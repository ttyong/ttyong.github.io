<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<script src="https://cdn.jsdelivr.net/gh/wallleap/cdn/js/piao.js"></script>
<head>
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="s8Pe1TBqyy">


  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/animal_bear_panda_32px_4023_easyicon.net.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/animal_bear_panda_16px_4023_easyicon.net.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="HBase,">





  <link rel="alternate" href="/atom.xml" title="TianYong's Blog" type="application/atom+xml">






<meta name="description" content="快速上手NoSQL数据库HBase-23 深入HBase架构原理Region概念解释123456Region可以翻译为区域，在HBase里面，一个表中的数据，会按照行被横向划分为多个Region。每个Region，按照存储的Rowkey的最小行键和最大行键指定的，使用区间[start Rowkey,end Rowkey)解释：-如果一个文件中数据量很大的时候，从这个大文件中读取数据肯定会">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据开发工程师-快速上手NoSQL数据库HBase-2">
<meta property="og:url" content="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BNoSQL%E6%95%B0%E6%8D%AE%E5%BA%93HBase-2.html">
<meta property="og:site_name" content="TianYong&#39;s Blog">
<meta property="og:description" content="快速上手NoSQL数据库HBase-23 深入HBase架构原理Region概念解释123456Region可以翻译为区域，在HBase里面，一个表中的数据，会按照行被横向划分为多个Region。每个Region，按照存储的Rowkey的最小行键和最大行键指定的，使用区间[start Rowkey,end Rowkey)解释：-如果一个文件中数据量很大的时候，从这个大文件中读取数据肯定会">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306212347130.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306212352303.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306212355799.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220009491.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220012225.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220013589.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220014851.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220019532.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220021198.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220028244.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220031358.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220035927.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306241006906.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306241006204.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306241015258.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306241016461.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306221529788.png">
<meta property="article:published_time" content="2023-06-13T01:24:19.000Z">
<meta property="article:modified_time" content="2023-07-04T15:46:08.857Z">
<meta property="article:author" content="TTYONG">
<meta property="article:tag" content="HBase">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306212347130.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://tianyong.fun/大数据开发工程师-快速上手NoSQL数据库HBase-2.html">





  <title>大数据开发工程师-快速上手NoSQL数据库HBase-2 | TianYong's Blog</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>


<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">
  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">TianYong's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">比你优秀的人都努力，有什么理由不努力！</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>
            
            站点地图
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BNoSQL%E6%95%B0%E6%8D%AE%E5%BA%93HBase-2.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="TTYONG">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianYong's Blog">
    </span>

    
      <header class="post-header">
	  
	  



        
        
          <h2 class="post-title" itemprop="name headline">大数据开发工程师-快速上手NoSQL数据库HBase-2</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-06-13T09:24:19+08:00">
                2023-06-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/" itemprop="url" rel="index">
                    <span itemprop="name">大数据开发工程师</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  13k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  57
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <hr>
<script type="text/javascript" src="/js/src/bai.js"></script>



<h1 id="快速上手NoSQL数据库HBase-2"><a href="#快速上手NoSQL数据库HBase-2" class="headerlink" title="快速上手NoSQL数据库HBase-2"></a>快速上手NoSQL数据库HBase-2</h1><h2 id="3-深入HBase架构原理"><a href="#3-深入HBase架构原理" class="headerlink" title="3 深入HBase架构原理"></a>3 深入HBase架构原理</h2><h3 id="Region概念解释"><a href="#Region概念解释" class="headerlink" title="Region概念解释"></a>Region概念解释</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Region可以翻译为区域，在HBase里面，一个表中的数据，会按照行被横向划分为多个Region。</span><br><span class="line">每个Region，按照存储的Rowkey的最小行键和最大行键指定的，使用区间[start Rowkey,end Rowkey)</span><br><span class="line"></span><br><span class="line">解释：</span><br><span class="line">-如果一个文件中数据量很大的时候，从这个大文件中读取数据肯定会比较慢</span><br><span class="line">-打开一个小文件查找数据和打开一个大文件查找数据的效率是不一样的</span><br></pre></td></tr></table></figure>

<img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306212347130.png" alt="image-20230621234751043" style="zoom: 67%;">

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在这个图里面，表t1刚创建的时候默认只有1个Region，后来数据量多了以后，Region会自动分裂，这样就产生了多个Region。</span><br><span class="line">那么我们在查询数据的时候，首先要知道数据在哪个Region中，再从Region中读取数据。</span><br><span class="line"></span><br><span class="line">如何知道数据在哪个Region中呢？</span><br><span class="line">我们在向表中插入数据的时候，Rowkey是必须指定，不能缺少的，并且Rowkey在存储的时候是有序存储的。</span><br><span class="line">那么我们在定位数据的时候，就可以拿Rowkey到对应的Region中进行对比，每个Region中都会有一个最小Rowkey和最大Rowkey，这样就能很快的判断出来我们要找的数据是不是在这个Region中了。</span><br><span class="line"></span><br><span class="line">其实每个Region中的最大Rowkey是有一个地方进行维护的，HBase内部默认提供了一个目录表来维护这个关系。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果表t1有两个列族c1和c2，那么在存储的时候，列族c1中的数据是一个独立的文件，列族c2中的数据也会是一个独立的文件，也就是说，每一个列族中的数据在底层存储的时候都是一个单独的文件。</span><br><span class="line">如下图所示，当表有多个Region的时候，每个Region内部的每个列族是一个单独的文件。</span><br></pre></td></tr></table></figure>

<img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306212352303.png" alt="image-20230621235252969" style="zoom: 67%;">

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">所以说我们在设计列族的时候，可以把经常读取的列存储到一个列族中，不经常读取的列放到另一个列族中。</span><br><span class="line">这样我们在读取部分列的数据的时候，就只需要读取对应列族文件中的数据，提高读取效率。</span><br><span class="line"></span><br><span class="line">在这里有几个问题：</span><br><span class="line"></span><br><span class="line">-如果一个列族中如果有2个列，那么这2个列会存储到2个列族文件中吗？不会的。</span><br><span class="line">-一行记录，会不会分到多个文件中存储？ 会</span><br><span class="line">-一个列族中的数据，会不会在多个Region中存储？会</span><br><span class="line">-一个Region中，会不会存储多个列族文件？会</span><br></pre></td></tr></table></figure>

<h3 id="HBase物理架构"><a href="#HBase物理架构" class="headerlink" title="HBase物理架构"></a>HBase物理架构</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306212355799.png" alt="image-20230621235550517" style="zoom:67%;">

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">主要包含以下内容：</span><br><span class="line"></span><br><span class="line">Zookeeper：为HBase集群提供基础服务。</span><br><span class="line">HBase Master：HBase集群的主节点。</span><br><span class="line">HBase Regionserver：HBase集群的从节点。</span><br><span class="line">Client：客户端节点。</span><br></pre></td></tr></table></figure>

<h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ZooKeeper为HBase集群提供协调服务，它管理着HMaster和HRegionServer的状态(available&#x2F;alive等)，并且会在HRegionServer宕机时通知给HMaster。</span><br><span class="line"></span><br><span class="line">ZooKeeper协调HBase集群所有节点的共享信息，在HMaster和HRegionServer连接到ZooKeeper后会创建Ephemeral（临时）节点，并使用心跳机制监控这个节点的存活状态，如果某个临时节点失效，则HMaster会收到通知，并做相应的处理，这块需要通过Watcher监视器实现。</span><br><span class="line"></span><br><span class="line">另外，HMaster通过监听ZooKeeper中的临时节点(默认：&#x2F;hbase&#x2F;rs&#x2F;*)来监控HRegionServer的加入和宕机。在第一个HMaster连接到ZooKeeper时会创建临时节点(默认：&#x2F;hbasae&#x2F;master)来表示Active的HMaster，后面加入进来的HMaster则监听该临时节点，如果当前Active的HMaster宕机，则该临时节点消失，因此其他HMaster会得到通知，然后将自身转换成Active的HMaster，在变为Active的HMaster之前，它会先在&#x2F;hbase&#x2F;back-masters&#x2F;下创建自己的临时节点。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">对Zookeeper的作用总结一下，一共有3点：</span><br><span class="line"></span><br><span class="line">-Zookeeper维护HBase集群的信息</span><br><span class="line">-HRegionserver启动的时候会在Zookeeper的&#x2F;hbase&#x2F;rs下面创建节点信息</span><br><span class="line">-HMaster会在Zookeeper的&#x2F;hbase下创建master节点 多余的HMaster会监听这个节点，发现这个节点失效的时候，会接管这个角色。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">登录Zookeeper，查看HBase的一些节点信息，HBase默认会在Zookeeper的根节点下面创建hbase节点</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] ls &#x2F;hbase</span><br><span class="line">[backup-masters, draining, flush-table-proc, hbaseid, master, master-maintenance, meta-region-server, namespace, online-snapshot, rs, running, splitWAL, switch, table]</span><br><span class="line"></span><br><span class="line">在Zookeeper中查看HBase的从节点信息，这里面显示的内容表示目前HBase集群有2个从节点bigdata02和bigdata03</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;hbase&#x2F;rs</span><br><span class="line">[bigdata02,16020,1777443334673, bigdata03,16020,1777443333539]</span><br></pre></td></tr></table></figure>

<h4 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">HMaster是HBase集群的主节点，HBase集群支持多个HMaster节点，可以实现HA。</span><br><span class="line">通过ZooKeeper的选举机制保证同时只有一个HMaster处于Active状态，其他的HMaster处于备份状态。一般情况下会启动两个HMaster，备份状态的HMaster会定期和Active HMaster通信以获取其最新状态，从而保证它是实时更新的，因此如果启动了多个HMaster反而增加了Active HMaster的负担。</span><br><span class="line"></span><br><span class="line">HMaster主要有以下职责：</span><br><span class="line">-管理HRegionServer，实现其负载均衡。</span><br><span class="line">-管理和分配Region，比如在Region分裂时分配新的Region；在HRegionServer退出时迁移里面的Region到其他HRegionServer上。</span><br><span class="line">-管理namespace和table的元数据（这些元数据实际存储在HDFS上面）</span><br><span class="line">-权限控制（ACL）</span><br></pre></td></tr></table></figure>

<h4 id="HRegionserver"><a href="#HRegionserver" class="headerlink" title="HRegionserver"></a>HRegionserver</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HRegionserver是HBase集群的从节点。</span><br><span class="line"></span><br><span class="line">HRegionServer一般建议和DataNode部署在同一台机器上，这样可以实现数据的本地化特性，因为DataNode是存储数据的，HBase的数据也是存储在HDFS上的，HRegionServer就是管理数据的，所以这样的话可以尽量保证HRegionServer读取本地的数据，只有磁盘IO，节省了网络IO。</span><br></pre></td></tr></table></figure>

<h3 id="HBase架构详解"><a href="#HBase架构详解" class="headerlink" title="HBase架构详解"></a>HBase架构详解</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220009491.png" alt="image-20230622000928203"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">图中虚线下面是HDFS部分 上面是HBase的部分</span><br><span class="line">Client(客户端)想要连接HBase的时候，需要先连接Zookeeper。</span><br><span class="line">首先会找Zookeeper中&#x2F;hbase&#x2F;meta-region-server这个节点，这个节点里面保存了HBase中meta表的数据（Region）所在的Regionserve节点信息。</span><br><span class="line">在Zookeeper执行get命令查看节点上的信息，显示出来的有乱码，不过大致可以看出来，里面显示的是bigdata03，也就意味着meta表的Region是在bigdata03这个节点上的。</span><br><span class="line">注意：大家在查看的时候显示的不一定是bigdata03节点，也可能是其他的节点。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] get &#x2F;hbase&#x2F;meta-region-server</span><br><span class="line">�master:16000)��K,&#125;�PBUF</span><br><span class="line"></span><br><span class="line">        bigdata03_x0010_�&#125;�ªڵ4_x0010_</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我们也可以通过HBase的UI界面进行查看验证。</span><br><span class="line">先在系统表中找到这个meta表</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220012225.png" alt="image-20230622001256147"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后点击进去，显示的这个表确实是在bigdata03这个节点上面</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220013589.png" alt="image-20230622001323492"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们可以在这里点击bigdata03:16030这个链接，然后进入到这个界面</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220014851.png" alt="image-20230622001458992"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">最终会发现meta表的数据确实就是在bigdata03这个节点上面的。</span><br><span class="line"></span><br><span class="line">客户端到Zookeeper中找到这个信息以后，就把这个信息加载到缓存中了，这样就不用每次都去重新加载了</span><br><span class="line"></span><br><span class="line">meta表中存储了所有表的相关信息，可能会有很多，都加载到内存的话可能会扛不住的，所以客户端并不会加载meta表中的所有数据，只会把meta表中我们目前需要的相关数据加载到内存。</span><br><span class="line"></span><br><span class="line">meta表里面存储的有HBase中所有表对应的RegionServer节点信息</span><br><span class="line"></span><br><span class="line">scan ‘hbase:meta’ 可以看到这个meta表里面的详细信息。</span><br><span class="line">所以客户端这个时候其实就可以获取到表对应RegionServer的IP和端口信息了，通过RPC机制就可以通信了。</span><br></pre></td></tr></table></figure>

<h4 id="详解HRegionServer"><a href="#详解HRegionServer" class="headerlink" title="详解HRegionServer"></a>详解HRegionServer</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面就来详细看一下HRegionServer里面的内容</span><br></pre></td></tr></table></figure>

<img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220019532.png" alt="image-20230622001912173" style="zoom: 67%;">

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HRegionServer里面有2块内容，一个是HLog ，另一个是HRegion(其实就是我们前面分析的Region，是同一个意思，Region是HRegion的简称)。</span><br><span class="line"></span><br><span class="line">在一个HRegionServer里面，HLog只有一个，HRegion会有多个，这个框后面是有三个点，表示是多个的意思。</span><br><span class="line">HLog是负责记录日志的，针对这个HRegionServer中的所有写操作，包括put、delete等操作，只要是会对数据产生变化的操作，都会记录到这个日志中，再把数据写到对应的HRegion中。</span><br><span class="line"></span><br><span class="line">HRegion就是负责存储实际数据了。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">看一下HRegion内部：</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220021198.png" alt="image-20230622002106137"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">每一个Store对应一个列族，所以一个HRegion里面可能会有多个Store。</span><br><span class="line">向HRegionserver中写数据的时候，会先写HLog，然后在把数据写入HRegion的时候，会根据指定的列族信息写入到不同的Store里面，我们之前在向表中put数据的时候，表名和列族名称都是必须要指定的。</span><br><span class="line"></span><br><span class="line">Store里面包含两部分：MemStore和StoreFile。</span><br><span class="line">用户写入的数据首先会放入MemStore【基于内存的Store】里面，当这个MemStore写满了以后，会把数据持久化到StoreFile中，每一次内存满了持久化的时候都会生成一个StoreFile，StoreFile底层对应的是一个HFile文件。</span><br><span class="line">HFile文件会通过下面的DFS Client写入到HDFS中。</span><br><span class="line">最终HLog和HFile都是用DFS Client写入到HDFS中的。</span><br></pre></td></tr></table></figure>

<h3 id="HBase物理存储模型"><a href="#HBase物理存储模型" class="headerlink" title="HBase物理存储模型"></a>HBase物理存储模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HBase中表的数据是存储在Region中的，表中的数据会越来越多，Region就会分裂，分裂出来的多个Region会分布到多个节点上面，因为单台机器的存储能力是有限的，这样对后期数据并行读取也有好处，有利于扩展。</span><br><span class="line">这样就可以保证一个表能存储海量数据，存放Region的服务器称之为Region Server。</span><br></pre></td></tr></table></figure>

<img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220028244.png" alt="image-20230622002810617" style="zoom: 80%;">

<h3 id="WAL-Write-Ahead-Logging-预写日志系统"><a href="#WAL-Write-Ahead-Logging-预写日志系统" class="headerlink" title="WAL(Write-Ahead Logging)预写日志系统"></a>WAL(Write-Ahead Logging)预写日志系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">WAL最重要的作用是灾难恢复。和MySQL的Binlog类似，它记录所有的数据改动。一旦服务器崩溃，通过重放log可以恢复崩溃之前的数据。这也就意味如果写入WAL失败，整个写入操作将认为失败。</span><br><span class="line">HBase中，HLog是WAL的实现类。一个HRegionServer对应一个HLog实例。</span><br><span class="line"></span><br><span class="line">WAL数据是存储在HDFS上面的，点进去可以看到是一个一个的文件。</span><br></pre></td></tr></table></figure>

<img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220031358.png" alt="image-20230622003106817" style="zoom:80%;">

<h3 id="HFile介绍"><a href="#HFile介绍" class="headerlink" title="HFile介绍"></a>HFile介绍</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HFile是HBase中重要的一个存在，可以说是HBase架构中最小的结构，HBase的底层数据都在HFile中。</span><br><span class="line">HFile从根本上来说是HDFS中的文件，只是它有自己特殊的格式。</span><br><span class="line"></span><br><span class="line">HFile文件由6部分组成：</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306220035927.png" alt="image-20230622003528698"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Data(数据块): 保存表中的数据(key-value的形式)，这部分可以被压缩，每个数据块都有一个Magic头,负责存储偏移量和第一个Key。</span><br><span class="line">Meta(元数据块)：存储用户自定义的key-value。</span><br><span class="line">File Info：定长，记录了文件的一些元信息，例如：AVG_KEY_LEN，AVG_VALUE_LEN，LAST_KEY等</span><br><span class="line">Data Index(数据块索引)：记录了每个数据块（Data）的起始索引。</span><br><span class="line">Meta Index(元数据块索引)：记录了每个元数据块（Meta）的起始索引。</span><br><span class="line">Trailer：定长，用于指向其他数据块的起始点。</span><br></pre></td></tr></table></figure>

<h3 id="从读-写流程角度理解Hbase架构"><a href="#从读-写流程角度理解Hbase架构" class="headerlink" title="从读/写流程角度理解Hbase架构"></a>从读/写流程角度理解Hbase架构</h3><h4 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306241006906.png" alt="image-20230624100622100"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">总结下来:</span><br><span class="line">--1. 具体流程：</span><br><span class="line">    1. Client先访问zookeeper，请求hbase:meta元数据位于哪个Region Server中。</span><br><span class="line">    2. zookeeper返回hbase:meta所在的Region Server地址</span><br><span class="line">    3. client访问对应的Region Server，请求hbase:meta元数据信息</span><br><span class="line">    4. Region Server返回meta元数据信息</span><br><span class="line">    5. client根据写请求的namespace:table&#x2F;rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。</span><br><span class="line">       并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</span><br><span class="line">    6. 与目标Region Server进行通讯；</span><br><span class="line">    7. 将数据顺序写入（追加）到WAL；</span><br><span class="line">    8. WAL将数据写入对应的MemStore，数据会在MemStore进行排序；</span><br><span class="line">    9. 完成写数据操作以后，向客户端发送ack；</span><br><span class="line">    10. 等达到MemStore的刷写时机后，将数据刷写到HFile。</span><br><span class="line"></span><br><span class="line">-- 2. hbase:meta包含哪些信息呢？</span><br><span class="line">   命令：scan ‘hbase:meta’</span><br><span class="line">   a、rowkey： test,,1592911245995.c42a3b247c7ed78f071</span><br><span class="line">               1) test：namespace,命名空间</span><br><span class="line">               2) ,,：startkey endkey，起止的rowkey</span><br><span class="line">               3) 1592911245995：time stamp ，时间戳</span><br><span class="line">               4) .c42a3b247c7ed78f071:前面3个参数的MD5值。</span><br><span class="line">   b、column&#x3D;info:regioninfo ： region的信息，</span><br><span class="line">                               value&#x3D;&#123;ENCODED &#x3D;&gt; c42a3b247c7ed78f071f60721bad78ad, NAME &#x3D;&gt; &#39;test,,</span><br><span class="line"> f60721bad78ad.1592911245995.c42a3b247c7ed78f071f60721bad78ad.&#39;, STARTKEY &#x3D;&gt; &#39;&#39;, ENDKEY &#x3D;&gt; &#39;&#39;&#125;</span><br><span class="line">   c、column&#x3D;info:seqnumDuringOpen ：序列号，value&#x3D;\x00\x00\x00\x00\x00\x00\x00\x02 </span><br><span class="line">   d、column&#x3D;info:server ： 该region所在的server，value&#x3D;hadoop106:16020</span><br><span class="line">   e、column&#x3D;info:serverstartcode ： 该region所在的server创建的时间戳，value&#x3D;1592887276902</span><br><span class="line">   f、column&#x3D;info:sn    ： value&#x3D;hadoop106,16020,1592887276902</span><br><span class="line">   g、column&#x3D;info:state ：该region的状态，value&#x3D;OPEN</span><br></pre></td></tr></table></figure>



<h4 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306241006204.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Hbase的读比写慢,因为读的时候,一定要扫描磁盘.</span><br><span class="line"></span><br><span class="line">思考:读数据</span><br><span class="line">读数据首先的找到Region所对应的RegionServer.找RegionServer就得找Meta表-&gt;找Zookeeper</span><br><span class="line"></span><br><span class="line">思考因为1.HDFS不支持随机写和2.Hbase写入数据时候,可以指定时间戳的原因,导致HBase中存放数据的位置有很多,那我们读哪些位置呢?</span><br><span class="line">答:全部都读.</span><br><span class="line"></span><br><span class="line">全部都读的话,同一个RowKey会有很多版本,怎么呢?</span><br><span class="line">答:merge合并</span><br><span class="line"></span><br><span class="line">还真么多读的位置,还要merge,HBase的读取效率的多慢啊?</span><br><span class="line">HBase肯定做了优化啊.做了哪些优化呢?</span><br><span class="line">1.Block Cache</span><br><span class="line">2.读HDFS时.会根据时间范围,RowKey范围,以及布隆过滤器进行优化.</span><br><span class="line"></span><br><span class="line">读的位置有很多:</span><br><span class="line">1.并不是先读内存,在读磁盘.因为hbase写入数据时候,可以指定时间戳.</span><br><span class="line">例如:先写入数据A,已经被flush到磁盘.后写入数据B并指定时间戳小. 假如先读内存,导致读出来B&#x3D;&#x3D;&#x3D;&#x3D;&gt;错误数据</span><br><span class="line">2.因此读MemStore,Hfile(Block cache没扫描过的Hfile),Block cache.3个都读</span><br><span class="line">3.HDFS时.有很多优化.被读的HFile,都是Block Cache没缓存的数据.</span><br></pre></td></tr></table></figure>

<h3 id="BloomFilter布隆过滤器"><a href="#BloomFilter布隆过滤器" class="headerlink" title="BloomFilter布隆过滤器"></a>BloomFilter布隆过滤器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">布隆过滤器是一种比较巧妙的概率型数据结构，可以用来告诉你 “某样东西一定不存在或者可能存在”。</span><br><span class="line">也就是说它告诉你某样东西不存在的话就一定不存在。</span><br><span class="line">如果它告诉你某样东西存在，也可能实际是不存在的。</span><br><span class="line"></span><br><span class="line">布隆过滤器是hbase中的高级功能，它能够减少特定访问模式（get&#x2F;scan）下的查询时间。进而提高HBase集群的吞吐率。</span><br><span class="line"></span><br><span class="line">当我们随机查询数据时，如果采用HBase的块索引机制，HBase会加载很多块文件。如果采用布隆过滤器后，它能够准确判断该HFile的所有数据块中，是否含有我们查询的数据，从而大大减少不必要的块加载，进而提高HBase集群的吞吐率。</span><br><span class="line"></span><br><span class="line">对于HBase而言，当我们选择采用布隆过滤器之后，HBase会在生成HFile时包含一份布隆过滤器结构的数据，开启布隆过滤器会有一定的存储及内存开销。但是在大多数情况下，这些负担相对于布隆过滤器带来的好处来说是可以接受的。</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306241015258.png" alt="image-20230624101548341"></p>
<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306241016461.png" alt="image-20230624101614330"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">优化:</span><br><span class="line">1.Hfile存储的是一定时间范围.一定RoWkey范围的.所以读数据时候会根据RowKey&#x2F;时间范围过滤,通过时间范围&#x2F;rowkey范围不在里边的storefile不会被扫描.</span><br><span class="line">2.布隆过滤器:告诉Hbase,这个文件中没有你想要的数据.怎么实现的呢?</span><br><span class="line">布隆过滤器:只能告诉你这个文件中不存在你想要的数据,不能告诉你是否存在.</span><br><span class="line">数据在往文件里边存储时.会经过几次算法计算.将维护的数组位置表示为1;</span><br><span class="line">读数据时,就不需要扫描一遍全文件.只需要在经过相同算法计算,看对应数组为位置是否全部为1.</span><br><span class="line">如果全部为1&#x3D;&#x3D;&gt;可能存在这个数据 否则:一定不存在</span><br><span class="line">(MemetoreHfile被刷写前,会进行排序.所以,RowKey有序,但是并不一定连续)</span><br><span class="line">4.merge:返回版本大的数据</span><br></pre></td></tr></table></figure>

<h3 id="HFile-compaction（合并）机制"><a href="#HFile-compaction（合并）机制" class="headerlink" title="HFile compaction（合并）机制"></a>HFile compaction（合并）机制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">当MemStore超过阀值的时候，就会持久化生成一个（StoreFile）HFile。</span><br><span class="line">因此随着数据不断写入，HFile的数量将会越来越多，HFile数量过多会降低读性能，因为每次查询数据都需要加载多个HFile文件。为了避免对读性能的影响，可以对这些HFile进行合并操作，把多个HFile合并成一个HFile。</span><br><span class="line"></span><br><span class="line">合并操作需要对HBase中的数据进行多次的重新读写，这个过程会产生大量的IO。因此可以发现合并机制的本质就是以IO操作换取后续读性能的提高。</span><br><span class="line"></span><br><span class="line">合并操作分为major（大合并）和minor（小合并）两种。</span><br><span class="line"></span><br><span class="line">minor（小合并）：只做部分文件的合并操作，生成新文件设置成激活状态，然后删除老的HFile文件(标记为删除状态，在major合并时删除)。小合并的过程一般较快，而且IO相对较低。</span><br><span class="line"></span><br><span class="line">major（大合并）：对Region下(同列族)的所有HFile执行合并操作，最终的结果是合并出一个HFile文件。在生成新的HFile时，会忽略掉已经标记为删除的数据、ttl过期的数据、版本超过限定的数据。大合并会产生大量的IO操作，对HBase的读写性能产生较大影响。</span><br><span class="line"></span><br><span class="line">注意：一般情况下，大合并会持续很长时间，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会关闭自动触发大合并功能，改为手动在业务低峰期触发。</span><br></pre></td></tr></table></figure>

<h3 id="Region-Split（分裂）机制"><a href="#Region-Split（分裂）机制" class="headerlink" title="Region Split（分裂）机制"></a>Region Split（分裂）机制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">前面我们分析了HFile文件的合并机制，当HFile文件合并多次之后，会导致Region中的数据过大，此时就需要涉及Region的分裂机制了。</span><br><span class="line">当HBase中的一个表刚被创建的时候，HBase默认只会分配一个Region给这个表。也就是说这个时候，所有的读写请求都会访问到同一个RegionServer中的同一个Region中，出现读写热点问题。</span><br><span class="line">并且当Region管理的数据量过多，或HFile文件较大时，都会影响性能。</span><br><span class="line"></span><br><span class="line">为了达到负载均衡，当Region达到一定的大小时就会将一个Region分裂成两个新的子Region，并对父 Region进行清除处理。</span><br><span class="line"></span><br><span class="line">HMaster会根据Balance策略，重新分配Region所属的RegionServer，最大化的发挥分布式系统的优点。</span><br><span class="line"></span><br><span class="line">触发Region Split的条件：</span><br><span class="line">-ConstantSizeRegionSplitPolicy （0.94版本前）：</span><br><span class="line">一个Region中最大HFile文件的大小大于设置的阈值（hbase.hregion.max.filesize）之后才会触发切分，HFile文件大小为压缩后的文件大小（针对启用压缩的场景），默认文件大小的阈值为10G。</span><br><span class="line">这种策略简单粗暴，但是弊端相当大。</span><br><span class="line">阈值设置偏大的话，对大表友好，小表可能不会触发分裂，极端情况下小表可能就只会有一个Region。</span><br><span class="line">阈值设置偏小的话，对小表友好，但一个大表可能会在集群中产生大量的Region，对于集群管理来说不是好事。</span><br><span class="line"></span><br><span class="line">-IncreasingToUpperBoundRegionSplitPolicy （0.94版本~2.x版本默认切分策略）：</span><br><span class="line">一个Region中最大HFile文件的大小大于设置的阈值就会触发切分，区别是这个阈值并不像 ConstantSizeRegionSplitPolicy是一个固定的值，这里的阈值是会不断调整的。调整规则和Region所属表在当前RegionServer上的Region个数有关系。</span><br><span class="line">公式：调整后的阈值 &#x3D; Region个数的3次方 * flush_size * 2</span><br><span class="line">注意：这里的阈值不会无限增大，会通过hbase.hregion.max.filesize来进行限制，不能超过这个参数的大小。</span><br><span class="line">这种策略能够自适应大小表，集群规模大的情况下，对大表很优秀，对小表会产生大量小Region（比第一种策略好一些）。</span><br></pre></td></tr></table></figure>

<h3 id="Region-Balance-负载均衡策略"><a href="#Region-Balance-负载均衡策略" class="headerlink" title="Region Balance(负载均衡策略)"></a>Region Balance(负载均衡策略)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Region分裂之后就会涉及到Region的负载均衡。</span><br><span class="line"></span><br><span class="line">HBase的HMaster进程会自动根据指定策略挑选出一些Region，并将这些Region分配到负载比较低的RegionServer上。</span><br><span class="line"></span><br><span class="line">由于HBase的所有数据都是写入到HDFS文件系统中的， 因此HBase的Region移动其实是非常轻量级。在做Region移动的时候，保持这个Region对应的HDFS文件位置不变，只需要将Region的元数据分配到对应的RegionServer中即可。</span><br><span class="line"></span><br><span class="line">官方目前支持两种挑选Region的策略：</span><br><span class="line">DefaultLoadBalancer和StochasticLoadBalancer。</span><br><span class="line">-DefaultLoadBalancer：这种策略能够保证每个RegionServer中的Region个数基本上都相等。</span><br><span class="line">-StochasticLoadBalancer：这种策略非常复杂，简单来讲是一种综合权衡6个因素的均衡策略。</span><br><span class="line"></span><br><span class="line">采用6个因素加权的方式算出一个代价值，这个代价值用来评估当前Region分布是否均衡，越均衡代价值越低。</span><br><span class="line">-每台服务器读请求数(ReadRequestCostFunction)</span><br><span class="line">-每台服务器写请求数(WriteRequestCostFunction)</span><br><span class="line">-Region个数(RegionCountSkewCostFunction)</span><br><span class="line">-移动代价(MoveCostFunction)</span><br><span class="line">-数据Locality(TableSkewCostFunction)</span><br><span class="line">-每张表占据RegionServer中Region个数上限(LocalityCostFunction)</span><br></pre></td></tr></table></figure>



<h2 id="4-HBase高级用法"><a href="#4-HBase高级用法" class="headerlink" title="4 HBase高级用法"></a>4 HBase高级用法</h2><h3 id="列族高级设置"><a href="#列族高级设置" class="headerlink" title="列族高级设置"></a>列族高级设置</h3><h4 id="生存时间-TTL"><a href="#生存时间-TTL" class="headerlink" title="生存时间(TTL)"></a>生存时间(TTL)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">应用系统经常需要从数据库里删除老数据，配置此项，可使数据增加生命周期，超过该配置时间的数据，将会在大合并时“被删除”。（单位：秒）</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):009:0&gt; create &#39;t3&#39;, &#123;NAME &#x3D;&gt; &#39;cf1&#39;, TTL &#x3D;&gt; &#39;18000&#39;&#125;</span><br><span class="line">Created table t3</span><br><span class="line">Took 1.2689 seconds                                                   </span><br><span class="line">&#x3D;&gt; Hbase::Table - t3</span><br><span class="line">hbase(main):053:0&gt; desc &#39;t3&#39;</span><br><span class="line">Table t3 is ENABLED                                                   </span><br><span class="line">t3                                                                    </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                           </span><br><span class="line">&#123;NAME &#x3D;&gt; &#39;cf1&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP</span><br><span class="line">_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;1800</span><br><span class="line">0 SECONDS (5 HOURS)&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, B</span><br><span class="line">LOOMFILTER &#x3D;&gt; &#39;ROW&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOC</span><br><span class="line">KCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;                               </span><br><span class="line"></span><br><span class="line">1 row(s)</span><br><span class="line">Quota is disabled</span><br><span class="line">Took 0.0359 seconds</span><br><span class="line"></span><br><span class="line">因此，HBase 中数据被删除的真正时机是在发生 Major Compaction 的时候。如果想要立即删除数据，可以手动执行 Major Compaction 命令；如果想要保留数据的历史版本，可以设置 KEEP_DELETED_CELLS&#x3D;true 参数。</span><br></pre></td></tr></table></figure>

<h4 id="版本数"><a href="#版本数" class="headerlink" title="版本数"></a>版本数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在0.96的版本之前默认每个列族是3个version， 0.96之后每个列族是1个version，在大合并时，会遗弃过期的版本。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):010:0&gt; create &#39;t4&#39;, &#123;NAME &#x3D;&gt; &#39;cf1&#39;, VERSIONS &#x3D;&gt; 3&#125;</span><br><span class="line">Created table t4</span><br><span class="line">Took 1.2638 seconds                                                   </span><br><span class="line">&#x3D;&gt; Hbase::Table - t4</span><br><span class="line">hbase(main):011:0&gt; desc &#39;t4&#39;</span><br><span class="line">Table t4 is ENABLED                                                   </span><br><span class="line">t4                                                                    </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                           </span><br><span class="line">&#123;NAME &#x3D;&gt; &#39;cf1&#39;, VERSIONS &#x3D;&gt; &#39;3&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW</span><br><span class="line">_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DAT</span><br><span class="line">A_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;</span><br><span class="line">, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;,</span><br><span class="line"> CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_O</span><br><span class="line">N_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt;</span><br><span class="line"> &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HFile可以被压缩并存放在HDFS上。这有助于节省硬盘IO，但是读写数据时压缩和解压缩会抬高CPU利用率。</span><br><span class="line">压缩是表定义的一部分，可以在建表或修改表结构时设定。建议打开表的压缩，除非你确定不会从压缩中受益。只有在数据不能被压缩或者因为某种原因服务器的CPU利用率有限制要求的情况下，有可能会关闭压缩特性。</span><br><span class="line"></span><br><span class="line">HBase可以使用多种压缩编码，包括LZO、SNAPPY和GZIP  (hadoop，hive，impala也有压缩概念)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):012:0&gt; create &#39;t5&#39;,&#123;NAME &#x3D;&gt; &#39;cf1&#39;, COMPRESSION &#x3D;&gt; &#39;SNAPPY&#39;&#125;</span><br><span class="line">Created table t5</span><br><span class="line">Took 1.3364 seconds                                                   </span><br><span class="line">&#x3D;&gt; Hbase::Table - t5</span><br><span class="line">hbase(main):013:0&gt; desc &#39;t5&#39;</span><br><span class="line">Table t5 is ENABLED                                                   </span><br><span class="line">t5                                                                    </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                           </span><br><span class="line">&#123;NAME &#x3D;&gt; &#39;cf1&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW</span><br><span class="line">_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DAT</span><br><span class="line">A_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;</span><br><span class="line">, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;,</span><br><span class="line"> CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_O</span><br><span class="line">N_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt;</span><br><span class="line"> &#39;SNAPPY&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="数据块-BLOCKSIZE-大小的配置"><a href="#数据块-BLOCKSIZE-大小的配置" class="headerlink" title="数据块(BLOCKSIZE)大小的配置"></a>数据块(BLOCKSIZE)大小的配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">随机查询：数据块越小，索引越大，查找性能更好</span><br><span class="line">顺序查询：更好的顺序扫描，需要更大的数据块</span><br><span class="line">所以在使用的时候根据业务需求来判断是随机查询需求多还是顺序查询需求多，根据具体的场景而定。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):014:0&gt; create &#39;t6&#39;,&#123;NAME &#x3D;&gt; &#39;cf1&#39;, BLOCKSIZE &#x3D;&gt; &#39;65537&#39;&#125;</span><br><span class="line">Created table t6</span><br><span class="line">Took 1.2451 seconds                                                   </span><br><span class="line">&#x3D;&gt; Hbase::Table - t6</span><br><span class="line">hbase(main):015:0&gt; desc &#39;t6&#39;</span><br><span class="line">Table t6 is ENABLED                                                   </span><br><span class="line">t6                                                                    </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                           </span><br><span class="line">&#123;NAME &#x3D;&gt; &#39;cf1&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW</span><br><span class="line">_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DAT</span><br><span class="line">A_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;</span><br><span class="line">, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;,</span><br><span class="line"> CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_O</span><br><span class="line">N_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt;</span><br><span class="line"> &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65537&#39;&#125;</span><br><span class="line"> </span><br><span class="line">HBase 表定义中的 blocksize 是指 HBase 存储数据的最小单元，也是 HBase 缓存数据的基本单位。HBase 数据是存储在 HFile 文件中的，每个 HFile 文件由多个 block 组成，每个 block 包含一定数量的 KeyValue 对。当 HBase 读取数据时，会从 HFile 中读取一个或多个 block 到内存中，并缓存在 BlockCache 中，以提高后续读取的性能。</span><br><span class="line"></span><br><span class="line">blocksize 的大小会影响 HBase 的读写效率和内存占用。blocksize 太小，会导致 HFile 文件数量过多，增加索引和元数据的开销；blocksize 太大，会导致读取数据时需要读取更多的无关数据，浪费内存和网络资源。因此，需要根据数据的特点和访问模式来调整 blocksize 的大小，以达到最佳的性能。</span><br><span class="line"></span><br><span class="line">HBase 的默认 blocksize 是 64 KB，但是可以针对不同的列族进行配置。一般来说，如果列族中的数据大小比较均匀，并且访问模式比较随机，可以使用较小的 blocksize；如果列族中的数据大小比较不均匀，并且访问模式比较顺序，可以使用较大的 blocksize。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HBase 的数据块和 HDFS 的数据块是不同的概念，它们用于不同的目的。HBase 的数据块是 HBase 存储数据的最小单元，也是 HBase 缓存数据的基本单位；HDFS 的数据块是 HDFS 存储数据的最小单元，也是 HDFS 分布数据的基本单位。HBase 的数据块通常比 HDFS 的数据块要小得多，因为 HBase 需要支持快速的随机访问，而 HDFS 需要支持高效的顺序访问。</span><br><span class="line"></span><br><span class="line">HBase 数据是存储在 HFile 文件中的，每个 HFile 文件由多个 HBase 数据块组成，每个 HBase 数据块包含一定数量的 KeyValue 对。HFile 文件又是存储在 HDFS 上的，每个 HFile 文件由多个 HDFS 数据块组成，每个 HDFS 数据块包含一定数量的字节。当 HBase 读取数据时，会先从内存中的 BlockCache 或 MemStore 中查找，如果没有找到，会从 HDFS 中读取一个或多个 HBase 数据块，并缓存在 BlockCache 中。当 HBase 写入数据时，会先写入内存中的 MemStore 和 WAL 中，当 MemStore 满了时，会将数据刷写到一个新的 HFile 文件中，并写入多个 HDFS 数据块。</span><br><span class="line"></span><br><span class="line">因此，HBase 读取一个数据块时，并不需要读取一个完整的 HDFS 数据块，而是可以根据索引和偏移量直接定位到所需的 HBase 数据块，并从 HDFS 中读取该部分数据。这样可以减少磁盘和网络的开销，提高随机访问的性能。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HFile 和 HDFS 数据块相比，一般来说 HFile 更大，因为 HFile 是由多个 HBase 数据块组成的，而 HBase 数据块又是存储在 HDFS 数据块中的。HFile 的大小取决于 HBase 数据的量和 MemStore 的刷新策略，通常是几百 MB 或者几 GB 的级别；HDFS 数据块的大小取决于 HDFS 的配置，通常是 64 MB 或者 128 MB 的级别。</span><br><span class="line"></span><br><span class="line">HFile 和 HDFS 数据块的大小会影响 HBase 的读写性能和存储效率。HFile 太大，会导致读取数据时需要扫描更多的索引和数据块，增加磁盘和网络的开销；HFile 太小，会导致 HFile 文件数量过多，增加元数据和管理的开销。HDFS 数据块太大，会导致读取数据时需要读取更多的无关数据，浪费内存和网络资源；HDFS 数据块太小，会导致 HDFS 文件分片过多，增加 NameNode 的负担。</span><br><span class="line"></span><br><span class="line">因此，需要根据数据的特点和访问模式来调整 HFile 和 HDFS 数据块的大小，以达到最佳的性能。一般来说，如果数据比较稀疏，并且访问模式比较随机，可以使用较小的 HFile 和 HDFS 数据块；如果数据比较密集，并且访问模式比较顺序，可以使用较大的 HFile 和 HDFS 数据块。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">根据我的搜索结果，hbase数据块的大小是可以在建表语句中通过参数BlockSize指定的，而memstore和blockcache的大小是可以在配置文件中设置的。因此，它们之间不一定相等，也不一定有固定的比例关系。12</span><br><span class="line"></span><br><span class="line">memstore是hbase的写缓存，用于存储最近更新的数据，blockcache是hbase的读缓存，用于存储最近访问的数据块。12</span><br><span class="line"></span><br><span class="line">hbase提供了几种blockcache方案，有LruBlockCache, SlabCache, BucketCache和ExternalBlockCache。123</span><br><span class="line"></span><br><span class="line">LruBlockCache是默认的方案，它将所有数据都放入JVM堆中，按照访问频率分为三个优先级队列：single, multi和in memory。12</span><br><span class="line"></span><br><span class="line">SlabCache是0.92版本提供的方案，它使用堆外内存存储固定大小的block，但是在1.0版本后被废弃了。12</span><br><span class="line"></span><br><span class="line">BucketCache是0.95版本提供的方案，它支持多种缓存方式和多种不同大小的bucket，以适应不同大小的block size。12</span><br><span class="line"></span><br><span class="line">ExternalBlockCache是1.10版本提供的方案，它使用外部进程管理缓存数据，以减少JVM内存压力。</span><br></pre></td></tr></table></figure>



<h4 id="数据块缓存-BLOCKCACHE"><a href="#数据块缓存-BLOCKCACHE" class="headerlink" title="数据块缓存(BLOCKCACHE)"></a>数据块缓存(BLOCKCACHE)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果一张表或表里的某个列族只被顺序化扫描访问或者很少被访问，这个时候就算Get或Scan花费时间是否有点儿长，你也不会很在意。在这种情况下，你可以选择关闭那些列族的缓存。</span><br><span class="line">如果你只是执行很多顺序化扫描，你会多次倒腾缓存，并且可能会滥用缓存把应该放进缓存获得性能提升的数据给排挤出去。如果关闭缓存，不仅可以避免上述情况发生，而且还可以让出更多缓存给其他表和同一个表的其他列族使用。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):016:0&gt; create &#39;t7&#39;,&#123;NAME &#x3D;&gt; &#39;cf1&#39;, BLOCKCACHE &#x3D;&gt; &#39;false&#39;&#125;</span><br><span class="line">Created table t7</span><br><span class="line">Took 2.2954 seconds                                                   </span><br><span class="line">&#x3D;&gt; Hbase::Table - t7</span><br><span class="line">hbase(main):017:0&gt; desc &#39;t7&#39;</span><br><span class="line">Table t7 is ENABLED                                                   </span><br><span class="line">t7                                                                    </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                           </span><br><span class="line">&#123;NAME &#x3D;&gt; &#39;cf1&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW</span><br><span class="line">_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DAT</span><br><span class="line">A_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;</span><br><span class="line">, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;,</span><br><span class="line"> CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_O</span><br><span class="line">N_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt;</span><br><span class="line"> &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;false&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="布隆过滤器-Bloom-filters"><a href="#布隆过滤器-Bloom-filters" class="headerlink" title="布隆过滤器(Bloom filters)"></a>布隆过滤器(Bloom filters)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase中存储额外的索引层次会占用额外的空间。布隆过滤器随着它们的索引对象的数据增长而增长，所以行级布隆过滤器比列标识符级布隆过滤器占用空间要少。当空间不是问题的时候，它们可以帮助你榨干系统的性能潜力。</span><br><span class="line"></span><br><span class="line">BLOOMFILTER参数的默认值是ROW，表示是行级布隆过滤器。</span><br><span class="line">使用行级布隆过滤器需要设置为ROW，使用列标识符级布隆过滤器需要设置为ROWCOL。</span><br><span class="line">行级布隆过滤器在数据块里检查特定行键是否不存在，列标识符级布隆过滤器检查行和列标识符联合体是否不存在。</span><br><span class="line">ROWCOL布隆过滤器的开销要高于ROW布隆过滤器。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; create &#39;t8&#39;,&#123;NAME &#x3D;&gt; &#39;cf1&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROWCOL&#39;&#125;</span><br><span class="line">Created table t8</span><br><span class="line">Took 1.2484 seconds                                                   </span><br><span class="line">&#x3D;&gt; Hbase::Table - t8</span><br><span class="line">hbase(main):019:0&gt; desc &#39;t8&#39;</span><br><span class="line">Table t8 is ENABLED                                                   </span><br><span class="line">t8                                                                    </span><br><span class="line">COLUMN FAMILIES DESCRIPTION                                           </span><br><span class="line">&#123;NAME &#x3D;&gt; &#39;cf1&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW</span><br><span class="line">_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DAT</span><br><span class="line">A_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;</span><br><span class="line">, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROWCO</span><br><span class="line">L&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOM</span><br><span class="line">S_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION</span><br><span class="line"> &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Scan-全表扫描"><a href="#Scan-全表扫描" class="headerlink" title="Scan (全表扫描)"></a>Scan (全表扫描)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">HBase中的Scan操作，类似于SQL中的select * from ...</span><br><span class="line">例子：</span><br><span class="line"></span><br><span class="line">scan &#39;hbase:meta&#39;</span><br><span class="line">scan &#39;hbase:meta&#39;,&#123;COLUMNS &#x3D;&gt; &#39;info:regioninfo&#39;&#125;</span><br><span class="line">scan &#39;t1&#39;,&#123;COLUMNS&#x3D;&gt;&#39;c1&#39;,TIMERANGE&#x3D;&gt;[1303668804,1303668904]&#125;</span><br><span class="line">scan &#39;t1&#39;,&#123;REVERSED&#x3D;&gt;true&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Scan的Java-API用法"><a href="#Scan的Java-API用法" class="headerlink" title="Scan的Java API用法"></a>Scan的Java API用法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Scan 在Java API中支持以下用法：</span><br><span class="line"></span><br><span class="line">scan.addFamily(); &#x2F;&#x2F;指定列族</span><br><span class="line">scan.addColumn(); &#x2F;&#x2F;指定列，如果没有调用任何addFamily或Column，会返回所有的columns；</span><br><span class="line">scan.readAllVersions(); &#x2F;&#x2F;读取所有版本数据。</span><br><span class="line">scan.readVersions(3); &#x2F;&#x2F;读取最新3个版本的数据</span><br><span class="line">scan.setTimeRange(); &#x2F;&#x2F;指定最大的时间戳和最小的时间戳，只有在此范围内的cell才能被获取.</span><br><span class="line">scan.setTimeStamp(); &#x2F;&#x2F;指定时间戳</span><br><span class="line">scan.setFilter(); &#x2F;&#x2F;指定Filter来过滤掉不需要的信息</span><br><span class="line">scan.withStartRow(); &#x2F;&#x2F;指定开始的行。如果不指定，则从表头开始</span><br><span class="line">scan.withStopRow(); &#x2F;&#x2F;指定结束的行（不含此行）</span><br><span class="line">scan.setBatch(); &#x2F;&#x2F;指定最多返回的Cell数目。用于防止一行中有过多的数据，导致OutofMemory错误。</span><br><span class="line">scan.setCaching(); &#x2F;&#x2F;指定scan底层每次连接返回的数据条数，默认值为1，适当调大可以提高查询性能，设置太大会比较耗内存</span><br></pre></td></tr></table></figure>

<h5 id="常见的Filter"><a href="#常见的Filter" class="headerlink" title="常见的Filter"></a>常见的Filter</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在进行Scan的时候，可以添加Filter实现数据过滤</span><br></pre></td></tr></table></figure>

<h6 id="RowFilter"><a href="#RowFilter" class="headerlink" title="RowFilter"></a>RowFilter</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">RowFilter：对Rowkey进行过滤。</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;小于等于 x</span><br><span class="line">Filter filter &#x3D; new RowFilter(CompareOperator.LESS_OR_EQUAL,new BinaryComparator(Bytes.toBytes(“x”)));</span><br><span class="line">&#x2F;&#x2F;正则 以x结尾</span><br><span class="line">Filter filter &#x3D; new RowFilter(CompareOperator.EQUAL,new RegexStringComparator(&quot;.*x&quot;));</span><br><span class="line">&#x2F;&#x2F;包含 x</span><br><span class="line">Filter filter &#x3D; new RowFilter(CompareOperator.EQUAL, new SubstringComparator(“x”));</span><br><span class="line">&#x2F;&#x2F;开头 x</span><br><span class="line">Filter filter &#x3D; new RowFilter(CompareOperator.EQUAL, new BinaryPrefixComparator(Bytes.toBytes(“x”)));</span><br><span class="line"></span><br><span class="line">CompareOperator.其他比较参数</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LESS   				小于   </span><br><span class="line">LESS_OR_EQUAL 	 	小于等于   </span><br><span class="line">EQUAL 				等于   </span><br><span class="line">NOT_EQUAL  			不等于   </span><br><span class="line">GREATER_OR_EQUAL 	大于等于</span><br><span class="line">GREATER  			大于</span><br><span class="line">NO_OP 				排除所有</span><br></pre></td></tr></table></figure>

<h6 id="PrefixFilter"><a href="#PrefixFilter" class="headerlink" title="PrefixFilter"></a>PrefixFilter</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PrefixFilter：筛选出具有特定前缀的行键的数据。</span><br><span class="line">Filter pf &#x3D; new PrefixFilter(Bytes.toBytes(“前缀”));</span><br></pre></td></tr></table></figure>

<h6 id="ValueFilter"><a href="#ValueFilter" class="headerlink" title="ValueFilter"></a>ValueFilter</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ValueFilter：按照具体的值来筛选列中的数据，这会把一行中值不能满足的列过滤掉</span><br><span class="line">Filter vf &#x3D; new ValueFilter(CompareOperator.EQUAL, new SubstringComparator(“ROW2_QUAL1”));</span><br></pre></td></tr></table></figure>

<h5 id="Scan和Filter结合的案例"><a href="#Scan和Filter结合的案例" class="headerlink" title="Scan和Filter结合的案例"></a>Scan和Filter结合的案例</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在执行下面代码之前，先创建表，初始化一批数据</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; create &#39;s1&#39;,&#39;c1&#39;,&#39;c2&#39;</span><br><span class="line">Created table s1</span><br><span class="line">Took 2.2063 seconds                                                   </span><br><span class="line">&#x3D;&gt; Hbase::Table - s1</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;a&#39;,&#39;c1:name&#39;,&#39;zs&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;a&#39;,&#39;c1:age&#39;,&#39;18&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;a&#39;,&#39;c2:score&#39;,&#39;99&#39;</span><br><span class="line"></span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;b&#39;,&#39;c1:name&#39;,&#39;jack&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;b&#39;,&#39;c1:age&#39;,&#39;21&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;b&#39;,&#39;c2:score&#39;,&#39;85&#39;</span><br><span class="line"></span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;c&#39;,&#39;c1:name&#39;,&#39;tom&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;c&#39;,&#39;c1:age&#39;,&#39;31&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;c&#39;,&#39;c2:score&#39;,&#39;79&#39;</span><br><span class="line"></span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;d&#39;,&#39;c1:name&#39;,&#39;lili&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;d&#39;,&#39;c1:age&#39;,&#39;27&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;d&#39;,&#39;c2:score&#39;,&#39;65&#39;</span><br><span class="line"></span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;e&#39;,&#39;c1:name&#39;,&#39;ww&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;e&#39;,&#39;c1:age&#39;,&#39;35&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;e&#39;,&#39;c2:score&#39;,&#39;100&#39;</span><br><span class="line"></span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;f&#39;,&#39;c1:name&#39;,&#39;jessic&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;f&#39;,&#39;c1:age&#39;,&#39;12&#39;</span><br><span class="line">hbase(main):026:0&gt; put &#39;s1&#39;,&#39;f&#39;,&#39;c2:score&#39;,&#39;77&#39;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.filter.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 全表扫描Scan+Filter</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意：首先需要创建表s1,然后初始化一批数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseScanFilter</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//获取配置</span></span><br><span class="line">        Configuration conf = HBaseConfiguration.create();</span><br><span class="line">        <span class="comment">//指定HBase使用的zk的地址，多个都逗号隔开</span></span><br><span class="line">        conf.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"bigdata01:2181,bigdata02:2181,bigdata03:2181"</span>);</span><br><span class="line">        <span class="comment">//指定HBase在hdfs上的根目录</span></span><br><span class="line">        conf.set(<span class="string">"hbase.rootdir"</span>,<span class="string">"hdfs://bigdata01:9000/hbase"</span>);</span><br><span class="line">        <span class="comment">//创建HBase连接，负责对HBase中数据的增删改查(DML操作)</span></span><br><span class="line">        Connection conn = ConnectionFactory.createConnection(conf);</span><br><span class="line">        <span class="comment">//获取Table对象，指定要操作的表名，表需要提前创建好</span></span><br><span class="line">        Table table = conn.getTable(TableName.valueOf(<span class="string">"s1"</span>));</span><br><span class="line"></span><br><span class="line">        Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">        <span class="comment">//范围查询：指定查询区间，提高查询性能</span></span><br><span class="line">        <span class="comment">//这是一个左闭右开的区间，也就是查询的结果中包含左边的，不包含右边的</span></span><br><span class="line">        scan.withStartRow(Bytes.toBytes(<span class="string">"a"</span>));</span><br><span class="line">        scan.withStopRow(Bytes.toBytes(<span class="string">"f"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//添加Filter对数据进行过滤：使用RowFilter进行过滤，获取Rowkey小于等于d的数据</span></span><br><span class="line">        Filter filter = <span class="keyword">new</span> RowFilter(CompareOperator.LESS_OR_EQUAL,<span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">"d"</span>)));</span><br><span class="line">        scan.setFilter(filter);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取查询结果</span></span><br><span class="line">        ResultScanner scanner = table.getScanner(scan);</span><br><span class="line">        <span class="comment">//迭代查询结果</span></span><br><span class="line">        <span class="keyword">for</span> (Result result: scanner) &#123;</span><br><span class="line">            List&lt;Cell&gt; cells = result.listCells();</span><br><span class="line">            <span class="comment">//RowKey</span></span><br><span class="line">            <span class="keyword">byte</span>[] row_key = result.getRow();</span><br><span class="line">            <span class="keyword">for</span> (Cell cell: cells) &#123;</span><br><span class="line">                <span class="comment">//注意：下面获取的信息都是字节类型的，可以通过new String(bytes)转为字符串</span></span><br><span class="line">                <span class="comment">//列族</span></span><br><span class="line">                <span class="keyword">byte</span>[] famaily_bytes = CellUtil.cloneFamily(cell);</span><br><span class="line">                <span class="comment">//列</span></span><br><span class="line">                <span class="keyword">byte</span>[] column_bytes = CellUtil.cloneQualifier(cell);</span><br><span class="line">                <span class="comment">//值</span></span><br><span class="line">                <span class="keyword">byte</span>[] value_bytes = CellUtil.cloneValue(cell);</span><br><span class="line">                System.out.println(<span class="string">"Rowkey："</span>+<span class="keyword">new</span> String(row_key)+<span class="string">",列族："</span>+<span class="keyword">new</span> String(famaily_bytes)+<span class="string">",列："</span>+<span class="keyword">new</span> String(column_bytes)+<span class="string">",值："</span>+<span class="keyword">new</span> String(value_bytes));</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">"================================================================"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        table.close();</span><br><span class="line">        conn.close();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="批量导入"><a href="#批量导入" class="headerlink" title="批量导入"></a>批量导入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">批量导入两种方式：</span><br><span class="line"></span><br><span class="line">-利用MapReduce中封装好的方法。在map阶段，把数据封装成Put操作，直接将数据入库。</span><br><span class="line">-利用Bulkload。首先使用MapReduce直接生成HFile文件，然后再通过Bulkload将HFile文件直接加载到表中。</span><br><span class="line"></span><br><span class="line">Bulkload的优势：通过MR生成HBase底层HFile文件，直接加载到表中，省去了大部分的RPC和写过程。</span><br></pre></td></tr></table></figure>

<h4 id="批量导入之-MapReduce"><a href="#批量导入之-MapReduce" class="headerlink" title="批量导入之 MapReduce"></a>批量导入之 MapReduce</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先初始化输入数据：</span><br><span class="line">在linux中创建文件：hbase_import.dat</span><br><span class="line">内容如下：</span><br><span class="line"></span><br><span class="line">注意：字段分隔符为制表符</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 soft]# vi hbase_import.dat</span><br><span class="line">a       c1      name    zs</span><br><span class="line">a       c1      age     18</span><br><span class="line">b       c1      name    ls</span><br><span class="line">b       c1      age     29</span><br><span class="line">c       c1      name    ww</span><br><span class="line">c       c1      age     31</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">把hbase_import.dat上传到HDFS中</span><br><span class="line">[root@bigdata04 soft]# hdfs dfs -put hbase_import.dat &#x2F;</span><br><span class="line"></span><br><span class="line">在HBase中创建需要用到的表batch1</span><br><span class="line">hbase(main):027:0&gt; create &#39;batch1&#39;,&#39;c1&#39;</span><br><span class="line">Created table batch1</span><br><span class="line">Took 1.3600 seconds                                                   </span><br><span class="line">&#x3D;&gt; Hbase::Table - batch1</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时项目的pom.xml文件中除了添加hbase-client的依赖，还需要添加hadoop-client和hbase-mapreduce的依赖，否则代码报错</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.2.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hbase-mapreduce&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.2.7&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">详细代码如下：</span><br></pre></td></tr></table></figure>

<h5 id="BatchImportMR"><a href="#BatchImportMR" class="headerlink" title="BatchImportMR"></a>BatchImportMR</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 批量导入：</span></span><br><span class="line"><span class="comment"> * 1：利用MapReduce中封装好的方法。</span></span><br><span class="line"><span class="comment"> * 在map阶段，把数据封装成Put操作，直接将数据入库</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意：需要提前创建表batch1</span></span><br><span class="line"><span class="comment"> * create 'batch1','c1'</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchImportMR</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchImportMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Put</span>&gt;</span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">                <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String[] strs = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">            <span class="keyword">if</span>(strs.length==<span class="number">4</span>)&#123;</span><br><span class="line">                String rowkey = strs[<span class="number">0</span>];</span><br><span class="line">                String columnFamily = strs[<span class="number">1</span>];</span><br><span class="line">                String name = strs[<span class="number">2</span>];</span><br><span class="line">                String val = strs[<span class="number">3</span>];</span><br><span class="line">                Put put = <span class="keyword">new</span> Put(rowkey.getBytes());</span><br><span class="line">                put.addColumn(columnFamily.getBytes(),name.getBytes(),val.getBytes());</span><br><span class="line">                context.write(NullWritable.get(),put);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(args.length!=<span class="number">2</span>)&#123;</span><br><span class="line">            <span class="comment">//如果传递的参数不够，程序直接退出</span></span><br><span class="line">            System.exit(<span class="number">100</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String inPath = args[<span class="number">0</span>];</span><br><span class="line">        String outTableName = args[<span class="number">1</span>];</span><br><span class="line">        <span class="comment">//设置属性对应参数</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.set(<span class="string">"hbase.table.name"</span>,outTableName);</span><br><span class="line">        conf.set(<span class="string">"hbase.zookeeper.quorum"</span>,<span class="string">"bigdata01:2181,bigdata02:2181,bigdata03:2181"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//封装Job</span></span><br><span class="line">        Job job = Job.getInstance(conf, <span class="string">"Batch Import HBase Table："</span> + outTableName);</span><br><span class="line">        job.setJarByClass(BatchImportMR<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定输入路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(inPath));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定map相关的代码</span></span><br><span class="line">        job.setMapperClass(BatchImportMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(Put<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        TableMapReduceUtil.initTableReducerJob(outTableName,<span class="keyword">null</span>,job);</span><br><span class="line">        TableMapReduceUtil.addDependencyJars(job);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//禁用Reduce</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">对代码打jar包：</span><br><span class="line">在打jar包之前，需要对pom.xml中的依赖添加&lt;scope&gt;provided&lt;&#x2F;scope&gt;配置</span><br><span class="line"></span><br><span class="line">注意：hbase-client和hbase-mapreduce不能设置provided，这两个依赖需要打进jar包里面，否则会提示找不到对应的类。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hbase-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.2.7&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.2.0&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hbase-mapreduce&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.2.7&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">还需要添加maven打包配置</span><br><span class="line"></span><br><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;!-- compiler插件, 设定JDK版本 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.3.2&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                &lt;showWarnings&gt;true&lt;&#x2F;showWarnings&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;descriptorRefs&gt;</span><br><span class="line">                    &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                &lt;archive&gt;</span><br><span class="line">                    &lt;manifest&gt;</span><br><span class="line">                        &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                    &lt;&#x2F;manifest&gt;</span><br><span class="line">                &lt;&#x2F;archive&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">    &lt;&#x2F;plugins&gt;</span><br><span class="line">&lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">执行打jar包命令</span><br><span class="line">C:\Users\yehua&gt;d:</span><br><span class="line"></span><br><span class="line">D:\&gt;cd IdeaProjects\db_hbase</span><br><span class="line"></span><br><span class="line">D:\IdeaProjects\db_hbase&gt;mvn clean package -DskipTests</span><br><span class="line">[INFO] Scanning for projects...</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Building db_hbase 1.0-SNAPSHOT</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] --- maven-clean-plugin:2.4.1:clean (default-clean) @ db_hbase ---</span><br><span class="line">[INFO] Deleting D:\IdeaProjects\db_hbase\target</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] --- maven-jar-plugin:2.3.2:jar (default-jar) @ db_hbase ---</span><br><span class="line">[INFO] Building jar: D:\IdeaProjects\db_hbase\target\db_hbase-1.0-SNAPSHOT.jar</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] --- maven-assembly-plugin:2.2-beta-5:single (make-assembly) @ db_hbase ---</span><br><span class="line">[INFO] Building jar: D:\IdeaProjects\db_hbase\target\db_hbase-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 7.491s</span><br><span class="line">[INFO] Finished at: Sun Nov 22 12:01:56 CST 2020</span><br><span class="line">[INFO] Final Memory: 99M&#x2F;356M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">将生成的db_hbase-1.0-SNAPSHOT-jar-with-dependencies.jar上传到bigdata04机器上</span><br><span class="line">然后向集群中提交此MapReduce任务</span><br><span class="line"></span><br><span class="line">注意：需要确保Hadoop集群、Zookeeper集群、HBase集群可以正常工作</span><br><span class="line">[root@bigdata04 hadoop-3.2.0]# hadoop jar db_hbase-1.0-SNAPSHOT-jar-with-dependencies.jar com.imooc.hbase.BatchImportMR hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;hbase_import.dat batch1</span><br><span class="line"></span><br><span class="line">执行成功之后，查询HBase中batch1表中的结果如下：</span><br><span class="line"></span><br><span class="line">hbase(main):013:0&gt; scan &#39;batch1&#39;</span><br><span class="line">ROW                COLUMN+CELL                                        </span><br><span class="line"> a                 column&#x3D;c1:age, timestamp&#x3D;1778305406350, value&#x3D;18   </span><br><span class="line"> a                 column&#x3D;c1:name, timestamp&#x3D;1778305406350, value&#x3D;zs  </span><br><span class="line"> b                 column&#x3D;c1:age, timestamp&#x3D;1778305406350, value&#x3D;29   </span><br><span class="line"> b                 column&#x3D;c1:name, timestamp&#x3D;1778305406350, value&#x3D;ls  </span><br><span class="line"> c                 column&#x3D;c1:age, timestamp&#x3D;1778305406350, value&#x3D;31   </span><br><span class="line"> c                 column&#x3D;c1:name, timestamp&#x3D;1778305406350, value&#x3D;ww  </span><br><span class="line">3 row(s)</span><br><span class="line">Took 0.0455 seconds</span><br></pre></td></tr></table></figure>

<h4 id="批量导入之-BulkLoad"><a href="#批量导入之-BulkLoad" class="headerlink" title="批量导入之 BulkLoad"></a>批量导入之 BulkLoad</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306221529788.png" alt="image-20230622152857361"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">测试数据继续使用hdfs中的hbase_import.dat</span><br><span class="line"></span><br><span class="line">在hbase中创建需要用到的表batch2</span><br><span class="line"></span><br><span class="line">hbase(main):027:0&gt; create &#39;batch2&#39;,&#39;c1&#39;</span><br><span class="line">Created table batch2</span><br><span class="line">Took 1.3600 seconds                                                   </span><br><span class="line">&#x3D;&gt; Hbase::Table - batch2</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">想要实现BulkLoad需要两步</span><br><span class="line">第一步：先生成HFile文件</span><br><span class="line">代码如下：</span><br><span class="line"></span><br><span class="line">注意：此代码会生成底层的HFile文件</span><br></pre></td></tr></table></figure>

<h5 id="BatchImportBulkLoad"><a href="#BatchImportBulkLoad" class="headerlink" title="BatchImportBulkLoad"></a>BatchImportBulkLoad</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.hbase;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.hbase.TableName;</span><br><span class="line">import org.apache.hadoop.hbase.client.Connection;</span><br><span class="line">import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 批量导入</span><br><span class="line"> *</span><br><span class="line"> * 2.利用BulkLoad</span><br><span class="line"> * 在map阶段，把数据封装成put操作，将数据生成HBase的底层存储文件HFile</span><br><span class="line"> * 再将生成的HFile文件加载到表中</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class BatchImportBulkLoad &#123;</span><br><span class="line">    public static class BulkLoadMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt;&#123;</span><br><span class="line">        @Override</span><br><span class="line">        protected void map(LongWritable key, Text value, Context context)</span><br><span class="line">                throws IOException, InterruptedException &#123;</span><br><span class="line">            String[] strs &#x3D; value.toString().split(&quot;\t&quot;);</span><br><span class="line">            if(strs.length&#x3D;&#x3D;4)&#123;</span><br><span class="line">                String rowkey &#x3D; strs[0];</span><br><span class="line">                String columnFamily &#x3D; strs[1];</span><br><span class="line">                String name &#x3D; strs[2];</span><br><span class="line">                String val &#x3D; strs[3];</span><br><span class="line">                ImmutableBytesWritable rowkeyWritable &#x3D; new ImmutableBytesWritable(rowkey.getBytes());</span><br><span class="line">                Put put &#x3D; new Put(rowkey.getBytes());</span><br><span class="line">                put.addColumn(columnFamily.getBytes(),name.getBytes(),val.getBytes());</span><br><span class="line">                context.write(rowkeyWritable,put);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        if(args.length!&#x3D;3)&#123;</span><br><span class="line">            &#x2F;&#x2F;如果传递的参数不够，程序直接退出</span><br><span class="line">            System.exit(100);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String inPath &#x3D; args[0];</span><br><span class="line">        String outPath &#x3D; args[1];</span><br><span class="line">        String outTableName &#x3D; args[2];</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;设置属性对应参数</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        conf.set(&quot;hbase.table.name&quot;,outTableName);</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;bigdata01:2181,bigdata02:2181,bigdata03:2181&quot;);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;封装Job</span><br><span class="line">        Job job &#x3D; Job.getInstance(conf, &quot;Batch Import HBase Table：&quot; + outTableName);</span><br><span class="line">        job.setJarByClass(BatchImportBulkLoad.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;指定输入路径</span><br><span class="line">        FileInputFormat.setInputPaths(job,new Path(inPath));</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;指定输出路径[如果输出路径存在，就将其删除]</span><br><span class="line">        FileSystem fs &#x3D; FileSystem.get(conf);</span><br><span class="line">        Path output &#x3D; new Path(outPath);</span><br><span class="line">        if(fs.exists(output))&#123;</span><br><span class="line">            fs.delete(output,true);</span><br><span class="line">        &#125;</span><br><span class="line">        FileOutputFormat.setOutputPath(job, output);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;指定map相关的代码</span><br><span class="line">        job.setMapperClass(BulkLoadMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">        job.setMapOutputValueClass(Put.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;禁用Reduce</span><br><span class="line">        job.setNumReduceTasks(0);</span><br><span class="line"></span><br><span class="line">        Connection connection &#x3D; ConnectionFactory.createConnection(conf);</span><br><span class="line">        TableName tableName &#x3D; TableName.valueOf(outTableName);</span><br><span class="line">        HFileOutputFormat2.configureIncrementalLoad(job,connection.getTable(tableName),connection.getRegionLocator(tableName));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(true);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">打jar包</span><br><span class="line">D:\IdeaProjects\db_hbase&gt;mvn clean package -DskipTests</span><br><span class="line">....</span><br><span class="line">[INFO] META-INF&#x2F;native&#x2F; already added, skipping</span><br><span class="line">[INFO] org&#x2F; already added, skipping</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 34.460s</span><br><span class="line">[INFO] Finished at: Sun Nov 22 17:47:48 CST 2020</span><br><span class="line">[INFO] Final Memory: 115M&#x2F;1075M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 hadoop-3.2.0]# hadoop jar db_hbase-1.0-SNAPSHOT-jar-with-dependencies.jar com.imooc.hbase.BatchImportBulkLoad hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;hbase_import.dat hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;hbase_out batch2</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">第二步：加载HFile文件</span><br><span class="line">在HBase客户端节点上执行下面命令，把HFile数据转移到表对应的region中。</span><br><span class="line">[root@bigdata04 hbase-2.2.7]# hbase org.apache.hadoop.hbase.tool.BulkLoadHFilesTool hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;hbase_out batch2</span><br><span class="line"></span><br><span class="line">查看表batch2中的数据</span><br><span class="line">hbase(main):001:0&gt; scan &#39;batch2&#39;</span><br><span class="line">ROW                COLUMN+CELL                                        </span><br><span class="line"> a                 column&#x3D;c1:age, timestamp&#x3D;1778308217857, value&#x3D;18   </span><br><span class="line"> a                 column&#x3D;c1:name, timestamp&#x3D;1778308217857, value&#x3D;zs  </span><br><span class="line"> b                 column&#x3D;c1:age, timestamp&#x3D;1778308217857, value&#x3D;29   </span><br><span class="line"> b                 column&#x3D;c1:name, timestamp&#x3D;1778308217857, value&#x3D;ls  </span><br><span class="line"> c                 column&#x3D;c1:age, timestamp&#x3D;1778308217857, value&#x3D;31   </span><br><span class="line"> c                 column&#x3D;c1:name, timestamp&#x3D;1778308217857, value&#x3D;ww  </span><br><span class="line">3 row(s)</span><br><span class="line">Took 1.0230 seconds</span><br></pre></td></tr></table></figure>

<h3 id="批量导出"><a href="#批量导出" class="headerlink" title="批量导出"></a>批量导出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">批量导出两种方式：</span><br><span class="line"></span><br><span class="line">利用TableMapReduceUtil将数据导出 (需要开发MapReduce代码)</span><br><span class="line">利用HBase内部提供的Export工具类</span><br></pre></td></tr></table></figure>

<h4 id="批量导出之TableMapReduceUtil"><a href="#批量导出之TableMapReduceUtil" class="headerlink" title="批量导出之TableMapReduceUtil"></a>批量导出之TableMapReduceUtil</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">将HBase中的表batch1中的数据导出到hdfs上面</span><br><span class="line">表batch1中的数据如下：</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt; scan &#39;batch1&#39;</span><br><span class="line">ROW                COLUMN+CELL                                        </span><br><span class="line"> a                 column&#x3D;c1:age, timestamp&#x3D;1778305406350, value&#x3D;18   </span><br><span class="line"> a                 column&#x3D;c1:name, timestamp&#x3D;1778305406350, value&#x3D;zs  </span><br><span class="line"> b                 column&#x3D;c1:age, timestamp&#x3D;1778305406350, value&#x3D;29   </span><br><span class="line"> b                 column&#x3D;c1:name, timestamp&#x3D;1778305406350, value&#x3D;ls  </span><br><span class="line"> c                 column&#x3D;c1:age, timestamp&#x3D;1778305406350, value&#x3D;31   </span><br><span class="line"> c                 column&#x3D;c1:name, timestamp&#x3D;1778305406350, value&#x3D;ww  </span><br><span class="line">3 row(s)</span><br><span class="line">Took 0.7332 seconds</span><br></pre></td></tr></table></figure>

<h5 id="BatchExportTableMapReduceUtil"><a href="#BatchExportTableMapReduceUtil" class="headerlink" title="BatchExportTableMapReduceUtil"></a>BatchExportTableMapReduceUtil</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.hbase;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.hbase.client.Result;</span><br><span class="line">import org.apache.hadoop.hbase.client.Scan;</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableMapper;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 批量导出</span><br><span class="line"> * 1.利用TableMapReduceUtil将数据导出</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class BatchExportTableMapReduceUtil &#123;</span><br><span class="line">    public static class BatchExportMapper extends TableMapper&lt;Text,Text&gt;&#123;</span><br><span class="line">        @Override</span><br><span class="line">        protected void map(ImmutableBytesWritable key, Result result, Context context)</span><br><span class="line">                throws IOException, InterruptedException &#123;</span><br><span class="line">            &#x2F;&#x2F;key在这里就是hbase的Rowkey</span><br><span class="line">            &#x2F;&#x2F;result是scan返回的每行结果</span><br><span class="line">            byte[] name &#x3D; null;</span><br><span class="line">            byte[] age &#x3D; null;</span><br><span class="line">            try&#123;</span><br><span class="line">                name &#x3D; result.getValue(&quot;c1&quot;.getBytes(), &quot;name&quot;.getBytes());</span><br><span class="line">            &#125;catch (Exception e)&#123;&#125;</span><br><span class="line">            try&#123;</span><br><span class="line">                age &#x3D; result.getValue(&quot;c1&quot;.getBytes(), &quot;age&quot;.getBytes());</span><br><span class="line">            &#125;catch (Exception e)&#123;&#125;</span><br><span class="line"></span><br><span class="line">            String v2 &#x3D; ((name&#x3D;&#x3D;null || name.length&#x3D;&#x3D;0)?&quot;NULL&quot;:new String(name))+&quot;\t&quot;+((age&#x3D;&#x3D;null || age.length&#x3D;&#x3D;0)?&quot;NULL&quot;:new String(age));</span><br><span class="line"></span><br><span class="line">            context.write(new Text(key.get()),new Text(v2));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        if(args.length!&#x3D;2)&#123;</span><br><span class="line">            &#x2F;&#x2F;如果传递的参数不够，程序直接退出</span><br><span class="line">            System.exit(100);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String inTableName &#x3D; args[0];</span><br><span class="line">        String outPath &#x3D; args[1];</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;设置属性对应参数</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;bigdata01:2181,bigdata02:2181,bigdata03:2181&quot;);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;组装Job</span><br><span class="line">        Job job &#x3D; Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(BatchExportTableMapReduceUtil.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;设置map相关的配置</span><br><span class="line">        job.setMapperClass(BatchExportMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;禁用Reduce</span><br><span class="line">        job.setNumReduceTasks(0);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;设置输入信息</span><br><span class="line">        TableMapReduceUtil.initTableMapperJob(inTableName,new Scan(),BatchExportMapper.class,Text.class,Text.class,job);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;设置输出路径</span><br><span class="line">        FileOutputFormat.setOutputPath(job,new Path(outPath));</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(true);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">打jar包</span><br><span class="line">D:\IdeaProjects\db_hbase&gt;mvn clean package -DskipTests</span><br><span class="line">......</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 34.797s</span><br><span class="line">[INFO] Finished at: Mon Nov 23 11:03:26 CST 2020</span><br><span class="line">[INFO] Final Memory: 66M&#x2F;1129M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">将jar包是上传到bigdata04上面，然后向集群提交任务</span><br><span class="line">[root@bigdata04 hadoop-3.2.0]# hadoop jar db_hbase-1.0-SNAPSHOT-jar-with-dependencies.jar com.imooc.hbase.BatchExportTableMapReduceUtil batch1 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;batch1</span><br><span class="line"></span><br><span class="line">查看导出结果数据：</span><br><span class="line">[root@bigdata04 hadoop-3.2.0]# hdfs dfs -cat &#x2F;batch1&#x2F;*</span><br><span class="line">a       zs      18</span><br><span class="line">b       ls      29</span><br><span class="line">c       ww      31</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：想要导出什么格式的数据，具体的逻辑代码在map函数内部根据需求实现即可。</span><br></pre></td></tr></table></figure>

<h4 id="批量导出之HBase内部方法"><a href="#批量导出之HBase内部方法" class="headerlink" title="批量导出之HBase内部方法"></a>批量导出之HBase内部方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">使用HBase提供的Export工具类直接导出数据</span><br><span class="line">[root@bigdata04 ~]# hbase org.apache.hadoop.hbase.mapreduce.Export batch1 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;batch2</span><br><span class="line"></span><br><span class="line">注意：此种方式导出的数据格式是固定的</span><br><span class="line">数据中的k1和v1是&lt;ImmutableBytesWritable key, Result result&gt;形式的</span><br><span class="line"></span><br><span class="line">查看结果是这样的：</span><br><span class="line">注意：直接使用cat命令查看会显示乱码，因为不是普通的文本文件</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -cat &#x2F;batch2&#x2F;*</span><br><span class="line">SEQ1org.apache.hadoop.hbase.io.ImmutableBytesWritable%org.apache.hadoop.hbase.client.Result#&#123;MdAa;</span><br><span class="line">_x0019_</span><br><span class="line">a_x0012_c1age 218</span><br><span class="line"></span><br><span class="line">a_x0012_c1name 2zs (Ab;</span><br><span class="line">_x0019_</span><br><span class="line">b_x0012_c1age 229</span><br><span class="line"></span><br><span class="line">b_x0012_c1name 2ls (Ac;</span><br><span class="line">_x0019_</span><br><span class="line">c_x0012_c1age 231</span><br><span class="line"></span><br><span class="line">c_x0012_c1name 2ww (</span><br><span class="line"></span><br><span class="line">建议优先选择使用第一种，更加灵活，根据需求导出希望的数据格式。</span><br></pre></td></tr></table></figure>

<h3 id="HBase连接池"><a href="#HBase连接池" class="headerlink" title="HBase连接池"></a>HBase连接池</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">注意：HBase2.1.2之后的版本，无需创建HBase线程池，HBase2.1.2提供的代码已经封装好，只需创建调用即可。</span><br><span class="line">Connection conn &#x3D; ConnectionFactory.createConnection(conf);</span><br><span class="line">&#x2F;&#x2F;conn在获取Table的时候，底层默认会从线程池中获取一个连接</span><br><span class="line">Table table &#x3D; conn.getTable(TableName.valueOf(&quot;s1&quot;));</span><br><span class="line"></span><br><span class="line">所以table在用完之后可以直接调用close方法即可，在程序关闭的时候调用conn的close方法即可。</span><br><span class="line"></span><br><span class="line">查看源码最终可以追踪到这里：</span><br><span class="line"></span><br><span class="line">org.apache.hadoop.hbase.client.ConnectionImplementation</span><br><span class="line">默认会创建一个线程池，256个连接。</span><br><span class="line">private ThreadPoolExecutor getBatchPool() &#123;</span><br><span class="line">  if (batchPool &#x3D;&#x3D; null) &#123;</span><br><span class="line">    synchronized (this) &#123;</span><br><span class="line">      if (batchPool &#x3D;&#x3D; null) &#123;</span><br><span class="line">        int threads &#x3D; conf.getInt(&quot;hbase.hconnection.threads.max&quot;, 256);</span><br><span class="line">        this.batchPool &#x3D; getThreadPool(threads, threads, &quot;-shared&quot;, null);</span><br><span class="line">        this.cleanupPool &#x3D; true;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  return this.batchPool;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><a href="https://blog.csdn.net/jason_bone_/article/details/123057482" target="_blank" rel="external nofollow noopener noreferrer">参考链接</a></p>
<hr>
<blockquote>
</blockquote>

      
    </div>
    
    
    
	
	<div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BNoSQL%E6%95%B0%E6%8D%AE%E5%BA%93HBase-2.html">大数据开发工程师-快速上手NoSQL数据库HBase-2</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 TTYONG 的个人博客">TTYONG</a></p>
  <p><span>发布时间:</span>2023年06月13日 - 09:06</p>
  <p><span>最后更新:</span>2023年07月04日 - 23:07</p>
  <p><span>原始链接:</span><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BNoSQL%E6%95%B0%E6%8D%AE%E5%BA%93HBase-2.html" title="大数据开发工程师-快速上手NoSQL数据库HBase-2">http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BNoSQL%E6%95%B0%E6%8D%AE%E5%BA%93HBase-2.html</a>
    <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BNoSQL%E6%95%B0%E6%8D%AE%E5%BA%93HBase-2.html" aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
    });
    });  
</script>



      
	</div>



    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>多少都是爱</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat_reward.jpg" alt="TTYONG 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay_reward.jpg" alt="TTYONG 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/HBase/" rel="tag"><i class="fa fa-tag"></i> HBase</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BNoSQL%E6%95%B0%E6%8D%AE%E5%BA%93HBase-1.html" rel="next" title="大数据开发工程师-快速上手NoSQL数据库HBase-1">
                <i class="fa fa-chevron-left"></i> 大数据开发工程师-快速上手NoSQL数据库HBase-1
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BNoSQL%E6%95%B0%E6%8D%AE%E5%BA%93HBase-3.html" rel="prev" title="大数据开发工程师-快速上手NoSQL数据库HBase-3">
                大数据开发工程师-快速上手NoSQL数据库HBase-3 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
      
        
          <ul class="sidebar-nav motion-element">
            <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
              文章目录
            </li>
            <li class="sidebar-nav-overview" data-target="site-overview">
              站点概览
            </li>
          </ul>
        
      

	<section class="site-overview sidebar-panel">
	  <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
		<img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="TTYONG">
		<p class="site-author-name" itemprop="name">TTYONG</p>
		 
			<p class="site-description motion-element" itemprop="description"></p>
		 
	  </div>
	  <nav class="site-state motion-element">

		
		  <div class="site-state-item site-state-posts">
			<a href="/archives/%7C%7C%20archive">
			  <span class="site-state-item-count">377</span>
			  <span class="site-state-item-name">日志</span>
			</a>
		  </div>
		

		
		  
		  
		  <div class="site-state-item site-state-categories">
			<a href="/categories/index.html">
			  <span class="site-state-item-count">52</span>
			  <span class="site-state-item-name">分类</span>
			</a>
		  </div>
		

		
		  
		  
		  <div class="site-state-item site-state-tags">
			<a href="/tags/index.html">
			  <span class="site-state-item-count">107</span>
			  <span class="site-state-item-name">标签</span>
			</a>
		  </div>
		

	  </nav>

	  
		<div class="feed-link motion-element">
		  <a href="/atom.xml" rel="alternate">
			<i class="fa fa-rss"></i>
			RSS
		  </a>
		</div>
	  

	  <div class="links-of-author motion-element">
		
		  
			<span class="links-of-author-item">
			  <a href="2364076207@qq.com || envelope" target="_blank" title="E-Mail">
				
				  <i class="fa fa-fw fa-globe"></i>
				
				E-Mail
			  </a>
			</span>
		  
			<span class="links-of-author-item">
			  <a href="https://wpa.qq.com/msgrd?v=3&uin=2364076207&site=qq&menu=yes || qq" target="_blank" title="QQ" rel="external nofollow noopener noreferrer">
				
				  <i class="fa fa-fw fa-globe"></i>
				
				QQ
			  </a>
			</span>
		  
			<span class="links-of-author-item">
			  <a href="https://weibo.com/p/1005051833844383/home || weixin" target="_blank" title="WeiXin" rel="external nofollow noopener noreferrer">
				
				  <i class="fa fa-fw fa-globe"></i>
				
				WeiXin
			  </a>
			</span>
		  
			<span class="links-of-author-item">
			  <a href="https://www.zhihu.com/people/he-he-ty || zhihu" target="_blank" title="ZhiHu" rel="external nofollow noopener noreferrer">
				
				  <i class="fa fa-fw fa-globe"></i>
				
				ZhiHu
			  </a>
			</span>
		  
		
	  </div>

	  
	  

	  
	  
		<div class="links-of-blogroll motion-element links-of-blogroll-block">
		  <div class="links-of-blogroll-title">
			<i class="fa  fa-fw fa-link"></i>
			友链
		  </div>
		  <ul class="links-of-blogroll-list">
			
			  <li class="links-of-blogroll-item">
				<a href="https://www.baidu.com/" target="_blank" rel="external nofollow noopener noreferrer">百度</a>
			  </li>
			
		  </ul>
		</div>
	  

	  


	</section>
	
	  
	  <!--noindex-->
		<section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
		  <div class="post-toc">

			
			  
			

			
			  <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#快速上手NoSQL数据库HBase-2"><span class="nav-number">1.</span> <span class="nav-text">快速上手NoSQL数据库HBase-2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-深入HBase架构原理"><span class="nav-number">1.1.</span> <span class="nav-text">3 深入HBase架构原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Region概念解释"><span class="nav-number">1.1.1.</span> <span class="nav-text">Region概念解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HBase物理架构"><span class="nav-number">1.1.2.</span> <span class="nav-text">HBase物理架构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Zookeeper"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Zookeeper</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HMaster"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">HMaster</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HRegionserver"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">HRegionserver</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HBase架构详解"><span class="nav-number">1.1.3.</span> <span class="nav-text">HBase架构详解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#详解HRegionServer"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">详解HRegionServer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HBase物理存储模型"><span class="nav-number">1.1.4.</span> <span class="nav-text">HBase物理存储模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#WAL-Write-Ahead-Logging-预写日志系统"><span class="nav-number">1.1.5.</span> <span class="nav-text">WAL(Write-Ahead Logging)预写日志系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HFile介绍"><span class="nav-number">1.1.6.</span> <span class="nav-text">HFile介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从读-写流程角度理解Hbase架构"><span class="nav-number">1.1.7.</span> <span class="nav-text">从读&#x2F;写流程角度理解Hbase架构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#写流程"><span class="nav-number">1.1.7.1.</span> <span class="nav-text">写流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#读流程"><span class="nav-number">1.1.7.2.</span> <span class="nav-text">读流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BloomFilter布隆过滤器"><span class="nav-number">1.1.8.</span> <span class="nav-text">BloomFilter布隆过滤器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HFile-compaction（合并）机制"><span class="nav-number">1.1.9.</span> <span class="nav-text">HFile compaction（合并）机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Region-Split（分裂）机制"><span class="nav-number">1.1.10.</span> <span class="nav-text">Region Split（分裂）机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Region-Balance-负载均衡策略"><span class="nav-number">1.1.11.</span> <span class="nav-text">Region Balance(负载均衡策略)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-HBase高级用法"><span class="nav-number">1.2.</span> <span class="nav-text">4 HBase高级用法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#列族高级设置"><span class="nav-number">1.2.1.</span> <span class="nav-text">列族高级设置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#生存时间-TTL"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">生存时间(TTL)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#版本数"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">版本数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#压缩"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">压缩</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据块-BLOCKSIZE-大小的配置"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">数据块(BLOCKSIZE)大小的配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据块缓存-BLOCKCACHE"><span class="nav-number">1.2.1.5.</span> <span class="nav-text">数据块缓存(BLOCKCACHE)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#布隆过滤器-Bloom-filters"><span class="nav-number">1.2.1.6.</span> <span class="nav-text">布隆过滤器(Bloom filters)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scan-全表扫描"><span class="nav-number">1.2.2.</span> <span class="nav-text">Scan (全表扫描)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Scan的Java-API用法"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Scan的Java API用法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#常见的Filter"><span class="nav-number">1.2.2.1.1.</span> <span class="nav-text">常见的Filter</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#RowFilter"><span class="nav-number">1.2.2.1.1.1.</span> <span class="nav-text">RowFilter</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#PrefixFilter"><span class="nav-number">1.2.2.1.1.2.</span> <span class="nav-text">PrefixFilter</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#ValueFilter"><span class="nav-number">1.2.2.1.1.3.</span> <span class="nav-text">ValueFilter</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Scan和Filter结合的案例"><span class="nav-number">1.2.2.1.2.</span> <span class="nav-text">Scan和Filter结合的案例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#批量导入"><span class="nav-number">1.2.3.</span> <span class="nav-text">批量导入</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#批量导入之-MapReduce"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">批量导入之 MapReduce</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#BatchImportMR"><span class="nav-number">1.2.3.1.1.</span> <span class="nav-text">BatchImportMR</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#批量导入之-BulkLoad"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">批量导入之 BulkLoad</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#BatchImportBulkLoad"><span class="nav-number">1.2.3.2.1.</span> <span class="nav-text">BatchImportBulkLoad</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#批量导出"><span class="nav-number">1.2.4.</span> <span class="nav-text">批量导出</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#批量导出之TableMapReduceUtil"><span class="nav-number">1.2.4.1.</span> <span class="nav-text">批量导出之TableMapReduceUtil</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#BatchExportTableMapReduceUtil"><span class="nav-number">1.2.4.1.1.</span> <span class="nav-text">BatchExportTableMapReduceUtil</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#批量导出之HBase内部方法"><span class="nav-number">1.2.4.2.</span> <span class="nav-text">批量导出之HBase内部方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HBase连接池"><span class="nav-number">1.2.5.</span> <span class="nav-text">HBase连接池</span></a></li></ol></li></ol></li></ol></div>
			

		  </div>
		</section>
	  <!--/noindex-->
	  
	

	

  </div>
</aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2020.3.4 &mdash; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">TTYONG</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">807.8k</span>
  
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io" rel="external nofollow>Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next rel="external nofollow">NexT.Pisces</a> v5.1.4</div>



-->  


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 访问总量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <div id="gitalk-container"></div>
  <script src="/js/src/md5.min.js"></script>
  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'd3f3299eb0cc69c8f171',
      clientSecret: '23bf8796e5dda2daf0a1b12964d89f4cc88f9ddf',
      repo: 'comments_repository',
      owner: 'ttyong',
      admin: ['ttyong'],
      id: md5(location.pathname),
      distractionFreeMode: 'false'
    })
    gitalk.render('gitalk-container')
  </script>

  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  
  

  

  

  

  <link rel="stylesheet" href="/dist/APlayer.min.css">
  <div id="aplayer"></div>
  <script type="text/javascript" src="/dist/APlayer.min.js"></script>
  <script type="text/javascript" src="/dist/music.js"></script>
  
  
  
  

  <canvas id="evanyou"></canvas>
  <style>
    #evanyou {
      position: fixed;
      width: 100%;
      height: 100%;
      top: 0;
      left: 0;
      z-index: -1;
    }
  </style>
  <script src="/js/src/evan-you.js"></script>




  <script src="/js/src/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>




  <script async src="/js/cursor/fireworks.js"></script>




<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
