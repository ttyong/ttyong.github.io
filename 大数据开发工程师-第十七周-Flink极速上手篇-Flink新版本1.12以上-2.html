<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<script src="https://cdn.jsdelivr.net/gh/wallleap/cdn/js/piao.js"></script>
<head>
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<meta name="baidu-site-verification" content="s8Pe1TBqyy">


  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/animal_bear_panda_32px_4023_easyicon.net.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/animal_bear_panda_16px_4023_easyicon.net.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="tianyong">





  <link rel="alternate" href="/atom.xml" title="TianYong's Blog" type="application/atom+xml">






<meta name="description" content="Flink新版本1.12以上-2State(状态)的容错与一致性123456789101112131415前面我们学习了State的原理和在代码中的使用。下面我们来深入分析一下State的容错和一致性。4.1 State的容错与一致性针对一个Flink流式任务，如何保证这个任务故障后恢复到之前的运行状态？注意：这里所说的恢复到之前的运行状态是表示将算子中计算的中间结果恢复到任务停止之前的">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据开发工程师-第十七周 Flink新版本1.12以上-2">
<meta property="og:url" content="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-2.html">
<meta property="og:site_name" content="TianYong&#39;s Blog">
<meta property="og:description" content="Flink新版本1.12以上-2State(状态)的容错与一致性123456789101112131415前面我们学习了State的原理和在代码中的使用。下面我们来深入分析一下State的容错和一致性。4.1 State的容错与一致性针对一个Flink流式任务，如何保证这个任务故障后恢复到之前的运行状态？注意：这里所说的恢复到之前的运行状态是表示将算子中计算的中间结果恢复到任务停止之前的">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021237900.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021240656.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021241563.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021241575.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021510607.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021517531.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021520073.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021526216.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021529424.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021530253.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021530596.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021536214.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021539955.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021540022.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021555606.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021603217.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021611292.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021617129.png">
<meta property="og:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021625881.png">
<meta property="article:published_time" content="2023-04-20T08:46:48.000Z">
<meta property="article:modified_time" content="2023-06-02T08:37:04.596Z">
<meta property="article:author" content="TTYONG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021237900.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://tianyong.fun/大数据开发工程师-第十七周-Flink极速上手篇-Flink新版本1.12以上-2.html">





  <title>大数据开发工程师-第十七周 Flink新版本1.12以上-2 | TianYong's Blog</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>


<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">
  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">TianYong's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">比你优秀的人都努力，有什么理由不努力！</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>
            
            站点地图
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-2.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="TTYONG">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianYong's Blog">
    </span>

    
      <header class="post-header">
	  
	  



        
        
          <h2 class="post-title" itemprop="name headline">大数据开发工程师-第十七周 Flink新版本1.12以上-2</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-04-20T16:46:48+08:00">
                2023-04-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/" itemprop="url" rel="index">
                    <span itemprop="name">大数据开发工程师</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  17.5k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  70
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <hr>
<script type="text/javascript" src="/js/src/bai.js"></script>



<h1 id="Flink新版本1-12以上-2"><a href="#Flink新版本1-12以上-2" class="headerlink" title="Flink新版本1.12以上-2"></a>Flink新版本1.12以上-2</h1><h2 id="State-状态-的容错与一致性"><a href="#State-状态-的容错与一致性" class="headerlink" title="State(状态)的容错与一致性"></a>State(状态)的容错与一致性</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了State的原理和在代码中的使用。下面我们来深入分析一下State的容错和一致性。</span><br><span class="line"></span><br><span class="line">4.1 State的容错与一致性</span><br><span class="line">针对一个Flink流式任务，如何保证这个任务故障后恢复到之前的运行状态？</span><br><span class="line">注意：这里所说的恢复到之前的运行状态是表示将算子中计算的中间结果恢复到任务停止之前的样子。</span><br><span class="line"></span><br><span class="line">其实咱们前面也提到过，想要实现状态的这种容错效果，需要借助于checkpoint机制。</span><br><span class="line"></span><br><span class="line">因为checkpoint可以将状态数据持久化保存到外部存储系统中，这样任务恢复时，可以基于之前存储到外部的状态数据进行恢复。</span><br><span class="line"></span><br><span class="line">针对流式计算任务，在故障后恢复状态数据的时候会涉及到三种语义：</span><br><span class="line"></span><br><span class="line">至少一次：At-least-once，这种语义可能会导致数据恢复时重复处理数据。</span><br><span class="line">至多一次：At-most-once，这种语义可能会导致数据恢复时丢失数据。</span><br><span class="line">仅一次：Exactly-once，这种语义可以保证数据只对结果影响一次，可以保证结果的准确性，不会出现重复或者丢失的情况。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：这里的仅一次语义，表示数据对最终结果的影响只有一次，并不是说数据只被处理一次。</span><br><span class="line"></span><br><span class="line">如果想要实现流式计算任务中数据的一致性，其实就是想要在流式计算任务中实现这种仅一次语义。</span><br></pre></td></tr></table></figure>

<h3 id="流计算中Exactly-once语义的多种实现思路"><a href="#流计算中Exactly-once语义的多种实现思路" class="headerlink" title="流计算中Exactly-once语义的多种实现思路"></a>流计算中Exactly-once语义的多种实现思路</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对流式计算任务中Exactly-once语义的实现思路其实是有多种的，下面我们来分析一下</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021237900.png" alt="image-20230602123745704"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">第一种思路：借助于At-least-once语义，再加上去重功能来实现。这种实现思路需要我们程序员自己来维护一个去重功能，因为At-least-once语义可以保证数据不丢，但是可能会出现数据重复，这样当任务故障后恢复的时候，对重复的数据进行去重，这样就可以间接实现Exactly-once这种仅一次语义了。</span><br><span class="line"></span><br><span class="line">这种实现思路属于中等难度，因为需要我们自己开发一个去重功能，保证数据不重复，对应的这个功能也会有一些性能开销，所以性能开销这方面属于中等。</span><br><span class="line"></span><br><span class="line">第二种思路：借助于At-least-once语义，再加上幂等操作，这种实现思路需要依赖于第三方存储系统的特性，也就是说这个第三方存储系统需要支持幂等操作，幂等操作表示一条命令重复执行多次，对最终的结果只有一次影响。</span><br><span class="line">举个例子：针对redis中的set命令，我们执行set a 1 ,这条命令不管执行多少次，a的值始终都是1，那么这个命令我们就可以认为是幂等操作。</span><br><span class="line">针对redis的incr命令，我们执行incr a，此时每执行一次，a的值就会加1，所以多次执行会导致结果发生变化，那么这个命令就不是幂等操作。</span><br><span class="line">这样在使用At-least-once语义的时候，他可能会导致数据重复，借助于幂等操作也是可以实现Exactly-once这种仅一次语义的。</span><br><span class="line"></span><br><span class="line">这种实现思路比较简单，对应的性能开销也比较低，不需要我们额外维护什么功能。</span><br><span class="line"></span><br><span class="line">第三种思路：借助于状态和checkpoint机制来实现，这种实现思路其实我们前面已经分析过了，他是可以实现仅一次语义的。</span><br><span class="line"></span><br><span class="line">这种实现思路也比较简单，对应的性能开销也比较低。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对这三种实现思路，下面有几张图我们来看一下，加深一下理解：</span><br><span class="line"></span><br><span class="line">思路1：</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021240656.png" alt="image-20230602124044160"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">相当于我们在中间的算子中需要借助于外部的存储系统实现一个去重功能，当任务故障后恢复的时候，遇到重复数据在处理的时候可以进行去重，这样可以保证实现仅一次语义的效果。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">思路2：</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021241563.png" alt="image-20230602124129333"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时需要依赖于外部存储系统的幂等特性，咱们前面以redis数据库为例进行了分析，如果是使用hbase数据库呢？</span><br><span class="line">hbase中rowkey是唯一的，我们只要保证重复数据的rowkey不变，那么执行多次put操作也是可以保证结果数据不变的。</span><br><span class="line"></span><br><span class="line">思路3：</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021241575.png" alt="image-20230602124152921"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时是借助于状态和checkpoint机制实现，任务运行期间，定期对状态生成快照，任务故障恢复时基于之前的快照数据恢复状态，这样可以实现仅一次语义。</span><br></pre></td></tr></table></figure>

<h3 id="如何实现Flink任务的端到端一致性？"><a href="#如何实现Flink任务的端到端一致性？" class="headerlink" title="如何实现Flink任务的端到端一致性？"></a>如何实现Flink任务的端到端一致性？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Flink可以借助于Checkpoint机制保证Flink系统内部状态的一致性。</span><br><span class="line"></span><br><span class="line">但是一个Flink流计算任务需要有Source、系统内部(也就是算子)，以及Sink这三部分组成。</span><br><span class="line"></span><br><span class="line">那这个时候应该如何实现Flink流计算任务整个链条的一致性保证？（也可以称为是端到端的一致性）</span><br><span class="line">端到端的一致性，意味着要保证从Source端到系统内部、再到最终的Sink端整个阶段的一致性。</span><br><span class="line">每一个阶段负责保证自己的一致性，整个端到端的一致性级别就取决于所有阶段中一致性最弱的那个阶段了</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">那下面我们来具体分析一下这三个阶段如何保证自己的一致性？</span><br><span class="line"></span><br><span class="line">首先是Source端：这个时候外部数据源需要支持任务故障恢复时的数据重放机制，不能说数据取出来之后数据源中就没有了，如果数据取出来之后任务失败了，肯定还是要重新再取一次的，所以说，外部数据源需要支持数据重放机制，在流计算任务中常用的外部数据源是kafka，kafka是可以支持数据重放机制的，我们可以通过控制消费者的消费偏移量实现数据重新消费。</span><br><span class="line"></span><br><span class="line">接下来是Flink系统内部，这块主要是依赖于Checkpoint机制实现。</span><br><span class="line"></span><br><span class="line">最后是Sink端：此时目的地存储系统需要保证数据恢复时不会重复写入。</span><br><span class="line"></span><br><span class="line">针对Sink端功能的具体实现包括两种方式，也就是说如何保证写入目的地存储系统时不会出现重复？</span><br><span class="line">第一种方式是：幂等写入</span><br><span class="line">幂等写入其实我们前面已经分析过了，就是说一个操作，可以重复执行很多次，但是只会导致一次结果更改，也就是说，后面再重复执行就不起作用了。</span><br><span class="line"></span><br><span class="line">第二种方式是：事务写入</span><br><span class="line">需要构建事务来写入外部系统，构建的事务对应着 checkpoint，等到 checkpoint 真正完成的时候，才把所有对应的结果写入目的地存储系统中。</span><br><span class="line"></span><br><span class="line">针对事务写入这种机制大致有两种实现方式：</span><br><span class="line"></span><br><span class="line">第一种实现方式：预写日志(WAL)的方式，这种方式无法100%保证Exactly-once语义，可能会出现数据重复。</span><br><span class="line">怎么理解呢？</span><br><span class="line">预写日志这种方式的通用性比较强，它会使用Operator State来存储数据，在任务发生故障时可以恢复，不会导致数据丢失，几乎适合所有外部系统，但是不能提供100%端到端的仅一次语义。</span><br><span class="line">因为基于预写日志的写入方式在某些极端情况下可能会将数据写入多次。</span><br><span class="line">例如：如果外部系统不支持原子性的写入多条数据，那么在向外部系统写入数据的时候可能就会出现部分数据已经写入，但是此时任务出现了故障，导致剩余一部分数据没有写入的情况。当下次恢复的时候会重写全部数据，这样数据就会出现部分重复。</span><br><span class="line"></span><br><span class="line">第二种实现方式：两阶段提交(2PC)的方式，这种方式可以100%保证Exactly-once语义。</span><br><span class="line">如果外部系统自身就支持事务（比如MySQL、Kafka），可以使用两阶段提交的方式，这样可以保证端到端的一致性。</span><br><span class="line">两阶段提交这种方式可以提供端到端的一致性保证，但是它的代价也是非常明显的，就是牺牲了延迟。输出数据不再是实时写入到外部系统，而是分批次地提交。</span><br><span class="line">目前来说，没有完美的故障恢复和仅一次语义保障机制，对于开发者来说，我们需要在不同需求之间权衡取舍。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对WAL预写日志这种方式，他提供的有一个接口是GenericWriteAheadSink，我们来看一下这个接口的大致实现：</span><br><span class="line">这是一个抽象类，目前还没有具体的实现。</span><br><span class="line">他里面用到了ListState，在initializeState方法中可以看到这个ListState属于Operator State这种类型。</span><br><span class="line">这个ListState负责在checkpoint的时候保存目前等待写出去的数据，便于任务故障时的数据恢复。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">private transient ListState&lt;PendingCheckpoint&gt; checkpointedState;</span><br><span class="line"></span><br><span class="line">checkpointedState &#x3D;</span><br><span class="line">        context.getOperatorStateStore()</span><br><span class="line">                .getListState(</span><br><span class="line">                        new ListStateDescriptor&lt;&gt;(</span><br><span class="line">                                &quot;pending-checkpoints&quot;, new JavaSerializer&lt;&gt;()));</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">两阶段提交这种方式会用到TwoPhaseCommitSinkFunction这个接口：</span><br><span class="line">这个接口里面主要有几个比较重要的方法：</span><br><span class="line">&#x2F;&#x2F;开启一个新事务</span><br><span class="line">protected abstract TXN beginTransaction() throws Exception;</span><br><span class="line">&#x2F;&#x2F;预提交-对应的就是第一阶段的提交</span><br><span class="line">protected abstract void preCommit(TXN transaction) throws Exception;</span><br><span class="line">&#x2F;&#x2F;提交-对应的是第二阶段的提交，此时数据才会真正提交到外部存储系统中，数据才对外可见</span><br><span class="line">protected abstract void commit(TXN transaction);</span><br><span class="line">&#x2F;&#x2F;回滚事务</span><br><span class="line">protected abstract void abort(TXN transaction);</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们在使用Flink向kafka中写入数据想要保证数据一致性的时候就会用到这个接口，等后面我们再具体分析这个接口在kafka中的具体实现。</span><br></pre></td></tr></table></figure>

<h2 id="Checkpoint-快照-机制详解"><a href="#Checkpoint-快照-机制详解" class="headerlink" title="Checkpoint(快照)机制详解"></a>Checkpoint(快照)机制详解</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Checkpoint机制是Flink中实现State容错和一致性最核心的功能。</span><br><span class="line">它能够根据配置周期性地基于流计算中的状态数据生成快照，从而将这些状态数据定期持久化存储下来，当Flink程序一旦意外故障崩溃时，在重新运行程序的时候可以有选择地从这些快照进行恢复，从而修正因为故障带来的数据异常。</span><br><span class="line"></span><br><span class="line">默认情况下Checkpoint机制是处于禁用状态的，如果想要开启需要在代码层面进行设置。</span><br><span class="line"></span><br><span class="line">Checkpoint支持两种语义级别：Exactly-once和At-least-once，其中Exactly-once是默认的语义级别。</span><br><span class="line"></span><br><span class="line">Exactly-once语义对于大多数应用来说是合适的。</span><br><span class="line">At-least-once语义可能用在某些延迟超低的应用程序（始终延迟为几毫秒），相对而言，At-least-once语义不需要保证数据的强一致性，所以数据的传输延迟会比较低。</span><br><span class="line">如果某个需求对数据的准确度要求不是特别高，但是需要计算和传输的数据量比较大，还需要低延迟，那么可以考虑使用At-least-once语义。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来具体看一下如何在代码中开启Checkpoint，以及Checkpoint相关的一些核心配置。</span><br><span class="line">创建package：com.imooc.scala.checkpoint</span><br><span class="line"></span><br><span class="line">核心代码如下：</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.storage.<span class="type">FileSystemCheckpointStorage</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">ExecutionCheckpointingOptions</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Checkpoint相关配置</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CheckpointCoreConf</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint，并且指定自动执行的间隔时间</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">60</span>*<span class="number">2</span>)<span class="comment">//2分钟</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//高级选项</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//设置语义模式为EXACTLY_ONCE（默认就是EXACTLY_ONCE）</span></span><br><span class="line">    cpConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//设置两次Checkpoint之间的最小间隔时间(设置为5秒：表示Checkpoint完成后的5秒内不会开始生成新的Checkpoint)</span></span><br><span class="line">    cpConfig.setMinPauseBetweenCheckpoints(<span class="number">1000</span>*<span class="number">5</span>)<span class="comment">//5秒</span></span><br><span class="line">    <span class="comment">//设置最多允许同时运行几个Checkpoint(默认值为1，也建议使用默认值1，这样可以减少资源占用)</span></span><br><span class="line">    cpConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//设置一次Checkpoint的执行超时时间，达到超时时间后会被取消执行，可以避免Checkpoint执行时间过长</span></span><br><span class="line">    cpConfig.setCheckpointTimeout(<span class="number">1000</span>*<span class="number">60</span>*<span class="number">6</span>)<span class="comment">//6分钟</span></span><br><span class="line">    <span class="comment">//设置允许的连续Checkpoint失败次数(默认值为0，表示Checkpoint只要执行失败任务也会立刻失败)</span></span><br><span class="line">    <span class="comment">//偶尔的Checkpoint失败不应该导致任务执行失败，可能是由于一些特殊情况(网络问题)导致Checkpoint失败</span></span><br><span class="line">    <span class="comment">//应该设置一个容错值，如果连续多次Checkpoint失败说明确实是有问题了，此时可以让任务失败</span></span><br><span class="line">    cpConfig.setTolerableCheckpointFailureNumber(<span class="number">3</span>)</span><br><span class="line">    <span class="comment">//设置在手工停止任务时是否保留之前生成的Checkpoint数据(建议使用RETAIN_ON_CANCELLATION)</span></span><br><span class="line">    <span class="comment">//RETAIN_ON_CANCELLATION：在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    <span class="comment">//DELETE_ON_CANCELLATION：只有在任务故障时才会保留，如果手工停止任务会删除之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    <span class="comment">//支持JobManagerCheckpointStorage(默认)和FileSystemCheckpointStorage</span></span><br><span class="line">    <span class="comment">//JobManagerCheckpointStorage：表示Checkpoint后的状态数据存储在JobManager节点的JVM堆内存中</span></span><br><span class="line">    <span class="comment">//FileSystemCheckpointStorage：表示Checkpoint后的状态数据存储在文件系统中(可以使用分布式文件系统HDFS)</span></span><br><span class="line">    <span class="comment">//可以简写为：cpConfig.setCheckpointStorage("hdfs://bigdata01:9000/flink-chk001")</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="keyword">new</span> <span class="type">FileSystemCheckpointStorage</span>(<span class="string">"hdfs://bigdata01:9000/flink-chk001"</span>))</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这些配置是针对单个任务有效的，并且需要放到具体的任务中才有意义。目前这个类里面的代码是不完整的，无法正常执行，等后面我们把这些代码拿到对应的任务中去使用。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">好的，我来解释一下。在Flink中，Checkpoint是用来保存程序运行状态的一种机制。env.enableCheckpointing(1000*60*2)是用来设置Checkpoint的间隔时间，即每隔多长时间进行一次Checkpoint。cpConfig.setMinPauseBetweenCheckpoints(1000*5)是用来设置两次 Checkpoint之间的最小暂停时间，即如果上一次Checkpoint完成后，距离下一次 Checkpoint的间隔时间小于这个值，则会等待直到满足最小暂停时间后再进行下一次 Checkpoint。</span><br><span class="line"></span><br><span class="line">这两个设置是相互独立的，可以根据实际情况进行调整。例如，如果你希望程序在出现故障时能够快速恢复，则可以缩短Checkpoint的间隔时间；如果你希望减少 Checkpoint 对程序性能的影响，则可以增加两次Checkpoint之间的最小暂停时间。</span><br></pre></td></tr></table></figure>

<h3 id="保存多个Checkpoint"><a href="#保存多个Checkpoint" class="headerlink" title="保存多个Checkpoint"></a>保存多个Checkpoint</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">默认情况下，如果在任务中开启了Checkpoint，则Flink只会保留最近成功生成的1份Checkpoint数据。</span><br><span class="line">当Flink程序故障重启时，可以从最近的这份Checkpoint数据来进行恢复。</span><br><span class="line"></span><br><span class="line">但是我们希望能够保留多份Checkpoint数据，并能够根据实际需要选择其中一份进行恢复，这样会更加灵活。</span><br><span class="line">例如：我们发现最近2个小时的数据处理有问题，希望将整个状态还原到2小时之前，这样就需要找到2个小时之前的Checkpoint数据进行恢复了。</span><br><span class="line"></span><br><span class="line">Flink可以支持保留多份Checkpoint数据，需要在Flink的配置文件flink-conf.yaml中，添加如下配置，指定最多需要保存最近多少份Checkpoint数据</span><br><span class="line">这个配置针对整个客户端有效，只要是在这个客户端上提交的任务，都会使用这个配置。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# vi flink-conf.yaml </span><br><span class="line">...</span><br><span class="line">state.checkpoints.num-retained: 20</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="从Checkpoint进行恢复"><a href="#从Checkpoint进行恢复" class="headerlink" title="从Checkpoint进行恢复"></a>从Checkpoint进行恢复</h3><h4 id="手工停止的任务-恢复数据"><a href="#手工停止的任务-恢复数据" class="headerlink" title="手工停止的任务,恢复数据"></a>手工停止的任务,恢复数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面我们分析了Checkpoint的详细配置，下面我们就结合具体的案例来演示一下Checkpoint是如何对状态数据进行持久化保存的，并且当任务故障后，我们如何基于Checkpoint产生的数据进行恢复</span><br><span class="line"></span><br><span class="line">首先我们重新开发一个有状态的单词计数案例，并且在代码中开启checkpoint。</span><br><span class="line"></span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 有状态的单词计数 + Checkpoint</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStateWithCheckpointDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="string">"hdfs://bigdata01:9000/flink-chk/wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WordCountStateWithCheckpointDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">修改项目的依赖配置，将所有依赖的scope都设置为provided。</span><br><span class="line"></span><br><span class="line">打jar包。</span><br><span class="line"></span><br><span class="line">将生成的jar包上传到bigdata04机器中。</span><br><span class="line"></span><br><span class="line">在bigdata04机器上开启Socket</span><br><span class="line"></span><br><span class="line">向集群中提交此任务。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时a出现2次，b出现了1次。</span><br><span class="line"></span><br><span class="line">此时可以在任务界面中查看checkpoint的执行情况：</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021510607.png" alt="image-20230602151049919"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line"></span><br><span class="line">Trriggered：表示截止到目前checkpoint触发的次数。</span><br><span class="line">In Progress：表示目前正在执行的checkpoint数量。</span><br><span class="line">Completed：表示成功执行结束的checkpoint次数。</span><br><span class="line">Failed：表示执行失败的checkpoint次数，这里显示为2的意思是失败了2次，这是因为我们现在设置的checkpoint间隔时间太短了，只有10秒，任务提交上去之后很快就会触发checkpoint，此时Flink任务可能还没有初始化完成，所以会出现一些失败次数，等Flink任务正常运行起来之后就没问题了。实际工作中，我们会把checkpoint的间隔时间设置为分钟级别，一般是2分钟，5分钟之类的，这样就不会出现这种问题了。</span><br><span class="line">Restored：0表示这个任务没有基于之前的checkpoint数据启动，显示为1表示基于之前的checkpoint数据启动。</span><br><span class="line">Path：表示当前任务的checkpoint数据保存目录，注意：同一个任务每次启动生成的checkpoint数据目录都不一样，因为这个路径里面用到了flink的任务id，任务id是每次都会重新生成的。</span><br><span class="line"></span><br><span class="line">此时到hdfs中查看一下具体生成的checkpoint目录</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 ~]# hdfs dfs -ls &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;</span><br><span class="line">Found 22 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:23 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-27</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:23 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-28</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:23 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-29</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-30</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-31</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-32</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-33</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-34</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-35</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-36</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-37</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-38</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-39</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-40</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-41</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-42</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-43</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-44</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-45</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-46</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:18 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;shared</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:18 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;taskowned</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这里会显示最近20次checkpoint生成的数据目录。</span><br><span class="line"></span><br><span class="line">接下来我们来手工停止任务。</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021517531.png" alt="image-20230602151730020"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：如果刚才是由于任务内部故障导致的任务停止，则Flink会基于默认的重启策略自动重启，在自动重启的时候会自动使用最新生成的那一份checkpoint数据进行恢复。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">但是刚才是我们手工停止的任务，那么任务在重启的时候想要恢复数据就需要手工指定从哪一份checkpoint数据启动。</span><br><span class="line">如果没有特殊情况就选择使用最新的那一份checkpoint数据进行恢复即可。</span><br><span class="line"></span><br><span class="line">先开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">选择一份checkpoint数据重启任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]#flink1-15 run -m yarn-cluster -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-54&#x2F;_metadata  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：-s后面指定的是某一份checkpoint数据，这个时候任务启动的时候会基于这份checkpoint数据进行恢复，这个路径必须写hdfs的全路径，必须要有hdfs路径的前缀，否则会被识别成linux本地路径。</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，查看任务界面中的checkpoint信息：</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021520073.png" alt="image-20230602152033829"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">此时图中显示的Restored为1，说明这个任务是基于之前的checkpoint数据恢复启动的。</span><br><span class="line">checkpoint ID目前为55，说明这个任务会延续之前的Checkpoint ID，继续递增增长。</span><br><span class="line">最下面的Latest Restore：ID为54，这个编号是启动任务时使用的Checkpoint 数据的ID编号。</span><br><span class="line">Restore Time表示恢复的时间。</span><br><span class="line">Type：SavePoint，这是因为我们是手工重启的，会显示为Savepoint，如果是任务故障时自动重启的，这里会显示为checkpoint。具体Savepoint什么含义，后面我们会具体分析。</span><br><span class="line">Path：这个表示启动任务时使用的checkpoint数据路径。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">此时在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line"></span><br><span class="line">查看任务日志：</span><br><span class="line">(a,3)</span><br><span class="line">(b,2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这样就说明此任务在启动的时候恢复到了之前的状态，因为上一次任务停止之前，a出现了2次，b出现了1次。这次任务重启后，可以累加之前状态中记录的次数。这样就实现了任务故障后的数据恢复，可以保证流计算中数据的准确性，如果没有使用状态和checkpoint，当任务重启后，所有的数据都会归0。</span><br><span class="line"></span><br><span class="line">当重启后的程序正常运行后，他还会按照Checkpoint的配置进行运行，继续生成Checkpoint数据。</span><br></pre></td></tr></table></figure>

<h4 id="任务自动重启时checkpoint数据的自动恢复"><a href="#任务自动重启时checkpoint数据的自动恢复" class="headerlink" title="任务自动重启时checkpoint数据的自动恢复"></a>任务自动重启时checkpoint数据的自动恢复</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">下面我们来演示一下任务在运行期间，任务内部故障导致的任务自动重启时checkpoint数据的自动恢复。</span><br><span class="line">重新开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交此任务。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br><span class="line"></span><br><span class="line">此时在任务界面中查看一下taskmanager节点信息</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021526216.png" alt="image-20230602152607843"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">从这个图里面可以看出来，taskmanager进程目前运行在bigdata02上。</span><br><span class="line">我们模拟一个故障场景，由于集群节点异常导致bigdata02节点宕机了，那么运行在这个节点上的taskmanager进程肯定也就没了。</span><br><span class="line"></span><br><span class="line">在这里我到bigdata02上使用kill命令直接把taskmanager进程杀掉就可以模拟这个场景了。</span><br><span class="line">首先在bigdata02上执行jps命令查看目前的进程信息</span><br><span class="line"></span><br><span class="line">[root@bigdata02 ~]# jps</span><br><span class="line">1680 NodeManager</span><br><span class="line">1570 DataNode</span><br><span class="line">11815 YarnTaskExecutorRunner</span><br><span class="line">11992 Jps</span><br><span class="line">11674 YarnJobClusterEntrypoint</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">其实11815 YarnTaskExecutorRunner 这个就是taskmanager进程了。</span><br><span class="line"></span><br><span class="line">想要进一步确认的话可以使用jps -ml命令：</span><br><span class="line">[root@bigdata02 ~]# jps -ml</span><br><span class="line">1680 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">12033 sun.tools.jps.Jps -ml</span><br><span class="line">1570 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">11815 org.apache.flink.yarn.YarnTaskExecutorRunner -D taskmanager.memory.network.min&#x3D;67108864b -D taskmanager.cpu.cores&#x3D;1.0 -D taskmanager.memory.task.off-heap.size&#x3D;0b -D taskmanager.memory.jvm-metaspace.size&#x3D;268435456b -D external-resources&#x3D;none -D taskmanager.memory.jvm-overhead.min&#x3D;201326592b -D taskmanager.memory.framework.off-heap.size&#x3D;134217728b -D taskmanager.memory.network.max&#x3D;67108864b -D taskmanager.memory.framework.heap.size&#x3D;134217728b -D taskmanager.memory.managed.size&#x3D;241591914b -D taskmanager.memory.task.heap.size&#x3D;26843542b -D taskmanager.numberOfTaskSlots&#x3D;1 -D taskmanager.memory.jvm-overhead.max&#x3D;201326592b --configDir . -Dblob.server.port&#x3D;32854 -Djobmanager.rpc.address&#x3D;bigdata02 -Djobmanager.memory.jvm-overhead.min&#x3D;201326592b -Dtaskmanager.resource-id&#x3D;container_1826267805863_0011_01_000002 -Dweb.port&#x3D;0 -Djobmanager.memory.off-heap.size&#x3D;134217728b -Dweb.tmpdir&#x3D;&#x2F;tmp&#x2F;flink-web-95fcafd5-13cb-44f8-adbf-e3e42f09aa15 -Dinternal.taskmanager.resource-id.metadata&#x3D;bigdata02:38033 -Djobmanager.rpc.port&#x3D;36024 -Dr</span><br><span class="line">11674 org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint -D jobmanager.memory.off-heap.size&#x3D;134217728b -D jobmanager.memory.jvm-overhead.min&#x3D;201326592b -D jobmanager.memory.jvm-metaspace.size&#x3D;268435456b -D jobmanager.memory.heap.size&#x3D;469762048b -D jobmanager.memory.jvm-overhead.max&#x3D;201326592b</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">确定了是哪个进程之后，使用kill 命令杀掉进程。</span><br><span class="line">直接强制杀进程</span><br><span class="line">[root@bigdata02 ~]# kill -9 11815</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个时候回到任务界面，可以看到界面中已经不显示这个taskmanager了</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021529424.png" alt="image-20230602152917231"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">稍等一会任务会重启：</span><br><span class="line">此时发现任务中有一些失败的task，这是因为刚才taskmanager进程挂掉之后，socket也停止了，任务重启后，连不上socket了。</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021530253.png" alt="image-20230602153006041"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">重新开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再看任务界面中的checkpoint相关的信息</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021530596.png" alt="image-20230602153045307"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">可以发现此时下面的restore部分有信息了，表示是基于之前的第36份数据进行启动的，此时type就是checkpoint了。</span><br><span class="line"></span><br><span class="line">这个时候我们在socket中再模拟产生一条数据a</span><br><span class="line"></span><br><span class="line">再验证一下输出结果，发现是没有问题的，是基于之前的状态进行累加的。</span><br><span class="line"></span><br><span class="line">(a,3)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这就是针对checkpoint数据的自动恢复和手动恢复。</span><br></pre></td></tr></table></figure>

<h4 id="Savepoint详解"><a href="#Savepoint详解" class="headerlink" title="Savepoint详解"></a>Savepoint详解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">咱们前面讲到的Checkpoint是为了保证应用在出现故障时可以顺利重启恢复。</span><br><span class="line"></span><br><span class="line">而Savepoint是为了有计划的备份任务，实现任务升级后可恢复。</span><br><span class="line"></span><br><span class="line">任务升级主要包括：增减并行度、调整业务逻辑、以及升级Flink版本时的任务迁移。</span><br><span class="line"></span><br><span class="line">Flink通过Savepoint功能可以做到程序升级后，继续从升级前的那个点开始执行计算，保证数据不中断。</span><br><span class="line"></span><br><span class="line">Savepoint会生成全局，一致性快照，可以保存数据源offset，operator操作状态等信息，可以从应用在过去任意做了savepoint的时刻开始继续消费。</span><br><span class="line"></span><br><span class="line">Savepoint的生成算法和Checkpoint是完全一样的，所以可以把Savepoint认为是包含了一些额外元数据的Checkpoint，所以Savepoint本质上是特殊的Checkpoint。</span><br><span class="line"></span><br><span class="line">Savepoint和Checkpoint可以同时执行，互不影响，Flink不会因为正在执行Checkpoint而推迟Savepoint的执行。</span><br></pre></td></tr></table></figure>

<h4 id="Checkpoint-VS-Savepoint"><a href="#Checkpoint-VS-Savepoint" class="headerlink" title="Checkpoint VS Savepoint"></a>Checkpoint VS Savepoint</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对Checkpoint 和 Savepoint的详细区别，在这里我整理了一个表格，我们来看一下</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021536214.png" alt="image-20230602153615972"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">中文翻译：Checkpoint可以翻译为检查点 或者快照。Savepoint可以翻译为保存点。</span><br><span class="line"></span><br><span class="line">触发方式：Checkpoint是由JobManager定时触发快照并自动清理，不需要用户干预，当任务故障重启时自动恢复。Savepoint面向用户，完全根据用户的需要触发与清理，恢复的时候也是根据需求进行恢复。</span><br><span class="line"></span><br><span class="line">作用：Checkpoint主要是为了实现任务故障恢复的，他的侧重点是容错，当Flink作业意外失败，重启时可以从之前生成的CheckPoint自动恢复运行，不影响作业逻辑的准确性。SavePoint侧重点是维护，当Flink作业需要在人工干预下手动重启、升级、或者迁移时，先将状态整体写入可靠存储，维护完毕之后再从SavePoint恢复，他属于有计划的备份。</span><br><span class="line"></span><br><span class="line">特点：Checkpoint属于轻量级的快照，因为Checkpoint的频率往往比较高，所以Checkpoint的存储格式非常轻量级，但作为权衡牺牲了一切可移植的东西，例如：不保证改变并行度和升级的兼容性。Savepoint属于重量级的快照，他会以二进制的形式存储所有状态数据和元数据，执行起来比较慢而且贵，但是能够保证程序的可移植性 ，例如并行度改变或代码升级之后，仍然能正常恢复。</span><br><span class="line">Checkpoint是支持增量快照的（如果状态数据存储在RocksDB里面），对于超大状态的 作业而言可以降低写入成本。Savepoint并不会连续自动触发，所以不支持增量，只支 持全量。</span><br></pre></td></tr></table></figure>

<h4 id="Savepoint保证程序可移植性的前提条件"><a href="#Savepoint保证程序可移植性的前提条件" class="headerlink" title="Savepoint保证程序可移植性的前提条件"></a>Savepoint保证程序可移植性的前提条件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">咱们刚才说了，Savepoint能够保证程序的可移植性，可以保证在代码升级之后，仍然可以恢复数据。</span><br><span class="line">但是他想要实现这个功能的前提条件是需要保证任务中所有有状态的算子都配置好下面这两个参数：</span><br><span class="line">第一个参数是：算子唯一标识。</span><br><span class="line">第二个参数是：算子最大并行度，这个参数只针对使用了keyed State的算子。</span><br><span class="line"></span><br><span class="line">这两个参数会被固化到Savepoint数据中，不可更改，如果新任务中这两个参数发生了变化，就无法从之前生成的Savepoint数据中启动并恢复数据了，只能选择丢弃之前的状态从头开始运行。</span><br><span class="line"></span><br><span class="line">下面我们来具体分析一下这两个参数</span><br></pre></td></tr></table></figure>

<h5 id="算子唯一标识"><a href="#算子唯一标识" class="headerlink" title="算子唯一标识"></a>算子唯一标识</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">首先是算子唯一标识这个参数：</span><br><span class="line"></span><br><span class="line">默认情况下Flink会给每个算子分配一个唯一标识。</span><br><span class="line">但是这个标识是根据前面算子的标识并且结合一些规则生成的，这也就意味着任何一个前置算子发生改变都会导致该算子的标识发生变化 。</span><br><span class="line"></span><br><span class="line">例如：我们添加或者删除一个算子，这样后面算子的唯一标识就变了，就不可控了。</span><br><span class="line"></span><br><span class="line">只要任务中的算子唯一标识发生了变化，Savepoint保存的状态数据基本上就无法用来恢复了，因为之前保存的算子标识和现在最新的算子标识不一样了。</span><br><span class="line"></span><br><span class="line">咱们前面说过，Savepoint会以二进制的形式存储所有状态数据和元数据，这里的算子唯一标识就属于元数据中的内容。当Flink任务从Savepoint启动时，会利用算子的唯一标识将Savepoint保存的状态映射到新任务对应的算子中，只有当新任务的算子唯一标识和Savepoint数据中保存的算子标识相同时，状态才能顺利恢复。</span><br><span class="line"></span><br><span class="line">所以说如果我们没有给有状态的算子手工设置唯一标识，那么在任务升级时就会受到很多限制。</span><br><span class="line"></span><br><span class="line">看下面这个图</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021539955.png" alt="image-20230602153912620"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">上面是最开始的任务版本，里面有Source、Map和Sink这三个组件，假设这三个组件都是有状态的，我们没有给这些组件手工设置唯一标识，使用的是默认的自动生成的。</span><br><span class="line">这里的唯一标识其实就是uid。</span><br><span class="line">假设source自动生成的唯一标识是uid-001，Map自动生成的唯一标识是uid-002，Sink自动生成的唯一标识是uid-003。</span><br><span class="line"></span><br><span class="line">当这个任务运行了一段时间之后，我们的业务逻辑发生了变化，所以对代码做了一些修改，增加了一个Flatmap组件，看下面这个图：</span><br><span class="line">此时还是使用的默认生成的唯一标识，那此时source自动生成的唯一标识可能还是uid-001，map自动生成的唯一标识也还是uid-002，但是flatmap自动生成的唯一标识可能是uid-003，最后sink自动生成的唯一标识就可能是uid-004了。</span><br><span class="line"></span><br><span class="line">这样新任务中sink的唯一标识和之前任务中sink的唯一标识就不一样了，那么再基于之前任务生成的savepoint数据就会导致无法恢复了。</span><br><span class="line"></span><br><span class="line">为了能够在任务的不同版本之间顺利升级，我们需要通过 uid(…) 方法手动的给算子设置uid。</span><br><span class="line">类似这样的：</span><br><span class="line">source.uid(...)</span><br><span class="line">transform.uid(...)</span><br><span class="line">sink.uid(...)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在DataSouce、中间的转换算子、DataSink上面都可以设置uid。</span><br><span class="line"></span><br><span class="line">其实也没必要给所有的组件都设置uid，最重要的是给包含了状态的组件设置uid，没有状态的组件也不会涉及到数据恢复，就没必要设置了。</span><br><span class="line"></span><br><span class="line">看这块代码中的设置</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021540022.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source和map中维护了状态，所以需要设置uid，其它的就不用设置了，让程序自动生成即可</span><br><span class="line"></span><br><span class="line">注意：在同一个任务内部，uid不能重复。</span><br><span class="line"></span><br><span class="line">此时我们手工指定uid之后，后期就算在任务中新增了一个组件，之前的组件的uid也不会变化了，任务基于之前生成的Savepoint数据启动的时候依然是可以恢复数据的。</span><br></pre></td></tr></table></figure>

<h5 id="算子最大并行度"><a href="#算子最大并行度" class="headerlink" title="算子最大并行度"></a>算子最大并行度</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来分析一下第二个参数：算子最大并行度</span><br><span class="line"></span><br><span class="line">Flink中Keyed State类型的状态数据在恢复时，是按照KeyGroup为单位恢复的，每个KeyGroup 中包含一部分key的数据。</span><br><span class="line">咱们前面在讲状态的扩缩容模式的时候也提到过KeyGroup，说的是一回事。</span><br><span class="line">针对Keyed State，状态在扩缩容的时候会以KeyGroup为单位进行重新分配。</span><br><span class="line"></span><br><span class="line">KeyGroup的个数等于算子的最大并行度。</span><br><span class="line">注意：算子的最大并行度并不是算子的并行度，</span><br><span class="line"></span><br><span class="line">算子的最大并行度是通过setMaxParallelism()方法设置的，</span><br><span class="line">算子的并行度是通过setParallelism()方法设置的。</span><br><span class="line"></span><br><span class="line">当我们设置的算子并行度大于算子最大并行度时，任务在重启的时候有些并行度就分配不到KeyGroup了，这样会导致Flink任务无法从Savepoint恢复数据。</span><br><span class="line"></span><br><span class="line">注意：此时也是无法从Checkpoint中恢复数据的。</span><br><span class="line"></span><br><span class="line">那这个算子最大并行度应该如何设置呢？</span><br><span class="line">可以通过这两种方式，一种全局的，一种局部的</span><br><span class="line"></span><br><span class="line">设置全局算子最大并行度：env.setMaxParallelism()</span><br><span class="line">设置某个算子最大并行度：.map(..).setMaxParallelism()</span><br><span class="line">那咱们之前如果没有设置过的话，这些算子的最大并行度默认是多少呢？</span><br><span class="line"></span><br><span class="line">先说结果，算子最大并行度默认是128，最大是32768。env.setMaxParallelism()</span><br><span class="line"></span><br><span class="line">我们来看一下这块逻辑的代码：</span><br><span class="line">算子的最大并行度可以在算子上独立设置，也可以通过env全局设置。</span><br><span class="line">通过这个方法作为入口查看底层源码：</span><br><span class="line">env.setMaxParallelism()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">往下面追踪，可以追踪到这里：</span><br><span class="line">public StreamExecutionEnvironment setMaxParallelism(int maxParallelism) &#123;</span><br><span class="line">    Preconditions.checkArgument(</span><br><span class="line">            maxParallelism &gt; 0</span><br><span class="line">                    &amp;&amp; maxParallelism &lt;&#x3D; KeyGroupRangeAssignment.UPPER_BOUND_MAX_PARALLELISM,</span><br><span class="line">            &quot;maxParallelism is out of bounds 0 &lt; maxParallelism &lt;&#x3D; &quot;</span><br><span class="line">                    + KeyGroupRangeAssignment.UPPER_BOUND_MAX_PARALLELISM</span><br><span class="line">                    + &quot;. Found: &quot;</span><br><span class="line">                    + maxParallelism);</span><br><span class="line"></span><br><span class="line">    config.setMaxParallelism(maxParallelism);</span><br><span class="line">    return this;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这是我们手工设置算子最大并行度时使用的方法，这里面会用到KeyGroupRangeAssignment这个类。</span><br><span class="line"></span><br><span class="line">在KeyGroupRangeAssignment这个类中有一个方法：computeDefaultMaxParallelism</span><br><span class="line">这个方法会计算默认的算子最大并行度</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public static int computeDefaultMaxParallelism(int operatorParallelism) &#123;</span><br><span class="line"></span><br><span class="line">    checkParallelismPreconditions(operatorParallelism);</span><br><span class="line"></span><br><span class="line">    return Math.min(</span><br><span class="line">            Math.max(</span><br><span class="line">                    MathUtils.roundUpToPowerOfTwo(</span><br><span class="line">                            operatorParallelism + (operatorParallelism &#x2F; 2)),</span><br><span class="line">                    DEFAULT_LOWER_BOUND_MAX_PARALLELISM),</span><br><span class="line">            UPPER_BOUND_MAX_PARALLELISM);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这块代码我们可以拿出来运行一下，测试一下效果</span><br><span class="line">创建一个Object：TestDefaultMaxParallelism</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">MathUtils</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 测试默认算子最大并行度</span></span><br><span class="line"><span class="comment"> * 结论：Flink生成的算子最大并行度介于128和32768之间</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestDefaultMaxParallelism</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//这个参数表示当前算子(任务)的并行度</span></span><br><span class="line">    <span class="keyword">val</span> operatorParallelism = <span class="number">1</span></span><br><span class="line">    <span class="comment">//最小值：2的7次方=128</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">DEFAULT_LOWER_BOUND_MAX_PARALLELISM</span>:<span class="type">Int</span> = <span class="number">1</span> &lt;&lt; <span class="number">7</span></span><br><span class="line">    <span class="comment">//最大值：2的15次方=32768</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">UPPER_BOUND_MAX_PARALLELISM</span>:<span class="type">Int</span> = <span class="number">1</span> &lt;&lt; <span class="number">15</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//计算算子最大并行度</span></span><br><span class="line">    <span class="keyword">val</span> res = <span class="type">Math</span>.min(</span><br><span class="line">      <span class="type">Math</span>.max(</span><br><span class="line">        <span class="type">MathUtils</span>.roundUpToPowerOfTwo(</span><br><span class="line">          operatorParallelism + (operatorParallelism / <span class="number">2</span>)),</span><br><span class="line">        <span class="type">DEFAULT_LOWER_BOUND_MAX_PARALLELISM</span>),</span><br><span class="line">      <span class="type">UPPER_BOUND_MAX_PARALLELISM</span>)</span><br><span class="line">    println(res)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">运行代码可以发现，如果Flink任务中算子的并行度比较小的时候，则算子最大并行度默认就是128。</span><br><span class="line"></span><br><span class="line">如果Flink任务中算子的并行度比较大，则会按照这个公式生成对应的值，这个值不是固定的。</span><br><span class="line"></span><br><span class="line">所以说如果我们想要保证Savepoint的可移植性，那么最好是手工设置一个固定的算子最大并行度。</span><br></pre></td></tr></table></figure>

<h6 id="算子最大并行度-注意事项"><a href="#算子最大并行度-注意事项" class="headerlink" title="算子最大并行度-注意事项"></a>算子最大并行度-注意事项</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">在工作中还遇到过一个关于算子最大并行度的问题，当时我们在计算某一个业务指标的时候，这个任务中也用到了基于keyed state类型的状态，这个业务指标前期数据量比较小，所以最开始给这个任务设置的全局并行度为20，也没有单独给这个任务中的算子设置最大并行度，根据前面的分析，此时默认生成的算子最大并行度就是128了。</span><br><span class="line">后期随着平台用户规模的增长，这个业务的数据量呈指数级增长，使用之前的并行度来处理增长后的数据规模就有点力不从心了，导致数据出现了积压，我们尝试调整任务的并行度，当把任务的并行度调整为128以上时，发现任务无法从checkpoint和savepoint进行恢复。</span><br><span class="line">这就是所谓的任务并行度调不上去了。</span><br><span class="line">这是因为我们前面分析的，当算子并行度大于算子最大并行度时，任务在重启的时候有些并行度就分配不到KeyGroup了，这样会导致Flink任务无法从Checkpoint或者Savepoint恢复数据。</span><br><span class="line">此时想要提高计算能力，只有一种方法，那就是放弃状态中保存的数据，不从状态中恢复，直接单独启动这个任务，这样是可以提高并行度的。</span><br><span class="line"></span><br><span class="line">其实合理的一点方案是这样的：我们在开发基于keyed state类型的有状态的任务的时候，需要提前预估一下这个任务后期可能处理的数据规模会达到哪种级别，提前设置一个合适的算子最大并行度。这样在前期数据量小的时候，我们可以给任务设置一个比较小的并行度，也不浪费资源，后期数据量上来之后，再调整为一个比较大的并行度。</span><br><span class="line"></span><br><span class="line">所以针对一个keyed state类型的有状态的Flink任务，它未来能扩展到的最大并行度其实取决于这个任务第一次启动时设置的算子最大并行度。</span><br></pre></td></tr></table></figure>

<h4 id="手工触发Savepoint"><a href="#手工触发Savepoint" class="headerlink" title="手工触发Savepoint"></a>手工触发Savepoint</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面对Savepoint的原理有了一定的理解，下面我们来看一下Savepoint的使用</span><br><span class="line">Savepoint需要手工触发，需要使用这种形式的命令：</span><br><span class="line">bin&#x2F;flink savepoint jobId hdfs:&#x2F;&#x2F;IP:9000&#x2F;flink&#x2F;sap -yid yarnAppId</span><br><span class="line"></span><br><span class="line">注意：针对flink on yarn模式一定要指定-yid参数。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面我们分别来演示一下：</span><br><span class="line">开启一个新的socket</span><br><span class="line">nc -l 9001</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">手工触发savepoint。</span><br><span class="line"></span><br><span class="line">注意：需要获取flink任务id和对应的yarn applicationid</span><br><span class="line"></span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint 09ded8897f6a33eda7bdbb32e42046 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0014</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时到任务界面查看，可以看到这里显示的savepoint信息</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021555606.png" alt="image-20230602155500749"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">手工停止这个任务。</span><br></pre></td></tr></table></figure>

<h3 id="从Savepoint进行恢复"><a href="#从Savepoint进行恢复" class="headerlink" title="从Savepoint进行恢复"></a>从Savepoint进行恢复</h3><h4 id="正常恢复"><a href="#正常恢复" class="headerlink" title="正常恢复"></a>正常恢复</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">从Savepoint进行恢复时使用的命令和前面我们讲的手动从checkpoint恢复的命令是一样的。</span><br><span class="line">其实这个命令本来就是Savepoint提供的，只不过也是支持基于checkpoint的数据进行恢复。</span><br><span class="line"></span><br><span class="line">接下来我们尝试使用之前生成的savepoint数据来重启恢复任务。</span><br><span class="line">开启一个新的socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line"></span><br><span class="line">向集群中提交任务，需要通过-s参数指定savepoint的数据目录。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-09ded8-e14f515ce7af&#x2F;_metadata  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,3)</span><br><span class="line"></span><br><span class="line">这样就说明任务正常基于savepoint的数据恢复到了之前的状态。</span><br></pre></td></tr></table></figure>

<h4 id="异常恢复"><a href="#异常恢复" class="headerlink" title="异常恢复"></a>异常恢复</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">咱们前面讲到了在某些特殊情况下会导致任务无法从Savepoint中恢复。</span><br><span class="line">下面来针对两个比较常见的故障场景进行分析：</span><br><span class="line">故障情况1：未手工设置uid，重启时任务中增加了新的算子</span><br><span class="line">故障情况2：未手工设置uid，重启时算子并行度发生了变化</span><br></pre></td></tr></table></figure>

<h5 id="故障情况1"><a href="#故障情况1" class="headerlink" title="故障情况1"></a>故障情况1</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">首先演示一下第一种故障情况：</span><br><span class="line">创建package：com.imooc.scala.savepoint</span><br><span class="line"></span><br><span class="line">原始代码如下：</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.savepoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 故障情况1：未手工设置uid，重启时任务中增加了新的算子</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStateForSavepoint1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="string">"hdfs://bigdata01:9000/flink-chk/wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WordCountStateForSavepoint1"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">我们首先基于这份代码生成一份savepoint数据，然后再对这个代码进行修改，增加一个算子，看看还能不能基于之前的数据进行恢复。</span><br><span class="line"></span><br><span class="line">编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -c com.imooc.scala.savepoint.WordCountStateForSavepoint1 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br><span class="line"></span><br><span class="line">手工触发savepoint。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint d552f822307d4b1ce2e991f4cbb0d065 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0016</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">然后停止任务。</span><br><span class="line"></span><br><span class="line">修改代码，在任务中增加一个算子，其他代码不变。</span><br><span class="line">其实增加的这个算子对结果数据没有任何影响。</span><br><span class="line"></span><br><span class="line">val keyedStream &#x3D; text.flatMap(_.split(&quot; &quot;))</span><br><span class="line">  .map((_, 1))</span><br><span class="line">  .map(tup&#x3D;&gt;(tup._1,tup._2))</span><br><span class="line">  .keyBy(_._1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">重新编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">基于之前生成的savepoint数据进行恢复。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-d552f8-ec06489b0100&#x2F;_metadata  -c com.imooc.scala.savepoint.WordCountStateForSavepoint1 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时发现任务提交上去之后会自动失败。</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021603217.png" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">查看日志中的错误信息。</span><br><span class="line"></span><br><span class="line">注意：此时至少需要开启Hadoop的historyserver服务。</span><br><span class="line"></span><br><span class="line">核心错误日志是这一行：</span><br><span class="line">Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Failed to rollback to checkpoint&#x2F;savepoint hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-d552f8-ec06489b0100. Cannot map checkpoint&#x2F;savepoint state for operator c27dcf7b54ef6bfd6cff02ca8870b681 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">日志中的意思是说不能把savepoint中的状态数据映射到uid发送了变化的算子中。</span><br><span class="line">如果你想忽略这个问题，可以指定--allowNonRestoredState，这样会忽略掉无法映射的状态数据，强制启动。</span><br><span class="line"></span><br><span class="line">想要解决这个问题，咱们之前分析了，需要手工设置算子的uid，至少是要指定有状态的算子的uid。</span><br><span class="line">修改之前的代码，在有状态的map算子后面设置uid，并且删掉之前增加的map算子。</span><br><span class="line">完整代码如下：</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.savepoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 故障情况1：未手工设置uid，并且任务中增加了新的算子</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStateForSavepoint1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="string">"hdfs://bigdata01:9000/flink-chk/wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).uid(<span class="string">"vs_map001"</span>).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WordCountStateForSavepoint1"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -c com.imooc.scala.savepoint.WordCountStateForSavepoint1 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">手工触发savepoint。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint ebb9f0efa183e6b3d57f497a4da86ec2 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0020</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">然后停止任务。</span><br><span class="line"></span><br><span class="line">再修改代码，增加一个map算子</span><br><span class="line">val keyedStream &#x3D; text.flatMap(_.split(&quot; &quot;))</span><br><span class="line">  .map((_, 1))</span><br><span class="line">  .map(tup&#x3D;&gt;(tup._1,tup._2))</span><br><span class="line">  .keyBy(_._1)</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">重新编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">基于之前生成的savepoint数据进行恢复。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-ebb9f0-ae46290b2f2c&#x2F;_metadata  -c com.imooc.scala.savepoint.WordCountStateForSavepoint1 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时发现任务可以正常启动。</span><br><span class="line">查看任务界面中的信息，可以看到任务是基于之前的savepoint数据进行恢复的。</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021611292.png" alt="image-20230602161106105"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在socket中模拟产生数据：</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line"></span><br><span class="line">看到这个结果就说明数据是正常恢复了。</span><br></pre></td></tr></table></figure>

<h5 id="故障情况2"><a href="#故障情况2" class="headerlink" title="故障情况2"></a>故障情况2</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来演示一下第二种故障情况：</span><br><span class="line"></span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.savepoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 未手工设置uid，重启时算子并行度发生了变化</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStateForSavepoint2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="string">"hdfs://bigdata01:9000/flink-chk/wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WordCountStateForSavepoint2"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">这个代码就是一个有状态的单词计数，没有给map这个有状态的算子设置uid。</span><br><span class="line">我一会先基于默认并行度1启动任务，并且生成savepoint，然后在基于savepoint恢复的时候调整并行度，看看会出现什么情况</span><br><span class="line"></span><br><span class="line">编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -c com.imooc.scala.savepoint.WordCountStateForSavepoint2 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">手工触发savepoint。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint 1a871d083497ae7ab775c168a5f75f6d hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0023</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">然后停止任务。</span><br><span class="line"></span><br><span class="line">再开启一个新的socket</span><br><span class="line"></span><br><span class="line">基于之前生成的savepoint数据进行恢复。</span><br><span class="line">通过-p 指定全局并行度为2,</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -p 2 -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-1a871d-b2fce8500a75&#x2F;_metadata  -c com.imooc.scala.savepoint.WordCountStateForSavepoint2 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">结果发现任务执行失败</span><br><span class="line">查看任务报错日志，发现和故障情况1里面的错误信息一样</span><br><span class="line">Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: </span><br><span class="line">Failed to rollback to checkpoint&#x2F;savepoint hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-1a871d-b2fce8500a75. Cannot map checkpoint&#x2F;savepoint state for operator c27dcf7b54ef6bfd6cff02ca8870b681 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">那也就意味着，改变并行度时，也会影响算子默认生成的uid。</span><br><span class="line">所以想要支持在恢复状态的时候修改并行度，需要给有状态的算子手工设置uid。</span><br><span class="line"></span><br><span class="line">修改代码，给这个有状态的map算子手工设置uid。</span><br><span class="line">keyedStream.map(new RichMapFunction[(String,Int),(String,Int)] &#123;</span><br><span class="line">  ......</span><br><span class="line">    &#125;).uid(&quot;vs_map001&quot;).print()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">重新编译打包</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -c com.imooc.scala.savepoint.WordCountStateForSavepoint2 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line"></span><br><span class="line">手工触发savepoint。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint 5e63967dbad8dbc32be5dcd77edcae88 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0027</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">然后停止任务。</span><br><span class="line"></span><br><span class="line">再开启一个新的socket</span><br><span class="line"></span><br><span class="line">基于之前生成的savepoint数据进行恢复。</span><br><span class="line">通过-p指定全局并行度为2,</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -p 2 -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-5e6396-b23c7c08f013&#x2F;_metadata  -c com.imooc.scala.savepoint.WordCountStateForSavepoint2 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">此时任务可以正常启动并恢复数据。</span><br><span class="line">查看任务界面，可以看到并行度变成2了。</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021617129.png" alt="image-20230602161722870"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在socket中模拟产生数据</span><br><span class="line">a</span><br><span class="line">查看输出结果</span><br><span class="line">2&gt; (a,3)</span><br><span class="line">能看到这个输出，说明任务成功恢复执行了。</span><br></pre></td></tr></table></figure>

<h2 id="State-Backend-状态的存储方式"><a href="#State-Backend-状态的存储方式" class="headerlink" title="State Backend(状态的存储方式)"></a>State Backend(状态的存储方式)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">咱们前面分析过，状态数据默认是存储在taskmanager节点的jvm堆内存中。</span><br><span class="line">当然还有一种选择是存储在本地的rocksdb数据库中。</span><br><span class="line"></span><br><span class="line">具体状态的存储位置，是由state backend来控制的：</span><br><span class="line"></span><br><span class="line">目前Flink提供了两种State Backend：</span><br><span class="line"></span><br><span class="line">看下面这个图：</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021625881.png" alt="image-20230602162512730"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一种是HashMapStateBackend，这是默认的State Backend，这种存储方式会将状态数据存储在TaskManager节点的JVM堆内存中。</span><br><span class="line"></span><br><span class="line">第二种是EmbeddedRocksDBStateBackend，这种存储方式是借助于内嵌的RocksDB数据库。</span><br><span class="line">Rocksdb这个数据库中的数据会存储在对应TaskManager节点的本地磁盘文件中，此时状态数据会作为本地磁盘上的序列化字节存在。</span><br></pre></td></tr></table></figure>

<h3 id="HashMapStateBackend"><a href="#HashMapStateBackend" class="headerlink" title="HashMapStateBackend"></a>HashMapStateBackend</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HashMapStateBackend这种存储方式，他的优点是，基于内存读写数据，效率高，不需要涉及序列化和反序列化。</span><br><span class="line"></span><br><span class="line">但是它也有缺点，基于内存操作的话，内存大小是有限的，所以不适合存储超大状态的数据，适合用于常规任务，就是状态数据量中等的任务，例如：分钟级别的窗口聚合操作。</span><br><span class="line"></span><br><span class="line">Flink在执行checkpoint的时候，每次都会从HashMapStateBackend中获取全量数据进行持久化。</span><br></pre></td></tr></table></figure>

<h3 id="EmbeddedRocksDBStateBackend"><a href="#EmbeddedRocksDBStateBackend" class="headerlink" title="EmbeddedRocksDBStateBackend"></a>EmbeddedRocksDBStateBackend</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">EmbeddedRocksDBStateBackend这种存储方式，他的优点是解决了内存受限的问题，所以可以适用于超大状态的任务，例如：天级别的窗口聚合操作。</span><br><span class="line"></span><br><span class="line">但是它也有缺点，它在存储状态数据的时候使用的是内嵌的Rocksdb数据库，Rocksdb数据库的数据是存储在磁盘文件中的，所以需要操作磁盘，那么它的效率就不如内存高了，并且基于磁盘的读写操作还需要涉及到数据的序列化和反序列，也会影响性能，所以这种方式适合用于对状态读写性能要求不是特别高的任务。</span><br><span class="line"></span><br><span class="line">针对EmbeddedRocksDBStateBackend这种存储方式，它可以支持增量checkpoint，这样可以进一步提高Checkpoint性能。当然了，他也可以支持全量checkpoint。</span><br><span class="line"></span><br><span class="line">所以在工作中，针对常规的任务还是建议使用HashMapStateBackend，如果某个任务需要在状态里面维护大量的数据，可以考虑使用EmbeddedRocksDBStateBackend。</span><br></pre></td></tr></table></figure>

<h3 id="State-Backend的配置"><a href="#State-Backend的配置" class="headerlink" title="State Backend的配置"></a>State Backend的配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">在flink的执行环境中通过setStateBackend方法进行设置。</span><br><span class="line">可以给这个方法传递new HashMapStateBackend()或者new EmbeddedRocksDBStateBackend(true)。</span><br><span class="line"></span><br><span class="line">注意：new EmbeddedRocksDBStateBackend(true)中的true表示开启增量checkpoint。如果不传参数，则是全量checkpoint。</span><br><span class="line"></span><br><span class="line">在代码中的具体使用是这样的：</span><br><span class="line">创建package：com.imooc.scala.statebackend</span><br><span class="line"></span><br><span class="line">想要使用rockdb数据库，需要引入对应的依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-statebackend-rocksdb&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.15.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;!--&lt;scope&gt;provided&lt;&#x2F;scope&gt;--&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于WordCountStateWithCheckpointDemo代码复制一份进行修改</span><br><span class="line"></span><br><span class="line">Object：WordCountStateWithStateBackendDemo</span><br><span class="line"></span><br><span class="line">核心代码是这些：</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;设置StateBackend</span><br><span class="line">&#x2F;&#x2F;默认使用HashMapStateBackend</span><br><span class="line">&#x2F;&#x2F;此时State数据保存在TaskManager节点的JVM堆内存中</span><br><span class="line">&#x2F;&#x2F;env.setStateBackend(new HashMapStateBackend())</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;此时State数据保存在TaskManager节点内置的RocksDB数据库中(TaskManager节点的本地磁盘文件中)</span><br><span class="line">&#x2F;&#x2F;在EmbeddedRocksDBStateBackend的构造函数中指定参数true会开启增量Checkpoint【建议设置为true】</span><br><span class="line">env.setStateBackend(new EmbeddedRocksDBStateBackend(true))</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;注意：具体Checkpoint后的状态数据存储在哪里是由setCheckpointStorage控制的</span><br><span class="line">env.getCheckpointConfig.setCheckpointStorage(new FileSystemCheckpointStorage(&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink-chk001&quot;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用的时候和之前使用默认的HashMapStateBackend没什么区别。</span><br></pre></td></tr></table></figure>

<h2 id="State的生存时间-TTL"><a href="#State的生存时间-TTL" class="headerlink" title="State的生存时间(TTL)"></a>State的生存时间(TTL)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">默认情况下State数据会一直存在，如果存储了过多状态数据，可能会导致内存溢出(针对HashMapStateBackend)。</span><br><span class="line"></span><br><span class="line">因此从Flink 1.6版本开始引入了State TTL特性。类似于Redis 中的TTL机制，超时自动删除。</span><br><span class="line"></span><br><span class="line">TTL特性可以支持对KeyedState中过期状态数据的自动清理，当状态中的某条数据到了过期时间，这条数据会被Flink自动删除，这样就有效解决了状态数据在无干预情况下无限增长导致内存溢出的问题。</span><br><span class="line"></span><br><span class="line">例如：我们在实时统计一段时间内的数据指标的时候，需要在状态中做去重，但是过了这段时间之后，之前的状态数据就没有用了，这样就可以用状态的ttl机制实现自动清理，当然我们也可以通过代码逻辑清空状态中的历史数据。</span><br><span class="line"></span><br><span class="line">针对OperatorState类型而言，基本上不需要自动清理，以我们之前开发的OperatorState_MyBufferSinkDemo代码为例。</span><br><span class="line">在MyBufferSink中，平时处理的数据是存储在任务内部的本地缓存中。</span><br><span class="line">只有在触发checkpoint的时候，才会把本地缓存中的数据写入到状态中，当下次触发checkpoint的时候我们会在代码中将之前的状态数据清空。所以状态中存储的数据不会一直增长，这样就没必要设置了状态的TTL了。</span><br><span class="line"></span><br><span class="line">数据过期判断依据：上次修改的时间戳 + 我们设置的状态TTL &gt; 当前时间戳</span><br><span class="line">如果满足这个条件，那么这条状态数据就过期了。</span><br><span class="line"></span><br><span class="line">本质上来讲，状态的TTL功能其实就是给每个Keyed State增加了一个“时间戳”，而 Flink 在状态创建、写入或读取的时候可以更新这个时间戳，并且判断状态是否过期。如果状态过期，还会根据可见性参数，来决定是否返回已过期但还未清理的状态。</span><br><span class="line">状态的清理并不是即时的，而是使用了一种 Lazy 的算法来实现，从而减少状态清理对性能的影响。</span><br><span class="line"></span><br><span class="line">举个例子，我要将一个 String 类型的数据存储到ValueState类型的状态中：</span><br><span class="line">如果没有设置状态的TTL ，则直接将 String 类型的数据存储到ValueState中。</span><br><span class="line">如果设置了状态的TTL，则Flink会将&lt;String, Long&gt;这种结构的数据存储到ValueState中，其中Long为时间戳，用于判断状态是否过期。</span><br><span class="line"></span><br><span class="line">在代码层面进行分析的话，是这样的。</span><br><span class="line">如果没有设置状态的TTL，我们存储 String 类型的数据使用的是ValueState。</span><br><span class="line">当我们设置了状态的TTL，那么就需要使用ValueState对应的TtlValueState。</span><br><span class="line"></span><br><span class="line">来看一下TtlValueState的源码：</span><br><span class="line">他里面有一个update方法：</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void update(T value) throws IOException &#123;</span><br><span class="line">    accessCallback.run();</span><br><span class="line">    original.update(wrapWithTs(value));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">update方法中会调用wrapWithTs(value)，看一下wrapWithTs这个方法的实现：</span><br><span class="line">其实在这里可以发现，TtlUtils.wrapWithTs方法接收了两个参数，一个是value，另外一个是当前时间戳。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;V&gt; TtlValue&lt;V&gt; wrapWithTs(V value) &#123;</span><br><span class="line">    return TtlUtils.wrapWithTs(value, timeProvider.currentTimestamp());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">继续往下面查看TtlUtils.wrapWithTs方法</span><br><span class="line">static &lt;V&gt; TtlValue&lt;V&gt; wrapWithTs(V value, long ts) &#123;</span><br><span class="line">    return new TtlValue&lt;&gt;(value, ts);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">最终发现它返回了一个TtlValue数据类型，来看一下这个类的实现：</span><br><span class="line">public class TtlValue&lt;T&gt; implements Serializable &#123;</span><br><span class="line">    private static final long serialVersionUID &#x3D; 5221129704201125020L;</span><br><span class="line"></span><br><span class="line">    @Nullable private final T userValue;</span><br><span class="line">    private final long lastAccessTimestamp;</span><br><span class="line"></span><br><span class="line">    public TtlValue(@Nullable T userValue, long lastAccessTimestamp) &#123;</span><br><span class="line">        checkArgument(!(userValue instanceof TtlValue));</span><br><span class="line">        this.userValue &#x3D; userValue;</span><br><span class="line">        this.lastAccessTimestamp &#x3D; lastAccessTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Nullable</span><br><span class="line">    public T getUserValue() &#123;</span><br><span class="line">        return userValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public long getLastAccessTimestamp() &#123;</span><br><span class="line">        return lastAccessTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">这个类里面封装了具体的数据和对应的时间戳。</span><br><span class="line">这个类里面的lastAccessTimestamp就表示上次访问的时间戳，这样就可以基于这个时间戳判断这个数据是否过期了。</span><br><span class="line"></span><br><span class="line">这个是TtlValueState的实现，对应的还有TtlMapState、TtlListState等。</span><br><span class="line"></span><br><span class="line">下面我们来具体演示一下TTL的使用：</span><br><span class="line"></span><br><span class="line">创建package：com.imooc.scala.ttl</span><br><span class="line"></span><br><span class="line">完整代码如下：</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.ttl</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.<span class="type">StateTtlConfig</span>.<span class="type">TtlTimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">StateTtlConfig</span>, <span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * State TTL的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StateTTLDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//设置TTL机制相关配置</span></span><br><span class="line">        <span class="keyword">val</span> ttlConfig = <span class="type">StateTtlConfig</span></span><br><span class="line">          <span class="comment">//1：指定状态的生存时间</span></span><br><span class="line">          .newBuilder(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br><span class="line">          <span class="comment">//2：指定什么时候触发更新延长状态的TTL时间</span></span><br><span class="line">          <span class="comment">//OnCreateAndWrite：仅在创建和写入时触发TTL时间更新延长。</span></span><br><span class="line">          <span class="comment">//OnReadAndWrite：表示在读取的时候也会触发(包括创建和写入)</span></span><br><span class="line">          .setUpdateType(<span class="type">StateTtlConfig</span>.<span class="type">UpdateType</span>.<span class="type">OnCreateAndWrite</span>)</span><br><span class="line">          <span class="comment">//3：过期数据是否可访问</span></span><br><span class="line">          <span class="comment">//NeverReturnExpired：表示永远不会返回过期数据(可能会存在数据已过期但是还没有被清理)</span></span><br><span class="line">          <span class="comment">//ReturnExpiredIfNotCleanedUp：表示数据只要没有被删除，就算过期了也可以被访问</span></span><br><span class="line">          .setStateVisibility(<span class="type">StateTtlConfig</span>.<span class="type">StateVisibility</span>.<span class="type">NeverReturnExpired</span>)</span><br><span class="line">          <span class="comment">//4：TTL的时间语义</span></span><br><span class="line">          <span class="comment">//判断数据是否过期时使用的时间语义，默认使用处理时间，目前只支持这一种</span></span><br><span class="line">          .setTtlTimeCharacteristic(<span class="type">TtlTimeCharacteristic</span>.<span class="type">ProcessingTime</span>)</span><br><span class="line">          <span class="comment">//5：过期数据删除策略</span></span><br><span class="line">          <span class="comment">//cleanupFullSnapshot：全量删除</span></span><br><span class="line">          <span class="comment">//此时只有当任务从checkpoint或者savepoint恢复时才会删除所有过期数据</span></span><br><span class="line">          <span class="comment">//这种方式其实并不能真正解决使用HashMapStateBackend时的内存压力问题，只有定时重启恢复才可以解决</span></span><br><span class="line">          <span class="comment">//注意：这种方式不适合Rocksdb中的增量Checkpoint方式</span></span><br><span class="line">          <span class="comment">//.cleanupFullSnapshot()</span></span><br><span class="line">          <span class="comment">//cleanupIncrementally：针对内存的增量删除方式</span></span><br><span class="line">          <span class="comment">//增量删除策略只支持基于内存的HashMapStateBackend，不支持EmbeddedRocksDBStateBackend</span></span><br><span class="line">          <span class="comment">//它的实现思路是在所有数据上维护一个全局迭代器。当遇到某些事件（例如状态访问）时会触发增量删除</span></span><br><span class="line">          <span class="comment">//cleanupSize=100 和 runCleanupForEveryRecord=true 表示每访问一个状态数据就会向前迭代遍历100条数据并删除其中过期的数据</span></span><br><span class="line">          .cleanupIncrementally(<span class="number">100</span>,<span class="literal">true</span>)</span><br><span class="line">          <span class="comment">//针对Rocksdb的增量删除方式</span></span><br><span class="line">          <span class="comment">//当Rocksdb在做Compact(合并)的时候删除过期数据</span></span><br><span class="line">          <span class="comment">//每Compact(合并)1000个Entry之后，会从Flink中查询当前时间戳，用于判断这些数据是否过期</span></span><br><span class="line">          <span class="comment">//.cleanupInRocksdbCompactFilter(1000)</span></span><br><span class="line">          .build</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment">//开启TTL机制</span></span><br><span class="line">        <span class="comment">//注意：开启TTL机制会增加状态的存储空间，因为在存储状态的时候还需要将状态的上次修改时间一起存储</span></span><br><span class="line">        valueStateDesc.enableTimeToLive(ttlConfig)</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StateTTLDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">这个可以可以直接在idea中运行。</span><br><span class="line">先开启socket：</span><br><span class="line"></span><br><span class="line">启动代码。</span><br><span class="line"></span><br><span class="line">在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">可以看到任务输出结果：</span><br><span class="line">2&gt; (b,1)</span><br><span class="line">6&gt; (a,1)</span><br><span class="line">由于我们设置了状态的TTL时间是10秒，所以10秒之后，再模拟产生一条数据 a</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a</span><br><span class="line">看一下输出的数据效果，发现此时a的次数还是1，说明之前在状态中存储的那个a已经过期了。</span><br><span class="line">6&gt; (a,1)</span><br><span class="line">立刻再模拟产生一条数据 a</span><br><span class="line">a</span><br><span class="line">此时发现输出的a的次数就是2了，在数据过期前是可以使用的，并且使用之后，数据的过期时间会对应的往后顺延，因为我们重新向状态中写入了a的数据，那么a对应的时间戳就会被修改为当前最新时间。</span><br><span class="line">6&gt; (a,2)</span><br><span class="line">这就是状态的TTL。</span><br><span class="line"></span><br><span class="line">其实在Flink 的DataStream API 中，状态的TTL功能的应用场景还是比较少的。状态的TTL功能在Flink SQL中是被大规模应用的，除了窗口类操作和ETL之类的任务之外，其余的Flink SQL任务基本都需要用到状态的TTL机制。</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Flink任务中设置了TTL 和不设置TTL 的状态是不兼容的，大家在使用的时候时一定要注意。避免出现任务从 Checkpoint &#x2F;Savepoint无法恢复的情况。但是我们是可以去修改TTL的时间的，因为修改时长并不会改变状态的存储结构。</span><br><span class="line">如果试图使用设置了TTL的状态恢复先前没有设置TTL时生成的状态，将导致任务恢复失败。</span><br></pre></td></tr></table></figure>



<h2 id="Window中的数据存在哪里？"><a href="#Window中的数据存在哪里？" class="headerlink" title="Window中的数据存在哪里？"></a>Window中的数据存在哪里？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">简单来说是内存中，严格来说其实是存储在状态中</span><br><span class="line"></span><br><span class="line">核心内容是在WindowOperator这个类里面。</span><br><span class="line"></span><br><span class="line">下面我们来从源码层面来具体分析一下。</span><br><span class="line">随便找一个类，在keyBy后面调用window(..)方法。</span><br><span class="line">@PublicEvolving</span><br><span class="line">def window[W &lt;: Window](assigner: WindowAssigner[_ &gt;: T, W]): WindowedStream[T, K, W] &#x3D; &#123;</span><br><span class="line">  new WindowedStream(new WindowedJavaStream[T, K, W](javaStream, assigner))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">进入new WindowedJavaStream()</span><br><span class="line">这里面会创建WindowOperatorBuilder。</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public <span class="type">WindowedStream</span>(<span class="type">KeyedStream</span>&lt;<span class="type">T</span>, <span class="type">K</span>&gt; input, <span class="type">WindowAssigner</span>&lt;? <span class="keyword">super</span> <span class="type">T</span>, <span class="type">W</span>&gt; windowAssigner) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.input = input;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.builder =</span><br><span class="line">            <span class="keyword">new</span> <span class="type">WindowOperatorBuilder</span>&lt;&gt;(</span><br><span class="line">                    windowAssigner,</span><br><span class="line">                    windowAssigner.getDefaultTrigger(input.getExecutionEnvironment()),</span><br><span class="line">                    input.getExecutionConfig(),</span><br><span class="line">                    input.getType(),</span><br><span class="line">                    input.getKeySelector(),</span><br><span class="line">                    input.getKeyType());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">进入WindowOperatorBuilder这个类：</span><br><span class="line">在这个类里面有一个buildWindowOperator方法，他会创建WindowOperator，</span><br><span class="line">private &lt;ACC, R&gt; WindowOperator&lt;K, T, ACC, R, W&gt; buildWindowOperator(</span><br><span class="line">        StateDescriptor&lt;? extends AppendingState&lt;T, ACC&gt;, ?&gt; stateDesc,</span><br><span class="line">        InternalWindowFunction&lt;ACC, R, K, W&gt; function) &#123;</span><br><span class="line"></span><br><span class="line">    return new WindowOperator&lt;&gt;(</span><br><span class="line">            windowAssigner,</span><br><span class="line">            windowAssigner.getWindowSerializer(config),</span><br><span class="line">            keySelector,</span><br><span class="line">            keyType.createSerializer(config),</span><br><span class="line">            stateDesc,</span><br><span class="line">            function,</span><br><span class="line">            trigger,</span><br><span class="line">            allowedLateness,</span><br><span class="line">            lateDataOutputTag);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">进入WindowOperator</span><br><span class="line">这个类中有一个核心方法是processElement，处理窗口中的数据</span><br><span class="line">在这个方法中可以看到里面用到了windowState</span><br><span class="line">windowState.setCurrentNamespace(stateWindow);</span><br><span class="line">windowState.add(element.getValue());</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">查看windowState的介绍：</span><br><span class="line">这是一个聚合类型的状态。</span><br><span class="line">从注释上可以发现，windowState负责存储窗口内的状态，每个窗口一个命名空间。</span><br><span class="line">&#x2F;** The state in which the window contents is stored. Each window is a namespace *&#x2F;</span><br><span class="line">private transient InternalAppendingState&lt;K, W, IN, ACC, ACC&gt; windowState;</span><br><span class="line">从这里可以看出来，窗口中的数据其实是在状态中维护的。</span><br><span class="line"></span><br><span class="line">当窗口触发执行完成之后，窗口中维护的状态数据是会被删除的。</span><br><span class="line">他里面有一个clearAllState方法，当窗口执行结束后这个方法会被触发</span><br><span class="line">他里面会调用windowState.clear(); 清空状态中的数据。</span><br><span class="line">private void clearAllState(</span><br><span class="line">        W window, AppendingState&lt;IN, ACC&gt; windowState, MergingWindowSet&lt;W&gt; mergingWindows)</span><br><span class="line">        throws Exception &#123;</span><br><span class="line">    windowState.clear();</span><br><span class="line">    triggerContext.clear();</span><br><span class="line">    processContext.window &#x3D; window;</span><br><span class="line">    processContext.clear();</span><br><span class="line">    if (mergingWindows !&#x3D; null) &#123;</span><br><span class="line">        mergingWindows.retireWindow(window);</span><br><span class="line">        mergingWindows.persist();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">所以窗口中的状态不会一直增加，除非你的窗口非常大，例如：天级别的窗口，这种情况的话最好是使用rocksdb存储状态，避免内存溢出。</span><br></pre></td></tr></table></figure>









<hr>
<blockquote>
</blockquote>

      
    </div>
    
    
    
	
	<div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-2.html">大数据开发工程师-第十七周 Flink新版本1.12以上-2</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 TTYONG 的个人博客">TTYONG</a></p>
  <p><span>发布时间:</span>2023年04月20日 - 16:04</p>
  <p><span>最后更新:</span>2023年06月02日 - 16:06</p>
  <p><span>原始链接:</span><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-2.html" title="大数据开发工程师-第十七周 Flink新版本1.12以上-2">http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-2.html</a>
    <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-2.html" aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
    });
    });  
</script>



      
	</div>



    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>多少都是爱</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat_reward.jpg" alt="TTYONG 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay_reward.jpg" alt="TTYONG 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-1.html" rel="next" title="大数据开发工程师-Flink新版本1.12以上-1">
                <i class="fa fa-chevron-left"></i> 大数据开发工程师-Flink新版本1.12以上-1
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-6.html" rel="prev" title="大数据开发工程师-第十一周 Spark性能优化的道与术-Spark Streaming-6">
                大数据开发工程师-第十一周 Spark性能优化的道与术-Spark Streaming-6 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
      
        
          <ul class="sidebar-nav motion-element">
            <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
              文章目录
            </li>
            <li class="sidebar-nav-overview" data-target="site-overview">
              站点概览
            </li>
          </ul>
        
      

	<section class="site-overview sidebar-panel">
	  <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
		<img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="TTYONG">
		<p class="site-author-name" itemprop="name">TTYONG</p>
		 
			<p class="site-description motion-element" itemprop="description"></p>
		 
	  </div>
	  <nav class="site-state motion-element">

		
		  <div class="site-state-item site-state-posts">
			<a href="/archives/%7C%7C%20archive">
			  <span class="site-state-item-count">372</span>
			  <span class="site-state-item-name">日志</span>
			</a>
		  </div>
		

		
		  
		  
		  <div class="site-state-item site-state-categories">
			<a href="/categories/index.html">
			  <span class="site-state-item-count">51</span>
			  <span class="site-state-item-name">分类</span>
			</a>
		  </div>
		

		
		  
		  
		  <div class="site-state-item site-state-tags">
			<a href="/tags/index.html">
			  <span class="site-state-item-count">115</span>
			  <span class="site-state-item-name">标签</span>
			</a>
		  </div>
		

	  </nav>

	  
		<div class="feed-link motion-element">
		  <a href="/atom.xml" rel="alternate">
			<i class="fa fa-rss"></i>
			RSS
		  </a>
		</div>
	  

	  <div class="links-of-author motion-element">
		
		  
			<span class="links-of-author-item">
			  <a href="2364076207@qq.com || envelope" target="_blank" title="E-Mail">
				
				  <i class="fa fa-fw fa-globe"></i>
				
				E-Mail
			  </a>
			</span>
		  
			<span class="links-of-author-item">
			  <a href="https://wpa.qq.com/msgrd?v=3&uin=2364076207&site=qq&menu=yes || qq" target="_blank" title="QQ" rel="external nofollow noopener noreferrer">
				
				  <i class="fa fa-fw fa-globe"></i>
				
				QQ
			  </a>
			</span>
		  
			<span class="links-of-author-item">
			  <a href="https://weibo.com/p/1005051833844383/home || weixin" target="_blank" title="WeiXin" rel="external nofollow noopener noreferrer">
				
				  <i class="fa fa-fw fa-globe"></i>
				
				WeiXin
			  </a>
			</span>
		  
			<span class="links-of-author-item">
			  <a href="https://www.zhihu.com/people/he-he-ty || zhihu" target="_blank" title="ZhiHu" rel="external nofollow noopener noreferrer">
				
				  <i class="fa fa-fw fa-globe"></i>
				
				ZhiHu
			  </a>
			</span>
		  
		
	  </div>

	  
	  

	  
	  
		<div class="links-of-blogroll motion-element links-of-blogroll-block">
		  <div class="links-of-blogroll-title">
			<i class="fa  fa-fw fa-link"></i>
			友链
		  </div>
		  <ul class="links-of-blogroll-list">
			
			  <li class="links-of-blogroll-item">
				<a href="https://www.baidu.com/" target="_blank" rel="external nofollow noopener noreferrer">百度</a>
			  </li>
			
		  </ul>
		</div>
	  

	  


	</section>
	
	  
	  <!--noindex-->
		<section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
		  <div class="post-toc">

			
			  
			

			
			  <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink新版本1-12以上-2"><span class="nav-number">1.</span> <span class="nav-text">Flink新版本1.12以上-2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#State-状态-的容错与一致性"><span class="nav-number">1.1.</span> <span class="nav-text">State(状态)的容错与一致性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#流计算中Exactly-once语义的多种实现思路"><span class="nav-number">1.1.1.</span> <span class="nav-text">流计算中Exactly-once语义的多种实现思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何实现Flink任务的端到端一致性？"><span class="nav-number">1.1.2.</span> <span class="nav-text">如何实现Flink任务的端到端一致性？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Checkpoint-快照-机制详解"><span class="nav-number">1.2.</span> <span class="nav-text">Checkpoint(快照)机制详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#保存多个Checkpoint"><span class="nav-number">1.2.1.</span> <span class="nav-text">保存多个Checkpoint</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从Checkpoint进行恢复"><span class="nav-number">1.2.2.</span> <span class="nav-text">从Checkpoint进行恢复</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#手工停止的任务-恢复数据"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">手工停止的任务,恢复数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#任务自动重启时checkpoint数据的自动恢复"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">任务自动重启时checkpoint数据的自动恢复</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Savepoint详解"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">Savepoint详解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Checkpoint-VS-Savepoint"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">Checkpoint VS Savepoint</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Savepoint保证程序可移植性的前提条件"><span class="nav-number">1.2.2.5.</span> <span class="nav-text">Savepoint保证程序可移植性的前提条件</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#算子唯一标识"><span class="nav-number">1.2.2.5.1.</span> <span class="nav-text">算子唯一标识</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#算子最大并行度"><span class="nav-number">1.2.2.5.2.</span> <span class="nav-text">算子最大并行度</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#算子最大并行度-注意事项"><span class="nav-number">1.2.2.5.2.1.</span> <span class="nav-text">算子最大并行度-注意事项</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#手工触发Savepoint"><span class="nav-number">1.2.2.6.</span> <span class="nav-text">手工触发Savepoint</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从Savepoint进行恢复"><span class="nav-number">1.2.3.</span> <span class="nav-text">从Savepoint进行恢复</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#正常恢复"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">正常恢复</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#异常恢复"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">异常恢复</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#故障情况1"><span class="nav-number">1.2.3.2.1.</span> <span class="nav-text">故障情况1</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#故障情况2"><span class="nav-number">1.2.3.2.2.</span> <span class="nav-text">故障情况2</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#State-Backend-状态的存储方式"><span class="nav-number">1.3.</span> <span class="nav-text">State Backend(状态的存储方式)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HashMapStateBackend"><span class="nav-number">1.3.1.</span> <span class="nav-text">HashMapStateBackend</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EmbeddedRocksDBStateBackend"><span class="nav-number">1.3.2.</span> <span class="nav-text">EmbeddedRocksDBStateBackend</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#State-Backend的配置"><span class="nav-number">1.3.3.</span> <span class="nav-text">State Backend的配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#State的生存时间-TTL"><span class="nav-number">1.4.</span> <span class="nav-text">State的生存时间(TTL)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Window中的数据存在哪里？"><span class="nav-number">1.5.</span> <span class="nav-text">Window中的数据存在哪里？</span></a></li></ol></li></ol></div>
			

		  </div>
		</section>
	  <!--/noindex-->
	  
	

	

  </div>
</aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2020.3.4 &mdash; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">TTYONG</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">714.5k</span>
  
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io" rel="external nofollow>Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next rel="external nofollow">NexT.Pisces</a> v5.1.4</div>



-->  


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 访问总量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <div id="gitalk-container"></div>
  <script src="/js/src/md5.min.js"></script>
  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'd3f3299eb0cc69c8f171',
      clientSecret: '23bf8796e5dda2daf0a1b12964d89f4cc88f9ddf',
      repo: 'comments_repository',
      owner: 'ttyong',
      admin: ['ttyong'],
      id: md5(location.pathname),
      distractionFreeMode: 'false'
    })
    gitalk.render('gitalk-container')
  </script>

  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  
  

  

  

  

  <link rel="stylesheet" href="/dist/APlayer.min.css">
  <div id="aplayer"></div>
  <script type="text/javascript" src="/dist/APlayer.min.js"></script>
  <script type="text/javascript" src="/dist/music.js"></script>
  
  
  
  

  <canvas id="evanyou"></canvas>
  <style>
    #evanyou {
      position: fixed;
      width: 100%;
      height: 100%;
      top: 0;
      left: 0;
      z-index: -1;
    }
  </style>
  <script src="/js/src/evan-you.js"></script>




  <script src="/js/src/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>




  <script async src="/js/cursor/fireworks.js"></script>




<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
