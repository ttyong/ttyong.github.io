<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2023-04-24T15:41:55.091Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v1.0-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-4.html</id>
    <published>2023-04-24T07:37:38.000Z</published>
    <updated>2023-04-24T15:41:55.091Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v1.0-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-3.html</id>
    <published>2023-04-24T07:37:33.000Z</published>
    <updated>2023-04-24T15:41:58.841Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v1.0-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-2.html</id>
    <published>2023-04-24T07:37:28.000Z</published>
    <updated>2023-04-25T16:35:49.838Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v1-0-2"><a href="#第十八周-直播平台三度关系推荐v1-0-2" class="headerlink" title="第十八周 直播平台三度关系推荐v1.0-2"></a>第十八周 直播平台三度关系推荐v1.0-2</h1><h2 id="数据采集架构详细设计"><a href="#数据采集架构详细设计" class="headerlink" title="数据采集架构详细设计"></a>数据采集架构详细设计</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242348422.png" alt="image-20230424234832289"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们就从我们整体架构里面的第一个模块数据采集模块开始。注意，在实际过程中，数据采集模块不是只针对某一个项目而言的，而是一个公共的采集平台，所有项目依赖的数据全部都来源于数据采集模块，所以在设计采集模块的时候要考虑通用性。不能仅仅是为了这一个项目而服务。咱们前面在分析整体架构的时候说过，filebeat采集的数据到达kafka以后，会通过flume 再做一下分发，为什么要有这个分发这个过程呢？这个分发过程实现了什么功能呢？我们来看一下这张图。这个图里面呢，针对数据采集模块做了详细的分析，把数据采集模块呢，又划分了三层，数据采集聚合层，数据分发省数据落盘层。在这个数据采集聚合层，我们为了保证采集程序的通用性，不至于每次新增一个业务指标的数据，就去重新增加一个采集进程，或者修改采集程序的配置文件。所以呢，我们定义了一个规则。所有的日志数据全部保存在服务器的一个特定的目录下面。我会让filebeat的监控这个目录下面的所有文件。如果后期有新增业务日志，那么就会在这个目录下新增一种日志文件，filebeat就可以自动识别。但是这个时候会有一个问题。filebeat的输出只有一个。多种类型的日志数据会被filebeat采集到同一个topic中。如果各种类型的日志数据全部混到一块儿，会导致后期处理数据的时候比较麻烦。本来呢，我只想计算一种数据，但是这个时候我就需要读取这个大的topic。它里面呢，包含了很多种数据类型。这里面的一个数据量也很大，那我计算的时候呢，我就需要把它里面所有数据全部都读出来，然后再过滤。这样在计算的时候就会影响计算效率，也间接的浪费了计算资源。所以针对这个问题，我们又定义了一个规则，所有的日志数据全部使用json格式，并且呢，在json中增加一个type字段，标识数据的类型，这样每一条数据都有自己的类型标识，然后汇聚到kafka中的一个大的topic中。为了后面使用方便，我们就需要把这个大的topic中的数据啊，根据业务类型进行拆分，把不同类型的数据啊分发到不同的topic中。那这块的话，其实就是我们这个数据分发层要干的事情，相当于filebeat呢，它呢会把这个数据啊，全部都采集到kafka里面这个大topic里面。所有业务类型的日全部都采集到这一个topic里面。然后呢，我们接着就可以使用这个flume，对kafka中这个大topic中的数据进行分发。利用flume中的拦截器解析数据中的那个type字段的值，把type字段的值作为输出topic的一个名称。这样就可以把相同类型的数据分发到同一个topic中了。当然了，这些topic呢，我们需要提前创建。如果想要提高这个数据分发的能力，我们还可以在这儿启动多个flume进程。只需要保证多个flume中指定相同的group.id就可以了。这样就可以并行执行这个数据分发操作。那把这个数据分发到对应的topic里面以后呀，后面的实施计算程序就可以直接消费这个topic进行计算了。不需要再去读取那个大的topic，这样就可以提高计算性能。并且呢，我们还可以把需要备份的topic里面的数据啊，使用flume进行落盘，把它保存到Hdfs里面。这个呢，就是数据采集架构的详细设计。</span><br></pre></td></tr></table></figure><h2 id="数据来源分析"><a href="#数据来源分析" class="headerlink" title="数据来源分析"></a>数据来源分析</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251026706.png" alt="image-20230425102607194"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251044421.png" alt="image-20230425104401098"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下面我们来分析一下，针对这个项目，我们需要采集哪些业务类型的数据，以及这些数据来源于什么地方。首先是服务端日志数据。什么是服务端日志呢？可以这样理解。就是我们在APP中点击一些按钮的时候，例如我们要关注一个主播，这个时候当我们点击这个关注按钮之后。APP呢，它会去请求对应的接口。接口中的代码逻辑就是将我们关注的数据保存到数据库里面。同时，这个接口也会记录一份日志。因为这个接口呢，是为APP提供后台服务的，所以它记录的日志我们称之为服务端日志。接着我们来简单画一个图来看一下。这个呢，是隔壁老王。那是我们的一个用户。画的形象一点。这是我们的一个APP。我们这个APP里面呢，它有一个关注功能。这时候呢，你看老王啊，他会点击这个关注功能。要关注某一个主播。当他点击这个关注功能之后，这个APP里面这个关注功能，它对应的它会调用一个接口。那我们这块呢，有一个后台服务器。这个服务器里面呢，部署的有一个接口服务，就是这种HTTP接口。那对应的这个关注功能，其实呢，还会调那个接口。所以你在这呢，点击APP里面这个关注按钮。它底层啊，其实会调这个接口。这个接口呢，其实呢，它会操作一个数据库。相遇老王在这儿呢，关注了一个主播，最终啊调这个接口。接口呢去操作这个数据库，最终把老王关注了，谁把这个数据呢存到数据库里面。就是这个逻辑啊。那这个时候注意我们在这个接入服务里面呢，因为它俩现在也是一个外部项目，所以什么它里面可以记录日志，那这里面记录的日志呢。我们就把它称为是服务端日志。</span><br><span class="line"></span><br><span class="line">那在我们这个项目里面，针对服务端日志。主要包含实时粉丝、观众数据以及视频数据。这个实时粉丝关注数据啊，是因为用户呢，在点击关注以及取消关注的时候，都需要调用服务端接口。所以这个数据呢，会在服务端通过日志记录。还有就是这个视频数据。下面这个视频啊，其实就是直播。当主播关闭直播的时候，会调用服务端接口上报本次直播的相关指标数据。其实服务端记录的还有很多其他类型的数据，只不过说呢，我们这个项目目前呢，只需要这两种数据。那接下来我们来看一下服务端数据库中的数据。注意服务端数据库中的数据啊，其实啊，就是我们刚才图里面这个数据库里面的数据。来看一下。就这块儿。就相当于啊，我们App某一些功能呢，它会调用这个后台的接口，然后调用这个接口之后呢，最终啊，其实操作的是这个数据库。我们所说的这个服务端数据库的数据，其实说的就是这块它里面的数据。那在这个项目里面，我们主要获取历史粉丝关注数据，以及呢主播等级数据。这里面我们需要历史粉丝关注数据，因为我们在做这个项目的时候，我们的直播平台已经运营了两三年了。所以说，我们需要把历史粉丝关注数据初始化到图数据库中。这些历史数据服务端存储在数据库中，所以说我们需要从数据库里面去取。还有就是这个主播的等级数据。其实这个数据呢，在服务端日志中也有，但是我们考虑到服务端数据库中的数据是最准确的。特别是针对用户相关的数据，最好是以服务端数据库中的为准。所以说呢，我们就从那个服务端数据库中，每天凌晨啊，定时把昨天等级发生了变化的这个主播等级数据呢，导入到hdfs，方便我们后面的离线计算时。最后呢，我们来看一下客户端日志数据。刚才我们分析了服务端日志。</span><br><span class="line"></span><br><span class="line">那什么是客户端日志呢？其实啊，就是用户在APP客户端操作的时候。直接通过埋点上报的日志数据，这种数据称之为客户端日志数据。我们在这再画一下这个图。如果说呀，你在这做了一些操作，你说呢，我不调用这个后台这个接口。我呢通过买点直接上报用户这个行为说你关注了谁对吧，直接通过买点上报。这个时候的话，在这呢，我们会有一个。这个日日金融服务器。他这个呢，就是直接通过这个买一点。上报日。那最终啊，可以把这个用户的行为数据啊，直接上报到这个日志汽车服务器里面。然后就没有，然后。他不会去操作数据库啊。心志里面啊，一般上报的都是一些用户的行为。咱们刚才你看服务端日志呢，现在这啊，我们是要调这个接口，最终呢，是要操作数据库的，要把我们这个关注行为。就是谁关注，谁要把这个数据给它保存到数据库里面。而这种呢，通过买点上报呢，其实呢，他就不会和那个数据库打交道。所以说这种话一般称之为是客服端日志。是通过客户端直接上报的，不需要和这个服务端这个接口去交互的。这里记录的日志，我们称之为后端日志。好，那这个服务端日志和客端日志有什么区别吗？为什么再分成两种日志呢？以及说为什么有的地方我们使用客户端日志，为什么有的地方我们使用服务端日志呢。针对我们这个APP里面啊，它这个关注功能服务端记录的这个日志啊，会更加准确。因为服务端接口里面，它会涉及到对于数据库的一个操作里面会有事务。只有这条数据真正保存成功的时候，才会记录日志，如果说你在操作数据库保存失败了。现在你会回滚，这时候就不需要去记录这个操作日志了。你顶多记录一条失败的日志。但是客户端日志，只要用户在APP里面点击了一次关注功能，他就会上报一次日志。最终啊，这个日志接收服务器啊，就会接收到这个日志，并且呢，把它记录下来。他可能呢，会由于网络等原因啊，导致最终关注失败，但是呢，你这条日志。你是发过来了，并且呢，这块啊也记录下来。所以相对来说，服务端数据的准确性是比这个客户端日志这个准确性高的。如果说一份数据在服务端日志中和客户端日志中同时都有。那我们肯定要优先选择服务端中的日志数据。一般啊，我们在客户端通过埋点上报的数据啊，都是一些用户行为数据，这些数据啊，就算有一些误差也没有多大影响。它不涉及到一些和数据库交互的一些操作。那在我们这个项目中，针对客户端日志，我们只要获取用户活跃数据。活跃呢表示啊，只要用户每天打开APP就认为用户活跃，用户的这些行为数据呢，会在客户端通过买点上报管。那这些呢，就是我们项目中需要的一些基础数据，主要是这五份儿数据。那刚才我们分析到服务端日志，服务端数据库数据，以及客户端日志，那这个这个图里面啊，其实这样这个呢，就是服务端日志数据。这块呢，就是服务端数据库里面的数据。那这块呢，其实就是客户端的一些日志数据。</span><br></pre></td></tr></table></figure><h2 id="模拟产生数据"><a href="#模拟产生数据" class="headerlink" title="模拟产生数据"></a>模拟产生数据</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251045006.png" alt="image-20230425104534521"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面我们分析了具体需要什么数据，以及数据从哪里来，下面我们来看一下如何通过代码模拟产生这些数据。先看这个图。我们通过执行generate date这个项目中的这五个入口类，可以模拟产生这五种数据。就是咱们前面分析的那五种数据。那在这里注意一下，因为我们不能直接使用企业中的真实数据啊，所以在这里我会根据企业中真实数据的格式去模拟生成。最终的效果是没有区别的。我们来看一下这些对应的代码。这是一个DB video，这是我们这个项目的代码。对，这是一些基础的一些接口，以及生成数据的一个代码，所以说我提前把这个代码啊都写好了。先看看这个生成数据的这个。12345对吧，先使用这几个。这里面真的每一个类，它可以生成什么类型的数据，在这的话，这个图里面都有体现啊。这个呢是服务端日志，就是实时粉丝关注数据，这个呢也是对吧，就是视频数据。这呢是服务端数据库里面的数据啊，就是历史粉丝关注数据。这呢也是服务端数据，会面数据，生成那个主播等级数据对吧，直接进到那个马赛克面。那下面这个呢，是这个后端日志对吧，就是用户活跃数据。注意这些服务端日志呢，在生成的时候会调这个接口，对吧，就是服务端那个HTTP接口。那客户端呢，通过买点上报的时候呢，会调用这个客户端日志接收服务。这个以及这个server对应的就是这里面这两个。对吧，data collect。以及这个server。那我们在执行这些代码的时候注意。这里面呢，也是需要获取这个校验码的。这个券码的获取的流程呢，和咱们第一个项目里边获取券码的流程是一样的。注意，在真正执行这些代码之前，我们需要先把基础环境给它搞定了。这些代码在执行的时候呢，它会对应的会调用那个服务端的接口，以及呢，客户端日志接收服务的一个接口。还有一项，买面去写数据。所以说我们需要提前把这个server，还有这个data collect这两个接口服务，以及my circle中数据库和对的表都给它初始化好，都给它部署起来。那接下来第一步我们先对这个data collect这个项目给他呢编译打包。嗯。CDDB。video。recommend。C的这个data collect。肯定package。港地skip test。好，编译成功。那把它上传到我们这个服务器上面，注意在这呢，我统一都上传到我们这个BD的零四这台机器上面。那接着呢，我统一再建一个目录。每个点啊。V6。recommend。所以说呢，针对这个项目相关的一些架包啊，那些东西全部都放到这个目录里面，这样方便管理维护啊。那我们进到这个目录下面，注意现在里面呢，我在创建一个目录。叫data collect。那这里面呢，就放我们这个data这个接口，这个夹包。打开这个图形化界面。把我们刚才生成那个架包给他传上去。it soft。在这。把这个家包传上来。好，那下面呢，我们把它启动起来。Java杠架。后面呢，直接指定那个另一个加包就行了，注意我们需要把它放到后台运行，所以说呢，前面加个no ho。后面呢，加个and符，这样就可以了。GPS一下。看到没有，这个架已经启动了，你可以这样GPS杠ML。这样看起来更加清晰，对吧，就这个data collect。那下面呢，我们来测试一下这个服务啊是否正常，因为它是一个接口服务。这里面呢，我们其实开放的有一个测试的一个接口。看到没有？你直接调用这个就行，它是一个测试结果，当然你在这需要传一个参数啊，它需要提出一个参数name。他会把它打印出来，这样可以确认一下你这个服务是否正常啊。这个怎么验证呢？它是一个HTTP请求，你可以选择在我们的浏览器里面去操作，或者说呢，我们直接在这个控制台里面也是可以的，通过curl这个命令杠X。盖着操作啊。HTTP冒号双斜线logo。冒号，注意它这个端口呢，是8080。我在这指定了。看到没有8080啊。VT对吧。name。等于test吧。你看状态是200是OK的啊，说明我那个服务呢是正常启动了。那接下来第二步，我们把这个server这个项目呢，给它编译打包。有这个项目。他是负责写收我们那个服务日志的。嗯。clean港。开始。好，编译成功。把它传上来。注意，那我们在这需要创建一个目录啊。你可以在这直接对吧，右键这样创建也是可以的啊都可以。serve。因此。嗯。好，上传成功。那这个呢，我们要把它启动一下。no Hu Java刚加。安福。嗯。先确认这个服务是不是在啊在吗。好，这个呢，我们也来验证一下。大家注意它这个接口名称呢，就不是这个，你可以到这儿来确认一下。这是S1T1对吧，以及呢，它的端口注意。因为我们是在同一台服务器上启动的，所以说呢，这两个项目啊，它监听的端口肯定是不能重复的啊，这个我给它改成80812。四上这个是8081。后面的是S1T1对吧。嗯。好，OK啊，还是这个size。好，那这样的话，这两个接口服务呢，就搞定了，这个以这个都部署好了。那接着第三步，我们需要把这个MY面这个数据库啊，还有表啊，给它触发好，这个呢，给大家提供了有这个触发脚本也比较方便啊。现在这右键。进行一个so文件。直接把这个脚本。指另外就行了。就是in my circle tables啊。这个脚本。好，执行成功。在这刷新一下。好，它呢，会产生这么些表啊。OK，这个后面我们具体用的时候再来具体查看。那接下来呢，我们就需要去执行这些代码了。这样的话呢，就可以模拟产生数据了。首先呢，我们先产生这个实时粉丝关注数据。就这个服务端呢。啊，服务端日志。模拟生成实时粉丝关注和取消关注数据。注意了，大家在下面呀，在执行我这个代码的时候，需要注意，你们需要改点东西。注意在上面，你看这个是调用我线上这个接口来获取这个模拟数据，获取到之后呢，注意。下面呢，会调用我们本地刚才部署的那个接口服务。因为这个呢是服务端日志，所以说它会调那个服务端接口。我呢是在BD的零四这台机箱部署的。对吧，这个服装接口，它这个对应的端口号是8081。对吧。后面这些东西呢，都不用改，你们下去改的话，主要改一下这个对吧，你在哪一台机器上去部署的这个什么server这个接入服务，那你在这就把那个IP或者主机名写到这就可以了。所以接着的话，就可以模拟产生这个服务端的这个调用请求，最终呢，去调用这个服务端这个接口，让这个接口呢记录日志。那你说这个接口，它具体把日志记录到哪个目录下面呢？注意，在这也能看懂啊。在resources这个目录下面有一个log back点查明。看到没有，我把这个日呢记到这个Di这个母下面了。然后他这块呢，具体的一个日志文件名称是server下换in加log。那对应的这个对collect看了也有。它日志呢，也是放到这个电log这个目录下面。然后它的日文名称呢，叫data collect。所以说这个到时候可以通过这个文件名就知道到底是哪个接口服务记录的日志啊，也好分析。那我们这个代码呢，都是OK的，以及这些接口这些东西呢，我们也不需要改，对吧，也都是OK的，下面呢，我们来执行一下。注意这个代码呢，它是实时产生数据，现在呢，你在这执行一次，它会产生一条数据，就是模拟那个实时产生啊。看到没有接口调用成功，注意这块是我在这记录的日志啊。借口立项成功，它最终产生的日是这个。好，那我们来看一下，我们具体去调这个服务端接口，这个服务端接口有没有把这个日给记录下来了，我们来确认一下。在这重新克隆一个绘画。对，log。到这个木下面。看到没有？这个里面其实已经有值了，对吧，我们可以在这摸看一下啊。没问题吧，他已经记录下来了，说明我们这个流程目前是通的啊。这个呢，就是服务端的实时粉丝关注数据。那接下来往下面看。这个generate video for。这个也是服务端日志对吧，是模拟生成视频相关的数据。就是你这个主播开播结束之后呢，其实它就会调用服务端接口。把当前这个直播的相关的一些信息上报过去啊。那注意你这个在用的时候也是一样的流程，对吧，这个上面还是就我那个接口。那下面的话呢，你需要在这对应改一下对吧。好，那我在这来执行一下。注意这个呢，一次性我可以采用多条数据啊。你看接口交换成功是OK的。那下面注意我们来确认一下数据。因为这时候它里面产生数据比较多，我就使用T杠一吧，查看里面最新的一条数据。对吧。也怪了。OK，所以说这个呢，是服务端的这两种类型的数据都采集过来了。往下面看。那接着呢，我们来看一下这个服务端数据库里面的数据。首先呢，是这个。就是这个历史，粉丝关注数据。注意。这个电板执行之后呢，它会把这个数据啊，初始化到我们那个数据库里面啊。注意，你在执行这个代码之前，你需要先执行我这个脚本。进行这个数据库和表的一个初始化。初始化之后的话就可以直接执行了，这里面别的不需要改动了，注意你要改点东西。看到没有，要改一下数据库啊。你的一个数据库在哪，把那个IP地址啊，以数据库啊，一个名称啊，对吧，用户名啊，密码啊对吧，这些东西都改一下就可以了。来，我们来执行一下。从那个里面你可以看出来，他最终把数据啊，写到这个FOLLOW00这个表里面。好，我们到这来看一下。好，数据呢，都写进来了。这些测数据啊。这个呢，其实就是历史的一个粉丝关注出去了。那接下来注意我们来看这个主播等级的。也是这个服务端数据库里面数据模拟生成主播等级数据啊。它也是操作数据库。那这里面的话，他会把数据写到这个level user这个表里面来执行一下。OK，我们来确认一下。好，数据都写进了。那这样的话，相当于服务端数据库里面这两份数据呢，我们也都给他触发好了。那其实还剩一份数据，就是这个客端数据，就是那个用户活跃的。gena use active。对吧，客端是模拟生成用户活跃数据。这个呢，还是通过调用我这个接口获取数据。那下面注意这时候呢，它会调用我们那个客户端的那个日志收集服务器，对吧，其实就是一个client。他们最终会调用他，把数据上报给他。所以你的用处呢，也要改成这个，看一下你这个data client，你部署到哪个机器，把这个改一下就可以了。来执行一下。好，这个data可爱呢，他接收到用户上报的这些数据之后呢，他呢也会把它记录日志记录到本地。嗯。嗯。在这他又起到这个did collect点里面。我们取头一条还杠一。看到没有，就这个这个类型是user active，看到没有，都是接格式的啊。那到此为止呢，项目中需要的数据呢，我们都可以正常产生了。</span><br></pre></td></tr></table></figure><h3 id="客服端日志接收服务"><a href="#客服端日志接收服务" class="headerlink" title="客服端日志接收服务"></a>客服端日志接收服务</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251102183.png" alt="image-20230425110236188"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251055893.png" alt="image-20230425105526920"></p><h3 id="服务端http接口"><a href="#服务端http接口" class="headerlink" title="服务端http接口"></a>服务端http接口</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251101505.png" alt="image-20230425110135498"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251101273.png" alt="image-20230425110105056"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251104599.png" alt="image-20230425110444470"></p><h3 id="服务端数据库"><a href="#服务端数据库" class="headerlink" title="服务端数据库"></a>服务端数据库</h3><h4 id="实时粉丝关注"><a href="#实时粉丝关注" class="headerlink" title="实时粉丝关注"></a>实时粉丝关注</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251136630.png" alt="image-20230425111454589"></p><h4 id="视频数据"><a href="#视频数据" class="headerlink" title="视频数据"></a>视频数据</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251136684.png" alt="image-20230425111654573"></p><h4 id="历史粉丝关注"><a href="#历史粉丝关注" class="headerlink" title="历史粉丝关注"></a>历史粉丝关注</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251136744.png" alt="image-20230425112531665"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251125314.png" alt="image-20230425112548087"></p><h4 id="历史主播等级"><a href="#历史主播等级" class="headerlink" title="历史主播等级"></a>历史主播等级</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251130414.png" alt="image-20230425113019519"></p><h4 id="客户端用户活跃日志"><a href="#客户端用户活跃日志" class="headerlink" title="客户端用户活跃日志"></a>客户端用户活跃日志</h4><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251133562.png" alt="image-20230425113331494" style="zoom:67%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251133869.png" alt="image-20230425113349659"></p><h2 id="数据采集聚合"><a href="#数据采集聚合" class="headerlink" title="数据采集聚合"></a>数据采集聚合</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251159150.png" alt="image-20230425115912855"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">咱们前面把数据产生好了，下面就可以进行数据采集了，首先呢是使用这个filebeat呢，将所有日志数据采集到kafka的一个topic中中，把这个日志数据采集到kafka里面。日志数据有三份，服务端有两份，客户端日志有一份。注意，在这具体执行之前，我们需要先把这个zookeeper以及kafka这两个服务给它启动起来。好，那接下来呢，我们需要在kafka中呢，去创建一些topic。我们主要创建哪些topic呢？就根据我们前面啊，在那个架构图里面分析的，首先第一个是一个大的topic，它里面呢，包含所有的这种日数据。all_type_data_r2p40，因为它是一个大topic里面数据量比较大，所以说呢，我们给它多分一些分区，后期的话呢，就可以提高你一个消费能力了。这个里面呢，存储所有采集过来的日志数据。</span><br><span class="line">我们后面其实还有一个数据分发层，分发层的话，相当于把这里面的数据啊，这个它分发出来。我们服务端有两种数据，一个呢是这个实时的一个粉丝的关注和取消关注数据。这个它具体那个类型的名称呢，叫user_follow。这个topic这个名称后面为什么不加这种后缀了呀。这是因为啊，这个数据它相当于是我们的原始json数据里面的某一个自动的值，就那个type自动的值，这个日志啊，是之前记录下来的，所以后期我们再用的话，就把它直接拿过来用就行了，就不要再去改它这个值了，改起来就比较麻烦了。并且呢，你这个对应起来也不太好对应，所以说呢，我们就使用那个原始那个数据里面那个type值作为这个topic贝这个名称。</span><br><span class="line"></span><br><span class="line">那下面还有一个video_info。这个里面存储视频信息</span><br><span class="line"></span><br><span class="line">还有一个客户端的。user_active就是用户的活跃数据。存储客户端上报的用户活跃数据。</span><br><span class="line"></span><br><span class="line">注意后面还有一个default_r2p5，所以这个topic是干什么呢？注意现在啊，我们要把这个里面的数据给它分发到这几个topic里面。那在分发的时候有可能有一些数据啊，它里面没有type字段呀，或者说那个type字段的值不是这三个里面的。也就是说那个数据异常，这样的话，你需要把那个异常数据放到这个topic。这里面的存储无法具体分配到对应topic的异常数据。</span><br><span class="line"></span><br><span class="line">所以说呢，我们下面就需要去创建这些topic。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251200726.png" alt="image-20230425120032836"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们呢，就可以使用filebeat的去采集数据了，注意现在filebeat呢，我们还没有安装呢。需要安装到哪些机器上面呢？注意你要看你之前那个客户端的那个接口，以及服务端的接口，你都部署到哪些服务器上面，我在这呢，都给它部署到这个bigdata04上面了。因为filebeat呢，要采集data_collect这个接口记录的日志，以及server_inter这个接口记录的日志，所以说呢，你后期这两个接口你部署到哪台机器上面，那么你在对应的机器上就需要部署这个filebeat去采集数据。</span><br><span class="line"></span><br><span class="line">解压一下</span><br><span class="line">注意这个filebeat就是一个采集工具。所以说呢，使用起来很简单，我们只需要在里面指令输入和输出就可以。我们先修改它的配置文件，在这个目录下面有一个这个filebeat.yml这个配置文件，注意它是一个YML格式的，不是那种XML这种格式的啊，这是种新型的那种配置文件格式。</span><br><span class="line"></span><br><span class="line">你可以在这配置它的输入。你看它这有一个什么呀，这个类型是日志的，但是目前这个呢没有开启。我们在这呢，想先测试一下。在测试的时候呢，我就在这个配置文件里面，把这个输入啊，给它指定成标准的一个输入，就是键盘输入，输出的话呢，就把它指定成控制台。这样分析起来好分析，也比较直观。那怎么加呢？注意其实我们在这里面需要加一个输入，就是配置一个输入。对吧，你就按照它这个格式去写就行了，冒号注意后面一个空格，注意这个空格是不能少的。找那个输出。没有output。输出注意你看它默认呢，现在开启了一个什么呀，这个输出呢，是把数据输出的这个elasticserch，那现在我们先不用这个，你把它注掉。就指定那个console，注意下面呢，我们来加一些参数，注意下面这个缩进，我能不能使用那个tab，不行，必须使用空格。你可以使用两个或者四个。pretty。注意这个表示什么意思啊，它表示啊，会把你这个输出的结果啊，给你格式化一下，要不然是非常乱的啊。大家后期啊，在改这个配置文件的时候一定要注意啊，首先是这个冒号后面这个空格不能少，还有呢，就是说你这个在做缩进的时候不能使用制表符，要使用空格，官方建议使用四个，你用两个啊三个啊都可以啊。那下面呢，我们需要把这个配置好的这个配置文件呢，给它上传上去行吗？保存一下。如果说你对这个文件熟悉了之后，你可以直接在这里面去改都是可以的。不熟悉的话，建议呢，你还是把这个配送件拿到本地去，改完之后呢再传上来。直接覆盖就行啊。那这样的话就可以啊，下面我们就可以启动filebeat</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251209159.png" alt="image-20230425120937212"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251211306.png" alt="image-20230425121132023"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251212200.png" alt="image-20230425121239236"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251218213.png" alt="image-20230425121824126"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输出了一堆东西。虽然是一堆啊，还是有格式的，是一个阶层格式的，注意这个呢，就是因为我们在这加了这个属性，等于true，它呢会对你返回的数据呢，你做一个格式化。反馈的数据，你看就这个message里面内容，其他内容呢，都是它里面默认加的啊。接着你想停掉的话，直接CTRLC对吧。</span><br><span class="line"></span><br><span class="line">接下来我们需要继续修改配置文件，因为我们是希望filebeat的可以采集日志文件中的数据，将数据呢采集到kafka中。把刚才我们那个配置啊都给它删掉。然后呢，我们把这个基于日志的这个类型啊，给它开启了。这样的话它就可以读取文件了，你看pass里面指定一些路径，注意你可以指定一个或者多个都可以。你指定一个的话，你可以这样，你指定一个目录下面可以使用那种通配符也是OK的啊。所以说的话，它可以监控多个目录里面的多个文件。这是输入就配置好了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251222076.png" alt="image-20230425122235895"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来我们需要配置输出。这是一个输出组件，注意在这我们需要指定那个kafka的一个输出组件，那在这都需要指定什么配置呢？这个信息呢，到他那个官网上面是可以找到的啊，所以在边我就直接拿过来，你看out布点卡夫卡，把数据呢，输入到卡夫卡里面，直行卡卡的一个博物块地址对吧。123。还有这个topic，我们要把这个数据呢写到这里面。下面这几个配置你不用改就行了，这个呢表示啊，它往这个topic里面这些分区写数据的一个规则。这个表示卡里面那个艾机制是吧。这个呢是就是往里面写数据的时候，对这个数据15度压缩，这块呢，使用那个机会压缩可以提高性能。这个呢，表示呢，每一条数据最大的一个字节数量。好这样就可以了，你们下用的时候呢，有可能要改这以及这对吧，如果说你都和我的一样的话，其实都不用改。OK，这样的话就可以了，下面呢，把这个配置文件重新再传一下。这样就可以了。这样呢，我们就可以实现将服务端日志和客端日志全部都采集到卡夫卡里面，这个auto data的RP40这个topic里面。好，注意这个F呢，我们前面说了，它需要部署在所有这个服务端接口机器和客端日志接收服务器上面。</span><br><span class="line"></span><br><span class="line">在实行工作中，服务端接口机器会有多个。我们当时的这个服务端接口机器呢，有100多台。客户端的这个日志接收服务器有六台。所以说呢，在部署的时候呢，我们需要在100多台机器上去部署这个filebeat。那你说这个东西让我手工去做啊，肯定会疯的，所以说当时我们是通过运维同学开发的一个部署工具批量部署的，就是批量把这个filebeat的部署到机器上，那接着呢，由于这个服务端接口和客户端这个日志接入服务器呢，都在这个bigdata04上面，所以说我们就只需要在这个bigdata04上面部署一套filebeat就可以了，注意在这呢，我们暂时先不启动这个filebeat了。等我们把后面那个采集模块中的所有的配置啊，全部都修改好了以后，我们再从后面挨个开始启动进程，这样的话就不会漏出去了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251225756.png" alt="image-20230425122506901"></p><h2 id="数据分发"><a href="#数据分发" class="headerlink" title="数据分发"></a>数据分发</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下数据分发层，就是使用实现对采集到kafka指定topic里面的数据啊进行分发。咱们前面呢，把这个日数据采集过来啊，全部都放到kafka一个大的topic里面了，所以说下面呢，我们需要使用flume对这个大的topic里面数据啊进行分发，分发到一些小的topic里面，这样可以方便使用。注意，那这样的话，我们就需要在这个flume里面增加一配置文件，就是从那个大的topic里面消费数据，通过拦截器获取数据中的一个type自段的值作为输出kafka的一个topic的名称。这个配置文件呢，在这儿我已经写好了，我们来直接看一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#source+channle+sink的名字</span><br><span class="line">agent.sources &#x3D; kafkaSource</span><br><span class="line">agent.channels &#x3D; fileChannl</span><br><span class="line">agent.sinks &#x3D; kafkaSink</span><br><span class="line"></span><br><span class="line"># 指定source使用的channel</span><br><span class="line">agent.sources.kafkaSource.channels &#x3D; fileChannl</span><br><span class="line"># 指定sink需要使用的channel的</span><br><span class="line">agent.sinks.kafkaSink.channel &#x3D; fileChannl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-------- kafkaSource相关配置-----------------</span><br><span class="line">agent.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agent.sources.kafkaSource.batchSize &#x3D; 1000</span><br><span class="line">agent.sources.kafkaSource.batchDurationMillis &#x3D; 1000</span><br><span class="line">agent.sources.kafkaSource.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line">agent.sources.kafkaSource.kafka.topics &#x3D; all_type_data_r2p40</span><br><span class="line">agent.sources.kafkaSource.kafka.consumer.group.id &#x3D; flume_con_id_1</span><br><span class="line">#----------------- 拦截器 -------------------</span><br><span class="line"># 定义拦截器</span><br><span class="line">agent.sources.kafkaSource.interceptors &#x3D; i2 i1</span><br><span class="line"># 设置拦截器类型</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.type &#x3D; regex_extractor</span><br><span class="line"># 设置正则表达式，匹配指定的数据，这样设置会在数据的header中增加topic&#x3D;aaa</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.regex &#x3D; &quot;type&quot;:&quot;(\\w+)&quot;</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.serializers &#x3D; s1</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.serializers.s1.name &#x3D; topic</span><br><span class="line"># 避免遇到数据中没有type字段的，给这些数据赋一个默认topic【注意：这个拦截器必须设置】</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.type &#x3D; static</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.key &#x3D; topic</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.preserveExisting &#x3D; false</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.value &#x3D; default_r2p5</span><br><span class="line">#------- fileChannel相关配置-------------------------</span><br><span class="line"># channel类型</span><br><span class="line">agent.channels.fileChannl.type &#x3D; file</span><br><span class="line">agent.channels.fileChannl.checkpointDir &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;all_type_data&#x2F;checkpoint</span><br><span class="line">agent.channels.kafka2HdfsShow.dataDirs &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;all_type_data&#x2F;data</span><br><span class="line">#---------kafkaSink 相关配置------------------</span><br><span class="line">agent.sinks.kafkaSink.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent.sinks.kafkaSink.kafka.topic &#x3D; default</span><br><span class="line">agent.sinks.kafkaSink.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line">agent.sinks.kafkaSink.kafka.flumeBatchSize &#x3D; 1</span><br><span class="line">agent.sinks.kafkaSink.kafka.producer.acks &#x3D; 1</span><br><span class="line">agent.sinks.kafkaSink.kafka.producer.linger.ms &#x3D; 1</span><br><span class="line">agent.sinks.kafkaSink.kafka.producer.compression.type &#x3D; snappy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来就把它呢传上去就可以了，注意如果说我们需要在一台机器中启动多个flume进程的时候，最好在里面复制多个conf目录，因为如果在一个conf目录中启动多个agent的进程的话，多个agent的进程的日志信息会混到一块，后期排查问题会很麻烦啊，这个我们之前呢讲过了对吧啊，所以说呢，在这。进到这个。都门面。我们来复制一个。打靠谱。告不告？卡不卡？高不高相以我们从卡夫卡里面读出去，再把数据写到卡夫卡里面。进到里面。改一下他这个log的这个配置文件。把那个改一下，搞搞不搞搞卡不卡。好，这样的话就可以了。嗯。OK，那接下来我们在这里面来创建这个配置文件，就是刚才我们分析的这个。敢不敢杠后杠？敢不敢点。com？嗯。我们就组织一下。好，这样就可以了，注意这块搞定之后呢，我们就需要往下面走了，在这啊，我们也先不启动啊，那我们把这个数据落盘这块也搞定之后呢，从后往前启动。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252144970.png" alt="image-20230425214444152"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252145763.png" alt="image-20230425214458087"></p><h2 id="数据落盘"><a href="#数据落盘" class="headerlink" title="数据落盘"></a>数据落盘</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252156385.png" alt="image-20230425215638800"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下这个数据落盘层。我们需要使用采集指定topic各种数据进行落盘，便于离线计算。你看咱们前面呢，把这个数据呢，全部都采集到kafka这个大的topic里面，接下来做了一个分发，分发之后后面我们就要做这个落盘了。把我们需要落盘的数据呢，给它呢，落到这个HTPS上面，注意如果这里面这个大topic里面的所有类型的数据呢，我们都需要落盘的话，我们就可以直接读取这个大topic里面的所有数据，然后呢，使用这个拦截器。根据数据类型把它们呢保存到hdfs的多个目录中，这样呢比较方便，不过在我们这个项目中，这个大的topic里面一共有三种类型的数据，其中呢，有两种数据我们是需要进行落盘后期进行离线计算的。有一种数据呢是不需要的，只有实时计算上讲会用的，所以说我们可以使用两个flume agent来对我们需要的数据执行落盘操作。当然了，我们也可以啊，只使用一个flume agent的，那你这里面啊，也可以使用拦截器，只获取我们需要落盘的那两种数据，这样也是可以的，这个呢，给大家留一个作业，大家下去自己研究一下。使用flume拦截器如何呢？获取我们需要的两种数据，然后呢，分别把它呢落盘到hdfs的不同目录下面。</span><br><span class="line"></span><br><span class="line">那我在这呢，就使用两个agent来实现了，我们需要落盘的这两个topic呀，分别是这个user_active和这个video_info。所以说呢，在这我还需要复制两个目录。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252211650.png" alt="image-20230425221141206"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那我们进去去改一下那个logo的配置。那接下来我们还需要在这两个目录里面创建对应的一个配置文件，这两个配置文件啊，我也提前写好了，我们来看一下。</span><br><span class="line"></span><br><span class="line">因为这个东西啊，它就是一个体力活。咱们前面用过很多次了啊，没有什么技术含量。首先看这个user active，这都没什么好处了。S呢，是一个卡夫卡S，你读取这个user active。那个group使用这个be China。注意，下面是这个H里边的think。按T分目录存储就行了，放到对目录下面一个user。这就可以。那这个呢，是一个video。对吧，它读取的是一个video in这个topic。然后呢，就是说放到这个data木下面有个video。对吧，这是那个年月日。看天分，母乳好。那我在这呢，把这个复制一下。这是一个user active。脸卡不卡？到as。到。user。active。com。好，这个搞定。下一个先进到这个目录里面这个音符，把这个呢复制一下。VI、卡夫卡杠杠、video info。好。好，那这两个呢，我们都配置好了。所以接下来我们需要先确认一下这个含毒不集群啊，是否启动。对吧，我们之前已经起来了啊。好，那到此为止啊，这个采集相关的这个配置呢，我们都修改好了，下面呢，我们就要来启动一下。我们要从后往前启动，所以说呢，我们先启动这个数据落盘的这个辅助。我们来启动一下。诺哈普。b my name。agent康复指定配置文件跟目录。这个是com user active先求这个。进行配置文件刚刚。靠谱。在康复，然后呢。有点active下面有一个。啊，不搞搞HS user active com这个文件。agent。好，接下来呢，是第二个，我把前面这些能敷的呢复制过来。这个呢是video。刚刚看。六。下面有一个卡杠，杠HS杠video杠infor.com。最后是这个大纲内。agents。嗯。确认一下。这是一个，这是一个对吧，这两个呢都起来，嗯。这就是这两个数据落盘的一个a的进程啊，那接下来再往回推，我们需要把那个数据分发那个给它起下来。说。NG到康复。到卡夫卡，到卡夫卡。刚刚康复杠。这个木下面有一个。卡夫卡，卡夫卡点。com。我们之前少写一个这个F的这个强迫症，我给他改一下啊。来确认一下，确实少一个F，这个也不影响啊，只不过看起来有点别扭。好。这就可以了。怎么呢，重新来启动的啊，刚才也没启动成功啊no。b agent。等到。卡夫卡，卡夫卡，然后呢，刚刚。哎。卡夫卡，卡夫卡下面有一个卡夫卡，卡夫卡加。祷告，name isn&#39;。好。可以确认一下啊。对吧，这个也启动好。那最后呢，我们就要启动那个Bob的采集程了。接着来启动。牛逼了，杠C。比点两秒。注意啊，针对这个feel进程啊，正式启动的时候也需要使用这个no ho，把它放到后台运行在这里，我们为了一会儿使用方便，所以说我先在前台启动了，对吧。我们按一个回车把它启动起来，那接下来呢，我们就可以启动生成数据的程序了。注意在这呢，我们先执行那个generate real time for这个实时生成那个粉丝关注和取消关注的数据啊。来执行一下。</span><br></pre></td></tr></table></figure><h3 id="kafka-hdfs-user-active-conf"><a href="#kafka-hdfs-user-active-conf" class="headerlink" title="kafka-hdfs-user-active.conf"></a>kafka-hdfs-user-active.conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#source+channel+sink的名字</span><br><span class="line">agent.sources &#x3D; kafkaSource</span><br><span class="line">agent.channels &#x3D; kafka2Hdfs</span><br><span class="line">agent.sinks &#x3D; hdfsSink</span><br><span class="line"></span><br><span class="line"># 指定source使用的channel名字</span><br><span class="line">agent.sources.kafkaSource.channels &#x3D; kafka2Hdfs</span><br><span class="line"># 指定sink需要使用的channel的名字</span><br><span class="line">agent.sinks.hdfsSink.channel &#x3D; kafka2Hdfs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-------- kafkaSource相关配置-----------------</span><br><span class="line"># 定义消息源类型</span><br><span class="line">agent.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 定义kafka所在zk的地址</span><br><span class="line">agent.sources.kafkaSource.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line"># 配置消费的kafka topic</span><br><span class="line">agent.sources.kafkaSource.kafka.topics &#x3D; user_active</span><br><span class="line"># 配置消费者组的id</span><br><span class="line">agent.sources.kafkaSource.kafka.consumer.group.id &#x3D; user_active_con_1</span><br><span class="line"></span><br><span class="line">#------- fileChannel-1相关配置-------------------------</span><br><span class="line"># channel类型</span><br><span class="line">agent.channels.kafka2Hdfs.type &#x3D; file</span><br><span class="line">agent.channels.kafka2Hdfs.checkpointDir &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;user_active&#x2F;checkpoint</span><br><span class="line">agent.channels.kafka2Hdfs.dataDirs &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;user_active&#x2F;data</span><br><span class="line"></span><br><span class="line">#---------hdfsSink 相关配置------------------</span><br><span class="line">agent.sinks.hdfsSink.type &#x3D; hdfs</span><br><span class="line"># 注意, 我们输出到下面一个子文件夹datax中</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;user_active&#x2F;%Y%m%d</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat &#x3D; Text</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType &#x3D; DataStream</span><br><span class="line">agent.sinks.hdfsSink.hdfs.callTimeout &#x3D; 3600000</span><br><span class="line"></span><br><span class="line">#当文件大小为104857600字节时，将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize &#x3D; 104857600</span><br><span class="line">#events数据达到该数量的时候，将临时文件滚动成目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount &#x3D; 0</span><br><span class="line">#每隔N s将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval &#x3D; 3600</span><br><span class="line"></span><br><span class="line">#配置前缀和后缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix&#x3D;run</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileSuffix&#x3D;.data</span><br></pre></td></tr></table></figure><h3 id="kafka-hdfs-video-info-conf"><a href="#kafka-hdfs-video-info-conf" class="headerlink" title="kafka-hdfs-video-info.conf"></a>kafka-hdfs-video-info.conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#source+channel+sink的名字</span><br><span class="line">agent.sources &#x3D; kafkaSource</span><br><span class="line">agent.channels &#x3D; kafka2Hdfs</span><br><span class="line">agent.sinks &#x3D; hdfsSink</span><br><span class="line"></span><br><span class="line"># 指定source使用的channel名字</span><br><span class="line">agent.sources.kafkaSource.channels &#x3D; kafka2Hdfs</span><br><span class="line"># 指定sink需要使用的channel的名字</span><br><span class="line">agent.sinks.hdfsSink.channel &#x3D; kafka2Hdfs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-------- kafkaSource相关配置-----------------</span><br><span class="line"># 定义消息源类型</span><br><span class="line">agent.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 定义kafka所在zk的地址</span><br><span class="line">agent.sources.kafkaSource.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line"># 配置消费的kafka topic</span><br><span class="line">agent.sources.kafkaSource.kafka.topics &#x3D; video_info</span><br><span class="line"># 配置消费者组的id</span><br><span class="line">agent.sources.kafkaSource.kafka.consumer.group.id &#x3D; video_info_con_1</span><br><span class="line"></span><br><span class="line">#------- fileChannel-1相关配置-------------------------</span><br><span class="line"># channel类型</span><br><span class="line">agent.channels.kafka2Hdfs.type &#x3D; file</span><br><span class="line">agent.channels.kafka2Hdfs.checkpointDir &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;video_info&#x2F;checkpoint</span><br><span class="line">agent.channels.kafka2Hdfs.dataDirs &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;video_info&#x2F;data</span><br><span class="line"></span><br><span class="line">#---------hdfsSink 相关配置------------------</span><br><span class="line">agent.sinks.hdfsSink.type &#x3D; hdfs</span><br><span class="line"># 注意, 我们输出到下面一个子文件夹datax中</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;video_info&#x2F;%Y%m%d</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat &#x3D; Text</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType &#x3D; DataStream</span><br><span class="line">agent.sinks.hdfsSink.hdfs.callTimeout &#x3D; 3600000</span><br><span class="line"></span><br><span class="line">#当文件大小为104857600字节时，将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize &#x3D; 104857600</span><br><span class="line">#events数据达到该数量的时候，将临时文件滚动成目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount &#x3D; 0</span><br><span class="line">#每隔N s将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval &#x3D; 3600</span><br><span class="line"></span><br><span class="line">#配置前缀和后缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix&#x3D;run</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileSuffix&#x3D;.data</span><br></pre></td></tr></table></figure><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><h3 id="数据落盘-1"><a href="#数据落盘-1" class="headerlink" title="数据落盘"></a>数据落盘</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252219426.png" alt="image-20230425221947341"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252220669.png" alt="image-20230425222021013"></p><h3 id="数据分发-1"><a href="#数据分发-1" class="headerlink" title="数据分发"></a>数据分发</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252225085.png" alt="image-20230425222459145"></p><h3 id="数据采集聚合-1"><a href="#数据采集聚合-1" class="headerlink" title="数据采集聚合"></a>数据采集聚合</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252227824.png" alt="image-20230425222700964"></p><h3 id="执行数据生成程序"><a href="#执行数据生成程序" class="headerlink" title="执行数据生成程序"></a>执行数据生成程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">验证</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252230600.png" alt="image-20230425223043250"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252231054.png" alt="image-20230425223117407"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以执行成功了，那我们来验证一下结果。我们到那个卡夫卡里面来消费一下。基于console的。注意这个内容还是比较多的，但是我们可以大致看一下啊，你看。这是一条数据，是不是很乱呀？我们的日志数据没有这么乱吧，并且我们的日志数据里面也没有这个东西。对吧，我们的日志数据里面其实是这些东西在这，你看它其实啊，其实在这面又封装了一层，他把我这个具体的业务数据啊，给封装到这个message字段里面。注意这些字段的话，相对来讲是filebeat默认生成的，那这些字段呢，它不是我们需要的，我们希望啊，只记录我们的原始日志即可。那怎么解决这个问题呢？可以解决啊，我们需要在那个filebeat那个配置文件里面加一个配置。在这相当于啊，你要对它输出这个数据啊，做一个格式化。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252235554.png" alt="image-20230425223504841"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CD format。做一个格式化。四六类形的。对。我们呢，只取它里面的这个东西，哎，问号首先呢，括括号中括号我们只取里面那个message。所以这个配置的意思呢，就是说我们从这个数据里面。你看它这个采集过来数据里面有很多字段，我们只需要message字段里面内容，其实这里面就是我们的原始的日志数据。然后把这个filebeat再停一下。你再来启动，我们再来执行这个生成数据的这个。看到没有？这个就正常了，这样就可以</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252237920.png" alt="image-20230425223717162"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们再来查看一下这个user_follow这个topic，这个看一看它里面能不能消费到数据，如果能消费到数据，就说明那个flume的数据分发过程是没有问题的啊</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252240353.png" alt="image-20230425224013711"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">收到了吧。注意你在这能收到就说明啊，我们中间那个flume的分发程序是OK的，他从那个大的topic里面把这个数据读出来，然后呢写到这里，这样就可以好。那接下来我们再来执行两个程序。一个是这个active。其实呢，我们之前已经执行过了，在这我们再重新执行一下啊。然后重新呢，再生成一批数据，就是往那个日志文件里面再写一批啊，因为你调用接口，这个接口就会记录了，以及这个video info。好，这个执行成功之后呀，注意我们就可以到那个HDPS里面去确认一下结果数据了。因为你这个执行之后啊，他呢，这个接口就会把那个日志记录到本地。这个接口呢，就会把那个日志数据啊，记录到这个本地的日文件里面，然后filebeat呢，发现你里面啊有新增数据，所以说他就把这个数据给读出来，读出来之后呢，把它呢，采集到那个大的topic，就是all type data那个topic里面。然后呢，这里面有数据之后，后面那个的数据分发程序，它就会读取这个数据，对吧，对这个数据做分发，然后呢，把这个数据分发到不同的那个子topic里面，那我们最后呢，还有一个flume数据落盘层。他呢，就会从那个子topic里面把那些数据呢读出来，最终呢落到hdfs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以下面我们来验证一下。看到没有，这个user active，对这个都是有的。好，这个目录来我们直接查看一下它里面的数据啊。使用管道来取，前一条取一条就行，好里面有数据。那说明是OK的，那其实对应的我可以直接把这个改一下，改成video。看一下他们的数据。也是有了。没有问题，好。那在这我们都可以获取到数据，那就说明了是没有问题的，这就意味着我们前面的整个数据采集流程是通的。那大家在下面做实验的时候呀，我估计啊，可能会遇到各种各样的问题啊，就是大家在操作这一块的时候，如果发现最终啊，看不到我们这个希望的结果。那你就需要一步一步去排查，你要确认一个数据到了哪一步。你先看一下那个大套贝里面有没有数据，然后呢，再看一下那个子套贝壳里面有没有数，如果子套贝格里面也有数据，那最终hps里面没数据，那肯定是你那个数据落盘程序有问题。所以说你需要一点点去分析啊。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252308990.png" alt="image-20230425230808125"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252309870.png" alt="image-20230425230907753"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252309790.png" alt="image-20230425230920558"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对这个filebeat呀，我再多说一点，这个filebeat它在采集日文件中的数据的时候呢，它会将日文件数据的采集的一个偏移量啊，记录到本地文件里。所以说它在这会记录一下，这样话你filebeat给重启之后，它呢会读取这个文件，然后呢，根据你上一次记录的off继续往下面消费。它可以保证，就算你那个filebeat要停了，那在它停的中间，你往那个日里面记录数据，它后期启动之后还可以把它呢采集过来是这样。注意了，如果说呢，我们想要让这个filebeat的重启之后啊，继续重新开始消费。这样怎么办呢？暴力一点的话，我们可以直接把这个。这个data目录啊，给它删掉，你删掉之后这些信息是不是就没了，没了之后它就认为它是一个新文件，就从头开始读了啊，这个需要注意一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252316813.png" alt="image-20230425231605565"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">还有一点就是我们在使用filebeat的时候啊，如果发现filebeat没有正常工作，这个时候呢，我们需要去查看filebeat的日志文件，来排查具体是什么问题，因为有时候有一些错误信息啊，他不会暴露到这个工作台上面，它会记录到日志文件里面。像few b的下面有一个move。看到没有，它下面有这个这文件，注意你看的话就看这个，它后面这个相当于是一些备份的啊。你直接看这个就行。这是最新的一些日志。这些呢，是之前的一些老的日志。如果说他有一些错误信息在这里面就可以看。你在这儿可能看不到，它不会暴露到这个地方啊，你说呢，我把这个飞票启动了，也没见他报错呀，结果呢，他也没有把数据采集到我的卡夫卡里面，那所以说你就需要来这儿来看啊，看那个日文面。好，那针对服务端日志和这个客端日志的一个数据采集啊，我们先讲到这儿，在这呢，大家主要掌握数据采集的整理思路，重点是里面那个数据聚合，数据分发这两个步骤，那个数据落盘呢，倒没什么特殊的，对吧，咱们之前已经用过很多次。</span><br></pre></td></tr></table></figure><h2 id="采集服务端数据库数据"><a href="#采集服务端数据库数据" class="headerlink" title="采集服务端数据库数据"></a>采集服务端数据库数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面呢，我们把这个服务端日志以及客户端日志呢采集过来，下面呢，我们还需要将服务端数据库的数据也采集过来。咱们前面分析了啊，由于历史粉丝关注数据呢，只需要导出一次，所以说呢，没必要使用sqoop，那还有一份数据呢。是那个每日的主播等级数据对吧？所以这份数据呢，我们在最开始的时候会将数据库里面的全量数据导出来一份，后期只需要根据表中的更新字段获取发生了变化的数据即可，这样每天需要导出的数据量就很小了，属于增量采集。这时候呢，可以选择使用sqoop，其实呢也可以选择使用shell脚本，sqoop的使用在上一个项目中我们已经用过了，所以接着呢，我们来讲点不一样的，我来使用shell脚本，将mysql中的数据导出来，所以说么，这两份数据我全部呢使用mysql脚本。把它倒出来。那首先呢，我们使用这个mysql里面那个-e这个命令，将这个具体的查询命令啊准备好。</span><br><span class="line"></span><br><span class="line">注意我其实在这呢，可以直接操作我Windows本地的那个MYSQL，这个里面它默上是有那个MYSQL客户端的，你如果没有的话，你就装一下啊，我们使用mysql -e后面的话就可以写具体的sql了。因为你现在也不是连本地的马，你是连其他机器的马，对吧。我们在这个list里面连接我们Windows里边那个马。那我们Windows的那个机器这个IP是也有2.168.182.1。这样就可以，这也可以编了哈。那这里面写了一个参考，我们先写那个历史粉丝关注数据啊。再来呢，它里面有两列，一个UID，一个呢是UID。from我们这个库的名称啊，是一个点。follow。零零，我们先查这一张表。看到没有，它是可以执行的。那以及呢，我们还要查询这个每日主播等级数据啊，就每天发生了变化的那些主播数据。注意它里面的话字段比较多。注意它里面有一个update time，就是更新字段啊。就这个，如果说这里面这个数据发生了变化。某一个字段被改了，那么这个就会变化，所以说我们每天呢，就根据它去抽取数据。up time。大于等于。所以我们这里面这个日期啊，目前的话看一下。那是2月1号对吧。26年2月1号，所以说我在里面这样来，我就把这些数据给它查出来，二六杠零二杠零幺。000000对吧。只要凌晨开始按着。update time。小于等于。2026杠零二杠零一。23:59:59，这样的话就可以把这些数据全都查出来。没问题吧，也是可以的啊。好，那这两条命令啊</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252328190.png" alt="image-20230425232805742"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252328755.png" alt="image-20230425232851743"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252350277.png" alt="image-20230425235033202"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252351285.png" alt="image-20230425235117301"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252351462.png" alt="image-20230425235132504"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">验证成功之后呢，我们就需要把这个命令啊公布到脚本里面，首先是这个用户历史关注数据，注意这份数据啊。它其实呢是分表存取。你看我在这其实只是创建了一部分表，它有很多张。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252352125.png" alt="image-20230425235203139"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个分表逻辑是什么样子的，你看它里面存储的是什么呀，是一个用户的一个UID啊。所以说这样的，它会根据这个用户的UID来计算一个MD5值。你这个MD5值是什么样子的呢？给他搜一下。对吧，你看它的MD5值其实就是这样。就是类似于这么一串。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252354258.png" alt="image-20230425235426902"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我们呢，会根据这个用户的UID去计算一个MD5值，然后呢获取最后两位字符，然后呢拼接到这个follow后面。组装一个新的表名。这样话呢，他会算一下你那个UID最后两位是什么，然后呢，存到对应的表里面。这是它一个分表逻辑啊，那你后期查的话呢，也会根据这个UID计算MD5获取最后两位，然后呢来前面呢拼那个follow下划线，后面的话拼上了两位，这样就找到它对应的一个表。</span><br><span class="line"></span><br><span class="line">注意这种组合呀。这两位你看它有可能是零到九以及a到z中的任意一个，所以说呢，这位它呢有36种情况，这一位呢也是36种情况，你26个英文字母加上零到九有十个，一共是36，每一位都三十六三十六乘以36是吧。1296张表。所以说呢，我们其实在实际过程中，我们需要将这1296张表中的数据全部都导出来，所以我在这呢，也没有建那么多张表，太多了啊，所以说在这建了一部分，在这大家知道它是一个分表的就行啊。然后呢，你要知道它这个分表规则。OK。这样的话，一会我们采集数据，我就先采集一张就行，但是呢，我会写脚本，让他可以支持把这个1296张表中的数据全部都采集出来。我们把那个脚本写好，但是具体采的时候，我们就只采这一张表就行啊，因为就它里面有数据。</span><br></pre></td></tr></table></figure><h3 id="extractFollower-sh"><a href="#extractFollower-sh" class="headerlink" title="extractFollower.sh"></a>extractFollower.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">好，下面我们来开发第一个脚本。我现在来写。简单号。它保存一下啊，给它起个名字叫attracted。要抽取follow。是脚本。只需要执行一次即可。你只需要抽取自己的是历史数据，后期的话我们还有一份实时数据，就可以实时维护了。针对1296张表。需要使用这个双层或循环。动态生成表明。I。12345678。所以后面还有呢a。FGH。I z KL mn pqr。ST。UVWXYZ，好吧，这题又是个直接画啊度。这是一层，你里面还要套一层呢。因为我们要批那个表明的最后两个字符嘛，对吧。G。这个我就不是文桥了。50块。好。那现在里面了，我来艾打印一下。就把那个表明啊给他批出来。哎。当然，这行吧。其实你在这儿只要能把这1000多张表的那个表明全都拼出来，那继续把它导出去，那不就很简单了吗？对吧？我先在这呢，先写一个导师的一个脚本。马杠u杠P杠H。18291。藏意后面是circle。这个UIDUID。from。video。零零。注意这样的话就可以获取那些数据了，然后我把这个数据呢，给它重定向。到这个里面这个soft。video recommend。就使用这个表名作为文件里面的前缀log，这样的话其实就可以把这张表数据给它导出来了。你如果把这个东西放到这儿。好把重要啊，我们一会执行不执行，你把里面改一下是不是就行了，你这个东西。还有那个。这东西，然后那个是不是可以了呀。你只要说这个循环执行完，其实就可以把所有数据全都倒出来。我在这通过IO把这个表名导一下就行，行吗。最后的话呢，我们只是把这一个标点数据给它打开就行。好，那接下来呢。先试一下啊，重新在这儿来建一个脚本。video&#39;。到了。OK，这样就可以了，那接下来我们来执行SH。看到没有，前面都打印出来了，没问题啊。那我们来确认一下，这个木下面有没有产生那个零零.log。产生了吧。看一下里面的数据。没问题吧，没问题啊好，</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 此脚本只需要执行一次即可</span><br><span class="line"></span><br><span class="line"># 针对1296张表，需要使用双层for循环动态生成表名</span><br><span class="line">for i in 0 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o p q r s t u v w x y z</span><br><span class="line">do</span><br><span class="line"> for j in 0 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o p q r s t u v w x y z</span><br><span class="line"> do</span><br><span class="line">echo follower_$&#123;i&#125;$&#123;j&#125;</span><br><span class="line">#mysql -uroot -padmin -h 192.168.182.1 -e &quot;select fuid,uid from video.follower_$&#123;i&#125;$&#123;j&#125;&quot; &gt;&gt; &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;follower_$&#123;i&#125;$&#123;j&#125;.log</span><br><span class="line"> done</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">mysql -uroot -padmin -h 192.168.182.1 -e &quot;select fuid,uid from video.follower_00&quot; &gt;&gt; &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;follower_00.log</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260002090.png" alt="image-20230426000249498"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260003322.png" alt="image-20230426000303163"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260003085.png" alt="image-20230426000343050"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260004849.png" alt="image-20230426000403681"></p><h3 id="extractUserLevel-sh"><a href="#extractUserLevel-sh" class="headerlink" title="extractUserLevel.sh"></a>extractUserLevel.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line"># 此脚本每天执行一次，添加到crontab或者Azkaban调度器中[每天0:30分开始执行]</span><br><span class="line"></span><br><span class="line"># 正常情况下获取昨天的数据，如果需要补数据，可以直接指定日期</span><br><span class="line">if [ &quot;X$1&quot; &#x3D;&#x3D; &quot;X&quot; ]</span><br><span class="line">then</span><br><span class="line">yesterday&#x3D;&#96;date --date&#x3D;&quot;1 days ago&quot; +%Y-%m-%d&#96;</span><br><span class="line">else</span><br><span class="line">yesterday&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">mysql -uroot -padmin -h 192.168.182.1 -e &quot;select * from video.cl_level_user where update_time &gt;&#x3D;&#39;$&#123;yesterday&#125; 00:00:00&#39; and update_time &lt;&#x3D; &#39;$&#123;yesterday&#125; 23:59:59&#39;&quot; &gt;&gt; &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;cl_level_user_$&#123;yesterday&#125;.log</span><br><span class="line"></span><br><span class="line"># 将数据上传到hdfs上，每天一个目录</span><br><span class="line"></span><br><span class="line"># 先在hdfs上创建日期目录</span><br><span class="line">hdfs dfs -mkdir -p &#x2F;data&#x2F;cl_level_user&#x2F;$&#123;yesterday&#x2F;&#x2F;-&#x2F;&#125;</span><br><span class="line"></span><br><span class="line"># 上传</span><br><span class="line">hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;cl_level_user_$&#123;yesterday&#125;.log &#x2F;data&#x2F;cl_level_user&#x2F;$&#123;yesterday&#x2F;&#x2F;-&#x2F;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来开发第二个脚本。这个也是抽取数据了。他呢是抽取了一个user level，有这个主播等级的啊。注意在这个脚本中啊，我们需要将数据上传到hdfs上面，并且呢，这个脚本啊，也需要添加到Crontab或者azakaban那个调度器中，每天凌晨00:30了，然后执行一次。就是每天呢抽取一次啊。</span><br><span class="line"></span><br><span class="line">那下面我们写一下，就是正常情况下。每天凌晨获取昨天的数据。你如果呢，你需要补数据的话。可以直接指定日期。所以这个的话，我来获取一个日期。呃，一。等等a。那说明道一为空。then。还有这个yesterday。等于。加个反引号。刚刚date。days ago，一天以前就是昨天嘛，对吧，加号百分号Y杠百分号M，杠百分号D。这就过去昨天日期了。else，如果说呢不相等，那说明了多少？有值，有值的话呢？二。这样就快。那下面的话其实就可以把我们那个circle语句给它拿回来，那个搜有点长，我在这呢。到这儿拿一下。我们在前面是不是写过呀。对，这里面写这个星号就可以啊，没问题。这边需要改一下。把那个日期拼上。注意你说我在这成这个单引号，它不是不解析吗？大家注意外面还有一层双引号，咱们之前讲过对吧，双引号里面这个单引号这个会解析这个变量啊。我反过来又不行。那最后呢，我们把这个日志数据呢，给它重进现到这个目下面，在这我可以把前面这个复制一下是吧过来。后面的话呢，其实就是这个。表明了。使它来拼接一个热面。注意其在后面呢，你最好拼一个日期对吧，因为他每天都有一次。填了一个日期。点到这就可以。注意，那你到这还没完，我们还需要把这个日上传到HDS上面。杨淑玉上传。的X元。每天一个目录。所以你在这呢，你先在hps上创建日期目录，因为每天一个目录嘛，对吧。CS杠那个地可以加个杠P对吧。it。后面呢，我们就使用这个level user。然后后面把这个复制过来。大家注意这个日期格式呢，中间是带这个横杠的年月日，我们其实想获取这个不带横杠的。但是由于这他确实需要带横杠的，那我们这又不想要怎么办呢？那你就改一下呗，是吧。把横杠给替换掉就行，对吧，全部横杠替换成空，这样的话就是这个年月日中间不带任何分隔符了。这个用法咱们前面也讲过吧。还没上班。高foot。那就是这。把它呢，上传到这个目录下。这一块。</span><br><span class="line"></span><br><span class="line">好，注意，我这个脚本其实就是一个增量脚本，咱们前面说了，你在最开始的时候，其实啊，你还需要将这个表里面的之前的全量数据也给它导一次。那个脚本我这里先不写。我直接写了一个每日的增量脚本啊，大家要知道这个事情。因为我们现在这里面这个数据其实都是认为是一些增量数据啊，我直接。使用这个日期过滤，就可以把里面所有东西全都过滤出来。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260033394.png" alt="image-20230426003329063"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来确认一下。杠，我们来查一下这个date。小level user。有吧？再来查一下这个。没问题吧，有数据啊，那我们来看一下里面内容呗。对吧，这就是我们采集过来的内容。</span><br><span class="line"></span><br><span class="line">但是你在这需要注意点，它是有一个表头的。所以说呢，我们后期啊，在处理这个数据的时候，需要把这些数据给它归掉就行。那到此为止，服务端数据库中的数据我们就采集完毕了，那在这里面啊，大家需要熟悉我们的数据来源，以及数据最终的存储位置。后面我们的计算程序呢，都需要依赖于这些数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260034352.png" alt="image-20230426003427969"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260034803.png" alt="image-20230426003455794"></p><h2 id="数据计算核心指标详细分析"><a href="#数据计算核心指标详细分析" class="headerlink" title="数据计算核心指标详细分析"></a>数据计算核心指标详细分析</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v1.0-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1.html</id>
    <published>2023-04-24T07:37:23.000Z</published>
    <updated>2023-04-24T15:42:04.630Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v1-0-1"><a href="#第十八周-直播平台三度关系推荐v1-0-1" class="headerlink" title="第十八周 直播平台三度关系推荐v1.0-1"></a>第十八周 直播平台三度关系推荐v1.0-1</h1><h2 id="项目"><a href="#项目" class="headerlink" title="项目"></a>项目</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面我们开始正式学习直播平台三度关系推荐系统这个项目，这个项目分为1.0和2.0这两个版本。本周我们先学习1.0这个版本。首先我们来看一下项目效果。大家呢，可以在这里面扫码体验。这个就是我们直播平台的首页，当我们点击某一个主播，会进入到主播的详情页，我们在点击这个follow关注按钮的时候。这里面呢，会插入一个模块，它里面显示的是关注了此主播的人，也关注了哪些主播。这就是三度关系推进的效果。这页面上看起来只是把数据展现出来，很简单，但是具体这些数据是怎么来的，如何保证推荐的主播也是用户感兴趣的，这才是我们这个项目的核心内容。下面我们来看一下针对这个项目官方一点的介绍。</span><br><span class="line"></span><br><span class="line">在直播平台中，用户在主播页面关注该主播时。粉丝状态栏下方插入三度关系推荐模块，显示该主播的粉丝同时又关注了哪些主播。按照推荐重合度且满足一定的筛选条件进行择优展示，这样推荐的主播才是用户最可能喜欢的，可以帮助用户发现更多他喜欢的主播，促进用户活跃，进而挖掘用户消费潜力。这就是我们这个项目最终想要达到的效果。想要实现我们前面所说的三种关系推荐是需要由数据来支撑的，那么这些数据从哪里来呢？这就涉及到我们的第一块内容，数据采集。我们需要将项目中需要用的所有数据全部采集过来，包括离线数据和实时数据。这些数据采集过来以后，就需要涉及第二块内容了，数据存储，离线数据一般存储到分布式文件系统中，实时数据一般存储到消息队列中，数据存储起来以后，就需要涉及到数据计算了。数据计算模块，对前面存储起来的数据进行计算，分为离线计算和实时计算，计算之后的结果数据还会进行存储。那计算出来结果之后呢，就会涉及到数据展示了。将数据在页面中展示，查看最终的一个推荐效果。所以这个项目中的四大模块，它们之间的关系就是这样。在这里我们先从整体上对这个项目进行一个划分，后面还会有更详细的划分。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241546289.png" alt="image-20230424154642727"></p><h2 id="技术选型之数据采集"><a href="#技术选型之数据采集" class="headerlink" title="技术选型之数据采集"></a>技术选型之数据采集</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241549395.png" alt="image-20230424154922323"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">练好。接下来我们先针对这四个模块分析一下他们里面是需要用到的具体的技术框架，俗称技术选型。我们首先来看一下数据采集工具的选择。针对日志数据，目前业内常用的采集工具有下面这些。首先是这个apache开源的这个顶级项目flume。还有这个elastic公司开源logstash以及filebeat。那在这呢，我们把这个kafka也列出来了，但是kafka并不算是日志采集工具。只是说呢，它一般会和采集工具一块使用，所以呢，在这就一块列出。以及呢，这个sqoop组件。下面呢，我们来详细分析一下这个flume、logstash，还有filebeat这几个日志采集工具。</span><br><span class="line">首先呢，是flume。如果它是基于Java语言实现的。flume主要由source、channel、sink这三个组件组成。针对这三个组件中提供了很多实现。针对source，有基于文件的，基于socket的，基于kafka的等等，还有很多我们常用的数据源，flume几乎都提供了支持。这个channel提供了有常见的基于文件的，基于内存的。这个sink有基于hdfs的，基于kafka的等等，还有很多我们常用的存储系统几乎都提供了支持。就算是部分特殊的source和sink，flume没有提供支持，那么也没有关系，flume允许我们自定义这些组件。由于它也是基于Java的，所以开发这些自定义的组件也没有多大问题，我们都是Java程序员对吧？所以目前啊，在企业中，针对日志数据采集这一块，flume占据了主要地位。</span><br><span class="line">OK，那接下来我们来看一下logstash。那是基于Jruby实现的。Jruby是ruby语言的Java实现。logstash的架构有点类似于flume，主要由输入、输出和过滤组成。这里的输入和输出类似于中的S和。那也提供了很多输入和输出的组件支持。常见的数据源和存储系统也都是支持的。并且带中还提供了强大的过滤功能，可以将采集到的数据进行一些处理之后再写出去。flume中的拦截器也可以实现类似的功能。logstash可以和elasticsearch、kibana轻松的实现一个日志收集检索、展现平台，非常方便。俗称ELK全家桶。那其实分析到这儿，我们会发现flume和那logstash还是非常相似的。但是呢，他们两个的典型应用场景是有一些区别的。logstash常用的场景是帮助运维人员采集服务器自身的运行日志，方便运维人员排查服务器的问题。这种场景下，对于数据的完整性和安全性要求不是特别高。因为logstash内部没有一个持久化的队列，所以在异常情况下是可能出现数据丢失的问题。而flume内部呢，是有自己的ack机制来去确保这个问题。所以说呢，它可以用于一些更重要的业务日志台词。</span><br><span class="line"></span><br><span class="line">OK，那接下来我们再来看一下这个filebeat采集工具。是采用这个go语言开发的。它只支持文件数据采集。它可以将文件中的数据采集到，kafka呀，ES啊等等这些常见的存储系统。它会记录文件采集到的offset信息。就算filebeat的采集进程挂掉，也不会导致数据丢失，它下一次重新启动之后，还会延续之前的offset，继续往下面采。并且呢，这个filebeat呢，它还是一个轻量级的采集工具。咱们前面分析的这个flume，还有这个logstash，它们都是一些重量级的产品。在某些特定场景下，这个轻量级的组件可能会更加合适。filebeat和logstash同属于elastic这个公司，这些公司呢，提供了很多的这种Beat组件。filebeat的只是其中一个。因为我们的数据采集呢，主要是基于文件的，所以在这呢，我就只分析了这个filebeat。</span><br><span class="line"></span><br><span class="line">那到目前为止，这三个采集工具啊，我们都分析了一遍。logstash、flume属于重量级的组件。它们都是基于JVM虚拟机运行的。filebeat的呢是一个轻量级的组件，它是基于go语言。从语言层面来分析。go语言开发的程序，性能消耗是比那个基于JVM虚拟机运行的程序要小的。并且我们也在实际的服务器上进行了测试。相同数据规模下，filebeat的内存和CPU消耗是flume和logstash低的。那我们就直接选择filebeat了吗？并不是因为它的优点是性能消耗低，但是它的功能是有点弱的。所以我们在实际过程中会这样做。在前端业务机器上部署Filebeat的，将日志数据采集到消息队列里面。因为这个时候的要求是尽可能少的占用服务器资源，保证服务器上面的其他业务正常运行。数据到消息队列以后，后面我们可能还需要对数据进行一些简单的预处理，之后再存储到不同的地方。那所以在这个地方就可以使用flume了，因为提供了丰富的source和sink，并且也可以使用拦截器对数据进行一些简单的处理。这个时候就不需要太纠结性能消耗了，因为flume是部署在单独的服务器上面，不会对其他应用程序造成影响。在这呢，我们先简单画图看一下。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后这个呢，是前端业务机器啊。那如果说我们想采集这个机器上面的一个日志数据的话，最好呢，是部署一个性能消耗低的一个采集工具。因为这上面除此还有其他进程。所以说我们在这里面部署于一个filebeat的性能消耗比较低。还是有那么两台吧。好，那我们在每一台上面都部署一个Bo的，让他呢去采集当前机器里面的人数数据。采集到之后呢，把这个日志数据啊，放到我们的消息队列里面。它的一个数据走向是这样。OK，这个数据进到这个消息页之后呢，后面我们其实就可以使用去对这些数据做一些简单的预处理，预处理之后呢，再把数据写到其他地方。所以接着呢就可以接一个flume，这个时候就不需要去考虑这个性能消耗。这个有可能，它可以直接去读它里面的数据，然后呢，再把数据再写进去，都是有可能的啊。类似于如果从第一个topic上面就是消费数据。把数据拿出来之后呢，对数据做一些处理，处理之后呢，再放到第二个topic都是OK的。以及呢，flume可以直接消费这个消息队列里面这个数据，然后呢，直接把数据呢，写到我们的hdfs分布式文件系统里面也是可以的。好，这就是这个流程。所以在这里我们就选择使用filebeat和flume。那针对这里面这个消息列，我们就直接使用卡夫卡，因为卡夫卡呢是大数据领域中最常用的消息队列。所以说我们最终的选择就是F贝塔加上卡布卡，再加上罗姆，这就是针对日志数据采集工具的选择。就这三个。后面呢，我们也会涉及到数据库数据的采集。在我们这个项目中，需要从数据库中采集的数据量比较小。我们可以选择使用sqoop，或者我们可以使用mysql -e这个命令直接导出数据也是可以的。上一个项目我们已经使用过sqoop采集MYSQL的数据了，那在这呢，我们就使用一个不一样的，我们自己来开发一个脚本，使用MYSQL命令直接导出数据。不过我们在最后呢，也是需要把hdfs中的结果数据啊，导出到mysql中啊，这个时候呢，还是需要用到sqoop。所以说呢，针对数据采集这一块，最终我们选择了这四个技术框架。</span><br></pre></td></tr></table></figure><h2 id="技术选型之数据存储"><a href="#技术选型之数据存储" class="headerlink" title="技术选型之数据存储"></a>技术选型之数据存储</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241618486.png" alt="image-20230424161849176"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们把数据采集工具分析完了，下面我们来分析一下数据存储系统的选择。我们采集到的数据最终会存储到分布式文件系统中，这个分布式文件系统一般就直接选择hadoop中的hdfs，这个就不需要额外的对比啊，因为我们在搭建大数据平台的时候，hdfs已经安装好了。并且它也可以和很多采集工具以及计算框架无缝衔接，所以大数据领域的分布式存储系统一般都直接使用hdfs。目前一些大的厂商也有分布式存储的一些服务，例如亚马逊的s3。我们也可以选择使用这些服务，但是这样的话，针对数据计算就不太友好了。分布式计算框架无法实现本地计算，因为数据和计算节点不在一块儿。所以在这呢，离线数据我们就使用HDFS来求我们的计算框架，计算的结果数据有一些是需要和前端交互的。这些数据呢，前期可以选择存储到MYSQL里。那我们在维护这个用户三度关系数据的时候呢，如果使用普通的关系型数据库进行存储的话，会造成很多数据冗余，并且查询起来也非常麻烦，所以一般啊会使用一些图数据库。这里面这个Graphx或者Gelly。Graphx是spark中的图计算。gelly属于flink中的图计算，它们只能实现分布式图计算，不能保存图数据，所以说呢，并不满足我们的需求。</span><br><span class="line"></span><br><span class="line">下面这几个neo4j、orientDB、JanusGraph这几个都是图数据库，它们几个又有什么区别呢？我们来看一下。在这里面，我通过这些层面。对这三个图数据库做了一些对比分析啊。其中这个neo4j啊，它是目前人气最高的图数据库，它可以支持高度扩展，完全支持acid acid是数据库里面的一个特性啊，neo4j啊，它提供的Cypher查询语言是比较人性化的，非常容易上手使用，并且 它支持社区版和商业版，社区版是开源的。社区版呢，不支持分布式，商业版支持分布式。neo4j入门，相当简单，学习成本比较低，并且比较稳定。还有就是neo4j，它对各种语言的支持也比较好，Java呀，python啊，这些语言它都支持，并且官方提供的还有一个connector插件，可以实现Spark直接操作neo4j非常方便，那在这呢，我们主要考虑到应用性以及快速上线这些特性，所以说neo4j是我们目前最优的选择。所以说呢，针对这些存储系统啊，我们最后的选择就是hdfs加上MYSQL加上neo4j。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241621070.png" alt="image-20230424162131164"></p><h2 id="技术选型之数据计算-数据展现"><a href="#技术选型之数据计算-数据展现" class="headerlink" title="技术选型之数据计算+数据展现"></a>技术选型之数据计算+数据展现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下数据计算框架的选择。目前啊，大数据领域最常用的几种计算框架包括mapreduce、storm、spark、flink。其中mapreduce是第一代计算引擎，它主要针对离线数据进行计算。由于mapreduce计算框架的模型啊是固定的，针对复杂的计算，需要开发多个mapreduce任务，代码量比较多，也比较麻烦，并且它的计算是基于磁盘的，计算效率也比较低，所以现在已经很少使用。接下来看一下storm这个计算框架，是一个比较早的实时计算框架，可以实现真正意义上的实时处理。在前期的数据计算领域立下了汗马功劳，但是由于此框架太过于独立，没有自己的生态圈，所以最近这几年呢，日渐没落。那接下来看一下spark的这个计算框架，它是一个分布式的内存计算框架，支持离线和实时数据计算，由于它是基于内存的，所以说呢，它的计算性能非常高。但是在这需要注意一下，虽然spark支持实时计算，但是它的实时计算并不是真正意义上的实时。这是由于它底层的计算模型决定。spark最快只能支持到秒级别的实时计算，相当于一秒执行一个小型的批处任务。最后我们来看一下flink这个计算框架，flink属于最近新兴起的一个流式计算框架，它侧重于的是实时计算。flink在支持实时计算的基础上，也可以实现离线数据计算，所以说flink也是支持离线和实时数据计算的。在我们这个项目中，既需要离线计算，也需要实时计算，所以单纯的使用mapreduce或者storm都不合适，并且呢，他们两个现在几乎呢已经快被淘汰了。用的非常少啊，所以说我们需要在Spark和flink中进行选择，当时我们在开发这个项目的时候，flink才刚出来。还不是很稳定，并且我们团队内部也是刚开始接触flink，之前我们一直是使用Spark，所以说为了保证项目快速稳定上线，我们当时决定先使用spark，等后期对项目进行迭代优化的时候再考虑使用flink。所以在这针对数据计算，我们选择Spark，最后是这个数据展现模块，数据展现模块不需要我们实现。这块是由安卓开发组还有iOS开发组负责的，我们只需要把结果数据计算好，存储起来就可以。好，最后我们做一个总结啊，就我们前面啊，针对各个模块进行技术选型的时候，大家不要盲目的追星，我们要根据具体的业务场景和不同框架的特点进行选择，同时还要考虑已经在使用的成熟的框架，不要盲目追求一些所谓的好的新的框架，因为技术成本也要考虑。所以说，技术选型不单单是选技术，是要在结合业务场景的前提下进行选择。</span><br></pre></td></tr></table></figure><h2 id="项目整体架构"><a href="#项目整体架构" class="headerlink" title="项目整体架构"></a>项目整体架构</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241727480.png" alt="image-20230424172738182"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">大家好，前面我们把技术选型搞定了，下面我们来看一下项目的整体架构设计。在这里，我把这个项目分为了三个模块，数据采集、数据计算、存储以及数据展现。因为这个计算以后啊，就涉及到存储了，所以说呢，我把这两个划分到一块儿，那接下来我们来详细分析一下这个项目的整体流程。</span><br><span class="line"></span><br><span class="line">首先呢，看这个数据采集模块。数据采集模块的数据啊，包含两大类，一个是服端数据，还有一个是客户端数据。其中服务端数据啊，它里面包含服务器中接口调用时记录的日志数据和数据库中的数据。在这里需要注意一下，针对服务端日志数据的采集，是在提供接口服务的机器上部署filebeat来采集。这样机器会有上百台，在这里我们先用一台server01来表示。这里面DB呢，表示的就是MYSQL数据库。接下来呢，是客户端数据，就是用户使用APP的时候上报的一些用户行为日志。例如打开关闭APP以及呢在APP中的滑动点击等行为，其实呢都会记录日志。这些数据呢，客户端会通过接口定时上报。那接口收到这个请求之后呀，会把请求中包含的日志信息呢，记录到本地文件中，然后使用filebeat进行采集。也就是说呢，我们会在server02上去部署一个接口服务，接收客户端上报的日志数据。那这样其实就可以统一流程了。针对服务端日志和客户端日志，最终啊，都是通过这个filebeat的来进行采集。那filebeat呢，最终把这个数据啊，都采集到这个卡夫卡里面。那针对服务端数据库里面的一个数据啊，我们会通过脚本直接呢，把它导入到hdfs里面。那filebeat的采集的实时数据啊，导入到卡夫卡里面之后呢，还会通过flume。对这个数据进行一些分发处理，以及落盘到hdfs的操作。落盘就是存储的意思。那针对这里面我们刚才所说这个数据分发的一个详细内容，我们在后面开发数据采集模块的时候，会详细分析它的架构。这就是数据采集模块的主要内容。</span><br><span class="line"></span><br><span class="line">那接下来我们来看一下数据计算，还有存储这个模块。计算模块主要呢是利用spark。针对卡夫卡中的数据呢，进行实时计算，针对hdfs中的数据啊，进行离线计算。那在计算的时候呢，它还会和这个noe4j这个图数据库进行交互。既会向里面写数据，也会从里面读数据。最终呢，会把这个spark计算的结果呀，使用sqoop导出到MYSQL里面。针对数据计算这一块，一共有六七种计算指标，具体直接计算指标我们在开发数据计算模块的时候会详细分析。这就是这块的一个流程。</span><br><span class="line"></span><br><span class="line">最后一个呢，是这个数据展现。那在这个模块里面，我们可以看到最终的一个项目效果，这里面其实就是一个手机端的一个项目。这个不是我们的重点。这就是我们这个项目的一个整体架构设计。注意，在这个架构里面其实存在三个主要的问题，第一个针对实时计算，Spark其实不是最优的选择，最好是使用flink。针对这个结果，数据的存储mysql也不是最优的选择。最好是使用redis。针对数据展现这一块，直接查mysql中的数据也不是最优的选择。最好是开发接口。对外提供接口查询数据。不过我们为了快速迭代上线，所以前期呢会使用相对来说比较简洁的架构，先把功能快速上线，后面再迭代优化。</span><br></pre></td></tr></table></figure><h2 id="Neo4j介绍及安装部署"><a href="#Neo4j介绍及安装部署" class="headerlink" title="Neo4j介绍及安装部署"></a>Neo4j介绍及安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">大家好，针对我们前面分析的这些技术组件，只有filebeat和neo4j我们没有使用过。不过非呢比较简单，它类似于在使用的时候主要是写配置文件，所以在后面用的时候我们再具体分析。下面我们就来学习一下neo4j的使用，让大家快速了解它，并掌握它的常见用。</span><br><span class="line"></span><br><span class="line">neo4j，它是一个高性能的图数据库。它和普通的关系型数据库是不一样的，它里面侧重于存储关系数据。针对各种明星之间错综复杂的关系，如果我们使用mysql这种数据库存储，在查询所有人之间的关系的时候是非常复杂的。但是使用neo4j这种数据库只需要一条命令就可以了。neo4j，它是一个嵌入式的基于磁盘的、具备完全的事物特性的持久化引擎。它将结构化数据存储在网络上，而不是表中。注意这块，这个网络，从数学角度我们可以把它称之为是图。这个并不是我们所说的4G网络，5G网络，不是这个意思。</span><br><span class="line"></span><br><span class="line">目前这个neo4j有两种发行版。一个呢是商业版，它是支持集群的，另一个是社区版。这个只支持单机。目前我们这个平台用户量啊，在三四千万这个规模，这个时候呢，我们使用单机也是足够用的。等后期单机无法支撑之后呢，再考虑使用商业版。那接下来我们来看一下neo4j的一个安装部署。用它支持在Windows以及Linux中进行安装，由于在实际过程中肯定是要在Linux中进行安装，所以说在这呢，我们就直接使用Linux环境。</span><br><span class="line"></span><br><span class="line">那下面呢，我们首先来下载一下。在这我已经起先打开了，因为它这个打开比较慢啊，你在这搜new，就这个new.com，这是它官网。接下之后把鼠标放在这个product上面，然后到这看到没有下载new，点那个。进入这个界面之后，注意。点这个。大陆的用户g serve。记下之后，注意，这呢是商业版。我们要用那个社区版的，就是这个。这是免费的。往下面走，你看他现在最新的版本呢，是4.1的，建议的话呢，我们可以往上面走一走，使用它之前比较稳定的是3.5的那个版本。往下边你看。3.5.21建议使用这个版本。看到没有，这是针对linknux或者麦克对吧？下这个是一个table包，如果想在Windows里面运行，你选那个Z。那我们在实际工作中，开发环境肯定是要用这个基于Linux的，所以说我就直接下载这个啊，你点这个就可以了，但是注意。你直接使用这个链接下载啊，很大概率可能会由于网络原因导致你下载失败。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241747028.png" alt="image-20230424174708992"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">呃，建议的话不要用这个链接下怎么办。现在呢，还有一个链接。这个。点，建议大家使用这个链接下载。你直接把这个复制到你的那个浏览器里面，打开就可以，它就会自动开始下载。这个现在呢是比较快。sentence。我在这呢已经卸载过了，到时候呢也会把去的安装包发给大家，所以说你想下的话就去下一下，不想下就算了。</span><br><span class="line"></span><br><span class="line">那这个呢，我们就需要把这个安装包啊，上传到我们的bigdata04机器上面。我呢，提前已经上传过了。到这看一下。我放到对soft这个下面。有这个文件对吧，已经有了。那下面呢，我们呢，先解压。解压之后我们来吸到里面。接下来我们需要修改配置。注意它下面有一个conf目录。嗯。在看下面有一个neo4j.conf这个文件，我们就要改这个配置文件。这里面呢，其实也比较简单，我们只需要改两个地方。dbms.connectors.default_listen_address。这个默认的一个监听地址在这呢，把它打开，你看它这个默认是注释掉的，把它打开。默认0.0.0.0，你使用这个也行，或者呢，我们建议把它改一下，直接改成我们那个bigdata04。除了这之外，还有一个。dbms.connectors.default_advertised_address。把注释呢给它去掉，在这呢，指定到bigdata这样就可以了啊。都是一个末接听地。</span><br><span class="line"></span><br><span class="line">那接下来我们就可以去启动了。退到上一级。现在bin面有一个neo4j这个脚本，后面传一个start，这边启动new。这样就起来了。起来之后呢，你可以通过JPS验证一下。看到没有，它确实有一个进程，对吧，对。这个时候呢，我们还可以访问一下neo4j的一个web界面。bigdata04:7474。看到没有，这样就可以了。你默认进来之后啊，你看它默认呢，会连那个bigdata04，这时候这个端口是7687，注意这个是真正连接neo4j这个服务的。这个端口我们注意的呢，是new负G，它的一个web界面的一个端口。这个需要注意下面需要输入用户名和密码，注意它这里面啊，默认用户名和密码都是neo4j。密码也是一样的。就这。注意你第一次使用的时候，它呢会让你重新去修改密码，因为那个默认的密码不安全。那我们接着把它的密码改成medin。好，这样也可以。注意，只有第一次你在访问你复制的时候，他才会强制让你修改密码，以后呢就不需要了。啊，能看到这个界面呢，也可以说明我们这个逆复瑞呢，是正常启动了。那如果说我们想把它停止掉，怎么停呢？很简单，你启动穿一个start，那停止了就穿一个stop。嗯。这样就可以了。这个进程又没了。这个就是用户队的一个安装部署以及启动行驶。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;neo4j stop</span><br></pre></td></tr></table></figure><h2 id="Neo4j之添加数据"><a href="#Neo4j之添加数据" class="headerlink" title="Neo4j之添加数据"></a>Neo4j之添加数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用neo4j可以很方便的展示一些人物或者事物之间的错综复杂的关系。下面我们来看一张图。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242111885.png" alt="image-20230424211116004"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这张图里面展示了这些人物之间的关系。使用这种展示形式看起来是很清晰的。也方便理解，后期如果说我们想查询某一个人的一个关系链，也是很方便的。这些数据如果让你去使用MY数据存储是很繁琐的，并且呢，查询起来也很烦。那这在这里面呢，有几个概念我们需要明确一下。因为neo4j，它是一个图数据库。我们可以认为它里面存储的呢，都是图数据。这个图到底是一个什么东西呢？注意图呢，它是由点边和属性组成的。我们这个图里面这个圆圈呢，它就是一个点。这里面这个线呢，它就是一个边圆圈中的这个姓名呢，就是属性。以及这个边里面这个值呢，也是属性。就是点和边上面都可以设置属性。对，这个点呢，你还可以把它称为是节点。这个边的话，可以把它称为是关系。就类似于这两个人之间的一个关系。每个节点和关系，它都可以有一个或者是多个属性。比如这上面呢，可以保存多属性。</span><br><span class="line"></span><br><span class="line">那在这里面大家啊，先对这个neo4j有一个整体的认识，下面呢，我们开始具体学习neo4j中的具体使用。在这儿我们主要学习neo4j的以下操作。添加数据、查询数据、更新数据、建立索引、批量导入数据。主要是这五种。那下面呢，我们把这个neo4j给它起来</span><br></pre></td></tr></table></figure><h3 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h3><h4 id="create"><a href="#create" class="headerlink" title="create"></a>create</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">创建一个点</span><br><span class="line">create (p1:Person &#123;name:&quot;zs&quot;&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create (p2:Person &#123;name:&quot;ls&quot;&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create (p1:Person &#123;name:&quot;zs&quot;&#125;) -[:like]-&gt; (p2:Person &#123;name:&quot;ls&quot;&#125;)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242125348.png" alt="image-20230424212534229"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242125430.png" alt="image-20230424212513916"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242129575.png" alt="image-20230424212950366"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p1,p2并不会实际存储，用create创建关系，不会检查之前是否存在待创建关系同名的节点</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">表示每次都创建新的点或者边。第二个表示每次创建点或者编之前呢，会先查询一下，如果存在则不创建。那下面我们就来演示一下。由于在这儿我们需要挑一些密径，所以说呢，在这儿我简单记录一下，这样看起来会更加清晰。a。每次都创建新的点。左边。那我们来在这先创建一个点。嗯。我先把meeting在这写一下，create。注意后面怎么写呢？注意先写一个小括号。英文的啊。冒号后面呢，表示你要创建这个点，你要给它起一个类型，它是什么类型的，表示一个person。还是那些。那你这个person的话，你可以给它设置一些属性，就相当于你在这出现点的时候呢，可以里面给它加些属性，属性怎么加呢？括号括号。我们给它加一个name属性。冒号。后面是这个属性的值，叫张三。那我们在这呢，还可以给它起一个别名P，就类似呢，我们在这创建了一个判对象。判断对象里面有一个内部属性，它的值呢是张三，最终呢，给它起了一个变量的名称叫P。我们来执行一下。放到这里面，点了一个play去执行就可以。添加成功看到没有，创建一个note，创建一个节点，然后设置一个属性。OK，那接下来我们再来创建一个节点。还使用这个。改一下。P2吧。第。把这个拿过来执行一下。那现在的话呢，我们就有两个点了。但是这两个点之间呢，还没有什么关系。那我们想他们两个之间有关系的话，就需要给它们设置一个边了。这个边的话呢，同样可以使用这个create命令来创业。注意，看我怎么实现啊。VISA。对前面的话呢，就类似是这个对吧，把它拿过来。注意，我们想给他设置一个边。就是一个关系。我们想让这个PE啊，这个张三，让他去喜欢李四。表示它们间的关系吗？后面一个横杠中括号冒号，这是固定格式。后面给他写个like，表示呢，张三喜欢李四。一个横杠，一个右键轴。对，这个表示呢，是张三喜欢李四，所以说李四是在后面。对，它这个箭头是往右边指啊，比如他喜欢它，所以呢，在这两个点之间呢，给它设置一个关系叫like。比如张三喜欢的，我们来执行一下。成功了对吧，两个基点。是这两个属性。以及呢，创建了一个关系，或者说呢，是一个边都可以啊。注意，这条命令执行之后，我们可以到这个界面上点这个device。来看一下，你看这是节点，这是关系，这个呢是属性。这是我们创建的person，以及这个like的关系，以及你person里面name的一个属性，在这都可以查看。我们接着可以点那个like。看到没有？张三like李四。这样的话，就可以很清晰的看到他们两个之间的一个关系了。但是呢，这时候呢，你回到这儿来看一下。你看点了一个person。你发现啊，它有四个person。你看两个张三，两个李四。然后你往这个位置看，你看这是内幕李四对吧，所以我们现在啊，选中这个李四和ID，你看是一，那这个呢。李四，它ID是21，看到没有，这个ID是它自动生成的，是唯一的。你看他们两个还不一样，那就意味着这是两个节点，对吧，不是同一个，虽然说他们两个名字一样，但是他们不是同一个极点。那这是什么原因呢？注意，因为这个create呀，它每次呢都会创建新的激烈或者关系。所以说呢，最开始啊，我们使用这个create，你看创建了两个节点，P1还有P2，对这个P1还有P2这个东西它不会扯到就里面。这个以及这个name对应的值是会存储到里面的，这个相当于你给它起了一个别名而已，这个东西不会存进去。那我们之前你看在这创建了一个P1，又创建一个P2，现在我们出现了两个P对象。这是新建的。那接着呢，我们使用这条命令，看到没有。它相当于又创建了两个，这个杠三和离子。所以说呢，你这时候在你的纽扣针里面就有四个。那其实啊，我们在这是想给最开始创建的这两个机制增加一个这个like关系的。因为在实际过程中也会有这种需求，就是节点已经存在了，需要我们后期给他们指定关系。你这种写法，它相当会重新生成。那肯定是不满足我们需求的。这个时候该怎么做呢？咱们前面说了，谬杯里面除了有这个create meaning，它还有一个me meaning。这个命令呢，表示啊，在创建几点之前都会先查询一下，如果存在则不创建。我。这个命令。在创建节点之前都会。先查询一下。如果存在。则不创建。所以说这个默认命令啊，你就算是重复执行，他也不会产生重复的结果。注意你这个奎的命令。你重复执行，你执行一次，他就给你创建一个这个person这个节点。这个需要注意一下啊。那我们看一下me的话，后面的写法是一样的。后面这种写法是一样的。现在我要看起来清晰一些，我给它起个别名叫P3吧。其实都无所谓啊。注意那下面呢，我再写一个，这是P4对吧。这个名字给他改一下吧，这个叫Jack。同时呢，我们想要这个Jack呢，去like to。这样写much。第三。like。kiss。这样写就可以了。现在呢，我先复制这一行，拿过来来执行一下。拖延成功了对吧，创建一个节点，设置一个属性，那你说进行明我再执行一下，你可以直接点那个。再执行一下。看到没有，没有改变。点着来确认一下。看到没有，还是一个，所以说这个默认命令啊，你重复执行，它是不会重新创建的，因为在这的话，它呢，会根据你在这使用的这个名称去查一下，看看有没有重复的，如果有的话，他就不再创建了。所以说呢，在工作中啊，建议使用这个墨。可以避免重复。注意。这条命令你说我单独执行行不行啊不行，你必须要保证这条命令和前面两个一块来执行，因为在这这个变量，我前面说变量它是不会存到u里面的。他呢，只在当前绘画有效，所以说呢，这三条命令需要一块儿来执行。把它们放到一块儿，这样来执行就可以。来确认一下。看到没有，Tom Jack。你点那个也是OK的啊，一样的。这个。like to。对吧，我们记住那个person的话，等于之前我们是四个，现在又加了两个，一共是六个，没有问题。在这里其实还有另外一种写法，如果节点已经存在了，我们只需要创建关系，我们还可以使用那个match来实现。再让我们先简单用一下。那。那么可以查询之前。已有的。节点或者关系。那其实就是电或者是编了嘛。一样的意思啊。那接着呢，我们就想要这个Tom和这个也产生一个like关系。对，你看之前的话呢，是这个。Jack like Tom，那现在的话，我们想让Tom也去like Jack一下，互相习惯，这样的话就不是单相思了嘛，对吧。单相思最难受。所以说我们在这呢这样来做，使用ma先查询那个节点，因为那个节点之前已经创建过了，对吧。可以这样，那我们要给他起个名字叫a吧，还一个person。他的name呢？the Tom。对吧，我们要查两个人啊，要把这个汤还有这个都得查出来，对吧，要找到这两个人，重新再给他们加一个关系。B。name。Jack。把他们两个都查出来，注意，查出来之后注意。a。然后a就是Tom，然后Tom呢去like。B注意这里面这个a还有B，只是为了在这去使用，没有其他含义，你给它起个什么XY也是可以的。来我们来执行一下，注意他们两个也需要一起来执行，要不然那你这个a他是找不到他的。好，执行成功，你看创建了一个关系。这个还是六对吧，没有变。只不过这时候你看没有，汤姆也喜欢Jack克，Jack克也喜欢汤姆，他们两个就互相喜欢了。所以说呢，我们就可以通过match呢，去查询之前已有的机械信息，然后再通过墨创建关系就行。这样也不会额外产生重复的节点。所以说呢，这两种方式啊都可以，你使用这种方式也行，使用这种方式也行。按需选择即可。效果是一样的。</span><br></pre></td></tr></table></figure><h4 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个命令在创建节点前都会先查询一下，如果存在则不创建</span><br><span class="line"></span><br><span class="line">merge (p3:Person &#123;name:&quot;jack&quot;&#125;)</span><br><span class="line">merge (p4:Person &#123;name:&quot;tom&quot;&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">merge (p3) -[:like]-&gt; (p4)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242135053.png" alt="image-20230424213515502"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242135568.png" alt="image-20230424213535288"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">把上述三条命令一起执行</span><br></pre></td></tr></table></figure><h4 id="match"><a href="#match" class="headerlink" title="match"></a>match</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">可以查询之前已有的节点(点)或者关系(边)</span><br><span class="line"></span><br><span class="line">match(a:Person &#123;name:&quot;tom&quot;&#125;),(b:Person &#123;name:&quot;jack&quot;&#125;)</span><br><span class="line">merge (a) -[:like]-&gt; (b)</span><br><span class="line"></span><br><span class="line">这种a,b只是别名，只在当前会话有效；这两个要一起执行，不然第二个命令找不到a,b</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242143938.png" alt="image-20230424214333671"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242144207.png" alt="image-20230424214401568"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">之前jack-&gt;tom</span><br><span class="line">现在jack&lt;-&gt;tom</span><br><span class="line"></span><br><span class="line">等同于之前的三条merge命令</span><br></pre></td></tr></table></figure><h2 id="Neo4j之查询数据"><a href="#Neo4j之查询数据" class="headerlink" title="Neo4j之查询数据"></a>Neo4j之查询数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下用户类中如何查询数据。针对这块我们主要学习以下内容，首先呢，学习一下这个match和return的用法，它们呢可以实现查看满足条件的数据，并且返回。以及最后我们会讲两个案例，如何查询二度关系和三度关系。</span><br></pre></td></tr></table></figure><h3 id="match-return"><a href="#match-return" class="headerlink" title="match+return"></a>match+return</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match(p:Person &#123;name:&quot;tom&quot;&#125;) return p</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">咱们前面呢说过这个match啊，它可以进行一个查询，下面咱们就来继续使用一下。这个呢，其实有点类似于mysql中的select。在这需要注意一下，match不能单独存在。咱们前面在使用的时候，那后面跟着也是有一个merge命令的。如果我们只想查询一些数据，并且把这个数据返回过去，呃，如何实现呢？</span><br><span class="line">就可以使那个match加return。就是查看满足条件数据，并且返回，那下面呢，我们就来查询一条数据。现在我们想查询一下这个Tom这条数据。nice。PAR。指定属性name。对吧，我们就想查他，那查出来之后呢，想要把这个结果啊给返回，怎么返回呢，后面加个return。这样这样就可来我们来执行一下。没问题吧，查出来了对吧，把那个汤姆查出来。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242150553.png" alt="image-20230424215018223"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们来查询一些复杂一点的内容。首先呢，在这我们来初始化一些数据。好，这些数据呢，刚才我已经把它复制过来了，就这些数据，注意这里面创建点的这些操作和创建边的操作需要在一个会话里面一起执行啊。否则它是无法识别这些变量的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242152676.png" alt="image-20230424215221280"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在这你看其实相当于我们初始化ABCXYZ是吧，这几个用户。在他们之间呢，给他加了一些关注关系，其实就类似于直播平台里面用户和主播之间的一个关注关系。好在这把这批数据给他做一下初始化，直接复制过来。好创建成功对吧，六个节点六个属性六个关系。那首先呢，现在我们要做一个查询。假设呢，这些都是主播，你看a follow了B对吧，a关注了B，那B的话，我们可以认为它是一个主播对吧？那所以说呢，这样我们要查询某个主播的粉丝信息。我们就查询这哥们儿，你看有这个a和C都关注了他对吧，他的名字呢叫B。B。好，那这时候怎么实现注意。查询嘛，使用。name。我们要查询这个用户，他的一些粉丝。注意看我下面怎么写，相当于啊，是别人关注了他，按照我们之前教的这种写法，就类似于这种，你说把这个拿过来，注意这个箭头是往右指的，也就是说呢，是他操作了别人，他关注了别人。我们现在要查的是谁关注了它，所以说了这个方向不是往右的往左。对吧？我们要查询哪些人关注到这个user b。所以后边的话呢，零。有的。这时候它后面呢，就不需要加这个括，括号里面也不需要指定什么属性了。相当于我们就要查询到底是谁关注了这个user b。这个人具体是谁，我们现在还不知道呢，所以说呢，后面也不需要加一些具体的限定。OK。那个范围就可以。来，我们来执行一下。看到没有a和C。你回过头来看一下。AC对吧，你看。a的话，它这个名字就是大a嘛，对吧，这是大C没问题。所以说呢，我们是可以查出来的。那其实这种写法呀，它还有一种写法。还有一种写法，矢量。mass。把他们反过来。把它放到前面。这个不是注意改一下。对吧。就是谁去follow了这个user。这样写也是可以的。如果说你感觉这种写法比较别扭，你可以用这种写，对吧，谁关着B。这样把它返回过来就可以，效果是一样。11下午啊。如果说我们只想返回满足条件的那个粉丝的一个name值，你看这个相当于它整个把这个几点都给返回来，如果说我们只想返回它里面这个内的属性的值怎么办呢？也简单。值返为零。没问题吧，也是可以的。好，这个其实啊，就是我们要查询的那个主播的二度关系。为什么这样说呢？我们来分析一下啊。你看这个时候呢，是这样的我。这个呢，是这个主播B。后面呢，是主播B的一个粉丝。那这个时候我和这个主播B的粉丝，我们之间是不是就属于一个二度关系呢？因为我和主播B我们之间呢，是一度关系。主币和粉丝之间呢，也是一度关系，但是我和这些粉丝之间就属于二度关系，我们是通过这个主币来认识。好，那我们在这个项目中呢，是想实现三个关系推荐。也就是说呀，当我要关注某个主播的时候，你呢，要给我推荐这个主播的粉丝，又关注了哪些主播，你把那些主播推荐给我。因为我和那些主播之间才属于三种关系。二的关系。三度关系。看到没有，这个时候我和主播N之间就属于三种关系。看到没有，我和他是一组，和它是二度，和它呢也是三组。那这个三的关系该如何查询呢？其实咱们刚才这个呀，查询的就是二楼关系，其实你只需要把这个主播币的粉丝查出来就行了，后期谁去关注主播币，那这个粉丝和那个人，他们之间是不是就是二度关系。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242153262.png" alt="image-20230424215318019"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查询某个人的粉丝</span><br><span class="line">match (:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (n:User) return n</span><br><span class="line"></span><br><span class="line">另一种写法</span><br><span class="line">match (n:User) -[:follow]-&gt; (:User &#123;name:&quot;B&quot;&#125;) return n</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242156274.png" alt="image-20230424215640635"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查询某个人的粉丝只返回name(属性)值</span><br><span class="line">match (:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (n:User) return n.name</span><br><span class="line"></span><br><span class="line">另一种写法</span><br><span class="line">match (n:User) -[:follow]-&gt; (:User &#123;name:&quot;B&quot;&#125;) return n.name</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1%5Cimage-20230424215940138.png" alt="image-20230424215940138">)<img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242159113.png" alt="image-20230424215940184"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这些属于二度关系(查询我关注的人的粉丝的信息)</span><br><span class="line">我-&gt;主播-&gt;粉丝</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">三度关系(给我推荐我关注的主播的粉丝关注的人)</span><br><span class="line">我-&gt;主播-&gt;粉丝-&gt;主播N</span><br><span class="line"></span><br><span class="line">match (a:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt;(c:User) return a.name as aname,b.name as bname,c.name as cname</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242213902.png" alt="image-20230424221314630"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那我们现在想查三度关系，那其实就是查出来主播B的粉丝又关注了哪些主播。只要把这些数据查出来就可以了。我们返回一下。看到没有？这是主播B，有这么几个粉丝啊，A和C看到没有？他有a和C这两个粉丝，分别关注了XYZ这三个主播，其中呢，A和C这两个粉丝呢，都关注了Y这个主播。那这个时候你再给我推荐深度关系的时候呢，就应该从这里面去挑了，那这个时候是不是应该把这个B的粉丝关注比较多的主播推荐给我呢？你看a和C都关注了Y，你是不是应该把它推荐给我呢？在这里，理论上来说，Y这个主播最有可能是我喜欢。这个Z和X呢，它那个可能性啊，就没那么大，所以说呢，在这里面啊，我们在获取这个三度关系的时候呢，针对这个c.name里面这个结果呀，最好呢是做一下过滤。我们统计一下c.name里面相同主播出现的次数。然后呢，按照倒序排序。最终再取一个topN是不是就可以了？重合度越多的说明了越有可能是我喜欢的。因为相当于我去关注这个主播B了，相当于我喜欢主播B。你看a和C这两个粉丝也关注他了，说明他们两个也喜欢他，那有可能我和这个a还有C这两个粉丝这个口味是一样的。那他们两个同时呢，又都关注了Y这个主播，所以说呢，Y这个主播也是最有可能是我喜欢的。所以说呢，这个其实就是三的关系，最终想要达到一个效果。</span><br><span class="line"></span><br><span class="line">好，那根据我们刚才分析，你在这儿还想对它做一个什么聚合，对吧，再做个排序，这东西怎么实现呢？我们来看一下。其实这个麦呀，后面。也支持什么抗就是抗函数。奥特曼。排序的以及呢，这个厘米上对吧，取多少条这些命令啊，都是支持的。那所以说呢，在这个基础之上，我们可以做一些调整。所以这时候你要把这个去掉。只保留这个a name，还有c name就行。然后后面注意你后面返回多个列的话，中间有多少个开啊。抗的星。也是做一个求和啊，后面order by。some。DSC倒序排序。那我们再做一个厘米。倒序排序之后呢，我们取前几条，这样不就是top n了吗，对吧。来，我们在这儿实现一下。看到没有？他最终统计的这个数量，你看对不对。这个Y西里Y嘛，是吧，两次。Z是一次，X是一次对吧，没问题吧。没问题，那所以说这个时候你其实可以把这个稍微改一下，你改成厘米则点一吧，因为这两个值都是一样的，我们就厘米的一就取了一条。没问题吧，是可以的。注意你这里面啊，你用康兴也行，或者说呢，你用那个什么呀，这种写法。对吧，这样也可以啊，都是一样的效果。要消毒一下。可以使用。它或者它效果是一样的。OK。那其实这里面啊，你看这里面就相当于我们根据这个a，还有这个c name去做一些分组。然后呢，使用抗的。去做了一个求和统计，每组的一个数据行数。好把这个加个a是吧，少了一个a啊。这是一样，这只是一个变量名称啊，无所谓。改过来之后呢，看起来顺眼一些对吧，有强迫症的话，感觉这里面少一个字母，感觉很难受对吧。好，这就是二度关系，还有三度关系的一个查询了。对，这里面呢，其实啊，我们还可以使用where去加一些过滤条件。就实现一下过滤。注意。你想使用where也可以啊。这个where需要放在。return前面因为你这个return啊，就直接返回了呀，你这个where啊，肯定是要放到return之前的，先过滤再返回嘛，对吧。把这个复制过来。注意现在前面加一个外过滤。我们过一下，where name。对，你在这还不能用那个C里，C里姆是在后面定义对吧，我们前面的话还只能用C点内。不等于X吗？把X这个过滤掉行吗？你先回到这儿。对吧，还是0.3。你看现在这个C里是不是有一个X呀。好，我们在它基础上加列过滤。对吧，C点内就是不等于X对吧。看到没有，只有Y和Z啊。OK，所以说呢，这里面也是可以用这个where或者条件的。OK，这个其实就是我们这个没忽略里面的一个查询操作啊。注意这里面这个查询语法呀，其实啊，就是用户这里面的S法语法，这个塞法语法呀，在查询的时候，你看其实有些地方它和circle那个查询还是有点类似啊。所以说呢，这种写法还是比较简单易用的，最起码看起来是比较清晰的。很直啊，所以说呢，我们学起来上手也很快。这就是我们当时为什么选择这个newd啊，这个查询语言确实用起来比较方便。</span><br></pre></td></tr></table></figure><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实match后面也支持count()、order by、limit等命令</span><br><span class="line">match (a:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt;(c:User) return a.name as aname,c.name as cname,count(*) as sum order by sum desc limit 3</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242224375.png" alt="image-20230424222424329"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：这里count(*)等同于count(cname)</span><br><span class="line"></span><br><span class="line">相当于对aname,和cname做了分组，在操作</span><br></pre></td></tr></table></figure><h4 id="where"><a href="#where" class="headerlink" title="where"></a>where</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：where放到return之前</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实match后面也支持count()、order by、limit等命令</span><br><span class="line">match (a:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt;(c:User) where c.name &lt;&gt; &quot;X&quot; return a.name as aname,c.name as cname,count(*) as sum order by sum desc limit 3</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242231619.png" alt="image-20230424223142375"></p><h2 id="Neo4j之更新数据"><a href="#Neo4j之更新数据" class="headerlink" title="Neo4j之更新数据"></a>Neo4j之更新数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下如何在应用中去更新数据。更新数据这块啊，其实总结一下有两种情况。第一种呢，就是更新节点的属性，使用match和set实现，你把它先查出来，然后呢使用set命令去修改。第二种啊，就是更新节点之间的关系，也就是边。这个其实就是删除边。我们使用那个ma和D的视线把它查出来，把它删掉。其实呢，你使用这个match和吉delete也可以实现删除节点。对吧，我们把这个节点查出来，然后把它删掉。那下面呢，我们就来演示一下。首先我们看一下就是如何修改节点中的属性。那。有了。name。我使用那个X这个用户吧，行吧。把它查出来之后呢，后面sa.H。等于八，注意如果说它里面没有这个属性，那就把这个属性给加上去，如果有这个属性了，那把这个属性值改成18。看到没有添加一个属性。现在我们可以查一下它。你看a。看到没有，它里面一个name是XH是什么？具有刚才我们给他加了一个属性啊。那接下来我们看一下如何删除关系。match。然后呢？name。a，对，你前面这个变量呢，你能用到了，那你就给它起个变量，如果你用不到，那你就不用起。我们在这呢用不到，所以说就不给它起变量，变成清不洗都无所谓啊。follow。我们看一下之前那个数据这个a。你看它其实呢，关注了BXY对吧，那我们随便找一个吧。name。X吧，对吧，它对它呢也有一个分关系。注意，我们最终啊，想把他们两个之间那个follow关系给它删掉。那怎么办，这时候啊，你要给这个合作关系啊，也起一个别名。起个名称，这样的话在后面呢，使用第一层。这样就可以把它给删掉。看到没有，删除了一个关系。点包。这时候这个a是不是就没有关注那个X了呀。你可以到这儿来查一下。看到没有对吧，X它现在就没有人去follow。就变成一个孤家寡人了，对吧，又没有连到这里面。OK，这就是用户内容针对更新数据的相关操作。如果说你想去删除一条数据啊，就删除一个节点，那你把它查出来对吧，把它查出来后面呢，直接给它就类似这种。前面呢写这个，后面呢加上一个a，就可以把这个name等于X的这个user给它删掉。这个呢，给大家留一个作业，下一周我们自己操作一下行吧，我这个呢就不再演示这个。</span><br></pre></td></tr></table></figure><h3 id="更新节点属性"><a href="#更新节点属性" class="headerlink" title="更新节点属性"></a>更新节点属性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">match (a:User &#123;name:&quot;X&quot;&#125;) set a.age &#x3D; 18</span><br><span class="line"></span><br><span class="line">有更新，无则添加</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242247278.png" alt="image-20230424224750421"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242248429.png" alt="image-20230424224819117"></p><h3 id="更新节点之间的关系-边"><a href="#更新节点之间的关系-边" class="headerlink" title="更新节点之间的关系(边)"></a>更新节点之间的关系(边)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match (:User &#123;name:&quot;A&quot;&#125;) -[r:follow]-&gt; (:User &#123;name:&quot;X&quot;&#125;) delete r</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242250704.png" alt="image-20230424225000475"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242250214.png" alt="image-20230424225026098"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意:删除节点</span><br><span class="line">match (a:User &#123;name:&quot;A&quot;&#125;) delete a</span><br></pre></td></tr></table></figure><h2 id="Neo4j之建立索引-批量导入数据"><a href="#Neo4j之建立索引-批量导入数据" class="headerlink" title="Neo4j之建立索引+批量导入数据"></a>Neo4j之建立索引+批量导入数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下用户中的索引。你有过滤动作，所以可以细分为两种。第一种呢，是普通索引。使用create index可以实现。指定给节点中的某个属性建立索引，具体建立索引的依据啊，就是后期我们在查询的时候，是否需要根据这个属性进行过滤，如果需要则建立索引，如果不需要则不建立索引。</span><br><span class="line"></span><br><span class="line">还有第二种，我们称之为是唯一约束。这种啊，就是类似于MYSQ数据库主键的一个唯一约束。使用这个create constraint来实现。那你说这两种在使用的时候具体该如何选择呢？注意，如果某个字段的值是唯一的，并且后期也需要根据这个字段进行过滤或者查询。那么我们就可以建立唯一约束，唯一约束的查询性能比索引更快。在这我们先不演示如何建立索引。等下面我们在学这个批量导入数据的时候会用到这块内容。</span><br></pre></td></tr></table></figure><h3 id="普通索引"><a href="#普通索引" class="headerlink" title="普通索引"></a>普通索引</h3><h3 id="唯一索引"><a href="#唯一索引" class="headerlink" title="唯一索引"></a>唯一索引</h3><h3 id="批量导入数据"><a href="#批量导入数据" class="headerlink" title="批量导入数据"></a>批量导入数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们将学习一下用户内容如何批量导入数据，针对项目一开始的时候有一批海量数据需要导入，我们就不能使用前面讲的那种命令一条一条导入了，性能太差，我们需要有一个批量导入的方式来快速导入这一批数据。neo4j批量导入数据有两种方式，一种是这个batch import的，还有一种呢是load csv，第一种这个batch import，它呢需要组装三个文件，导入性能呢比较快，但是呢比较麻烦。第二种这个load csv呢，它呢只需要把数据组装到一个CSV文件即可。导入性能呢，没有这个外的快，但是也没有我们想象中的那么慢，还是可以接受的啊，它的优点呢，就说使用起来很方便，直接把所有需要的数据直接都组装到一个CSV文件即可。那在这里啊，我们考虑到一个应用性。由于我们的原始数据都在MYSQL中，我们可以通过MYSQL命令直接把数据导出为一个文件。所以接着呢，我们直接使用load CSV会更加的方便。但是在这有一点需要注意。在load csv中啊，我们如果使用到了merge或者match这些命令的时候，我们需要确认关键字段是否有索引，否则呢，性能会很差，怎么理解呢？来看一下。就针对这种match，或者咱们前面讲的那种merge。你这个merge，它在执行的时候，其实呢，它会根据这个name看看有没有这条数据，对吧，如果没有的话，它才会新增，如果有的话，他就不会再新增了，所以说呢，它需要根据name这个字段去查询数据，那所以说呢，你就需要根据name了。去建立索引，如果你没有建立索引，后期用户内容数据量大之后，这块平行效率会很差。以及这个match也是一样的，match里面你看没有，你这个其实是根据那个name去查的。所以这时候的话，这个name字段对吧，它也是需要有索引。以及这个where后面这些过滤条件，对吧，也是需要有索引。如果这些关键字段没有建立索引的话，那其实这些操作它就相当于是一个全表扫描了，所以说呢，你数据量越多，它的查询性能会越差。那下面呢，我们就来演示一下load csv它的一个使用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接着啊，我们先把这个neo4j的数据啊给它清空了，那如何清空数据呢。接着呢，就给大家一种暴力的方式啊。你把它的data目录给删了。因为他的所有数据啊，都放到那个date目录里面。然后呢，重启一下neo4j。stop一下。再启动一下，那接着呢，我们来看一下，我准备了一个测试的一个数据文件。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">fuiduid</span><br><span class="line">10011000</span><br><span class="line">10011004</span><br><span class="line">10011005</span><br><span class="line">10012001</span><br><span class="line">10021000</span><br><span class="line">10021004</span><br><span class="line">10022001</span><br><span class="line">10031000</span><br><span class="line">10031004</span><br><span class="line">10061000</span><br><span class="line">10061005</span><br><span class="line">20021004</span><br><span class="line">20021005</span><br><span class="line">20022004</span><br><span class="line">20031000</span><br><span class="line">20031005</span><br><span class="line">20032004</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个文件里面一共有两列。fuid是关注者。uid是被关注者。你可以把它认为是主播，这是观众对吧。那我们需要把这个数据啊，给他做一下初始化。那怎么初始化呢，注意。你想要对这个数据做初始化的话，你首先啊，要把这个数据文件上传到neo4j的一个import目录下面才可以使用。注意你必须要放在这个目录下面才能使用啊，否则neo4j会找不到。这个需要注意一下，那下面呢，我们就把这个批量导入命令啊，先给他写一下。</span><br><span class="line"></span><br><span class="line">批量初始化数据。注意现在呢，首先啊，我们对它里面的一个关键字段啊，要建个索引。再来看看这个数据。我们需要在用中去维护他们之间的一个关注关系。那我们首先呢，需要在六中去创建这些节点。接节点名，其实啊，主要有一列啊。对吧，其实就是这个UID这些列先出现节点，然后呢，再维护这些节点之间的一些关系，谁关注了谁好，那所以说现在呢，我们在出现索引的时候呢，其实呢，就可以给一个user里面啊，一个UID的一个字段，再创建一个唯一索引，你看也是唯一的。a。啊。哦。有点。起个笔名叫小写的优点对吧。一二。然后呢？UID。is。这样也可以。这里我也。字段建立索引。那接着呢，是这个批量导入语句，把这个批量导入语句你给他写一下。using。PO。commit。1000。好，下面。no。CSV是不让我们去读取一个CSV文件啊。注意你看这个文件里面啊。它第一行是不是表头啊，表头的话其实我们是不需要的。那所以说你在这来指定一下with。hi。表示了你在读取这个文件的时候呢，它是有表头的，有忽略第一行就行，然后from这个文件的一个地址。第。表示了读取一个本地文件啊，后面呢，直接就是follow，它的名字叫follow。DEMO。你只要放到你后那个一的木下面这块，你直接写一个斜线对吧，从根下面去读就可以读得到。爱烂。这样呢，那下面其实又具体我们这个me与。比。这表示一个关注。还是一个u的，主要是一个普通的一个用户啊，形成这个UID。烂点ID。这个表示观众嘛，这一列表示观众，这一表示是具体的一个主播。所以说先创建它使用。因为它这两个你看它们有重合，所以说你在这需要使用墨，你要使用create的话，它就重复了。改成。注意，我不在这读。我在哪儿录，在这录。我们推到上一幕。可以呀。在闭幕下面有一个S。吧，S杠share这个脚本后面通过杠a来指定这个地址，其实就是BOO。括号，然后第这次。零四。7687。冒号啊，端口是7587，然后后面指用户名杠u，然后呢杠P。我的命。你说你这个地址是到哪找的呢？其实它在这呢，显示的也有，你看没有。就这。就是这样我们就连进来了，那这里面我们就可以使用这个SFA查询语言去操作我们的new了啊其实啊。他就和他是类似的。你这个只不过是一个页面版的，这个呢是运行版。在这把我们刚才写的命令组织过来。好，创业成功。接着来使用那个。注意一定要确认，你提前把这个文件放到六后缀的引port目录下面，你看。添加了11个节点，创建了17个关系，设置了11个属性，对吧。那这时候呢，我们再来。看一下。看到没有？点了一个哦。这个呢，就是最终我们把那个数据初始化之后，就是这样。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">批量初始化数据</span><br><span class="line"></span><br><span class="line">针对关键字段建立索引</span><br><span class="line">create constraint on (user:User) assert user.uid is unique;</span><br><span class="line"></span><br><span class="line">批量导入语句</span><br><span class="line">using periodic commit 1000</span><br><span class="line">这什么意思呢？注意这个表示呀，可以是每1000条起交一次，这个表示设置一个事物提交的一个大小啊。这个参数呢非常关键，如果说你这个数据量非常大，那你把这个全量的数据全部都读出来，全部都放到内存里面，这样的话内存可能扛不住，所以说呢，建议呢批量去提交事务。这样可以减小任务失败的风险，并且呢，也可以提高数据导入的速度。当然，这需要设置一个合适的数量，这个数量太大或者太小其实都不合适啊，就类似于我们平时往mysql里面批量提交数据一样，提交数据也是一批一批的。</span><br><span class="line">load csv with headers from &#39;file:&#x2F;&#x2F;&#x2F;follower_demo.log&#39; as line fieldterminator &#39;\t&#39;</span><br><span class="line">注意这个其实呢，就相当于从本地这个根目录下面读取了。我们之前把这个文件放到那个neo4j的import目录下了，注意你只要放到了import目录下面，那其实呢就是相当于是从根目录读取，这个是neo4j来设定的。他就会读取这个文件里面内容，一次读一行，一行数据这个字段之间是分隔符这指定一下。那这样的话，其实前面这两行呢，基本就把这个功能属性设置好了，</span><br><span class="line"></span><br><span class="line">merge (viewer:User &#123;uid: toString(line.fuid)&#125;)</span><br><span class="line">把那个第一列取出来，注意外面这个呢，我们使用的是一个tostring，它是一个函数啊，你本来一读出来之后呢，这个是一个数字，我们要把它转成一个字符串啊，因为本身我们这个UID就是一个字符串。</span><br><span class="line"></span><br><span class="line">merge (anchor:User &#123;uid: toString(line.uid)&#125;)</span><br><span class="line">下面这个呢，是一个主播anchor。</span><br><span class="line"></span><br><span class="line">merge (viewer) -[:follow]-&gt; (anchor);</span><br><span class="line">那下面把他们之间的关系给watch。V。WS。安。这样就可，那我们来执行一下这个命令，注意这个命令呢，你可以在这个外部界面去执行。在这执行也可以啊，但是在这执行的时候呢。你执行这条命令可以直接执行，但是你在执行这个时候，它会提示让你在前面加一个什么auto，有一个自动提交事务。这是一种方式，这个给大家留个作业，下一周呢自己实验一下，我呢先不用这种方式。我用哪种方式呢？我就直接在我们的雷命令行里面去做。注意咱们之前不是把这个六库率给它重新删了相，那重新启动了吗？现在于是一个新的六扣率了，那所以说你在这啊。还需要去修改一下密码。相当于我们把那个对的目录删了之后，它就是一套新的东西。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;cypher-shell -a bolt:&#x2F;&#x2F;bigdata04:7687 -u neo4j -p admin</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242334611.png" alt="image-20230424233404885"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242334532.png" alt="image-20230424233420853"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242335175.png" alt="image-20230424233527600"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242337963.png" alt="image-20230424233725604"></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那这个大家有没有感觉到有一些看起来不太一样的地方？咱们前面在创建这个节点的时候，你看节点上面是不是显示的那个内蒙的一个值啊。那你说我现在其实也有一个UID的值啊，你为什么这边没显示出来呢。注意了啊，之前啊，我在用这个纽破利的3.2那个版本的时候呢，是没有这个问题的，那现在呢，使用新的这个3.5这个版本之后啊，发现它这个页面显示的时候呢。会出现这种现象。这种现象啊，倒也没什么影响啊，就说了我们在页面中看起来啊，不太方便而已。那通过我的测试发现呀。我们在添加节点数据的时候呢，如果说你给这个节点啊，指定的一个内部属性，那么内部属性的值默认会显示在这个圆圈里面。如果不是内部的一个字段。就不显示你现在有这个UID字段，不显示这个呢，相当于是这个新版本的一些特性啊。不过这倒不影响啊，只不过说在这看起来啊，有点不太习惯，它这个数据呢，你看它其实存点对吧，UID这个值都是有的啊。好。下面呢，我们来验证一下，我们把UID这个属性的一个名称啊，给它改一下，把它改成name，看看这个值啊，会不会显示到这个圆圈里面。那个只是一个显示形式而已啊，我们来验证一下。验证呢很简单，把这个复制出来一份。对吧，然后在这呢改一下。看了什么？这个测试的。这块呢，这个属性名称改成name。这个也是name对吧。好，这个时候呢，我就在这里面来执行，注意我直接拿过来执行啊，它其实呢会报错。先看一下。看到没有？我搞错了。它下面有个提示。就说啊，你需要加一个什么呀，冒号凹凸。这样才行。所以说呢，也就意味着在它前面啊，加一个冒号。而是自行提交。这就可以了。该成功了，你看这又有了。这样的一个test。看到没有，这样就显示了。是什么？遇到这个问题啊，也不要太感到惊讶，这个只是在新版本上做一些改动，之前那个老版本是没有问题的，就是我们之前线上那个版本是OK的啊。后来呢，给大家在这讲的时候，我们用了一个新的版本，稍微新一点就有一些变化啊。其实两个效果是完全一样的啊，这个只是在这显示而已啊。就是看起来清晰一些，这样可能看起来不太清晰啊，有点B。那这样的话，我们现在就实现了一个批量数据的一个初始化，其实就很简单了，现在呢，我们后期啊，可以把我们想要初始化那些数据啊呃，提前导成这种文件。然后在这写一个这个P，触发一个脚本就OK。所以说呢，那CSV这种方式还是比较方便的，你直接把你需要的数据全部都组装到这一个文件里。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-Spark Streaming-6</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-6.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-6.html</id>
    <published>2023-04-23T14:22:49.000Z</published>
    <updated>2023-04-24T03:11:56.136Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-Spark-Streaming-6"><a href="#第十一周-Spark性能优化的道与术-Spark-Streaming-6" class="headerlink" title="第十一周 Spark性能优化的道与术-Spark Streaming-6"></a>第十一周 Spark性能优化的道与术-Spark Streaming-6</h1><h2 id="SparkStreaming-wordcount程序开发"><a href="#SparkStreaming-wordcount程序开发" class="headerlink" title="SparkStreaming wordcount程序开发"></a>SparkStreaming wordcount程序开发</h2><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面我们来学习一下Spark中的Spark streaming。针对Spark Streaming，我们主要讲一些基本的用法，因为目前在实时计算领域，flink的应用场景会更多。Spark streaming啊，它是Spark Core API的一种扩展。它可以用于进行大规模、高吞吐量、容错的实时数据流的处理。大家注意这个实时啊，属于近实时。最小可以支持秒级别的实时处理。</span><br><span class="line"></span><br><span class="line">那spark streaming的工作原理呢？是这样的。它呢会接收实时输入的数据流，然后呢，将数据啊拆分成多个Batch。比如呢，每收集一秒数据给它封装为一个batch，然后将每个batch呢交给这个spark计算引擎进行处理。最后呢，会产生出一个结果数据流。这个结果数据流里面数据呢，也是由一个一个的batch所组成的，所以说呢，spark streaming的实时处理，其实呢，就是一小批一小批的处理。那下面呢，我们就来开发一个spark streaming的实时wordcount程序来感受一下。</span><br><span class="line"></span><br><span class="line">现在我们来创建一个项目。it。点击这个auto。把它那个基本环境再配置一下，因为我们在这呢，也要写了一个SC，所以说在这点右键。到这儿。在这下面创建一个GALA。对吧，然后注意右键把它视为S对吧。接下来到这个depends里面，注意添加那个scholar的SDK，注意针对这个我们需要添加02:11的。因为我们使用的那个卡夫卡集群，它那个是02:11编译的啊。那个SC的版本，所以说呢，在这儿使用02:11。好，这样就行了。那下面呢，我们需要找一下它对应的一个依赖，SPA，人命的依赖。现在我们来说一下。就那个。用02:11的，注意我们之前用的是2.4.3这个版本。S。把这个除掉是吧。好。那这样基本环境就OK了。下面呢，我们来开发这个word的程序，在这呢，先建一个package。I1克点。八。the stream count。需求呢，是这样的。通过socket。模拟产生数据。实时计算数据中。单词出现的次数。这个没方法。好，那在这注意，我们需要先创建一个streaming。context。然后呢，指定数据处理。间隔。所以五秒吧。因为我们前面说了，你SPA死命，他这个实时处理，其实还是一小批一小批的处理，所以说你需要指定它这个一小批这个间隔是多少秒。那现在我们直接利用一个streaming。这边呢，首先传一个。配一样康复。注意第二个呢，才是这个距离的时间叫。五秒。SSC吧。注意，那我们在上面来创建这个。mark。康复配置对象。嗯。嗯。了，我们现在本意来执行。注意咱们之前啊，开发这个发个离线代码的时候，我们呢，穿的都是logo对吧，注意这时候呢。你需要这样来写LOGO2。什么意思呢？所以。止住了。LOCAL2。表示启动两个进程。一个进程。否则读取。数据源的数据一个进程。负责处理数据。ABB name。好，这样就行了。嗯。好，接下来我们来通过socket。获取实时产生的数据。B04端口9001。这个可以叫SRD。是吧，这里面也是RDD啊。下面我们就对接收到的数据使用。空格进行切割。转换成单个单词。改个e lines RD。第二，find map。加你的SP。按空格切就行啊。这样返回的就是wasd是里面呢包含了每个单词。这样把每个单词。转换成。淘宝兔的形式啊。what map对吧？这个RD。下面来执行reduce。BYK操作啊，所以基于K进行求和。嗯。it is by k。嗯。有我。啊。下面呢，将这个结果数据打印到控制台。嗯。认识。对吧，你看这个代码是不是也和咱们前面写那个link代码很像呀。启动任务。嗯。注意它下面这种写法不太一样，和那个SPA离线写法都不一样啊。等待任务形式啊。嗯。啊。好，这样的话你就可以开启一个发达人命实时流处理程序了。那我们在这呢，把这个socket给它打开。来执行。这个没事啊，还是那个when you choose啊，这个不用管了。把这个日志清一下，好，下面注意在这我来输入点数据。右。the me。回来看到没有， hello2161。只要说你输入的那两项数据在它的一个时间段之内，对吧，在五秒之内，它其实就把它切到一块儿了。这就可以啊。所以说呢，你可以这样理解，它相当于是每隔五秒把前五秒的数据给你封装成一个batch。然后后面呢，其实执行的就类似于Spark核心的那个代码Spark I的对吧。其实就是离线的那一套。它前面的话是按照时间去切这个小批，后面的话把你一小批一小批去处理，这样的话可以达到一个进食时的一个效果啊，所以说这个就是方向十命它的一个执行的原理。好，这是实现，接下来呢，我们使用加来实现一下。键package。SPA。world can&#39;t。加。把这个需求拿过来。那下面呢，是一个密方法，好，那接下注意首先还是要获取这套什么使命contact这些东西。先获取。创建。sten。啊。所以这里面你去Java这面你要获取这个。Java。streaming。context。嗯。后面呢，传的还是一个。时间。R。u减。second。嗯嗯。SIC。那上面还是要创建这个SPA配对项啊。嗯。but。嗯。master。LOCAL2。name。好，这样也可以。注意这块报错。对，报错了，一般是你那个包引错了。你可以看一下，把鼠标放到这个上面，你看。说什么这是什么Java FX里面什么？这是有问题的。对吧，我们用的话肯定是用Spark里面。对，其实啊，你这后面是少了一个S啊。嗯。这样也可以。你看这个时候用的是发使命里面的。好。下面是通过。获取实时产生的数据。soirit。这个点零四。等零一。来阿。对接收到的数据使用空格进行切割。转换成。三个单词。嗯。哪一点find map？对，那这里面的话，我们就需要写一个函数了啊。你有一个map function。然后要返回一个swim。我们就不把那个map的代码也写进去啊。这样的话，它是一个。我可以这样来直接来写ari。there as list，它里面呢，直接line there。后面做。直接。这样就可以啊，因为它最终返回一个联系啊。这种写法啊，它返回的速度，这样把这个速度转成list，再把它转成这个就可以。我咋？那接下来是把每个。单词转换。喂。double two的形式。我1MAP。所以呢，这里面也是需要写一个函数的啊。你有一个T。嗯。这个呢是in。嗯。因为你最终要法是一个P2列，这就淘宝里面第一列，淘宝里面第二列。new。double two。这呢，其实就是一个over了，把名字改一下啊，看起来清晰点。war。一。这个呢，叫派。安你。接下来实行。YK。嗯。reduce。function。I1 I2。这个呢，就叫word count。最后将结果数据啊引到。台。我们直接使用那个不好1D啊。这里面呢，我们给它传一个你一个VID方。这里面其实也好办，这个呢，就是一个。higher。不好意思。然后这里面的话，再给它传一个VD方式。对，这个就是具体那个他。嗯。嗯。相量二。这样的话就可以把里面这些数据啊，给它迭代出来。然后呢，就剩下最后这个。行任务。start。还有一个，等待任务停止。好一场，好一起。嗯。嗯。好，这就可以了。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket模拟产生数据，实时计算数据中单词出现的次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCountScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      <span class="comment">//注意：此处的local[2]表示启动2个进程，一个进程负责读取数据源的数据，一个进程负责处理数据</span></span><br><span class="line">      .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      .setAppName(<span class="string">"StreamWordCountScala"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建StreamingContext，指定数据处理间隔为5秒</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//通过socket获取实时产生的数据</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD = ssc.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对接收到的数据使用空格进行切割，转换成单个单词</span></span><br><span class="line">    <span class="keyword">val</span> wordsRDD = linesRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把每个单词转换成tuple2的形式</span></span><br><span class="line">    <span class="keyword">val</span> tupRDD = wordsRDD.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行reduceByKey操作</span></span><br><span class="line">    <span class="keyword">val</span> wordcountRDD = tupRDD.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将结果数据打印到控制台</span></span><br><span class="line">    wordcountRDD.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232245799.png" alt="image-20230423224530561"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232245196.png" alt="image-20230423224547229"></p><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket模拟产生数据，实时计算数据中单词出现的次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamWordCountJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//创建SparkConf配置对象</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">                .setAppName(<span class="string">"StreamWordCountJava"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建StreamingContext</span></span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过socket获取实时产生的数据</span></span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; linesRDD = ssc.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//对接收到的数据使用空格进行切割，转换成单个单词</span></span><br><span class="line">        JavaDStream&lt;String&gt; wordsRDD = linesRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//把每个单词转换为tuple2的形式</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairRDD = wordsRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行reduceByKey操作</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; wordCountRDD = pairRDD.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> i1 + i2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//将结果数据打印到控制台</span></span><br><span class="line">        wordCountRDD.foreachRDD(<span class="keyword">new</span> VoidFunction&lt;JavaPairRDD&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(JavaPairRDD&lt;String, Integer&gt; pair)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                pair.foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        System.out.println(tup._1+<span class="string">"---"</span>+tup._2);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//启动任务</span></span><br><span class="line">        ssc.start();</span><br><span class="line">        <span class="comment">//等待任务停止</span></span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SparkStreaming整合Kafka"><a href="#SparkStreaming整合Kafka" class="headerlink" title="SparkStreaming整合Kafka"></a>SparkStreaming整合Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这个sparkstreaming和kafka的整合。我们的需求是这样的，使用sparkstreaming实时消费kafka中的数据。这种场景也是比较常见的。注意，在这你想使用kafka，我们需要引入对应的一个依赖。它那个依赖是什么呢？到官网来看一下，进到官网点那个。文档SPA人命。你这里面来搜一下啊。就梳里的卡夫卡往下边走。到这块。看到没有，把这个点开右键啊，打开一个新页面。看到没有？你这个SPA命想要和卡夫卡进行交互，想要从卡夫卡里面去消费数据，你需要添加对应的依赖。这个呢是针对0.8.2.1级以上的，这个呢是针对卡夫卡零点十几以上的，那我们用的话肯定要用这个了，要用新一点的啊。那就这个东西。你可以。把它复制过来，到这儿来搜一下。就这个。2.4.3。格拉，02:11。把这个拿回来就可以了。嗯。这样就可以了。那下面呢，我们就来写一下具体的一个代码。swim。卡不卡？bug消费。卡夫卡中的数据。嗯。首先呢，还是希望见。streaming。嗯。context。康复。然后呢，还有一个second。嗯。有没有？我们还使用这个五秒。具体这个时间间隔啊，需要根据你们的业务而定啊。等于你有一个Spark。said master。LOCAL2。set name。嗯。获取消费卡夫卡的数据流。那怎么获取呢？现在我们需要用的这个卡夫卡u。csa。direct。这里面需要传两个泛型参数，three。SH。这里面呢，首先把这个SSC传给他。嗯，所以接下来需要第二个参数。什么那个location。street。这个。它这边会有提示啊。对吧，这几个参数。好使，用它呢，来调一个P开头的这个。就第一个就行。接下来只听下面那个。street。用这个。这个发型呢，还是string？对，这里面你要指定一下topic，对他接触的是一个topic s啊是一个。然后后面呢是一个map map里面传的是卡夫卡的一些参数。这里面这些都是固定写法啊。那下面我们就要呈现这两个。指定。指定卡夫卡的配置信息。好不好？等于一个map。object。我们直接在里面给它初始化就行啊。显得不报错。还有这个topic，在这一个它指定性。嗯。只要topics。注意，在这我们需要传一个。里面呢，可以同时使用多套贝。也就是说，它可以同时从多个topic里面去读取数据，都是可以的。那我们在这一个，我们就写一个就行。那既然把这个参数给它完善一下啊，嗯，首先需要指定卡不卡的。broke地址信息。would strive。B01。9092。零二。9092。039092。嗯。接下来我们需要指定那个K的序列化类型。K点。s Li。展开一个反序变化啊D。Siri。a。对啊。你如果怕拼数的话，可以先把后面这个写。后面的话，我们使用这个class of。spring。size对吧。可以把这个给它复制过去，把这个D改成小写就行了啊。这样也可以。嗯。还有这个value的。序列化类型。嗯。那就把这个改一下就行，改成V。都是死顿类型啊对，这就是咱们前面指定的那个对吧，配合没有一个泛型啊。嗯。那下面来指定那个。消费者ID。就那个YD。胳膊的ID。好，下面呢，再指定一下消费策略。there there。这块呢，只能一个。最后我们来指定一个自动提交。在设置啊。enable。there also？好。这样就可以了。这是一些核心的参数。OK，这样的话就可以从它里面去读取数据啊，这样就可以获取到一个类似于卡夫卡。我在这里面，我们可以把它称为。嗯，这是他们的一个概念啊。这个数据流。那下面我们就可以处理数据了。后面你可以调map啊，map啊这些算子去处理就可以。嗯。这样每次获取到一条数据啊，每次几的一条数据。然后把里面数据迭代出来之后呢，把它封装成一个他报，因为它本身呢是一个record一行记录，那我们在这呢。对的点。先获取的，你们K。然后再获取value。嗯。在这我们就把这个数据打印出来。将数据。印到后来。因为其实你在这只要能获取到数据，你后期你想做map map reduce go是不是都可以呀，对吧，那个就没什么区别了。启动任务。start。等待任务停止。嗯。嗯，好。那下面呢，我们把这个运行起来。但是呢，你发现这块他报错了。看到没有？31行。遇到问题不要怕啊，这个问题我们要排查一下。他说这个类型啊，有点问题。他呢，发现了是一个布尔类型，结果他需要了是一个OB。所以这块的话，你需要在这这样来指定一下。强制执行类型Java点拉点布尔。嗯。这样就可以了啊来执行。好，这样就可以了。那接下来呢，我们来开启一个生产者，往里面写点儿数据。其中一个卡不卡，剩下的我里面写着数据啊。其控制台的生产者。嗯。hello Spark。嗯。看到没有打一回啊。我把它停一下啊。注意。它这个呢，we know，为什么呀。因为现在我们卡卡里面数据啊，其实只有value是没有那个K的啊，所以说你K答出来是no，我们G的那些数据啊，一般都放到value里面啊。就是放到外。这个K的话是为了判定你这个数据到底是放哪个分区里面啊，一般会传一个K。所以说我们那种说法一般是不传的，然后随机分啊。这是没有问题的。那这样的话，我们就可以把那个卡字卡里面数据给他消费出来。是吧，那后面就可以实现你的业务逻辑。OK。那接下来呢，我们使用这个加代码来实现一下。卡夫卡。加了。嗯。把这个注释拿过来。好，首先呢，获取这个streaming context。在指定读取数据的。时间间隔为五秒。嗯。有一个Java。streaming context。好点。这个大家看五秒。嗯。new。said master。logo。嗯。下载APP name。把这个拿过来，嗯。而且这个变量Co。嗯。那接下来我们来获取消费卡夫卡的数据流。还是那个卡不卡。great。direct，那首先SC。后面还是一样的。嗯。嗯。嗯。嗯。嗯。好，接下来是这个。consumer。这个。there。首先是一个topics，还有一个。搞不搞？注意这块啊。你需要指定泛型，你这个泛型写到哪了。我们在SKY面里面是放在这个位置，但是在这里面你写这还是不对的啊，你在写前面。three。W。好。接下来创建这两个啊，把这个topic，还有这个卡夫卡。在线行。指令要读取的。名称。先写这个。three。topics。嗯。挨着。七。那接下来是这个。有一个map。object。好，不搞。下面就往里面添加参数了啊。RI。service。我们俩复制一下吧。这个又是提花。嗯。嗯嗯。第二个呢，是这个K的这个虚化类型。所以这个你别导错包了啊，你要导这个。巴阿巴奇，看到没有，卡布卡点common这个body。name。嗯。嗯嗯。说不爱你。嗯。also。offset。there reet。at。ne。点点commit。好，这样就可以了。of Australia。好，那接下来数数去。嗯。嗯嗯。选一个map，因为一个function。我们最终返回是一个double two。里面是一个string。LW。好，这个就是一个record。我们可以在这直接。你了一个。two，嗯。你告你。six。嗯。我看点160。这样转换成淘宝之后，后期用起来也方便。SP对吧。将数据打印的。启动任务，嗯。start。等待任务停止。嗯。好一场。嗯。嗯。好，这样就可以了，来。把它执行一下。好看没有，这是之前那条数据啊。我们可以再往里面加一条。哈哈哈。可以吧，也是可以的啊。好，这就是Java代码的一个实现。好，那针对Spark命这一块呢，我们暂时就讲到这儿，因为后期大部分的实施计算需求，我们需要使用link去实现了。在这呢，我们是把这个SPA命最常见那个消费卡不卡数据这种案例呢给大家讲一下。</span><br></pre></td></tr></table></figure><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spark 消费Kafka中的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"StreamKafkaScala"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定Kafka的配置信息</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Object</span>](</span><br><span class="line">      <span class="comment">//kafka的broker地址信息</span></span><br><span class="line">      <span class="string">"bootstrap.servers"</span>-&gt;<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>,</span><br><span class="line">      <span class="comment">//key的序列化类型</span></span><br><span class="line">      <span class="string">"key.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//value的序列化类型</span></span><br><span class="line">      <span class="string">"value.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//消费者组id</span></span><br><span class="line">      <span class="string">"group.id"</span>-&gt;<span class="string">"con_2"</span>,</span><br><span class="line">      <span class="comment">//消费策略</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span>-&gt;<span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">//自动提交offset</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span>-&gt;(<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//指定要读取的topic的名称</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"t1"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">    <span class="keyword">val</span> kafkaDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    kafkaDStream.map(record=&gt;(record.key(),record.value()))</span><br><span class="line">      <span class="comment">//将数据打印到控制台</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232301727.png" alt="image-20230423225935601"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232301407.png" alt="image-20230423225949397"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spark 消费Kafka中的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//创建StreamingContext，指定读取数据的时间间隔为5秒</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">                .setAppName(<span class="string">"StreamKafkaJava"</span>);</span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafka的配置信息</span></span><br><span class="line">        HashMap&lt;String, Object&gt; kafkaParams = <span class="keyword">new</span> HashMap&lt;String, Object&gt;();</span><br><span class="line">        kafkaParams.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"key.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"value.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"group.id"</span>,<span class="string">"con_2"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"auto.offset.reset"</span>,<span class="string">"latest"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"enable.auto.commit"</span>,<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定要读取的topic名称</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        topics.add(<span class="string">"t1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; kafkaStream = KafkaUtils.createDirectStream(</span><br><span class="line">                ssc,</span><br><span class="line">                LocationStrategies.PreferConsistent(),</span><br><span class="line">                ConsumerStrategies.&lt;String, String&gt;Subscribe(topics, kafkaParams)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">//处理数据</span></span><br><span class="line">        kafkaStream.map(<span class="keyword">new</span> Function&lt;ConsumerRecord&lt;String, String&gt;, Tuple2&lt;String,String&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(ConsumerRecord&lt;String, String&gt; record)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(record.key(),record.value());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();<span class="comment">//将数据打印到控制台</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//启动任务</span></span><br><span class="line">        ssc.start();</span><br><span class="line">        <span class="comment">//等待任务停止</span></span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-3.html</id>
    <published>2023-04-20T08:46:48.000Z</published>
    <updated>2023-04-24T15:42:18.541Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-2.html</id>
    <published>2023-04-20T08:46:43.000Z</published>
    <updated>2023-04-23T14:21:05.429Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十七周-Flink极速上手篇-Flink高级进阶之路-2"><a href="#第十七周-Flink极速上手篇-Flink高级进阶之路-2" class="headerlink" title="第十七周 Flink极速上手篇-Flink高级进阶之路-2"></a>第十七周 Flink极速上手篇-Flink高级进阶之路-2</h1><h2 id="Kafka-Consumer的使用"><a href="#Kafka-Consumer的使用" class="headerlink" title="Kafka Consumer的使用"></a>Kafka Consumer的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们来看一下flink中针对kafka connect的专题，提供了很多的connect组件，其中应用比较广泛的就是kafka这个connect。我们就针对kafka在flink的应用做详细的分析。针对flink流处里啊，最常用的组件就是kafka。原始日志数据产生后，会被日志采集工具采集到kafka中，让flink去处理。处理之后的数据可能也会继续写入到kafka中。kafka可以作为flink的datasource和datasink来使用。并且kafka中的partition机制和flink的并行度机制可以深度结合，提高数据的读取效率和写入效率。那我们想要在flink中使用kafka，需要添加对应的依赖。(先在flink官网中找到依赖的名字，再到maven中去找符合的版本)</span><br><span class="line"></span><br><span class="line">那在具体执行这个代码之前啊，我们先需要把那个zookeeper集群，还有kafka集群给他起来。我这些相关的服务呢，已经起来了。这是入K班了，这是卡不卡都已经起来啊。好，那下面注意，我们还需要做一件事情。因为我们在这里面呢，用到了一个T1这个topic，所以说在这我们需要去创建这个topic。找一下之前的命令。嗯。其一。这个分区设置为五，后面因子是为二。发现一个T。好，创建成功。那下面呢，我们就可以去启动代码。启动电板之后，那我们需要往那个卡夫卡里面模拟产生数据。这个时候呢，我们可以启动一个基于控制台的一个生产者来模拟产生数据。嗯。使用这个卡不卡console producer。把这个复制一下。好，这个套背上就是T1。那这个时候呢，我们接着就来模拟产生数据。hello。看到没有消费到。再加一个。hello。没问题吧，是可以的，这样的话我们就可以消费卡夫卡中的数据了。好，这个是代码实践，接下来我们使用这个Java代码来实现一下。先创建一个package。嗯。stream。搞不搞？SS。把这个复制过来。嗯。嗯嗯。嗯。好，首先呢，还是获取一个连环应。execution。因为第2GET。因为。下面的env.S。嗯。在这儿，我们需要去利用这个。Li。卡不卡？three。那这里面啊，传一个topic。嗯。第一，嗯。第二个，你有一个simple。视频，game。第三个pop。嗯嗯。有一个薄。嗯。嗯嗯嗯。首先呢，我们在里面set property。我可以把这个呢直接拿过来。嗯。好，下面呢，said。格布利。卡不卡星本。把它拿过来。嗯。感注释，这个就是指定卡夫卡作为S。嗯。这是指令。普林格卡夫卡consumer的相关配置。接下来呢，我们将读取到的数据啊，一到控制台。嗯。嗯。嗯嗯。嗯。嗯。包的异常。嗯。来把这个启动起来。好，那我们在这边呢，再模拟产生的数据。哈哈哈。没问题吧，是可以的。好，这就是Java代码的一个实现。</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">      env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>() prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304231050885.png" alt="image-20230423105004127"></p><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.kafkaconnector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaSourceJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">        String topic = <span class="string">"t1"</span>;</span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">        prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), prop);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafka作为source</span></span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.addSource(kafkaConsumer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将读取到的数据打印到控制台</span></span><br><span class="line">        text.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"StreamKafkaSourceJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KafkaConsumer消费策略设置"><a href="#KafkaConsumer消费策略设置" class="headerlink" title="KafkaConsumer消费策略设置"></a>KafkaConsumer消费策略设置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对卡夫卡康消费数据的时候会有一些策略，我们来看一下。首先这个是默认的消费策略。下面还有一个ear，从最早的开始消费，latest，从最新的开始消费。已经呢，现在要出来一个C呢。按照指定的时间戳往后面开始消费。下面呢，我们来演示一下。我直接在这里面来设置一下。嗯。卡夫卡consumer。的消费策略设置。这个其实我们在讲卡不卡的时候也详细分解过啊，其实是类似的。首先我们看下这个默认策略。嗯。直接使用它来设置点带。start from group of，注意这个呢，其实就默认你不设置它默就类。它是什么意思呢？它会读取。group ID。对应。保存的outside。开始消费数据。那读取不到的话呢。则根据卡夫卡中。这个参数auto点。reite。参数的值开始。消费数据。因为如果说你是第一次使用这个消费者，那么他之前肯定是没有保存这个对应的office的信息，那这样的话呢，他就会根据这个参数的值来开始进行消费。那这个值的话，它那要么是early latest对吧，要么是从最新的，要么是从最近的。那下一次的话呢，他就会根据你之前指定的这个global ID对应的保存的那个开始往下面继续消费数据。那既然下面这个呢，是从那个最早的记录开始，消费主义啊，不搞consumer there that。from earliest。从最早的记录。开始消费。独具。忽略。你提交。信息。这样的话，他就不管你有没有提交，都会每次都从那个最早的数据开始消费。那对了，还有一个从。最新的记录开始消费。也是忽略这个已提交的奥赛的信息。嗯。start。home latest。嗯。那还有一个是从指定的时间戳开始消费数据。对于每个分区。其时间戳大于或等于指定时间戳的记录。江北。作为70位。嗯。that。大的from。这里面你给他传一个时间戳就行了啊，我这边随便写一个行吗？这就是这几种测量啊。其实我们在讲卡不卡的时候，也详细分析过这几种词，那在那就把这个默认的给它打开吧。就你这个呢，你在这儿设置不设置，它其实都是一个默认的策略。这个呢，就是针对这里面这个卡夫卡抗性板，它这个消费策略的一个设置。其实咱们在工作中啊，一般最常见的，那其实就是一种默认的技术。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304231057297.png" alt="image-20230423105719815"></p><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KafkaConsumer的容错"><a href="#KafkaConsumer的容错" class="headerlink" title="KafkaConsumer的容错"></a>KafkaConsumer的容错</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下kafka consumer的容错。flink中呢也有这个checkpoint机制，checkpoint呢是flink实现容错机制的核心功能，它能够根据配置周期性的基于流中各个算子任务的state来生成快照。从而将这些state的数据定期持久化存储下来。当flink的程序一旦意外崩溃时，重新运行程序时，可以有选择的从这些快照进行恢复。从而修正因为故障带来的程序数据异常。</span><br><span class="line"></span><br><span class="line">当这个checkpoint机制开启的时候。consumer呢？它会定期把kafka的offset的信息，还有其他算子任务的信息一块保存起来。当job失败重启的时候，弗会从最近一次的checkpoint中进行恢复数据。重新消费kafka中的数据。那为了能够使用支持容错的kafka，我们需要开启checkpoint，那如何开启呢？很简单。直接呢，就env.enableCheckpointing，指定一个周期就行。这个5000呢，表示五秒，就是每隔五秒执行一次checkpoint。他会周期性的执行。</span><br><span class="line">来，我们来看一下。我呢，就直接在这儿。来配置啊。en enable check。这毫秒啊。每隔5000毫秒。执行一次checkpoint。这个呢，其实就是设置。这个point的周期啊。那针对这个呢，它还有一些相关的配置。那我接着呢，把这个配置拿过来。把这个复制一下。搞一下包。这个是。针对checkpoint的相关配置。下面这个参数的意思呢？表示设置一下checkpoint的一个语义，它可以提供这种锦一词的语义。下面这个呢，表示两次切之间它的一个时间间隔。这个呢，表示呢，必须要在指定时间之内完成one。其实就是给这个check呢，设置一个超时时间，超过这个时间了就被丢弃了。下面这个呢，表示呢，同一时间只允许执行一个checkpoint。下面这个三注意。他呢表示呀，当我们对这个link程序执行一个cancel之后，就是把这个link程序停掉之后，我们呢，会保留这个这个波段数据，这样的话，我们可以根据实际需要，后期呢来恢复这些数据。这是它相关的一些配置啊，</span><br><span class="line"></span><br><span class="line">那其实呢，在这块还有一个配置。设置这个state数据存储的位置，默认情况下的数据会保存在task manager内存中。当我们执行checkpoint的时候呢，会将这个实际的数据存储到jobmanager内存中。这个具体的存储位置呢，取决于StateBackend的配置。FLink呢，一共提供了三种存储方式，第一种是MemoryStateBackend，第二种呢是FsStateBackend，第三种是RocksSBStateBackend。</span><br><span class="line"></span><br><span class="line">我们先分析一下第一种基于内存的。这个时候呢，state数据保存在这个Java堆内存中，当我们执行checkpoint的时候，它呢会把state快照数据啊保存到job manager的内存中。基于内存的呢，在生产环境下面不建议使用。对吧，因为你重启之后，它内存里面数据就没了，所以说是没有意义的。</span><br><span class="line"></span><br><span class="line">第二种呢，FsStateBackend。数据呢？保存在task measure内存中，当我们执行check point的时候，会把的快照数据保存到配置的文件系统中。我们可以使用hdfs等分布式文件系统。这个呢是可以用的。</span><br><span class="line"></span><br><span class="line">当然还有一种叫RocksSBStateBackend啊，它跟上面的都略有不同，它会在本地文件系统中维护这个state。state的会直接写入本地的RocksDB中。同时它需要配置一个远端的文件系统，一般呢是Hdfs。那我们在做checkpoint的时候。会把本地的数据直接复制到远端的文件系统中。故障切换的时候，直接从远端的文件系统中恢复数据到本地。RocksDB克服了state的受内存限制的缺点，同时又能够持久化到远端文件系统中。推荐在生产环境中使用。</span><br><span class="line"></span><br><span class="line">所以在这里我们使用第三种RocksSBStateBackend</span><br><span class="line">maven添加依赖flink-statebackend-rocksdb</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">      env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106563.png" alt="image-20230423210612299"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106374.png" alt="image-20230423210628262"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106556.png" alt="image-20230423210654942"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232107250.png" alt="image-20230423210749485"></p><h3 id="Kafka-Consumers-Offset自动提交"><a href="#Kafka-Consumers-Offset自动提交" class="headerlink" title="Kafka Consumers Offset自动提交"></a>Kafka Consumers Offset自动提交</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink没有开启checkpoint时，offset的提交由之前的enable.auto.commit和auto.commit.interval.ms决定</span><br><span class="line"></span><br><span class="line">当开启了，由checkpoint每次执行时提交</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232113418.png" alt="image-20230423211307520"></p><h2 id="KafkaProducer的使用"><a href="#KafkaProducer的使用" class="headerlink" title="KafkaProducer的使用"></a>KafkaProducer的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下在flink中如何向kafka中写数据，此时需要用到kafka producer。</span><br><span class="line"></span><br><span class="line">所有的数据都写入指定topic的一个分区里面。注意，他会把所有数据写到这个topic的一个分区。那这样的话，其实呢，在我们实习当中，这样是不合适的啊。我们使用操作的肯定是要使用多个分区，你要把数据分别写到不同的分区里面，这样的话后期我们去消费也可以并行消费，提高消费能力，对吧？那你如果都搞一个分区里面，那其实相当于我这个topic卡就一个分区。这样后期我这个处理能力是有限制的，所以说呢，如果不想自定义分具体。也不想使用默认的可以直接。使用一个null即可。</span><br></pre></td></tr></table></figure><h3 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaProducer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.internals.<span class="type">KafkaSerializationSchemaWrapper</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.partitioner.<span class="type">FlinkFixedPartitioner</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink向Kafka中生产数据 </span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSinkScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启checkpoint</span></span><br><span class="line">    <span class="comment">//env.enableCheckpointing(5000)</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaProducer的相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t2"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为sink</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     KafkaSerializationSchemaWrapper的几个参数</span></span><br><span class="line"><span class="comment">     1：topic：指定需要写入的topic名称即可</span></span><br><span class="line"><span class="comment">     2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中</span></span><br><span class="line"><span class="comment">     默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面</span></span><br><span class="line"><span class="comment">     如果不想自定义分区器，也不想使用默认的，可以直接使用null即可</span></span><br><span class="line"><span class="comment">     3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳</span></span><br><span class="line"><span class="comment">     如果写入了，那么在watermark的案例中，使用extractTimestamp()提起时间戳的时候</span></span><br><span class="line"><span class="comment">     就可以直接使用recordTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> kafkaProducer = <span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">KafkaSerializationSchemaWrapper</span>[<span class="type">String</span>](topic, <span class="literal">null</span>, <span class="literal">false</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()), prop, <span class="type">FlinkKafkaProducer</span>.<span class="type">Semantic</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    text.addSink(kafkaProducer)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSinkScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232135912.png" alt="image-20230423213547295"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232137221.png" alt="image-20230423213734278"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232138202.png" alt="image-20230423213834818"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmak里查看</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232144712.png" alt="image-20230423214449177"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.kafkaconnector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink向Kafka中生产数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaSinkJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定FlinkKafkaProducer相关配置</span></span><br><span class="line">        String topic = <span class="string">"t2"</span>;</span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafak作为sink</span></span><br><span class="line">        FlinkKafkaProducer&lt;String&gt; kafkaProducer = <span class="keyword">new</span> FlinkKafkaProducer&lt;&gt;(topic, <span class="keyword">new</span> KafkaSerializationSchemaWrapper&lt;String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">new</span> SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE);</span><br><span class="line">        text.addSink(kafkaProducer);</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"StreamKafkaSinkJava"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KafkaProducer的容错"><a href="#KafkaProducer的容错" class="headerlink" title="KafkaProducer的容错"></a>KafkaProducer的容错</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下kafkaProducer的容错。如果flink开启了checkpoint，那针对flink kafka producer可以提供仅一次予以保证。我们可以通过这个参数来指定三种不同的语义。</span><br><span class="line">点就是不支持任何语音，这个呢是至少一次，这个呢仅一次，那more呢是至少一次。好，那在代码里面怎么体现呢？来看一下。注意看到没有，我们刚才指的就是一个锦一四啊。但是这个时候注意你还要开启这个table。开启这个了。下面那些参数我暂时就先不指定了，行吗？好，那下面呢，我们来执行一下。来开启这个稍的。好注意这时候呢，给大家看一个比较神奇的现象。你看刚才我们把这个搜下打开啊，结果它停了，你再把它打开。他还会请。看没有？什么原因呢？那时候我这块也没有报错呀。注意这个呢，是因为这个原因。我们之前啊，在这加了一个logo的配置文件，对吧。注意这个日级别，我之前给它改成error。我们把这个调一下。调成那个警告级别。因为这个时候有一些日他没有打出来警告信息看不到啊。来再启动把这个打开。对，他这个其实应该是error级别的，但是他写的什么写的不太好，他把这个日志写成那种warning级别，警告级别的，所以说呢，我们之前使用那个error级别的，监控不到这些日志信息啊。啊，停一下吧。</span><br><span class="line"></span><br><span class="line">来分析一下啊。不要往后面看这。还有什么呀，这个事物时间比这个博客里面配置的这个时间还要大。就是说，生产者中设置的事物超时时间大于卡夫卡博客中设置的事物超时时间。因为卡夫卡服务中默认事物的超时时间是15分钟，但是呢，弗林格卡夫卡保留它里面设置的事物超时间默认是一小时，这个仅一次语义啊，它需要依赖这个事物。如果从Li应用程序崩溃到完全重启的时间超过了卡夫卡的事物超时时间，那么将会有数据丢失，所以我们需要合理的配置事物超时时间。因此，在使用这个仅一次语义之前，建议增加卡夫卡博克中这个transaction.max.timeout.ms的值。把这个值啊给它调大。那下面呢，我们就来修改一下卡夫卡里面这个配置，这个配置在哪啊，其实就那个server.properties里面啊。买那个可以试一下。它里面是没有这个参数的，你直接在这把它拿过来。给它做个值。我们也给它改成一小时吧。这个你转换成毫秒是3600000。那么是五个零啊，这样的话就一小时。把这个复制一下。对，这个集群里面所有机器都要改啊。嗯。好，可以了，注意改完之后我们需要重写。那你先把这个卡夫卡集群停掉。好停掉之后再去启动，启动的话，我们使用它这个命令啊。前面加了一个GMX，这样的话我们可以使那个CMA来减轻它里面一些信息啊。嗯。好，这个起来了。嗯。嗯。这个呢也可以啊。好，这个也可以了。那接下来我们重新再执行这个样本，对吧，把这个再看一下。嗯。看到没有，此时他就不报错了啊，我们可以在这来验证一下，先确一下里面的数据对吧。是这样。5211。这个停了，因为刚才我们把那个卡夫卡停掉之后啊，这个c map哎，就停掉了。把它起来。所以这个没不对。嗯。嗯。第三。对吧，这里面是这了来。我们输点作业。好变了吧，对吧。说明这个数据写进来了，并且这块呢也没报错啊。OK，这样就可以了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232152824.png" alt="image-20230423215255115"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line">import java.util.Properties</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Flink向Kafka中生产数据 </span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object StreamKafkaSinkScala &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;开启checkpoint</span><br><span class="line">    env.enableCheckpointing(5000)</span><br><span class="line">      </span><br><span class="line">    val text &#x3D; env.socketTextStream(&quot;bigdata04&quot;, 9001)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;指定FlinkKafkaProducer的相关配置</span><br><span class="line">    val topic &#x3D; &quot;t2&quot;</span><br><span class="line">    val prop &#x3D; new Properties()</span><br><span class="line">    prop.setProperty(&quot;bootstrap.servers&quot;,&quot;bigdata01:9092,bigdata02:9092,bigdata03:9092&quot;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;指定kafka作为sink</span><br><span class="line">    &#x2F;*</span><br><span class="line">     KafkaSerializationSchemaWrapper的几个参数</span><br><span class="line">     1：topic：指定需要写入的topic名称即可</span><br><span class="line">     2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中</span><br><span class="line">     默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面</span><br><span class="line">     如果不想自定义分区器，也不想使用默认的，可以直接使用null即可</span><br><span class="line">     3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳</span><br><span class="line">     如果写入了，那么在watermark的案例中，使用extractTimestamp()提起时间戳的时候</span><br><span class="line">     就可以直接使用recordTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp</span><br><span class="line">     *&#x2F;</span><br><span class="line">    val kafkaProducer &#x3D; new FlinkKafkaProducer[String](topic, new KafkaSerializationSchemaWrapper[String](topic, null, false, new SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE)</span><br><span class="line">    text.addSink(kafkaProducer)</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;StreamKafkaSinkScala&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-1.html</id>
    <published>2023-04-20T08:44:51.000Z</published>
    <updated>2023-04-24T07:32:38.535Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十七周-Flink极速上手篇-Flink高级进阶之路-1"><a href="#第十七周-Flink极速上手篇-Flink高级进阶之路-1" class="headerlink" title="第十七周 Flink极速上手篇-Flink高级进阶之路-1"></a>第十七周 Flink极速上手篇-Flink高级进阶之路-1</h1><h2 id="Window的概念和类型"><a href="#Window的概念和类型" class="headerlink" title="Window的概念和类型"></a>Window的概念和类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">大家好，前面我们学习了flink中的基本概念，集群部署以及核心API的使用，下面我们来学习一下flink中的高级特性的使用。首先，我们需要掌握中的window、time以及whatermark使用。然后我们需要掌握kafka-connector使用，这个是针对kafka一个专题。最后我们会学习一下Spark中的流式计算sparkStreaming，之前在学习spark的时候我们没有涉及这块，在这儿我们和flink一块来学习，可以加深理解，因为它们都是流式计算引擎。</span><br><span class="line"></span><br><span class="line">下面呢，我们首先进入第一块flink中的window和time。flink认为批处理是流处理的一个特例，所以flink底层引擎是一个流式引擎，这上面呢实现了流处理和批处理。而window呢，就是从流处理到批处理的一个桥梁。通常来讲啊，这个window啊，是一种可以把无界数据切割为有界数据块的手段，例如对流动的所有元素进行计数是不可能的，因为通常流是无限的。或者呢，可以称之为是无界了。所以说流上的聚合需要由window来划分范围，比如计算过去五分钟或者最后100个元素的和。</span><br><span class="line"></span><br><span class="line">window可以是以时间驱动的time window，例如每30秒，或者是以数据驱动的count window，例如每100个元素。DataStream API提供了基于time和count的window。同时，由于某些特殊的需要，dataStreamAPI也提供了定制化的window操作，供用户自定义window。</span><br><span class="line"></span><br><span class="line">这个window呀，根据类型可以分为这两种。第一种是滚动窗口，它呢表示窗口内的数据没有重叠，第二种呢是滑动窗口，它呢表示窗口内的数据有重叠。</span><br><span class="line"></span><br><span class="line">那下面我们来看个图分析一下，首先看这个滚动窗口，这个S轴呢是一个时间轴，你看这个是一个窗口的大小，这是WINDOW1 window2 window3，注意每个窗口内的数据是没有重叠的，这个就是滚动窗口。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201704808.png" alt="image-20230420170417289"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面来看这个滑动窗口，这个S轴呢，还是一个时间轴，你看这个是一个window的大小。这个表示是一个window的滑动间隔，这是WINDOW1这个红色的，它这个窗口从这到这儿，下面这个呢，WINDOW2，注意这个窗口它是从这儿到这儿，这个蓝色的看到没有，它里面呢，包含了WINDOW1里面的一部分数据。那你看WINDOW3 window3里面它包含了WINDOW2里面的一部分数据，所以说这个滑动窗口，它们每个窗口之间呀，会有数据重叠，这个就这两种窗口它的一个区别</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201705935.png" alt="image-20230420170515739"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我针对这个窗口的类型做了一个汇总。你看这是window window下面有time window有count window还有自定义window，那这些window再往下面你看它呢，可以实现滚动窗口或者滑动窗口，对吧？不管你是基于time的，还是基于count的，还是自定义的，你们都可以实现滚动窗口或者是滑动窗口。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201707926.png" alt="image-20230420170745169"></p><h3 id="TimeWindow的使用"><a href="#TimeWindow的使用" class="headerlink" title="TimeWindow的使用"></a>TimeWindow的使用</h3><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这些window的具体应用，首先来看第一个time window。time window呢是根据时间对数据流切分窗口，time window可以支持滚动窗口和滑动窗口。</span><br><span class="line"></span><br><span class="line">其中它有这么两种用法，来看一下time window。</span><br><span class="line">timeWindow(Time.seconds(10))</span><br><span class="line">注意，首先这个。他呢是表示。滚动窗口的窗口大小为十秒。对每十秒内的数据,进行聚合计算。这个呢，其实就是设置一个滚动窗口。</span><br><span class="line"></span><br><span class="line">timeWindow(Time.seconds(10),Time.seconds(5))</span><br><span class="line">那下面这个呢，对应的它设置的就是一个滑动窗口，因为它除了有一个窗口大小，它还滑动一个间隔。表示滑动窗口的窗口大小为十秒,滑动间隔为五秒,就是每隔五秒计算前十秒内的数据，所以说是两种用法，一种是滚动，一种是滑动。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TimeWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TimeWindowOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TimeWindow之滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    <span class="comment">/*text.flatMap(_.split(" "))</span></span><br><span class="line"><span class="comment">      .map((_,1))</span></span><br><span class="line"><span class="comment">      .keyBy(0)</span></span><br><span class="line"><span class="comment">      //窗口大小</span></span><br><span class="line"><span class="comment">      .timeWindow(Time.seconds(10))</span></span><br><span class="line"><span class="comment">      .sum(1)</span></span><br><span class="line"><span class="comment">      .print()*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TimeWindow之滑动窗口：每隔5秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">        .timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>),<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print()</span><br><span class="line">    env.execute(<span class="string">"TimeWindowOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeWindow滚动窗口</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201729458.png" alt="image-20230420172901278"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201729476.png" alt="image-20230420172914398"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeWindow滑动窗口，黑色第一次输入，蓝色第二次输入</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201732506.png" alt="image-20230420173241489"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第三次打印，蓝色</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201736543.png" alt="image-20230420173625064"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TimeWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeWindowOpJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//TimeWindow之滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        <span class="comment">/*text.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">            @Override</span></span><br><span class="line"><span class="comment">            public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span><br><span class="line"><span class="comment">                    throws Exception &#123;</span></span><br><span class="line"><span class="comment">                String[] words = line.split(" ");</span></span><br><span class="line"><span class="comment">                for (String word: words) &#123;</span></span><br><span class="line"><span class="comment">                    out.collect(new Tuple2&lt;String, Integer&gt;(word,1));</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;).keyBy(0)</span></span><br><span class="line"><span class="comment">                //窗口大小</span></span><br><span class="line"><span class="comment">                .timeWindow(Time.seconds(10))</span></span><br><span class="line"><span class="comment">                .sum(1)</span></span><br><span class="line"><span class="comment">                .print();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//TimeWindow之滑动窗口：每隔5秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word: words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">10</span>),Time.seconds(<span class="number">5</span>))</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"TimeWindowOpJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CountWindow的使用"><a href="#CountWindow的使用" class="headerlink" title="CountWindow的使用"></a>CountWindow的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下count的Window的使用，count Window是根据元素个数对数据流切分窗口。count window也可以支持滚动窗口和滑动窗口。</span><br><span class="line">countWindow(5)表示滚动窗口的大小,是五个元素。也就是当窗口中填满五个元素的时候，就会对窗口进行计算</span><br><span class="line">countWindow(5,1)</span><br><span class="line">表示滑动窗口的窗口大小是五个元素，滑动的间隔为一个元素，也就是说每新增一个元素就会对前面五个元素计算一次</span><br><span class="line"></span><br><span class="line">那我们再验证一下。有有有。写四个啊。看到没有，这个又要直行了。对吧，你后面再加哈，注意。此时呢，它就不会再执行了，因为你是一个滚动窗口啊，最终呢，你再满足有五个元素之后，它才会重新执行，这是一个滚动窗口。把这个听一下。把这个注意事项啊，我们给它加进来，这是一个解释啊。由于我们在这里使用了可以。会相对数据分组。如果某个分组对应的数据窗口，数据窗口内达到了五个元素，这个窗口才会被主发执行，如果你不使用KPI的话，他就不会在这儿做区分了，所以他接收到所有的数据，在这儿会统一计算。不过那个时候你就需要使用这个count window or这个咱们后面再分析啊，接着我们先使用这个K方式，后面呢直接使用这个count window，好，这是一个滚动窗口，下面呢，我们来实现一个滑动窗口。它的豌豆之滑动窗口。每隔一个元素计算一次前五个元素。map。空格切一下。小点一。零零，它的window，注意第一个参数是窗口大小，第二个是滑动间隔。嗯。窗口大小。第二个参数。滑动间隔。一。BA。好，那接着要把上面这个的读调，嗯。把这个socket呢，再给它打开。嗯。好，那我们到这儿来数数句，hello you。注意它直行了，为什么呀，因为它的滑动间隔是一，只要间隔一个元素，它就会执行，它呢会往前推找五个元素，但是它前面并没有五个元素，就只有这一个，所以说最终的结果呢，就是这样好。那下面呢，我继续往里面添加元素。hello，你。看那个效果，看到没有，hello就两次了，me是一次对吧，hello已经变成两次了，那下面我们还按照刚才这个逻辑。hello，加三次，你看加三次，它其实最终呢，输出了三条如玉，这次是三，这次是四，这次是五。没问题吧，因为你新增一条数据，它就会往前推五条数据去统计。嗯。看到没有2345。这也是可以的啊，然后再加个什么，hello。还是50。you。为什么一直是五次呢？因为它只会往前面统计五个元素啊。好，这就滑动窗口，下面我们来使用Java代码来实现一下。放着window。op加。嗯。嗯嗯。先获取一个环境。get。嗯。嗯。嗯。看window。直滚动方口。每隔五个元素计算一次。前五个元素。加个小碟red map，你有一个red map。注意我们在这呢，还把这个map和map它这个逻辑整合一块，说输入是词频输出是。in。嗯。来。慢点，split。不了。我。在这个。嗯。WORD1。对吧，这样看一下后面一个a。零。嗯。放了window。五。嗯嗯。some。嗯。这个是窗口大小。好，接下来讲第二个把这个注释呢，从这复制一下吧。所以这是每格啊。嗯。好，这个前面啊，其实都一样啊，只有一个地方不一样，对吧。就是把这个放到温度这块，给它改一下就行。和两个参数，嗯。第一个参数窗口大小，第二个参数。滑动间隔。嗯嗯。因为一点。嗯。顺便抛个异常。好，这就可以了，在这我们可以助调一个。验证一下这个滑动窗口。赶紧回来把这个打开。OK。好。没问题吧，没问题啊。这就是Java代码，实现这个count window。</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CountWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CountWindowOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意：由于我们在这里使用keyBy，会先对数据分组</span></span><br><span class="line"><span class="comment">     * 如果某个分组对应的数据窗口内达到了5个元素，这个窗口才会被触发执行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//CountWindow之滚动窗口：每隔5个元素计算一次前5个元素</span></span><br><span class="line">    <span class="comment">/*text.flatMap(_.split(" "))</span></span><br><span class="line"><span class="comment">      .map((_,1))</span></span><br><span class="line"><span class="comment">      .keyBy(0)</span></span><br><span class="line"><span class="comment">      //指定窗口大小</span></span><br><span class="line"><span class="comment">      .countWindow(5)</span></span><br><span class="line"><span class="comment">      .sum(1)</span></span><br><span class="line"><span class="comment">      .print()*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//CountWindow之滑动窗口：每隔1个元素计算一次前5个元素</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">        .countWindow(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print()</span><br><span class="line">    env.execute(<span class="string">"CountWindowOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201756278.png" alt="image-20230420175629114"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201756533.png" alt="image-20230420175643559"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以我在这再输三个，看到没有，到这儿才刚开始执行了一次，他把这个hello打印出来五个。那这个you和me为什么没有打印呢？注意了，所以啊，我们在这啊执行了keyby会对这个数据进行分组，如果某个分组对应的数据窗口内达到了五个元素，这个窗口才会被处罚执行，所以说这个时候相当于是hello对应的那个窗口，它里面够五个元素了，它才会执行。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201758363.png" alt="image-20230420175819357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201758141.png" alt="image-20230420175806865"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">看一下count滑动窗口执行结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801677.png" alt="image-20230420180125565"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801117.png" alt="image-20230420180113979"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801863.png" alt="image-20230420180150478"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201802271.png" alt="image-20230420180220305"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201803247.png" alt="image-20230420180355733"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201805077.png" alt="image-20230420180527334"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201806314.png" alt="image-20230420180609855"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201806384.png" alt="image-20230420180630486"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CountWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowOpJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//CountWindow之滚动窗口：每隔5个元素计算一次前5个元素</span></span><br><span class="line">        <span class="comment">/*text.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">            @Override</span></span><br><span class="line"><span class="comment">            public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span><br><span class="line"><span class="comment">                    throws Exception &#123;</span></span><br><span class="line"><span class="comment">                String[] words = line.split(" ");</span></span><br><span class="line"><span class="comment">                for (String word : words) &#123;</span></span><br><span class="line"><span class="comment">                    out.collect(new Tuple2&lt;String, Integer&gt;(word,1));</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;).keyBy(0)</span></span><br><span class="line"><span class="comment">                //窗口大小</span></span><br><span class="line"><span class="comment">                .countWindow(5)</span></span><br><span class="line"><span class="comment">                .sum(1)</span></span><br><span class="line"><span class="comment">                .print();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//CountWindow之滑动窗口：每隔1个元素计算一次前5个元素</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">                .countWindow(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"CountWindowOpJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自定义Window的使用"><a href="#自定义Window的使用" class="headerlink" title="自定义Window的使用"></a>自定义Window的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下自定义window。其实呢，window还可以再细分一下。可以把它分为呢，一种是基于Key的window。一种是不基于Key的window。其实就是说咱们在使用window之前是否执行了key操作啊，咱们前面演示的都是这种基于Key的window。你看我们在做window之前，前面呢都做了Keyby对吧，那如果呢，需求中不需要根据Key进行分组，你在使用window的时候啊，我们需要对应的去使用那个timeWindowAll和countWindowAll。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201829109.png" alt="image-20230420182825536"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">你使用KeyBy之后的话，它就只能调那个timeWindow,countWindow，这个需要注意一下啊，那如果说是我们自定义的window。如何使用呢？对吧，针对这两种情况。来看一下。针对这个基于Key的window呀，我们需要使用这个window函数</span><br><span class="line"></span><br><span class="line">那针对下面这种不基于Key的window呢，我们可以直接使用这个windowAll就可以了。其实呀，我们前面所说的那个timewindow和timewindowall底层用的就是这个window和windowall，你可以这样理解timewindow是官方封装好的window。所以说呢，timewindow和countwindow呢，都是官方封装好了。</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingProcessingTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：自定义MyTimeWindow</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyTimeWindowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//自定义MyTimeWindow滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_,<span class="number">1</span>))</span><br><span class="line">      .keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//窗口大小</span></span><br><span class="line">  .window(<span class="type">TumblingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)))<span class="comment">//注意这里和后面的基于eventtime计算有点不一样</span></span><br><span class="line">      .sum(<span class="number">1</span>)</span><br><span class="line">      .print()</span><br><span class="line">    env.execute(<span class="string">"MyTimeWindowScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个呢，和咱们之前啊使用的什么timewindow那个效果是一样的。这样的话更加灵活一些，我们想怎么定义都可以啊。如果你不使用这个KeyBy的话，那下面你就可以使用windowAll是一样的效果</span><br></pre></td></tr></table></figure><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：自定义MyTimeWindow</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTimeWindowJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自定义MyTimeWindow滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//窗口大小</span></span><br><span class="line">                .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"MyTimeWindowJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Window中的增量聚合和全量聚合"><a href="#Window中的增量聚合和全量聚合" class="headerlink" title="Window中的增量聚合和全量聚合"></a>Window中的增量聚合和全量聚合</h3><h4 id="增量聚合"><a href="#增量聚合" class="headerlink" title="增量聚合"></a>增量聚合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下Window聚合。就是在进行Window聚合操作的时候呢，可以分为两种情况。一种呢是增量聚合，还有一种是全量聚合。</span><br><span class="line"></span><br><span class="line">那下面我们首先来看一下这个增量聚合。增量聚合呢，它表示呀，窗口中每进入一条数据就进行一次计算，常见的一些增量聚合函数如下:</span><br><span class="line">reduce() aggregate() sum() min() max()</span><br><span class="line"></span><br><span class="line">那下面呢，我们来看一个增量聚合的案例啊，就是累加求和</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202150610.png" alt="image-20230420215009349"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">它的具体执行过程是这样的。第一次进来一条数据，则立刻进行累加，求和结果为八，第二次进来一条数据12，则立刻进行累加，求和结果为20。第三次进来一条数据七，则立刻进行累加求和，结果为27。第四次进来一条数据，则立刻进行累加求和，结果为37。这就是这个增量聚合它的一个执行流程。</span><br><span class="line"></span><br><span class="line">那下面呢，我们来看一下reduce函数的一个使用，从这里面我们可以看出来，reduce是每次获取一条数据和上一次的执行结果求和。也就是来一条数据，立刻计算一次，这个就是增量聚合。</span><br></pre></td></tr></table></figure><h4 id="全量聚合"><a href="#全量聚合" class="headerlink" title="全量聚合"></a>全量聚合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们来看一下全量集合。全量集合呀，它就是等属于窗口的数据都到齐了，才开始进行聚合计算，可以实现对窗口内的数据进行排序等需求。常见的一些全量聚合函数为：</span><br><span class="line">apply(windowFunction)，还有这个process(processWindowFunction)</span><br><span class="line">apply呢，它里面接触的是windowfunction,process里面接触是process windowfunction</span><br><span class="line">注意这个processwindowfunction比windowfunction提供了更多的上下文信息啊。那下面呢，我们来看一个全量聚合的一个案例，求最大值</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202157070.png" alt="image-20230420215701906"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第四次进来调数据10，此时窗口触发，这时候才会对窗口内的数据进行排序，然后获取最大值。</span><br></pre></td></tr></table></figure><h5 id="全量聚合apply"><a href="#全量聚合apply" class="headerlink" title="全量聚合apply"></a>全量聚合apply</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202158547.png" alt="image-20230420215834620"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这个apply函数的一个使用。从这你可以看出来，他接触的是一个iterable，可以认为是一个集合。他可以把这个窗口的数据啊，一次性全都传过来，当这个窗口触发的时候，才会真正执行这个代码。</span><br></pre></td></tr></table></figure><h5 id="全量聚合process"><a href="#全量聚合process" class="headerlink" title="全量聚合process"></a>全量聚合process</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202200468.png" alt="image-20230420220026246"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢是一个process。你看他接触的也是一个iterable，所以说呢，你在这里面就可以获取到这个窗口里面的所有数据了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个呢就是Windows中的全量聚合和增量聚合，后面呢我们就会用到这个apply，还有process它的一个使用，因为有时候我们需要对这个窗口内的所有数据去做一些全量的操作，这样的话就不能用这种增量聚合，而要用这种全量聚合。</span><br></pre></td></tr></table></figure><h3 id="Flink中的Time"><a href="#Flink中的Time" class="headerlink" title="Flink中的Time"></a>Flink中的Time</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对流数据的time可以分为以下三种。第一个Event Time表示事件产生的时间，它通常由事件中的时间戳来描述。第二个ingestion time表示事件进入flink的时间。第三个processing time，它表示事件被处理时当前系统的时间，那这几种时间呀，我们通过这个图可以很清晰的看出来它们之间的关系。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202206362.png" alt="image-20230420220641443"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">首先是even time，这个就是数据产生的时间。第二个是ingestion time表示呢，他进入flink时间，其实就是被那个source把它读取过来那个时间。第三个呢，是这个processing time，它其实呢，就是flink里面具体的算子，在处理的时候它的一个时间，那接下来我们来看一个案例。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202209327.png" alt="image-20230420220902227"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意你看数据呢，是在十点的时候产生的。结果呢，在晚上八点的时候才被flink读取走。那flink真正在处理的时候呢？是8.02秒。</span><br><span class="line">注意，如果说呀，我们想要统计每分钟内接口调用失败的错误日志个数。那这个时候使用哪个时间才有意义呢？因为数据有可能会出现延迟。如果使用那个数据进入flink的时间或者window处理的时间，其实是没有意义的。这个时候我们需要使用原始日中的时间才是有意义的，这个才是数据产生的时间，我们基于这个时间去统计才有意义。</span><br><span class="line">那我们在flink流水中默认使用的是哪个时间呢？某种情况下，flink在流处理中使用的时间是这个processingtime。那如果说我们想要修改的话，怎么改呢？可以使用这个env去改env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)可以设置这个time或者是这个IngestionTime。好，这就是flink中的三种time。</span><br></pre></td></tr></table></figure><h3 id="Watermark的分析"><a href="#Watermark的分析" class="headerlink" title="Watermark的分析"></a>Watermark的分析</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211000567.png" alt="image-20230421100013693"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实时计算中，数据时间比较敏感。有eventTime和processTime区分，一般来说eventTime是从原始的消息中提取过来的，processTime是Flink自己提供的，Flink中一个亮点就是可以基于eventTime计算，这个功能很有用，因为实时数据可能会经过比较长的链路，多少会有延时，并且有很大的不确定性，对于一些需要精确体现事件变化趋势的场景中，单纯使用processTime显然是不合理的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面提到了Time的概念，如果我们使用Processing Time，那么在Flink消费数据的时候，它完全不需要关心数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为Processing Time只是代表数据在Flink被处理时的时间，这个时间是顺序的。</span><br><span class="line">但是如果你使用的是Event Time的话，那么你就不得不面临着这么个问题：事件乱序&amp;事件延迟。</span><br><span class="line"></span><br><span class="line">所以…</span><br><span class="line">为了解决这个问题，Flink中引入了WaterMark机制，即水印的概念。、</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211008547.png" alt="image-20230421100850143"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">然而在有些场景下，尤其是特别依赖于事件时间而不是处理时间，比如：</span><br><span class="line">错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</span><br><span class="line">设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</span><br><span class="line">比如我做过的充电桩实时报文分析，就必须依赖报文产生的时间，即事件时间</span><br><span class="line">…</span><br><span class="line">针对上面的问题（事件乱序 &amp; 事件延迟），Flink引入了Watermark 机制来解决。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">统计8:00 ~ 9:00这个时间段打开淘宝App的用户数量，Flink这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在8:00 ~ 9:00中用户打开 App的事件数据，但又不能无限期的等下去？</span><br><span class="line"></span><br><span class="line">当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是Watermark的思想。</span><br><span class="line"></span><br><span class="line">Watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的Watermark。Watermark本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有Watermark大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink也有相应的机制（下文会讲）去处理。</span><br></pre></td></tr></table></figure><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个timestamp.watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现。</span><br><span class="line"></span><br><span class="line">流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（out-of-order或者说late element）。</span><br><span class="line"></span><br><span class="line">但是对于late element，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">比如：</span><br><span class="line">08:00任务开启，设置1分钟的滚动窗口，在08:00:00-08:01:00为第一个窗口，08:01:00-08:02:00为第二个窗口；</span><br><span class="line">现在有一条数据的事件时间是08:00:50，但是这条数据却在08:01:10到达，按照正常的处理，窗口会在结束时间（08:01:00）的时候就触发计算，那么这条数据就会被丢弃；</span><br><span class="line">但是开启WaterMark后，窗口在08:01:00时不会触发；</span><br><span class="line">因为采用的是EventTime，而数据本身时间是08:00:50，所以该条数据肯定会落到第一个窗口；</span><br><span class="line">假设在08:01:10时的WaterMark为08:01:00（WaterMark可以理解为一个时间戳），发现这个WaterMark和第一个窗口的结束时间相等，此时触发第一个窗口的计算操作，此时这条延迟数据正好参与到计算中；</span><br><span class="line">此时只有水印大于或等于窗口结束时间才会触发窗口的关闭和计算；</span><br><span class="line">此时就不会丢数据。</span><br></pre></td></tr></table></figure><h4 id="WaterMark的传递"><a href="#WaterMark的传递" class="headerlink" title="WaterMark的传递"></a>WaterMark的传递</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Watermark在向下游传递时，是广播到下游所有的子任务中，如果多并行度下有多个watermark传递到下游时，取最小的watermark。</span><br></pre></td></tr></table></figure><h4 id="WaterMark设置"><a href="#WaterMark设置" class="headerlink" title="WaterMark设置"></a>WaterMark设置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注：如果你采用的是事件时间，即你设置了 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">那么你就必须设置获取事件时间的方法，否则会报错（如果是从kafka消费数据，不设置水印的话，默认采用kafka消息自带的时间戳作为事件时间）</span><br><span class="line"></span><br><span class="line">数据处理中需要通过调用DataStream中的 assignTimestampsAndWatermarks方法来分配时间和水印，该方法可以传入两种参数，一个是AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。</span><br><span class="line">所以设置Watermark是有如下两种方式：</span><br><span class="line"></span><br><span class="line">AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime都会产生一个Watermark。</span><br><span class="line">AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实际生产中用第二种的比较多，它会周期性产生Watermark的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时。</span><br></pre></td></tr></table></figure><h3 id="开发Watermark代码"><a href="#开发Watermark代码" class="headerlink" title="开发Watermark代码"></a>开发Watermark代码</h3><h4 id="乱序数据处理"><a href="#乱序数据处理" class="headerlink" title="乱序数据处理"></a>乱序数据处理</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了what mark的一些基本原理，可能大家对它还不够了解，下面我们来通过这个案例加深大家对what mark的理解。我们来分析一下这个案例。乱序数据处理</span><br><span class="line"></span><br><span class="line">通过socket模拟数据。数据的格式是这样的。前面的话代表的是具体的业务数据，后边的话是一个时间戳，这是一个毫秒的时间戳。中间用逗号分隔。</span><br><span class="line"></span><br><span class="line">其中，时间戳是数据产生的时间。也就是even time。那产生这个数据之后呢？然后使用map函数，把数据转换为tuple2的形式。接着再调用这个函数assignTimestampsAndWatermarks。使用这个方法来抽取timestamp并生成watermark。</span><br><span class="line">接着，再调用window打印信息，来验证window被触发的时机。最后验证乱序数据的处理方式，这是我们一个大致的一个处理流程。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">      env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳(EventTime)和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>) <span class="comment">// currentMaxTimstamp它的第一个参数值应该是传错了</span></span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样(这里和前面自定义window时，传的参数有点不一样，这里是event)</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>],<span class="type">String</span>,<span class="type">Tuple</span>,<span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup=&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr+<span class="string">","</span>+arr.length+<span class="string">","</span>+sdf.format(arr.head)+<span class="string">","</span>+sdf.format(arr.last)+<span class="string">","</span>+sdf.format(window.getStart)+<span class="string">","</span>+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="根据数据跟踪观察Watermark"><a href="#根据数据跟踪观察Watermark" class="headerlink" title="根据数据跟踪观察Watermark"></a>根据数据跟踪观察Watermark</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211425983.png" alt="image-20230421142543318"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211428308.png" alt="image-20230421142830892"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211435405.png" alt="image-20230421143511791"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211438988.png" alt="image-20230421143817886"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211440736.png" alt="image-20230421144017493"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211441254.png" alt="image-20230421144059007"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211441192.png" alt="image-20230421144132566"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211442566.png" alt="image-20230421144203353"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到这里，window仍然没有被触发，此时watermark的时间已经等于第一条数据的eventtime了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211445272.png" alt="image-20230421144516628"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211445660.png" alt="image-20230421144534532"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">window仍然没有被触发，此时，我们数据已经发送到2026-10-01 10:11:33了，根据eventtime来算，最早的数据已经过去了11s了，window还没开始计算，那到底什么时候会触发window呢？</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211450426.png" alt="image-20230421145025475"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211451804.png" alt="image-20230421145103798"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211452915.png" alt="image-20230421145208979"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到这里，我们做了一个说明。</span><br><span class="line">window的触发机制，是先按照自然时间将window划分，如果window大小是3s，那么1min内会把window划分成如下的形式(左闭右开的区间)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211459275.png" alt="image-20230421145940525"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211500839.png" alt="image-20230421150008692"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">window的设定无关数据本身，而是系统定义好了的。</span><br><span class="line">输入的数据，根据自身的eventtime，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;&#x3D;eventtime时，就符合了window触发的条件了，最终决定window触发，还是由eventtime所属window中的window_end_time决定。</span><br><span class="line"></span><br><span class="line">上面的测试中，最后一条数据到达后，其水位线(watermark)已经上升至10:11:24，正好是最早的一条记录所在window的window_end_time，所以window就被触发了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211511116.png" alt="image-20230421151156542"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211513051.png" alt="image-20230421151309893"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时，watermark时间虽然已经等于第二条数据的时间，但是由于其没有达到第二条数据所在window，但是由于其没有达到第二条数据所在window的结束时间，所以window并没有被触发。那么，第二条数据所在的window时间区间如下。 </span><br><span class="line">[00:00:24,00:00:27)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">也就是说，我们必须输入一个10:11:37的数据，第二条数据所在的window才会被触发，我们继续输入。</span><br><span class="line">0001,1790820697000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211520479.png" alt="image-20230421152050886"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时，我们已经看到，window的触发条件要符合以下几个条件：</span><br><span class="line">1.watermark时间&gt;&#x3D;wind_end_time</span><br><span class="line">2.在[window_start_time,window_end_time)区间中有数据存在(注意是左闭右开的区间)</span><br></pre></td></tr></table></figure><h3 id="Watermark-EventTime处理乱序数据"><a href="#Watermark-EventTime处理乱序数据" class="headerlink" title="Watermark+EventTime处理乱序数据"></a>Watermark+EventTime处理乱序数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我们前面那个测试啊，数据呢，都是按照这个时间顺序递增的，都是有序的，那现在呢，我们来输入一些的数据，来看看这个whatmark，结合这个一的eventtime机制是如何处理这些乱写数据的。那我们在上面那个基础之上啊，再输入两行数据。</span><br><span class="line">注意这个呢，没有触发对吧</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211904910.png" alt="image-20230421190455535"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211905517.png" alt="image-20230421190548292"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们再输入一条43秒的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211916109.png" alt="image-20230421191608587"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211919959.png" alt="image-20230421191923374"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211920175.png" alt="image-20230421192030363"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">大家注意他没有这个33的。我这个窗口呢，是[30,33)，你看我这个一的代表里面数据是有这个33的，为什么这个33没有输出呢。因为这个窗口啊，它是一个左闭右开的。那这个33的话，它其实啊，属于下一个窗口，就是33到36的那个窗口。</span><br><span class="line">好。所以上面这个结果其实已经表明对迟到的数据了，flink可以通过这个watermark来实现处理一定范围内的乱序数据。因为现在我们允许的最大乱序时间是十秒。就是十秒之内乱序是OK的，那如果超过了这个十秒怎么办？也就是说呢，对于这个迟到(late element)太久的数据，flink是怎么处理的呢？</span><br></pre></td></tr></table></figure><h3 id="延时数据的三种处理方式"><a href="#延时数据的三种处理方式" class="headerlink" title="延时数据的三种处理方式"></a>延时数据的三种处理方式</h3><h4 id="丢弃"><a href="#丢弃" class="headerlink" title="丢弃"></a>丢弃</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们就来看一下针对迟到太久的数据，它的一些处理方案，现在呢一共有三种。</span><br><span class="line">第一种是丢弃默认的啊，那我们再来演示一下。那我们首先呢，来输入一个乱序很多的数据来测试一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221025938.png" alt="image-20230422102536854"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意，下面呢，我们再来输入几个一定的eventtime小于whatmark的时间</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221027933.png" alt="image-20230422102748626"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">看到没有，这三条他都没有触发这个窗口的执行啊，因为你现在你输入的数据所在的窗口已经执行过了。flink默认对这些迟到的数据的处理方案就是丢弃。这几条数据，30对应的那个窗口数据是不是已经执行过了呀，那这样过来它直接丢弃，这是默认的一个处理方案。</span><br></pre></td></tr></table></figure><h4 id="allowedLateness"><a href="#allowedLateness" class="headerlink" title="allowedLateness"></a>allowedLateness</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那接下来看第二种，你可以通过这个allowedLateness来指定一个允许数据延迟的时间本身啊，我们之前通过那个watermark已经设置了一个数据的延迟时间是十秒，对吧。你可以通过这个参数啊，再给他指定一个延迟时间，就类似于我们上班打卡官方延迟对吧，类似于公司统一层面允许大家呢弹性半小时。但是你们这个部门呢，可以再多谈十分钟，有这种效果。</span><br><span class="line"></span><br><span class="line">在某些情况下，我们希望对迟到的数据再提供一个宽容时间。那flink提供了这个方法，可以实现对迟到的数据啊，再给它设置一个延迟时间，在指定延迟时间内到达数据还是可以触发window执行的。所以这时候我们需要去改一下代码了。主要呢，就增加这一行就行。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpForAllowedLatenessScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//允许数据迟到2秒</span></span><br><span class="line">      .allowedLateness(<span class="type">Time</span>.seconds(<span class="number">2</span>))</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>],<span class="type">String</span>,<span class="type">Tuple</span>,<span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup=&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr+<span class="string">","</span>+arr.length+<span class="string">","</span>+sdf.format(arr.head)+<span class="string">","</span>+sdf.format(arr.last)+<span class="string">","</span>+sdf.format(window.getStart)+<span class="string">","</span>+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221044535.png" alt="image-20230422104446863"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221047820.png" alt="image-20230422104738555"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们再输入几条二苯酮 time小于watermark的一个数据来验证一下效果。好，来看一下。注意你会发现，你看。这三条数据过来的时候，窗口同样被触发了，因为之前的话，我们是这个30到33这个窗口对吧。我在这输的这三条数据，一个是30秒了，31、32，它们都属于那个窗口。岁数，你看窗口都被吃光。你看这时候打印的窗口数据是两条，这是三条，这是四条对吧。所以说呢，每条数据都触发了window的执行啊。这三条数据。那下面我们再输一条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221050530.png" alt="image-20230422105039949"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221058268.png" alt="image-20230422105851955"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221100903.png" alt="image-20230422110036594"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这时候啊，我们把这个whatmark呀，给它调到34。往上面调一下。看到没有，这次呢，它是没有触发的啊是34。数据呢是44，这样的话，whatmark变成了34。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221103748.png" alt="image-20230422110328208"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221105374.png" alt="image-20230422110533985"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时呢，把whatmark上升到了34。此时呢，我们再输入几条这种迟到的数据来验证一下效果。因为刚才的话，我们验证了它是可以执行的啊。嗯。结果你会发现，看到没有，这三条又执行了。我们发现数的这三条数据呢，它都触发了window执行。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221107423.png" alt="image-20230422110715967"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221108095.png" alt="image-20230422110824496"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们再输入一行数据，把这个我的妈再调一下，调到35。嗯。对吧，这是刚才调到35。给你输入一下45的数据，我慢了变成35，我把这个清一下。这时候它就上升到了35，</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221112669.png" alt="image-20230422111217143"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意下面我们再输入几条十到的数据，还是那个三十三十一三十二啊。嗯。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221114742.png" alt="image-20230422111430693"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221116286.png" alt="image-20230422111654208"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意此时这个窗口就不会触发，相当于啊，你这个时候你这个迟到的数据啊，我就不管了。来分析一下啊。你看我们又发现这几条数据啊，它都没有触发window啊。那分析一下，当这个呀等于这个33的时候，它正好呢，是属于这个window n time对吧，正好相等，所以说呢，它会触发这个30到33这个窗口执行。当这个窗口执行过后啊，我们再输入30到33这个窗口内的数据的时候呢，会发现这个窗口是可以被触发的。当我们把这个mark提升到34秒的时候，我们再输入这个窗口内的数据，发现温度还是可以被触发的。那当我们把这个mark牺升到35的时候，再输入这个窗口数据，发现window不会被触发。这是为什么呢？这是因为我们在前面设置了这个参数。又给它多加了两秒延迟，因此呢，可以允许延迟在两秒内的数据继续触发温度执行。所以说当你这个我的ma等于34的时候，是可以触发温度的，但是35就不行了，这个需要注意一下。这块有个总结啊，对于这个窗口而言啊，它允许两秒的迟到数据，也就是说呢，你第一次触发是在这个o ma呢，它大于等于这个window and time的时候，对吧，那第二次或者啊以后多次触发条件是这样的。小于window n time加上这个。就是允许的迟到时间加这个二。对吧，并且呢，这个窗口有迟到数据的时候，它就会被触发。那所以说你看，当我们这个omark等于34的时候，我们输入什么三十三十一三十二秒的数据，它是可以出发的，因为这些数据它们的window n time呢，都是这个33。那又是说了，你这个你看是三十四三十四的话，你小于33加二对吧，它是小于的这个是不是出了啊，但是呢，当这个等于35的时候呢，我们再去输入这个三十三十一三十二，这个其实就迟到太久太久太久了。这些数据呢，我n time呢，还是这个33，此时注意我mark是三十五三十五小于这个33加二嘛，你不要不要去较这个真啊。这个是秒，你说一个秒加个二，这啥意思呢？对吧，是尾电门啊，就是33秒加上两秒就是三十五三  十五是不小于35。所以说就是暴走，那所以说最终这些数据呢，迟到时间太久了。本来呢，公司打卡弹性半小时，你们部门啊，又谈了十分钟，结果呢，你还是没有满足这个要求啊。这样就没办法对吧，那这时候呢，就不会再触发温度的执行了，就把你扔掉不管你了。这就是第二种处理方案。</span><br></pre></td></tr></table></figure><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221121053.png" alt="image-20230422112107568"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221121425.png" alt="image-20230422112129357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221603475.png" alt="image-20230422160343061"></p><h4 id="sideOutputLateData"><a href="#sideOutputLateData" class="headerlink" title="sideOutputLateData"></a>sideOutputLateData</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面呢，还有一种处理方案。这个呢，就是收集迟到数据。通过这个函数呢，可以把迟到的数据啊，给它统一收集，统一存储，方便后期排查，问题就是你为什么迟到这么久对吧，这个呢也需要去调整代码，你呢先创建这个out，咱们前面是不是已经用过呀</span><br><span class="line"></span><br><span class="line">注意咱们刚才讲那个第二种方案，其实可以和第三种结合到一块儿来使用，都是可以的啊，你再给他延迟两秒，如果说他还是没有到达，对吧，那你就把它保存起来，丢了保存起来。当然也可以单独使用，都是可以的啊，这个需要具体根据你们的业务需求来定。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">OutputTag</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpForSideOutputLateDataScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间 10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//保存被丢弃的数据</span></span><br><span class="line">    <span class="keyword">val</span> outputTag = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>]](<span class="string">"late-data"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resStream = waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//保存被丢弃的数据</span></span><br><span class="line">      .sideOutputLateData(outputTag)</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>], <span class="type">String</span>, <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup =&gt; &#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr + <span class="string">","</span> + arr.length + <span class="string">","</span> + sdf.format(arr.head) + <span class="string">","</span> + sdf.format(arr.last) + <span class="string">","</span> + sdf.format(window.getStart) + <span class="string">","</span> + sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;) </span><br><span class="line"></span><br><span class="line">    <span class="comment">//把迟到的数据取出来，暂时打印到控制台，实际工作中可以选择存储到其它存储介质中</span></span><br><span class="line">    <span class="comment">//例如：redis，kafka</span></span><br><span class="line">    <span class="keyword">val</span> sideOutput = resStream.getSideOutput(outputTag)</span><br><span class="line">    sideOutput.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将流中的结果数据也打印到控制台</span></span><br><span class="line">    resStream.print()</span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们来验证一下，先输入这两行数据。第一条。所以你看第一次发了一个30，这是43对吧。此时，这头的mark是33。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221620417.png" alt="image-20230422162002780"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221620154.png" alt="image-20230422162014105"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们再输入几条event time小于watermark的一个时间来测试一下啊。现在你这个窗口已经执行过了，我们再往里添加数据来看一下效果。还这三条数据啊。来看一下，注意这个呢，是正常他都会打印的，你看这时候你在输入这三条的时候，他注意它那个窗口就没有执行了，下面这个数据不是那个窗口打印出来，窗口打印出来数据是这种格式啊。这个是谁打印的呀？所以说呢，针对这个迟到的数据，我们就把它放到这里边儿了，这样后期呢，你就可以把这数据可以存到其他地方，方便你们去排查问题，为什么这个数据来这么迟啊，对吧，可以分析一下问题，看是网络原因啊或者是其他原因。是这个啊。那这时候呢，你看针对这个迟到的数据，我们就可以通过这个set out来保存到这个out中。后期你想在保存的其他存储介质中也是没有任何问题的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221622695.png" alt="image-20230422162202360"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221623442.png" alt="image-20230422162306215"></p><h3 id="在多并行度下的watermark应用"><a href="#在多并行度下的watermark应用" class="headerlink" title="在多并行度下的watermark应用"></a>在多并行度下的watermark应用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面呢，我们演示了在单并行度下whatmark的使用，下面呢我们来看一下在多并行度下面watermark的一个使用。咱们前面的话在这设置为一。如果说你把这行代码给它做掉的话，你不设置的话。那我们在IDE中去执行的时候，默认呢，它会读取我本地的CPU的数量来设置默认命度。那所以说我在这把这个给它直接做掉就行了。做了之后啊，你可以在这啊加一个系统ID，这样的话我们就知道了是哪条数据被哪个线程所处理。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.window</span><br><span class="line"></span><br><span class="line">import java.text.SimpleDateFormat</span><br><span class="line">import java.time.Duration</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.eventtime.&#123;SerializableTimestampAssigner, WatermarkStrategy&#125;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.api.scala.function.WindowFunction</span><br><span class="line">import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line">import org.apache.flink.streaming.api.windowing.windows.TimeWindow</span><br><span class="line">import org.apache.flink.util.Collector</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line">import scala.util.Sorting</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Watermark+EventTime解决数据乱序问题</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object WatermarkOpMoreParallelismScala &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    &#x2F;&#x2F;设置使用数据产生的时间：EventTime</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">    &#x2F;&#x2F;设置全局并行度为1</span><br><span class="line">    env.setParallelism(2)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;设置自动周期性的产生watermark，默认值为200毫秒</span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(200)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val text &#x3D; env.socketTextStream(&quot;bigdata04&quot;, 9001)</span><br><span class="line">    import org.apache.flink.api.scala._</span><br><span class="line">    &#x2F;&#x2F;将数据转换为tuple2的形式</span><br><span class="line">    &#x2F;&#x2F;第一列表示具体的数据，第二列表示是数据产生的时间戳</span><br><span class="line">    val tupStream &#x3D; text.map(line &#x3D;&gt; &#123;</span><br><span class="line">      val arr &#x3D; line.split(&quot;,&quot;)</span><br><span class="line">      (arr(0), arr(1).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;分配(提取)时间戳和watermark</span><br><span class="line">    val waterMarkStream &#x3D; tupStream.assignTimestampsAndWatermarks(WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)) &#x2F;&#x2F;最大允许的数据乱序时间 10s</span><br><span class="line">      .withTimestampAssigner(new SerializableTimestampAssigner[Tuple2[String, Long]] &#123;</span><br><span class="line">        val sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)</span><br><span class="line">        var currentMaxTimstamp &#x3D; 0L</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;从数据流中抽取时间戳作为EventTime</span><br><span class="line">        override def extractTimestamp(element: (String, Long), recordTimestamp: Long): Long &#x3D; &#123;</span><br><span class="line">          val timestamp &#x3D; element._2</span><br><span class="line">          currentMaxTimstamp &#x3D; Math.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          &#x2F;&#x2F;计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark&#x3D;currentMaxTimstamp-OutOfOrderness</span><br><span class="line">          val currentWatermark &#x3D; currentMaxTimstamp - 10000L</span><br><span class="line">          </span><br><span class="line">          val threadId &#x3D; Thread.currentThread().getId</span><br><span class="line">          &#x2F;&#x2F;此print语句仅仅是为了在学习阶段观察数据的变化</span><br><span class="line">          println(&quot;threadId:&quot;+threadId+&quot;,key:&quot; + element._1 + &quot;,&quot; + &quot;eventtime:[&quot; + element._2 + &quot;|&quot; + sdf.format(element._2) + &quot;],currentMaxTimstamp:[&quot; + currentWatermark + &quot;|&quot; + sdf.format(currentMaxTimstamp) + &quot;],watermark:[&quot; + currentWatermark + &quot;|&quot; + sdf.format(currentWatermark) + &quot;]&quot;)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(0)</span><br><span class="line">      &#x2F;&#x2F;按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span><br><span class="line">      .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class="line">      &#x2F;&#x2F;使用全量聚合的方式处理window中的数据</span><br><span class="line">      .apply(new WindowFunction[Tuple2[String,Long],String,Tuple,TimeWindow] &#123;</span><br><span class="line">        override def apply(key: Tuple, window: TimeWindow, input: Iterable[(String, Long)], out: Collector[String]): Unit &#x3D; &#123;</span><br><span class="line">          val keyStr &#x3D; key.toString</span><br><span class="line">          &#x2F;&#x2F;将window中的数据保存到arrBuff中</span><br><span class="line">          val arrBuff &#x3D; ArrayBuffer[Long]()</span><br><span class="line">          input.foreach(tup&#x3D;&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          &#x2F;&#x2F;将arrBuff转换为arr</span><br><span class="line">          val arr &#x3D; arrBuff.toArray</span><br><span class="line">          &#x2F;&#x2F;对arr中的数据进行排序</span><br><span class="line">          Sorting.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          val sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)</span><br><span class="line">          &#x2F;&#x2F;将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span><br><span class="line">          val result &#x3D; keyStr+&quot;,&quot;+arr.length+&quot;,&quot;+sdf.format(arr.head)+&quot;,&quot;+sdf.format(arr.last)+&quot;,&quot;+sdf.format(window.getStart)+&quot;,&quot;+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;WatermarkOpScala&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这时候窗口并没有触发。比如我们发现这个window没有被触发，因为这个时候呢，这七条数据啊，都是被不同的线程处理的，每个线程呢，都有一个watermark。我们前面分析了，在这种多并行度的情况下呢，whatmark呢，它呢有一个对齐机制，它呢会取所有材中最小的那个wordmark。所以说我们现在有八个并行度，你这七条数据呢，都被不同的线路所处理啊。到现在呢，还没有获取到最小的那个我。所以说呢，这个window是无法被处罚执行的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221654047.png" alt="image-20230422165413793"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221654240.png" alt="image-20230422165427792"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为这个线路太多了，验证起来了，不太好验证，所以说啊这样。把这个稍微再改一下。我们也不用了一个默认的八个了，我们给它改成两个吧。这个也是多并行度了。好。接下来呢，我们往里面输这么三条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221700312.png" alt="image-20230422170004992"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221702331.png" alt="image-20230422170229823"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一条，没有触发。接下来，第二条，其实理论上如果是单线程的话，这个时候这个窗口已经被触发，但是现在呢，还没有触发。这第三条数据。嗯。好。看到没有，这个时候他就出发。看一下这块的一个总结。此时呢，我们会发现，当第三条数据输入完以后，这个窗口呢，它就被触发了。你前两条数据啊，输入之后呢，它获取到的那个具体的wordmark是20。这个时候呢，它对应的window中呢，是没有数据的，所以说呢，什么都没有执行，当你第三条数据输入之后呢，它获取到那个最小的mark呢，就是33了，这个时候呢，它对应的窗口就是它，它里面有数据，所以说呢，这个window就触发了。</span><br></pre></td></tr></table></figure><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来针对这个watermark案例做一个总结。我们在flink中，针对这个watermark，我们该如何设置它的最大乱序时间？注意。最大乱序时间。首先第一点这个要结合我们自己的业务以及呢数据的实际情况去设置，如果说呢，这个最大落地时间设置的太小，而我们那个自身数据啊，发送时由于网络等原因导致乱序或者迟到太多，那么呢，最终的结果就是会有很多数据被丢弃。这样的话，对我们数据的正确性影响太大。那对于这个严重外序的数据呢？我们需要严格统计数据的最大延迟时间。这样才能最大程度保证计算数据的一个准确度。延时时间呢？实时太小会影响数据准确性。延时时间是太大，不仅影响数据的一个实时性。更加的会加重flink作业的一个负担。所以说不是对eventtime要求特别严格的数据，尽量呢不要采用这种eventtime的方式来处理数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221721954.png" alt="image-20230422172108445"></p><h2 id="Flink并行度"><a href="#Flink并行度" class="headerlink" title="Flink并行度"></a>Flink并行度</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们来分析一下Flink的并行度。一个flink程序由多个组件组成，datasource、transformation、datasink。</span><br><span class="line">一个组件呢，由多个并行的实例来执行，或者说呢，是由多个线程来执行。一个组件的并行实际数目呢？就被称之为该组件的并行度。其实就是说你这个组件有多少个线程去执行，那么它的并行度就是多少。</span><br></pre></td></tr></table></figure><h3 id="task-manager和slot之间的关系"><a href="#task-manager和slot之间的关系" class="headerlink" title="task manager和slot之间的关系"></a>task manager和slot之间的关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，在具体分析这个并行度之前，我们先分析一下这个task manager和slot之间的关系。flink的每个task manager为集群提供的slot的数量通常与每个task manager的可用CPU数量成正比。一般情况下的数量就是每个task manager的可用CPU数量。这个task manager节点就是我们集群的一个从节点。那上面这个slot数量就是这个task manager具有的一个并发执行能力。这里面啊，实行的就是具体的一些实例。source、map、keyBy、sink。还有这个图也是一样的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221751734.png" alt="image-20230422175152422"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221755921.png" alt="image-20230422175512133"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们就来看一下这个并行度该如何来设置。任务的并行度可以通过四个层面来设置。首先第一个是算式层面。第二个是执行环境层面。第三个是客户端层面。第四个呢，是系统层面。</span><br><span class="line">那这四个层面，他们执行的优先级是什么样的？注意。这个算式层面了大于执行环境层面的，执行环境层面了大于客户端层面了，客户端层面了大于系统层面。这是他们之间的优先级。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221757173.png" alt="image-20230422175706876"></p><h3 id="Operator-Level"><a href="#Operator-Level" class="headerlink" title="Operator Level"></a>Operator Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面我们来具体分析一下这四种。首先看这个算子层面的。算子层面其实很简单，首先呢，在这去设置就可以了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221758256.png" alt="image-20230422175809024"></p><h3 id="Execution-Environment-Level"><a href="#Execution-Environment-Level" class="headerlink" title="Execution Environment Level"></a>Execution Environment Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个执行环境层面的。主要呢，是在这个ENV后面来设置一个并行度。这设置的是一个全局的并行度。当然，你也可以选择在下面针对某一个算子再去改它的并行度也是可以的。因为你那个算子层面并行度是大于这个执行环境层面这个并行度的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221758507.png" alt="image-20230422175856304"></p><h3 id="Client-Level"><a href="#Client-Level" class="headerlink" title="Client Level"></a>Client Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来是一个客户端层面。这个并行度呢，可以在客户端提交Job的时候来设定。通过那个-P参数来动态指令就可以了。具体呢，是这样的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221802253.png" alt="image-20230422180247995"></p><h3 id="System-Level"><a href="#System-Level" class="headerlink" title="System Level"></a>System Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那最后呢，是这个系统层面了。我们在系统层面可以通过在这个配置文件里面来设定。parallelism.default属性来指定所有执行环境的默认并行度啊，当然了，你是可以在具体的任务里面再去动态的去改这个并行度。因为他们呢，可以覆盖这个系统层面的并行度。</span><br></pre></td></tr></table></figure><h3 id="并行度案例分析"><a href="#并行度案例分析" class="headerlink" title="并行度案例分析"></a>并行度案例分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来通过一些案例来具体分析一下Flink中的并行度。首先看这个图。这个图里面呢，它表示啊，我们这个集群是有三个从节点。M1，M2，M3，注意每个节点上面具有三个slot。这个表示这个从节点，它具有的3个并发处理能力。那如何实现三个呢？在这个flink-conf.yaml里面来配置了taskmanager.numberOfTaskSlots，把它设置为3。这样话相当于我这个节点上面有三个空闲CPU。那这样的话，我这个集群啊，目前具有的一个处理能力就是9 slot</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221811958.png" alt="image-20230422181155611"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一个案例，它的并行度为1，那如何让它的并行度为1呢？很简单，你在提交这个任务的时候，什么参数都不设置就行。并且我们在开发这个word代码的时候，里面啊，也不设置并行度相关的代码，这样就可以了，这样它就会默认呢，读取这个flink-conf.yaml里面的parallelism的值。这个参数的默认值为1。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221817269.png" alt="image-20230422181742166"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第二个案例，如何实现让它的并行度为2呢？你可以通过这几种方式，首先呢，去改了一份文件。把里面这个默认参数值改为二，或者说我们在动态提交的时候通过-P来指定。或者我们通过这个env来设置都是可以的。这样的话呢，我抗里面它的一个冰度都为二。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221823152.png" alt="image-20230422182315555"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221827716.png" alt="image-20230422182700747"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第三个案例，它的并行度为9，那如何实现呢？你要么在这个配置文件里面，把这个参数设置为9，要么呢动态指定。要么呢，通过env来设置都是可以的。这样的话，它就是9份了。这样就占满了，那说我能不能把这个并行度设为10呢？不能，因为你现在最终呢，只有九个slot。这个需要注意啊。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221831842.png" alt="image-20230422183105341"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第四个。我看呀，它这个并行度呢，还是9，但是注意针对这个sink组件的并行度啊，给它设置为1啊。我们在这主要分析一下这个新的组件并行度，全局设置为9，就是根据咱们前面这个案例。这三种你用哪种都可以。但是呢，我们还需要把这个新的组件并行度设置为1，那怎么设置呢？就说你在代码里面啊，通过算式层面来把这个新组件的并行度设置为1，这样的话它就会覆盖那个全局的那个9。当然你其他组件还是按那个九那个并行度去执行，而我这个组件的话，我在这给它覆盖掉，使用一给它覆盖掉。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-5</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-5.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-5.html</id>
    <published>2023-04-17T08:47:07.000Z</published>
    <updated>2023-04-19T08:27:36.561Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第四章-Hbase"><a href="#第四章-Hbase" class="headerlink" title="第四章 Hbase"></a>第四章 Hbase</h1><h2 id="01-Hbase基本原理"><a href="#01-Hbase基本原理" class="headerlink" title="01 Hbase基本原理"></a>01 Hbase基本原理</h2><h3 id="Region定位–region"><a href="#Region定位–region" class="headerlink" title="Region定位–region"></a>Region定位–region</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182307738.png" alt="image-20230418230749193"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在HBase中，表的所有行都是按照RowKey的字典序排列的，表在行的方向上分割为多个分区（Region）。如图1所示。</span><br><span class="line">每张表一开始只有一个Region，但是随着数据的插入，HBase会根据一定的规则将表进行水平拆分，形成两个Region，当表中的行越来越多时，就会产生越来越多的Region，而这些Region无法存储到一台机器上时，需要分布存储到多台机器上。每个Region服务器负责管理一个Region，通常在每个Region服务器上会放置10~1000个Region，HBase中Region的物理存储如图2所示。</span><br><span class="line"></span><br><span class="line">客户端在插入，删除，查询数据时需要知道哪个Region服务器上存有自己所需的数据，这个查找Region的过程称之为Region定位。</span><br><span class="line"></span><br><span class="line">HBase中每个Region由三个主要要素组成，包括Region所属的表、包含的第一行和包含的最后一行。</span><br></pre></td></tr></table></figure><h3 id="Region定位–META表"><a href="#Region定位–META表" class="headerlink" title="Region定位–META表"></a>Region定位–META表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">有了Region标识符，就可以唯一标识每个Region。为了定位每个Region所在的位置，就可以构建一张映射表，映射表的每个条目包含两项内容，一个是Region标识符，另一个是Region服务器标识，这个条目就表示Region和Region服务器之间的对应关系，从而就可以知道某个Region被保存在哪个Region服务器中。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182308191.png" alt="image-20230418230821649"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">映射表包含了关于Region的元数据，因此也被称为“元数据表”，又名“meta表”。可以scan命令查看meta</span><br><span class="line"></span><br><span class="line">表的结构如图3所示。</span><br><span class="line">Meta表中每一行记录了一个Region的信息。</span><br><span class="line">首先RowKey包含表名、起始行键和时间戳信息。</span><br><span class="line">中间用逗号隔开，第一个Region的起始行键为空。</span><br><span class="line">时间戳只有用.隔开的为分区名称的编码字符串，该信息是由前面的表名、起始行键和时间戳进行字符串编码后形成。</span><br><span class="line">Meta表里有一个列族info。info包含了三个列，分别为regionInfo、server和serverstartcode。</span><br><span class="line">Regioninfo中记录了Region的详细信息，包括行键范围StartKey和EndKey、列族列表和属性。</span><br><span class="line">Server记录了管理该Region的Region服务器的地址，如localhost:16201。</span><br><span class="line">Serverstartcode记录了Region服务器开始托管该Region的时间。</span><br></pre></td></tr></table></figure><h3 id="Region定位–Region定位"><a href="#Region定位–Region定位" class="headerlink" title="Region定位–Region定位"></a>Region定位–Region定位</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182309337.png" alt="image-20230418230932193"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在HBase的早期设计中，Region的查找是通过三层架构来进行查询的，即在集群中有一个总入口ROOT表，记录了meta表分区信息及各个入口地址，这个ROOT表存放在某个Region服务器上，但是在Zookeeper中保存有它的地址。这种早期的三层架构通过先找到ROOT表，从中获取分区meta表位置，然后再获取分区meta表信息，得到Region所在的Region服务器。</span><br><span class="line">从0.96版本以后，三层架构被改为二层架构，去掉了ROOT表，同时Zookeeper中的&#x2F;hbase&#x2F;root-region-server也被去掉。meta表所在的RegionServer信息直接存储在Zookeeper中的&#x2F;hbase&#x2F;meta-region-server中。如图所示</span><br><span class="line">当客户端进行数据操作时，根据操作的表名和行键通过一定的顺序寻找对应的分区数据。</span><br><span class="line">客户端通过Zookeeper获取到Meta表分区存储的地址，然后在对应Region服务器上获取meta表的信息，得到所需表和行键所在的Region信息，然后在从Region服务器上找到所需的数据。一般客户端获取到Region信息后会进行缓存，下次再查询不必从Zookeeper开始寻址。</span><br></pre></td></tr></table></figure><h3 id="数据存储与读取"><a href="#数据存储与读取" class="headerlink" title="数据存储与读取"></a>数据存储与读取</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">HBase集群数据的构成如图所示</span><br><span class="line">HBase的核心模块是Region服务器。</span><br><span class="line">Region服务器由多个Region块构成，Region块中存储的一系列连续的数据集。</span><br><span class="line">Region服务器主要构成部分是：HLog和Region块。</span><br><span class="line">HLog记录该Region的操作日志。</span><br><span class="line">Region对象由多个Store组成，每个Store对应当前分区中的一个列族，每个Store管理一块内存，即MemStore。</span><br><span class="line">当MemStore中的数据达到一定条件时会写入到StoreFile文件中，因此每个Store包含若干个StoreFile文件。StoreFile文件对应HDFS中的HFile文件。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182338403.png" alt="image-20230418233830777"></p><h4 id="MemStore"><a href="#MemStore" class="headerlink" title="MemStore"></a>MemStore</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当Region服务器收到写请求的时候，Region服务器会将请求转至相应的Region。数据首先写入到Memstore，然后当到达一定的阀值的时候，Memstore中的数据会被刷到HFile中进行持久性存储。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase将最近接收到的数据缓存在MemStore中，在持久化到HDFS之前完成排序，再顺序写入HDFS，为后续数据的检索做了优化。因为MemStore缓存的是最近增加的数据，所以也提高了对近期数据的操作速度。在持久化写入之前，在内存中对行键或单元格做些优化。</span><br></pre></td></tr></table></figure><h4 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Store是Region服务器的核心，存储的是同一个列族下的数据，每个Store包含有一块MemStore和0个或多个StoreFile。StoreFile是HBase中最小的数据存储单元。</span><br><span class="line"></span><br><span class="line">  Store存储是HBase存储的核心，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。MemStore是Sorted Memory Buffer（内存写缓存），用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile(底层实现是HFile)， 当StoreFile文件数量增长到一定阈值，会触发Compaction合并操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase将最近接收到的数据缓存在MemStore中，在持久化到HDFS之前完成排序，再顺序写入HDFS，为后续数据的检索做了优化。因为MemStore缓存的是最近增加的数据，所以也提高了对近期数据的操作速度。在持久化写入之前，在内存中对行键或单元格做些优化。</span><br></pre></td></tr></table></figure><h4 id="Store的合并分裂"><a href="#Store的合并分裂" class="headerlink" title="Store的合并分裂"></a>Store的合并分裂</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182339676.png" alt="image-20230418233949215"></p><h4 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182340842.png" alt="image-20230418234015451"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MemStore内存中的数据写到StoreFile文件中，StoreFile底层是以HFile的格式保存。</span><br><span class="line">HFile的存储格式如图7所示</span><br><span class="line">HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。</span><br><span class="line">Trailer中有指针指向其他数据块的起始点，FileInfo记录了文件的一些meta信息。</span><br><span class="line">每个Data块的大小可以在创建一个Table的时候通过参数指定（默认块大小64KB）。</span><br><span class="line">每个Data块除了开头的Magic以外就是一个键值对拼接而成，Magic内容就是一些随机数字，目的是防止数据损坏。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HFile里面的每个键值对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182340877.png" alt="image-20230418234045908"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">键值对结构以两个固定长度的数值开始，分别表示Key的长度和Value的长度。</span><br><span class="line">紧接着是Key，Key以RowLength开始，是固定长度的数值，表示RowKey的长度，</span><br><span class="line">紧接着是RowKey，然后是固定长度的数值ColumnFamilyLength，表示Family的长度，</span><br><span class="line">然后是Family列族，接着是Qualifier列标识符，Key最后以两个固定长度的数值Time Stamp和Key Type（Put&#x2F;Delete）结束。</span><br><span class="line">Value部分没有这么复杂的结构，就是纯粹的二进制数据。</span><br></pre></td></tr></table></figure><h3 id="数据存储与读取-1"><a href="#数据存储与读取-1" class="headerlink" title="数据存储与读取"></a>数据存储与读取</h3><h4 id="HBase写文件流程"><a href="#HBase写文件流程" class="headerlink" title="HBase写文件流程"></a>HBase写文件流程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">客户端首先访问zookeeper，从meta表得到写入数据对应的region信息和相应的region服务器。</span><br><span class="line">找到相应的region服务器,把数据分别写到HLog和MemStore上一份</span><br><span class="line">MemStore达到一个阈值后则把数据刷成一个StoreFile文件。（若MemStore中的数据有丢失，则可以总HLog上恢复）</span><br><span class="line">当多个StoreFile文件达到一定的大小后，会触发Compact合并操作，合并为一个StoreFile，（这里同时进行版本的合并和数据删除。）</span><br><span class="line">当Storefile大小超过一定阈值后，会把当前的Region分割为两个（Split分裂），并由Hmaster分配到相应的HRegionServer，实现负载均衡。</span><br></pre></td></tr></table></figure><h4 id="HBase读文件流程"><a href="#HBase读文件流程" class="headerlink" title="HBase读文件流程"></a>HBase读文件流程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">客户端先访问zookeeper，从meta表读取Region的信息对应的服务器。</span><br><span class="line">客户端向对应Region服务器发送读取数据的请求，Region接收请求后，先从MemStore找数据，如果没有，再到StoreFile上读取，然后将数据返回给客户端。</span><br></pre></td></tr></table></figure><h3 id="WAL机制"><a href="#WAL机制" class="headerlink" title="WAL机制"></a>WAL机制</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191129537.png" alt="image-20230419112916756"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">分布式环境下，必须要考虑到系统出错的情形，比如Region服务器发生故障时，MemStore缓存中还没有被写入文件的数据会全部丢失。</span><br><span class="line">因此，HBase采用HLog来保证系统发生故障时能够恢复到正常的状态。</span><br><span class="line">如图所示，每个Region服务器都有一个HLog文件，同一个Region服务器的Region对象共用一个HLog，HLog是一种预写日志（Write Ahead Log）文件，就是说，用户更新数据必须首先被记入日志后才能写入MemStore缓存，当缓存内容对应的日志已经被写入磁盘后，即日志写成功后，缓存的内容才会被写入磁盘。</span><br><span class="line">HBase系统中，每个Region服务器只需要一个HLog文件，所有Region对象共用一个HLog，而不是每个Region使用一个HLog。在这种Region对象共用一个HLog的方式中，多个Region对象的进行更新操作需要修改日志时，只需要不断把日志记录追加到单个日志文件中，而不需要同时打开、写入到多个日志文件中，因此可以减少磁盘寻址次数，提高对表的写操作性能。</span><br></pre></td></tr></table></figure><h2 id="02-Hbase-Region管理"><a href="#02-Hbase-Region管理" class="headerlink" title="02 Hbase Region管理"></a>02 Hbase Region管理</h2><h3 id="HFile合并"><a href="#HFile合并" class="headerlink" title="HFile合并"></a>HFile合并</h3><h4 id="Minor合并"><a href="#Minor合并" class="headerlink" title="Minor合并"></a>Minor合并</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191135924.png" alt="image-20230419113533020"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面讲过</span><br><span class="line">用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile(底层实现是Hfile)， 当StoreFile文件数量增长到一定阈值，会触发Compaction合并操作。</span><br><span class="line">HFile的合并分为两种类型，分别是Minor合并和Major合并，这两种合并都发生在Store内部，不是Region的合并。</span><br><span class="line">Minor合并是把多个小HFile合并生成一个大的Hfile</span><br><span class="line">执行合并时，HBase读出已有的多个HFile的内容，把记录写入到一个新文件中。然后把新文件设置为激活状态，并标记旧文件为删除。在Minor合并中，这些标记为删除的旧文件是没有被移除的，任然会出现在HFile中，只有在进行Major合并时才会移除这些旧文件。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191136210.png" alt="image-20230419113609308"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">对需要进行Minor合并的文件的选择是触发式的，当达到触发条件才会进行Minor合并，而触发条件有很多，比如在将MemStore的数据刷到HFile时会申请对Store下符合条件的HFile进行合并，或者定期对Store内的HFile进行合并。另外对选择合并的HFile也是有条件的，如表1所示。</span><br><span class="line">在执行Minor合并时，会根据上述配置参数选择合适的HFile进行合并。Minor合并对HBase的性能是有轻微影响的，所以合并的HFile数量是有限的，默认最多为10个。</span><br></pre></td></tr></table></figure><h4 id="Major合并"><a href="#Major合并" class="headerlink" title="Major合并"></a>Major合并</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191136792.png" alt="image-20230419113630473"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Major合并针对的是给定Region的一个列族的所有Hfile。</span><br><span class="line">它将Store中的所有HFile合并成一个大文件，有时也会对整个表的同一列族的HFile进行合并，这是一个耗时和耗费资源的操作，会影响集群性能。</span><br><span class="line">一般情况下都是做Minor合并，不少集群是禁止Major合并的，只有在集群负载较小时进行手动Major合并，或者配置Major合并周期，默认为7天。</span><br><span class="line">另外Major合并时会清理Minor合并中被标记删除的HFile。</span><br><span class="line">如上右图所示</span><br></pre></td></tr></table></figure><h3 id="Region拆分"><a href="#Region拆分" class="headerlink" title="Region拆分"></a>Region拆分</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191146636.png" alt="image-20230419114652571"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Region拆分是HBase能够拥有良好扩展性的最重要因素。一旦一个Region的负载过大或者超过阈值时，会被分裂成新的两个Region，如图所示。</span><br><span class="line">这个过程是由RegionServer完成，其拆分流程如下：</span><br><span class="line">（1）将需要拆分的Region下线，阻止所有对该Region的客户端请求，master会检测到Region的状态为SPLITING；</span><br><span class="line">（2）将一个Region拆分成两个子Region，先在父Region下建立两个引用文件，分别指向Region的首行和末行，这时两个引用文件并不会从父Region中拷贝数据；</span><br><span class="line">（3）之后在HDFS上建立两个子Region的目录，分别拷贝上一步建立的引用文件，每个子Region分别占父Region的一半数据。拷贝完成后删除两个引用文件。</span><br><span class="line">（4）完成子Region创建后，向.META.表发送新产生的region的元数据信息；</span><br><span class="line">（5）Region的拆分信息更新到Hmaster，并且每个Region进入可用状态。</span><br></pre></td></tr></table></figure><h4 id="拆分策略"><a href="#拆分策略" class="headerlink" title="拆分策略"></a>拆分策略</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191147462.png" alt="image-20230419114724603"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上表列举的切分策略中，切分点的定义是一致的，即当Region中最大Store的大小大于设置阈值之后才会触发拆分。而不同策略中，阈值的定义是不同的，且对集群中Region的分布有很大的影响。</span><br></pre></td></tr></table></figure><h3 id="Region合并"><a href="#Region合并" class="headerlink" title="Region合并"></a>Region合并</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">当RegionServer中的Region数到达最大阈值时，RegionServer就会发起Region合并。其合并过程如下：</span><br><span class="line">（1）客户端发起Region合并处理并发送Region合并请求给Master；</span><br><span class="line">（2）Master在RegionServer上把Region移到一起并发起一个Region合并操作的请求；</span><br><span class="line">（3）RegionServer将准备合并的Region下线，然后进行合并；</span><br><span class="line">（4）从.META.表删除被合并的Region元数据，新的合并了的Region的元数据被更新写入.META.表中；</span><br><span class="line">（5）合并的Region被设置为上线状态并接受访问，同时更新Region信息到Master。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从Region的拆分可以看到随着表的增大，Region的数量是越来越大的，如果很多Region，它们中Memstore也过多，内存大小会触发RegionServer级别的限制，会频繁出现数据从内存刷到HFile的操作，就会对用户请求产生较大的影响，可能阻塞该RegionServer上的更新操作。过多Region会增加ZooKeeper的负担。因此当RegionServer中的Region数到达最大阈值时，RegionServer就会发起Region合并。</span><br></pre></td></tr></table></figure><h3 id="Region负载均衡"><a href="#Region负载均衡" class="headerlink" title="Region负载均衡"></a>Region负载均衡</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191148622.png" alt="image-20230419114829628"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在分布式系统中，负载均衡是一个非常重要的功能，在HBase中通过Region的数量来实现负载均衡。</span><br><span class="line">每次负载均衡操作分为两步进行，首先生成负载均衡计划表，然后按照计划表执行Region的分配。</span><br><span class="line">Master内部使用一套集群负载评分的算法，来评估HBase某一个表的Region是否需要进行重新分配。</span><br><span class="line">这套算法分别从RegionServer中Region的数目、表的Region数，MenStore大小、StoreFile大小，数据本地性等几个维度来对集群进行评分，评分越低代表集群的负载越合理。</span><br><span class="line">确定需要负载均衡后，在根据不同策略选择Region进行分配，负载均衡策略有三种，如表所示。</span><br><span class="line">根据上述策略选择分配Region后再继续对整个表的所有Region进行评分，如果依然未达到标准，循环执行上述操作直至整个集群达到负载均衡的状态。</span><br></pre></td></tr></table></figure><h2 id="03-Hbase集群管理"><a href="#03-Hbase集群管理" class="headerlink" title="03 Hbase集群管理"></a>03 Hbase集群管理</h2><h3 id="运维管理"><a href="#运维管理" class="headerlink" title="运维管理"></a>运维管理</h3><h4 id="移除RegionServer节点"><a href="#移除RegionServer节点" class="headerlink" title="移除RegionServer节点"></a>移除RegionServer节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当集群由于升级或更换硬件等原因需要在单台机器上停止守护进程时，需要确保集群的其他部分正常工作，并且确保从客户端应用来看停用时间最短。满足此条件必须把这台RegionServer服务的Region主动转移到其他RegionServer上，而不是让HBase被动地对此RegionServer的下线进行反应。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">集群运行时，有些操作任务是必需的，包括增加和移除节点。</span><br><span class="line">用户可以在指定节点的HBase目录下使用hbase-damon.sh stop命令来停止集群中的一个RegionServer。执行此命令后，RegionServer先将所有Region关闭，然后再把自己的进程停止，RegionServer在ZooKeeper中对应的临时节点将会过期。Master检测到RegionServer停止服务后将此RegionServer上的Region重新分配到其他机器上。</span><br><span class="line">HBase也提供了脚本来主动转移Region到其他RegionServer，然后下掉下线的RegionServer这样会让整个过程更加安全。在HBase的bin目录下提供了graceful_stop.sh脚本可以完成这种主动移除节点的功能。此脚本停止一个RegionServer的过程如下：</span><br><span class="line">（1）关闭Region均衡器；</span><br><span class="line">（2）从需要停止的RegionServer上移出Region，并随机把他们分配给集群中其他服务器；</span><br><span class="line">（3）停止RegionServer进程</span><br></pre></td></tr></table></figure><h4 id="增加RegionServer节点"><a href="#增加RegionServer节点" class="headerlink" title="增加RegionServer节点"></a>增加RegionServer节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">随着应用系统需求的增长，整个HBase集群需要进行扩展，这时就需要往HBase集群中增加一个节点。添加一个新的RegionServer是运行集群的常用操作，首先需要修改conf目录下的regionserver文件，然后将此文件复制到集群中所有机器上，这样可以使用启动脚本就能够添加新的服务器。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HBase底层是以HDFS来存储数据的，一般部署HBase集群时，HDFS的DataNode和HBase的RegionServer位于同一台物理机上。</span><br><span class="line">所以往HBase集群增加一个RegionServer之前需要往HDFS里增加DataNode，</span><br><span class="line">等待DataNode进程启动并加入HDFS集群后，再启动HBase的RegionServer进程。</span><br><span class="line">启动新增节点上的RegionServer可以使用命令hbase-damon.sh start，启动成功后可以在Master用户界面看到此节点。</span><br><span class="line">如果需要重新均衡分配每个节点上的Region，则使用HBase的负载均衡功能。</span><br></pre></td></tr></table></figure><h4 id="增加Master备份节点"><a href="#增加Master备份节点" class="headerlink" title="增加Master备份节点"></a>增加Master备份节点</h4><h3 id="数据管理"><a href="#数据管理" class="headerlink" title="数据管理"></a>数据管理</h3><h4 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h4><h4 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h4><h4 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h4><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-4</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-4.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-4.html</id>
    <published>2023-04-17T08:47:02.000Z</published>
    <updated>2023-04-19T08:35:25.572Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第三章-Hbase"><a href="#第三章-Hbase" class="headerlink" title="第三章 Hbase"></a>第三章 Hbase</h1><h2 id="05-Hbase过滤器"><a href="#05-Hbase过滤器" class="headerlink" title="05 Hbase过滤器"></a>05 Hbase过滤器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">可以根据主键、列簇、列、版本等更多的条件来对数据进行过滤。</span><br><span class="line">类似SQL中的WHERE</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; show_filters</span><br><span class="line">DependentColumnFilter                                       KeyOnlyFilter                                               ColumnCountGetFilter                                       SingleColumnValueFilter                                     PrefixFilter                                               SingleColumnValueExcludeFilter                             FirstKeyOnlyFilter                                         ColumnRangeFilter                                           TimestampsFilter                                           FamilyFilter                                               QualifierFilter                                             ColumnPrefixFilter                                         RowFilter                                                   MultipleColumnPrefixFilter                                 InclusiveStopFilter                                         PageFilter                                                 ValueFilter                                                 ColumnPaginationFilter</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">过滤器语法格式：</span><br><span class="line"></span><br><span class="line">scan&#x2F;get  ‘表名’，&#123;Filter &#x3D;&gt; “过滤器 ( 比较运算符，’比较器’)”&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182137386.png" alt="image-20230418213720968"></p><h3 id="RowFilter"><a href="#RowFilter" class="headerlink" title="RowFilter"></a>RowFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RowFilter：针对rowkey进行字符串的比较过滤器。</span><br><span class="line"></span><br><span class="line">举例：</span><br><span class="line">例1：显示行键包含0的键值对；</span><br><span class="line">scan &#39;student&#39;,&#123;FILTER&#x3D;&gt;&quot;RowFilter(&#x3D;,&#39;substring:0’)”&#125;</span><br><span class="line">例2：显示行键字节顺序大于002的键值对；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;RowFilter(&gt;,&#39;binary:002&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="PrefixFilter"><a href="#PrefixFilter" class="headerlink" title="PrefixFilter"></a>PrefixFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PrefixFilter：rowkey前缀比较器，一种更简单的比较行键前缀的命令，等值比较。</span><br><span class="line">举例：</span><br><span class="line"></span><br><span class="line">例1：显示行键前缀为0开头的键值对；</span><br><span class="line">scan &#39;student&#39;,&#123;FILTER&#x3D;&gt;&quot;RowFilter(&#x3D;,&#39;substring:0’)”&#125;</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;PrefixFilter(&#39;003&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="KeyOnlyFilter"><a href="#KeyOnlyFilter" class="headerlink" title="KeyOnlyFilter"></a>KeyOnlyFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KeyOnlyFilter：</span><br><span class="line">只对cell的键进行过滤和显示，但不显示值。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182142565.png" alt="image-20230418214240115"></p><h3 id="FirstKeyOnlyFilter"><a href="#FirstKeyOnlyFilter" class="headerlink" title="FirstKeyOnlyFilter"></a>FirstKeyOnlyFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FirstKeyOnlyFilter：只扫描相同键的第一个cell，其键值对都会显示出来。</span><br><span class="line"></span><br><span class="line">例4：统计表的逻辑行数；</span><br><span class="line">count &#39;student’</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;FirstKeyOnlyFilter()&quot;</span><br><span class="line"></span><br><span class="line">hbase(main):008:0&gt; scan &#39;student&#39;, FILTER&#x3D;&gt;&quot;FirstKeyOnlyFilter()&quot;</span><br><span class="line">ROW                    COLUMN+CELL                           001                   column&#x3D;grades:englisg, timestamp&#x3D;1541485306878, value&#x3D;80        </span><br><span class="line"> 002                   column&#x3D;grades:bigdata, timestamp&#x3D;1541485403649, value&#x3D;88        </span><br><span class="line"> 003                   column&#x3D;grades:bigdata, timestamp&#x3D;1541485412686, value&#x3D;80        </span><br><span class="line">3 row(s) in 0.0400 seconds</span><br></pre></td></tr></table></figure><h3 id="InclusiveStopFilter"><a href="#InclusiveStopFilter" class="headerlink" title="InclusiveStopFilter"></a>InclusiveStopFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">InclusiveStopFilter：替代ENDROW返回终止条件行；</span><br><span class="line">例5：显示起始行键为001，结束行为003的记录；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182149501.png" alt="image-20230418214950938"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182153377.png" alt="image-20230418215313360"></p><h3 id="FamilyFilter"><a href="#FamilyFilter" class="headerlink" title="FamilyFilter"></a>FamilyFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FamilyFilter：针对列族进行比较和过滤。</span><br><span class="line"></span><br><span class="line">例1：显示列族前缀为stu开头的键值对；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,&#39;substring:stu’)”</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,‘binary:stu’)”</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182155971.png" alt="image-20230418215532681"></p><h3 id="QualifierFilter"><a href="#QualifierFilter" class="headerlink" title="QualifierFilter"></a>QualifierFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">QualifierFilter：列标识过滤器。</span><br><span class="line"></span><br><span class="line">例2：显示列名为name的记录；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;substring:name&#39;)&quot;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182200092.png" alt="image-20230418220036827"></p><h3 id="ColumnPrefixFilter"><a href="#ColumnPrefixFilter" class="headerlink" title="ColumnPrefixFilter"></a>ColumnPrefixFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ColumnPrefixFilter：对列名前缀进行过滤。</span><br><span class="line">例2：显示列名为name的记录；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPrefixFilter(&#39;name’)”</span><br><span class="line"></span><br><span class="line">等价于scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;substring:name&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MultipleColumnPrefixFilter：可以指定多个前缀</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">例3：显示列名为name和age的记录；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;MultipleColumnPrefixFilter(&#39;name&#39;,&#39;age&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="ColumnRangeFilter"><a href="#ColumnRangeFilter" class="headerlink" title="ColumnRangeFilter"></a>ColumnRangeFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ColumnRangeFilter ：设置范围按字典序对列名进行过滤；</span><br><span class="line">例4：查询列名在bi和na之间的记录</span><br><span class="line">Student表中有以下列族和列名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182205876.png" alt="image-20230418220523574"></p><h3 id="TimestampsFilter"><a href="#TimestampsFilter" class="headerlink" title="TimestampsFilter"></a>TimestampsFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TimestampsFilter ：时间戳过滤器。支持等值方式比较，但可以设置多个时间戳</span><br><span class="line"></span><br><span class="line">例5：只查询时间戳为1和2的键值对；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;TimestampsFilter(2,4)&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):030:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;TimestampsFilter(2,4)&quot;</span><br><span class="line">ROW                    COLUMN+CELL                                   004                   column&#x3D;stuinfo:age, timestamp&#x3D;2, value&#x3D;19      004                   column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry 004                   column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male   1 row(s) in 0.0150 seconds</span><br></pre></td></tr></table></figure><h3 id="ValueFilter"><a href="#ValueFilter" class="headerlink" title="ValueFilter"></a>ValueFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ValueFilter ：值过滤器。</span><br><span class="line"></span><br><span class="line">例6：查询值等于19的所有键值对</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;binary:19’)”</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;substring:19&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="SingleColumnValueFilter"><a href="#SingleColumnValueFilter" class="headerlink" title="SingleColumnValueFilter"></a>SingleColumnValueFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SingleColumnValueFilter：在指定的列族和列中进行值过滤器。</span><br><span class="line">例7：查询stuinfo列族age列中值等于19的所有键值对</span><br><span class="line"></span><br><span class="line">scan &#39;student&#39;,&#123;COLUMN&#x3D;&gt;&#39;stuinfo:age’,</span><br><span class="line">FILTER&#x3D;&gt;&quot;SingleColumnValueFilter(&#39;stuinfo&#39;,&#39;age&#39;,&#x3D;,&#39;binary:19&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">SingleColumnValueExcludeFilter：在指定的列族和列中进行值过滤器，与SingleColumnValueFilter功能相反。</span><br></pre></td></tr></table></figure><h3 id="ColumnCountGetFilter"><a href="#ColumnCountGetFilter" class="headerlink" title="ColumnCountGetFilter"></a>ColumnCountGetFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ColumnCountGetFilter：限制每个逻辑行返回的键值对数</span><br><span class="line">例7：返回行键为001的前3个键值对</span><br><span class="line"></span><br><span class="line">hbase(main):004:0&gt; get &#39;student&#39;, &#39;001&#39;,FILTER&#x3D;&gt;&quot;ColumnCountGetFilter(3)&quot;</span><br><span class="line">COLUMN                    CELL                                       grades:englisg           timestamp&#x3D;1541485306878, value&#x3D;80           grades:math              timestamp&#x3D;1541485384199, value&#x3D;90           stuinfo:age              timestamp&#x3D;1541485224974, value&#x3D;18           3 row(s) in 0.0950 seconds</span><br></pre></td></tr></table></figure><h3 id="PageFilter"><a href="#PageFilter" class="headerlink" title="PageFilter"></a>PageFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">PageFilter ：基于行的分页过滤器，设置返回行数。</span><br><span class="line"></span><br><span class="line">hbase(main):005:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;PageFilter(1)&quot;</span><br><span class="line">ROW                       COLUMN+CELL                                   001                      column&#x3D;grades:englisg, timestamp&#x3D;1541485306878, value&#x3D;80                 </span><br><span class="line"> 001                      column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90                    </span><br><span class="line"> 001                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485224974, value&#x3D;18                    </span><br><span class="line"> 001                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485170696, value&#x3D;alice                </span><br><span class="line"> 001                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485235855, value&#x3D;female                </span><br><span class="line">1 row(s) in 0.0680 seconds</span><br></pre></td></tr></table></figure><h3 id="ColumnPaginationFilter"><a href="#ColumnPaginationFilter" class="headerlink" title="ColumnPaginationFilter"></a>ColumnPaginationFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ColumnPaginationFilter：基于列的进行分页过滤器，需要设置偏移量与返回数量 。</span><br><span class="line"></span><br><span class="line">例9：显示每行第1列之后的2个键值对。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):006:0&gt; scan &#39;student&#39;</span><br><span class="line">ROW                       COLUMN+CELL                                                     001                      column&#x3D;grades:englisg, timestamp&#x3D;1541485306878, value&#x3D;80         001                      column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90           001                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485224974, value&#x3D;18           001                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485170696, value&#x3D;alice       001                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485235855, value&#x3D;female       002                      column&#x3D;grades:bigdata, timestamp&#x3D;1541485403649, value&#x3D;88         002                      column&#x3D;grades:englisg, timestamp&#x3D;1541485319132, value&#x3D;85         002                      column&#x3D;grades:math, timestamp&#x3D;1541485376414, value&#x3D;78           002                      column&#x3D;stuinfo:class, timestamp&#x3D;1541485278646, value&#x3D;1802       002                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485187403, value&#x3D;nancy       002                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485245291, value&#x3D;male</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):007:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(2,1)&quot;</span><br><span class="line">ROW                       COLUMN+CELL                                                     001                      column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90           001                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485224974, value&#x3D;18           002                      column&#x3D;grades:englisg, timestamp&#x3D;1541485319132, value&#x3D;85         002                      column&#x3D;grades:math, timestamp&#x3D;1541485376414, value&#x3D;78           003                      column&#x3D;grades:englisg, timestamp&#x3D;1541485328260, value&#x3D;90         003                      column&#x3D;grades:math, timestamp&#x3D;1541485368087, value&#x3D;80           004                      column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry                   004                      column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                     </span><br><span class="line">4 row(s) in 0.0840 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">003                      column&#x3D;grades:bigdata, timestamp&#x3D;1541485412686, value&#x3D;80       003                      column&#x3D;grades:englisg, timestamp&#x3D;1541485328260, value&#x3D;90       003                      column&#x3D;grades:math, timestamp&#x3D;1541485368087, value&#x3D;80           003                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485209410, value&#x3D;19           003                      column&#x3D;stuinfo:class, timestamp&#x3D;1541485271479, value&#x3D;1803       003                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485198223, value&#x3D;harry       003                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485253075, value&#x3D;male         004                      column&#x3D;stuinfo:age, timestamp&#x3D;2, value&#x3D;19                       004                      column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry                   004                      column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                              </span><br><span class="line">4 row(s) in 0.0980 seconds</span><br></pre></td></tr></table></figure><h3 id="组合使用过滤器"><a href="#组合使用过滤器" class="headerlink" title="组合使用过滤器"></a>组合使用过滤器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">组合使用过滤器：使用AND或OR等连接符，组合多个过滤器进行组合扫描。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">例10：组合过滤器的使用</span><br><span class="line">hbase(main):008:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(2,1) AND ValueFilter(&#x3D;,&#39;substring:ma&#39;)&quot;</span><br><span class="line">ROW                       COLUMN+CELL                                                              </span><br><span class="line"> 004                      column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                              </span><br><span class="line">1 row(s) in 0.1040 seconds</span><br><span class="line">hbase(main):010:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(1,1) OR ValueFilter(&#x3D;,&#39;substring:ma&#39;)&quot;</span><br><span class="line">ROW                           COLUMN+CELL                                                 001                          column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90       001                          column&#x3D;stuinfo:sex, timestamp&#x3D;1541485235855, value&#x3D;female   002                          column&#x3D;grades:englisg, timestamp&#x3D;1541485319132, value&#x3D;85   002                          column&#x3D;stuinfo:sex, timestamp&#x3D;1541485245291, value&#x3D;male     003                          column&#x3D;grades:englisg, timestamp&#x3D;1541485328260, value&#x3D;90   003                          column&#x3D;stuinfo:sex, timestamp&#x3D;1541485253075, value&#x3D;male     004                          column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry               004                          column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                                        </span><br><span class="line">4 row(s) in 0.0440 seconds</span><br></pre></td></tr></table></figure><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">FamilyFilter：针对列族进行比较和过滤。</span><br><span class="line">QualifierFilter：列标识过滤器。</span><br><span class="line">ColumnPrefixFilter：对列名前缀进行过滤。</span><br><span class="line">MultipleColumnPrefixFilter：可以指定多个前缀</span><br><span class="line">ColumnRangeFilter ：设置范围按字典序对列名进行过滤；</span><br><span class="line">TimestampsFilter ：时间戳过滤器。支持等值方式比较，但可以设置多个时间戳</span><br><span class="line">ValueFilter ：值过滤器。</span><br><span class="line">SingleColumnValueFilter ：在指定的列族和列中进行值过滤器。</span><br><span class="line">SingleColumnValueExcludeFilter：在指定的列族和列中进行值过滤器，与SingleColumnValueFilter功能相反。</span><br><span class="line">ColumnCountGetFilter ：限制每个逻辑行返回的键值对数</span><br><span class="line">PageFilter ：基于行的分页过滤器，设置返回行数。</span><br><span class="line">ColumnPaginationFilter ：基于列的进行分页过滤器，需要设置偏移量与返回数量 。</span><br></pre></td></tr></table></figure><h2 id="06-Hbase-Java编程方法"><a href="#06-Hbase-Java编程方法" class="headerlink" title="06 Hbase Java编程方法"></a>06 Hbase Java编程方法</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182232258.png" alt="image-20230418223225014"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">首先介绍基于JAVA的编程方法，HBase是基于java语言开发，用户可以利用包含java语言在内的多种语言进行调用开发。由于java是原生语言，因此利用java进行应用开发，以及过滤器等内容的开发最为方便。</span><br><span class="line"></span><br><span class="line">HBase对java开发环境并无特殊要求，只要将用到的HBase的库包加入引用路径即可。</span><br><span class="line">本节讲述以eclipse为java的集成开发环境，具体步骤如下：</span><br><span class="line">首先在eclipse中建立标准的java工程，给工程命名为hbase，其他使用默认配置，按步骤完成项目的创建。</span><br><span class="line">然后打开hbase项目的属性（对应图中序号1），选择java构建路径标签页（2），选择库（3），点击添加外部jar按钮（4）。然后弹出框会让你选择hbase安装目录下的lib。将需要的包导入工程，导入成功后会在工程里引用的库中出现你所选择的jar包。（解说完后播放操作视频hbase_java环境配置.mp4）</span><br></pre></td></tr></table></figure><h3 id="包导入"><a href="#包导入" class="headerlink" title="包导入"></a>包导入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">使用Hadoop和HBase的环境配置</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line"></span><br><span class="line">HBase的客户端接口</span><br><span class="line">import org.apache.hadoop.hbase.*;</span><br><span class="line">import org.apache.hadoop.hbase.client.*;</span><br><span class="line"></span><br><span class="line">HBase工具包</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line">HBase过滤器</span><br><span class="line">import org.apache.hadoop.hbase.filter.*;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工程目录src下新建类文件，在java文件中import需要的HBase包，比如HBase的环境配置包需要导入Configuration包（对应显示红色）</span><br><span class="line">HBase客户端接口需要导入hbase.client(绿色两行)</span><br><span class="line">工具包需要导入hbase.util.Bytes（蓝色），而如果使用过滤器需要导入hbase.filter包。如果还需要其他包可以从hbase lib里去寻找，并导入工程即可。</span><br></pre></td></tr></table></figure><h3 id="建立连接"><a href="#建立连接" class="headerlink" title="建立连接"></a>建立连接</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public static Configuration conf;</span><br><span class="line">public static Connection connection;</span><br><span class="line">           public static Admin admin;</span><br><span class="line"></span><br><span class="line">public void getconnect() throws IOException</span><br><span class="line">&#123;</span><br><span class="line">conf&#x3D;HBaseConfiguration.create();</span><br><span class="line">conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;cm-cdh01&quot;);</span><br><span class="line">conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</span><br><span class="line">try&#123;</span><br><span class="line">connection&#x3D;ConnectionFactory.createConnection(conf);</span><br><span class="line">admin&#x3D;connection.getAdmin();</span><br><span class="line">&#125;</span><br><span class="line">catch(IOException e)&#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">环境配置好后，接下来开始连接数据库，在分布式环境下，客户端访问HBase需要通过ZooKeeper的地址和端口来获取当前活跃的Master和所需的RegionServer地址，</span><br><span class="line">首先建立三个全局变量，conf、connection和admin（高亮显示红色字体），conf用来描述zookeeper集群的访问地址，connect用来建立连接，admin是创建的数据库管理员，执行具体的数据表操作。</span><br><span class="line">Conf使用set方法来设置集群地址和端口号，然后使用ConnectionFactory来建立连接，并让admin获取管理员的操作权限，至此已经java客户端已经连接上hbase数据库。</span><br></pre></td></tr></table></figure><h3 id="Admin接口"><a href="#Admin接口" class="headerlink" title="Admin接口"></a>Admin接口</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182235010.png" alt="image-20230418223516389"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来具体看下HBase API中常用的接口 ，首先是Admin接口，Admin用于管理HBase数据库的表信息，如创建表，用到了createtable，删除表用到disabletable和deletetable，与Hbase中的create，delete和disable命令对应。另外admin接口还可以使用listtables方法列出hbase中所有的表，使用getTableDescriptor来获取表的结构信息，分别于hbase shell中的，list 和describe命令。</span><br><span class="line"></span><br><span class="line">注意listtables和getTableDescriptor方法返回的是HTableDescriptor类结构的数据。</span><br></pre></td></tr></table></figure><h3 id="创建和删除表"><a href="#创建和删除表" class="headerlink" title="创建和删除表"></a>创建和删除表</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182236492.png" alt="image-20230418223634346"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接着通过具体代码来看下Admin接口提供的方法的使用。首先是创建表，在创建表之前用tablesexists来判断表是否存在，注意设定tableName对象的语法，使用valueOf方法设置表名。如果表已经存在则使用disableTable和deleteTable方法来删除表，注意这里与hbaseshell中操作一样，先禁用表再删除表（红框）。</span><br><span class="line"></span><br><span class="line">（这一段对应蓝色文字框）HTableDescriptor类用来描述表结构，包括HBase中表格的详细信息，例如表中的列族、该表的类型、是否只读、MemStore的最大空间等，并且提供了一些操作表的方法，比如增加列族addFamliy()、删除列族removeFamily()和设置属性值setValue()等方法。</span><br><span class="line"></span><br><span class="line">（这一段对应绿色文字框）HColumnDescriptor类则用来描述列族，比如列族的版本数，压缩设置等。此类通常在添加列族或者创建表的时候使用，一旦列族建立就不能被修改，只有通过删除列族，再创建新的列族来间接修改。HCloumnDescriptor类提供getName()、getValue()和setValue等方法对列族的数据进行操作。</span><br><span class="line"></span><br><span class="line">（对应绿色虚线框的内容）这里的代码使用了两种方式创建列族（绿色框），建立stuinfo时（黄色三行），先通过对象HColumnDescriptor自定义列族属性，比如列族名和块大小，然后根据列族属性使用addfamily建立列族。而建立grades列族时（绿色行）直接使用addfamily方法根据默认属性建立的。</span><br><span class="line"></span><br><span class="line">描述和建立完列族信息后，通过createTable建立表格。</span><br></pre></td></tr></table></figure><h3 id="Table接口"><a href="#Table接口" class="headerlink" title="Table接口"></a>Table接口</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182236561.png" alt="image-20230418223657412"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再来看下另一个接口Table，如果不需要创建表，直接插入数据，可以不用建立Admin对象，使用Table接口即可。Table接口主要用来进行数据的操作，比如删除指定行使用delete，获取指定行的数据使用get，以及向表中添加数据使用put方法。</span><br></pre></td></tr></table></figure><h3 id="Put数据更新"><a href="#Put数据更新" class="headerlink" title="Put数据更新"></a>Put数据更新</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public static void addData() throws IOException &#123;</span><br><span class="line">Table table &#x3D; connection.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">Put put &#x3D; new Put(Bytes.toBytes(“001&quot;));</span><br><span class="line">put.addColumn(Bytes.toBytes(&quot;StuInfo&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(“harry&quot;));</span><br><span class="line">                               put.addColumn(Bytes.toBytes(&quot;StuInfo&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(“harry&quot;));</span><br><span class="line">table.put(put);</span><br><span class="line">table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Put put1 &#x3D; new Put(Bytes.toBytes(&quot;002&quot;));</span><br><span class="line">put1.addColumn(Bytes.toBytes(&quot;StuInfo&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;jess&quot;));</span><br><span class="line">Put put2 &#x3D; new Put(Bytes.toBytes(&quot;003&quot;));</span><br><span class="line">put2.addColumn(Bytes.toBytes(&quot;Grades&quot;), Bytes.toBytes(&quot;english&quot;), Bytes.toBytes(&quot;98&quot;));</span><br><span class="line">List&lt;Put&gt; putList &#x3D; new ArrayList&lt;Put&gt;();</span><br><span class="line">putList.add(put1);</span><br><span class="line">putList.add(put2);</span><br><span class="line">table.put(putList);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先来看下put方法，put主要用来插入，也可以对已有的记录进行更新，在使用put方法前，需要根据表名建立和它的连接（红色行）</span><br><span class="line"></span><br><span class="line">与HBase shell相同，使用put方法可以逐条的插入数据，put对象中首先指明行键为001，并通过addColumn方法加入键值对，addColumn方法的参数分别为列族stuinfo，列name和值harry。这里加入了2个键值对，说明这一行有2列，table.Put方法将put对象写入内存和日志，此时数据已经可以被查出。</span><br><span class="line"></span><br><span class="line">另外可以采用链表的方式一次性插入多个键值对，如下图，put1插入1键值对，put2插入1个键值对，然后建立链表putList，使用add方法将两个put对象插入到链表中，最后一次性插入到表中。</span><br></pre></td></tr></table></figure><h3 id="查询数据-Get"><a href="#查询数据-Get" class="headerlink" title="查询数据-Get"></a>查询数据-Get</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public static void getRow(String tableName, String rowKey) throws IOException &#123;  </span><br><span class="line">Table table &#x3D; connection.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">Get get &#x3D; new Get(Bytes.toBytes(rowKey));  </span><br><span class="line">Result result &#x3D; table.get(get);  </span><br><span class="line">for (Cell cell : result.rawCells()) &#123;  </span><br><span class="line">System.out.println(  </span><br><span class="line">&quot;行键:&quot; + new String(CellUtil. getCellKeyAsString(cell)) + &quot;\t&quot; +  </span><br><span class="line">&quot;列族:&quot; + new String(CellUtil.cloneFamily(cell)) + &quot;\t&quot; +   </span><br><span class="line">&quot;列名:&quot; + new String(CellUtil.cloneQualifier(cell)) + &quot;\t&quot; +   </span><br><span class="line">&quot;值:&quot; + new String(CellUtil.cloneValue(cell)) + &quot;\t&quot; +  </span><br><span class="line">&quot;时间戳:&quot; + cell.getTimestamp());  </span><br><span class="line">&#125;  </span><br><span class="line">table.close();  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">插入数据后，可以根据行键进行查询，比如get方法，此代码中，先建立连接，通过get对象描述查询条件，再通过table.get方法进行实际查询（紫色两行）。查询结构写入result中，由于get方法一次获取一个逻辑行，即可能包含多个键值对，因此查询结构通过循环的方法将逐个键值对输出显示。</span><br><span class="line"></span><br><span class="line">显示时，getCellKeyAsString用来获取行键，cloneFamily用来获取列族，cloneQualifier用来获取列名，cloneValue获取具体值，getTimestamp获取时间戳</span><br></pre></td></tr></table></figure><h3 id="查询数据-Scan"><a href="#查询数据-Scan" class="headerlink" title="查询数据-Scan"></a>查询数据-Scan</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public static void scanTable(String tableName) throws IOException &#123;  </span><br><span class="line">       Table table &#x3D; connection.getTable(TableName.valueOf(&quot;student&quot;)); </span><br><span class="line">       Scan scan &#x3D; new Scan();  </span><br><span class="line">       ResultScanner results &#x3D; table.getScanner(scan);  </span><br><span class="line">       for (Result result : results) &#123;  </span><br><span class="line">           for (Cell cell : result.rawCells()) &#123;  </span><br><span class="line">               System.out.println(  </span><br><span class="line">                       &quot;行键:&quot; + new String(CellUtil.getCellKeyAsString(cell)) + &quot;\t&quot; +  </span><br><span class="line">                       &quot;列族:&quot; + new String(CellUtil.cloneFamily(cell)) + &quot;\t&quot; +   </span><br><span class="line">                       &quot;列名:&quot; + new String(CellUtil.cloneQualifier(cell)) + &quot;\t&quot; +   </span><br><span class="line">                       &quot;值:&quot; + new String(CellUtil.cloneValue(cell)) + &quot;\t&quot; +  </span><br><span class="line">                       &quot;时间戳:&quot; + cell.getTimestamp());  </span><br><span class="line">           &#125;  </span><br><span class="line">       &#125;   </span><br><span class="line">       table.close();</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Scan的操作语法与get类似，都需要建立连接，描述查询条件，再进行实际查询，核心语句是table.getScanner，后续的显示页和之前代码基本相同，但是，由于scan的结果得到的多个逻辑行，且每个逻辑行包含多个键值对，因此采用二层循环的方式来显示每一个键值对的内容，（红框所示）。</span><br></pre></td></tr></table></figure><h3 id="删除行和列"><a href="#删除行和列" class="headerlink" title="删除行和列"></a>删除行和列</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Table table &#x3D; connection.getTable(TableName.valueOf(tableName));</span><br><span class="line"></span><br><span class="line">1，删除指定行       </span><br><span class="line">Delete delete1 &#x3D; new Delete(Bytes.toBytes(‘001’));</span><br><span class="line"></span><br><span class="line">2，删除指定列族</span><br><span class="line">Delete delete2 &#x3D; new Delete(Bytes.toBytes(‘002’));</span><br><span class="line">delete2.addFamily(Bytes.toBytes(‘stuinfo’));</span><br><span class="line"></span><br><span class="line">3，删除指定列族中的列</span><br><span class="line">Delete delete3 &#x3D; new Delete(Bytes.toBytes(‘003’));</span><br><span class="line">delete3.addColumn(Bytes.toBytes(‘grades’),Bytes.toBytes(‘math’));</span><br><span class="line"></span><br><span class="line">调用 table.delete执行删除  </span><br><span class="line">table.delete(delete);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">完成了数据的新增和查询，Hbase api也提供删除数据，可以指定行键，列族和列进行删除，</span><br><span class="line">如第一个delete对象只有行键属性，因此会删除一个逻辑行，即所有行键为001的键值对都将被删除；</span><br><span class="line">第二个delete对象通过addfamily方法加入了列族参数stuinfo，因此只会删除再stuinfo列族中，行键为002的键值对；</span><br><span class="line">第三个delete对象通过addColumn方法加入列名参数math，同时也指定了列族grades，因此只会删除行键为003，grades列族中math列的键值对。</span><br><span class="line"></span><br><span class="line">最后调用table.delete方法来完成删除。</span><br></pre></td></tr></table></figure><h2 id="07-Hbase-Python编程方法"><a href="#07-Hbase-Python编程方法" class="headerlink" title="07 Hbase Python编程方法"></a>07 Hbase Python编程方法</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-3</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-3.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-3.html</id>
    <published>2023-04-17T08:46:58.000Z</published>
    <updated>2023-04-17T15:26:37.989Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第三章-Hbase"><a href="#第三章-Hbase" class="headerlink" title="第三章 Hbase"></a>第三章 Hbase</h1><h2 id="01-Hbase数据模型"><a href="#01-Hbase数据模型" class="headerlink" title="01 Hbase数据模型"></a>01 Hbase数据模型</h2><h3 id="逻辑模型"><a href="#逻辑模型" class="headerlink" title="逻辑模型"></a>逻辑模型</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172154639.png" alt="image-20230417215406265"></p><h3 id="HBase相关概念"><a href="#HBase相关概念" class="headerlink" title="HBase相关概念"></a>HBase相关概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（1）表（table）：HBase采用表来组织数据；</span><br><span class="line">（2）行（row）：每个表都由行组成，每个行由行键（row key）来标识，行键可以是任意字符串；</span><br><span class="line">（3）列族（column family）：一个table有许多个列族，列族是列的集合，属于表结构，也是表的基本访问控制单元；</span><br><span class="line">（4）列标识（column qualifier）：属于某一个Column Family：Column Qualifier形式标识，每条记录可动态添加</span><br><span class="line">（5）时间戳（timestamp）：时间戳用来区分数据的不同版本；</span><br><span class="line">（6）单元格（cell）：在table中，cell中存储的数据没有数据类型，是字节数组byte[] ，通过&lt;RowKey，Column Family: Column Qualifier，Timestamp&gt;元组来访问单元格</span><br></pre></td></tr></table></figure><h3 id="物理模型"><a href="#物理模型" class="headerlink" title="物理模型"></a>物理模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库特点：</span><br><span class="line">表结构预先定义；</span><br><span class="line">每列的数据类型不同；</span><br><span class="line">空值占用存储空间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HBase特点：</span><br><span class="line">只需定义表名和列族，可以动态添加列族和列；</span><br><span class="line">数据都是字符串类型；</span><br><span class="line">空值不占用存储空间；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172203728.png" alt="image-20230417220304381"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172214589.png" alt="image-20230417221413176"></p><h3 id="实际存储方式"><a href="#实际存储方式" class="headerlink" title="实际存储方式"></a>实际存储方式</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172218013.png" alt="image-20230417221843341"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172220818.png" alt="image-20230417222012709"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172220613.png" alt="image-20230417222052208"></p><h2 id="02-Hbase数据定义"><a href="#02-Hbase数据定义" class="headerlink" title="02 Hbase数据定义"></a>02 Hbase数据定义</h2><h3 id="HBase-Shell"><a href="#HBase-Shell" class="headerlink" title="HBase Shell"></a>HBase Shell</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase Shell：HBase的命令行工具，最简单的接口，适合HBase管理使用；</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost bin]# hbase shell</span><br><span class="line">HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.</span><br><span class="line">Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell</span><br><span class="line">Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt; </span><br><span class="line"></span><br><span class="line">命令：help,status,version,exit,quit</span><br></pre></td></tr></table></figure><h3 id="数据定义"><a href="#数据定义" class="headerlink" title="数据定义"></a>数据定义</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172230877.png" alt="image-20230417223050379"></p><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法：creat‘表名’，‘列族名’</span><br><span class="line">描述：</span><br><span class="line">●  必须指定表名和列族；</span><br><span class="line">●  可以创建多个列族；</span><br><span class="line">●  可以对标和列族指明一些参数；</span><br><span class="line">●  参数大小写敏感；</span><br><span class="line">●  字符串参数需要包含在单引号中；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172233954.png" alt="image-20230417223313646"></p><h4 id="表相关操作"><a href="#表相关操作" class="headerlink" title="表相关操作"></a>表相关操作</h4><h5 id="exsit"><a href="#exsit" class="headerlink" title="exsit"></a>exsit</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exsit：查看某个表是否存在</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172239630.png" alt="image-20230417223907126"></p><h5 id="List"><a href="#List" class="headerlink" title="List"></a>List</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List：查看当前所有的表名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172239913.png" alt="image-20230417223934111"></p><h5 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe：查看选定表的列族及其参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172236116.png" alt="image-20230417223645853"></p><h5 id="Alter"><a href="#Alter" class="headerlink" title="Alter"></a>Alter</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Alter：修改表结构</span><br><span class="line">功能：</span><br><span class="line">修改表中列族的参数信息；</span><br><span class="line">增加列族；</span><br><span class="line">移除或删除已有的列族；</span><br><span class="line"></span><br><span class="line">注意：删除列族时，表中至少有两个列族组成；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172241746.png" alt="image-20230417224143643"></p><h5 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop：删除表</span><br><span class="line">注意：删除表之前需要先禁用表。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172242902.png" alt="image-20230417224227529"></p><h5 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate：删除表中所有数据，想到于对表完成禁用、删除，按原结构重新建立表结构的过程</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172242617.png" alt="image-20230417224251643"></p><h2 id="03-Hbase数据操作"><a href="#03-Hbase数据操作" class="headerlink" title="03 Hbase数据操作"></a>03 Hbase数据操作</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172254865.png" alt="image-20230417225437539"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在HBase中对数据的增删改查命令如表所示，由put命令向表中添加和修改数据，get和scan命令用来查询数据，delete删除列族或列的数据。接下来详细介绍这几个命令的具体用法。</span><br></pre></td></tr></table></figure><h3 id="put"><a href="#put" class="headerlink" title="put"></a>put</h3><h4 id="为单元格插入数据"><a href="#为单元格插入数据" class="headerlink" title="为单元格插入数据"></a>为单元格插入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">语法：put  ‘表名’，‘行键’，‘列族：列限定符’，‘单元格值’，时间戳</span><br><span class="line">描述：必须指定表名、行键、列族、列限定符。</span><br><span class="line">参数区分大小，字符串使用单引号。</span><br><span class="line">只能插入单条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172255143.png" alt="image-20230417225550657"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在HBase中，更新数据时，不管是添加新的数据还是修改数据都使用put命令，它的语法结构如ppt所示，put命令所带的第一个参数为表名，指定某一张表，第二参数为行键的名称，用来指定某一行，第三个参数是为列族和列的名称，中间用冒号隔开，列族名必须是已经创建的，否则HBase会报错；列名是临时定义的，所以列族里的列是可以随意扩展的。第四个参数为单元格的值，在HBase里，所有数据都是字符串的形式。最后一个参数为时间戳，如果不设置时间戳，系统会自动插入当前时间为时间戳。</span><br><span class="line">HBase中所有命令参数是区分大小写的，字符串是需要包含在单引号中的，这一点在介绍后面操作命令不再提示。</span><br><span class="line">从命令形式来看，put只能插入单元格的数据，如果需要将逻辑表中的一行数据插入到HBase中需要执行几条put命令。</span><br><span class="line">比如，需要将此逻辑表的第一行数据（左边图和红色虚线框）插入HBase中，需要执行5条命令（右图）</span><br></pre></td></tr></table></figure><h4 id="更新单元格数据"><a href="#更新单元格数据" class="headerlink" title="更新单元格数据"></a>更新单元格数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">描述：</span><br><span class="line">如果指定的单元格已经存在，则put为更新数据；</span><br><span class="line">单元格会保存指定version&#x3D;&gt;n的多个版本数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172302591.png" alt="image-20230417230227658"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">另外如果由‘表名’，‘行键’，‘列族：列限定符’指定的单元格已经存在表中，则执行put命令为数据更新操作，</span><br><span class="line"></span><br><span class="line">比如，在执行了左边的5条命令后（左图），再执行这条命令（鼠标指向“put ‘Student’, ‘0001’, ‘StuInfo:Name’,‘Tom Green‘,1），学号为1的学生姓名将改成了tom green。</span><br><span class="line"></span><br><span class="line">默认情况下数据更新后，旧版本的数据将不可见，但如果建表时对列族指定了Version属性值，则旧版数据依然存在，用户查询时可以获得最新的多个版本；</span><br></pre></td></tr></table></figure><h3 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">语法：delete  ‘表名’，‘行键’，‘列族&lt;：列限定符&gt;’，&lt;时间戳&gt;</span><br><span class="line">描述：必须指定表名、行键和列族，列限定符和时间戳是可选参数；</span><br><span class="line">Delete最小删除粒度为单元格，且不能跨列族删除。</span><br><span class="line"></span><br><span class="line">(1)delete ‘Student’, ‘0001’, ‘Grades’</span><br><span class="line">(2)delete ‘Student’, ‘0001’, ‘Grades:Math’ </span><br><span class="line">(3)delete ‘Student’, ‘0001’, ‘Grades:Math’,2</span><br><span class="line">(4)Deleteall ‘Student’, ‘0001’</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase中删除数据采用delete命令，其语法与put命令类似，必须指定表名，行键，和列族。而列限定符和时间戳是可选的</span><br><span class="line">下面通过举例说明delete的使用，如第一条命令，只指定了表名行键和列族，表示删除student表中，学号为0001的学生所有的成绩信息。即将表中第一行grades列族的信息全部删除。</span><br><span class="line">第二条命令，指定了列族和列限定符，表示只删除这个学生的数学成绩。</span><br><span class="line">第三条命令，指定了列族和列限定符的同时，还指定了时间戳，表示所有时间戳小于等于2的数据都会被删掉。注意这里不是只删除时间戳等于2的数据。</span><br><span class="line"></span><br><span class="line">从上面语法和命令来看，delete最小的删除粒度为单元格，而且不能跨列族删除，如果想删除表中所有列族在某个行键上的数据，也就是说想删除一个逻辑行，可以使用deleteall命令，例如第四条命令，则删除0001学号学生的所有信息，包括stuinfo列族中的基本信息和grades列族中的所有成绩信息。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete操作并不会马上删除数据，只是将对应的数据打上删除标记（tombstone），只有在数据产生合并时，数据才会被删除。</span><br></pre></td></tr></table></figure><h3 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h3><h4 id="get"><a href="#get" class="headerlink" title="get"></a>get</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">get：根据行键获取一条数据</span><br><span class="line">scan：扫描一个表，可以指定行键范围，或使用过滤器</span><br><span class="line">语法：get  ‘表名’，‘行键’，&lt;‘列族：列限定符’，时间戳&gt;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172313248.png" alt="image-20230417231312108"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第三条命令指定了列族和时间戳范围，</span><br><span class="line">第四条命令则指定列族和显示的版本数，其结果如图所示（蓝色图），在执行此命令之前先向表的stuinfo列族的name列插入了三个版本的数据，注意这里前提是stuinfo列族在创建时已指定VERSION参数可以保存最近的3个版本的数据。在向同一单元格put三条数据后，再执行第四条命令，显示的结果可以看到，只将最近两个更新的数据显示出来了（红色虚线框）</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172315547.png" alt="image-20230417231510110"></p><h4 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">语法：scan  ‘表名’，&#123;&lt; ‘列族：列限定符’，时间戳&gt;&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172321582.png" alt="image-20230417232102333"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">另外一种数据查询方式使用scan命令进行全表扫描，scan命令必须带的参数是表名，其他参数都可选，还可以指定输出行键范围，以及使用过滤器来对全表数据进行过滤显示。</span><br><span class="line">依然通过举例说明scan命令的方法。</span><br><span class="line">第一条命令指定表名查询全表数据；如图所示将表中所有行和所有列族信息都显示出来了。</span><br><span class="line">第二条命令指定列族名称，显示student表中stuinfo列族的所有数据，注意与get不同的是，get只获得某一行的，而scan获取所有行的stuinfo列族数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172324207.png" alt="image-20230417232442038"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第五条命令指定输出行的范围；显示结果输出起始行和结束行但不包括结束行的数据，如图的命令只显示了001行的数据，并没有显示003行。</span><br><span class="line"></span><br><span class="line">另外这些限定条件可以组合使用，中间使用逗号隔开，如第六条命令所示：查询起始行为001，结束行为002的所有行的stuinfo列族的数据信息。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-2</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.html</id>
    <published>2023-04-17T08:46:52.000Z</published>
    <updated>2023-04-19T13:24:55.759Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第二章-Hbase"><a href="#第二章-Hbase" class="headerlink" title="第二章 Hbase"></a>第二章 Hbase</h1><h2 id="01-Hbase简介"><a href="#01-Hbase简介" class="headerlink" title="01 Hbase简介"></a>01 Hbase简介</h2><h3 id="什么是HBase"><a href="#什么是HBase" class="headerlink" title="什么是HBase"></a>什么是HBase</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase是一个开源的NoSQL数据库，参考google的BigTable建模，使用Java语言实现，运行于HDFS文件系统上，为Hadoop提供类似BigTable的服务，可以存储海量稀疏的数据，并具备一定的容错性、高可靠性及伸缩性。</span><br><span class="line"></span><br><span class="line">具备NoSQL数据库的特点：</span><br><span class="line">不支持SQL的跨行事务</span><br><span class="line">不满足完整性约束条件</span><br><span class="line">灵活的数据模型</span><br></pre></td></tr></table></figure><h3 id="HBase的发展历程"><a href="#HBase的发展历程" class="headerlink" title="HBase的发展历程"></a>HBase的发展历程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Apache HBase最初是Powerset公司为了处理自然语言搜索产生的海量数据而开展的项目</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171750807.png" alt="image-20230417175013383"></p><h3 id="HBase特性"><a href="#HBase特性" class="headerlink" title="HBase特性"></a>HBase特性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">容量巨大</span><br><span class="line">列存储</span><br><span class="line">稀疏性</span><br><span class="line">扩展性</span><br><span class="line">高可靠性</span><br></pre></td></tr></table></figure><h4 id="容量巨大"><a href="#容量巨大" class="headerlink" title="容量巨大"></a>容量巨大</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171755056.png" alt="image-20230417175551796"></p><h4 id="列存储"><a href="#列存储" class="headerlink" title="列存储"></a>列存储</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171758370.png" alt="image-20230417175839159"></p><h4 id="稀疏性"><a href="#稀疏性" class="headerlink" title="稀疏性"></a>稀疏性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。</span><br></pre></td></tr></table></figure><h4 id="扩展性"><a href="#扩展性" class="headerlink" title="扩展性"></a>扩展性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">纵向扩展：不断优化主服务器的性能，提高存储空间和性能</span><br><span class="line"></span><br><span class="line">横向扩展：不断向集群添加服务器来提供存储空间和性能</span><br><span class="line"></span><br><span class="line">HBase是横向扩展的，理论上无限横向扩展</span><br></pre></td></tr></table></figure><h4 id="高可靠性"><a href="#高可靠性" class="headerlink" title="高可靠性"></a>高可靠性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于HDFS的多副本机制</span><br><span class="line"></span><br><span class="line">WAL（Write-Ahead-Log）预写机制</span><br><span class="line"></span><br><span class="line">Replication机制</span><br></pre></td></tr></table></figure><h3 id="Hbase安装"><a href="#Hbase安装" class="headerlink" title="Hbase安装"></a>Hbase安装</h3><h4 id="单机模式"><a href="#单机模式" class="headerlink" title="单机模式"></a>单机模式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以本地文件系统作为基础，所有进程运行在一个JVM上，一般用于测试</span><br></pre></td></tr></table></figure><h4 id="伪分布式"><a href="#伪分布式" class="headerlink" title="伪分布式"></a>伪分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">主从模式，以hdfs文件系统为基础，所有进程运行在一个JVM中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改：hbase-env.sh，hbase-site.xml文件</span><br><span class="line">hbase-env.sh:配置java路径，配置zookeeper是否随hbase一起启动，还是先启动zookeeper，再hbase</span><br><span class="line"></span><br><span class="line">hbase-site.xml：主要是zookeeper,hadoop，是否集群部署的一些设置</span><br></pre></td></tr></table></figure><h4 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">主从模式，以hdfs文件系统为基础，守护进程运行在多个jvm上</span><br><span class="line"></span><br><span class="line">除了上述两个文件，还需设置集群有哪些节点文件的配置</span><br></pre></td></tr></table></figure><h2 id="02-HDFS原理"><a href="#02-HDFS原理" class="headerlink" title="02 HDFS原理"></a>02 HDFS原理</h2><h3 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS- 分布式文件系统"></a>HDFS- 分布式文件系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HDFS即Hadoop分布式文件系统（Hadoop Distributed File System）</span><br><span class="line">提供高可靠性和高吞吐量的文件存储服务</span><br><span class="line"></span><br><span class="line">通过软件设计来保证系统的可靠性</span><br><span class="line"></span><br><span class="line">具有容错性，高可靠性，高可扩展性，高吞吐率。</span><br></pre></td></tr></table></figure><h3 id="HDFS基本架构"><a href="#HDFS基本架构" class="headerlink" title="HDFS基本架构"></a>HDFS基本架构</h3><h3 id="HDFS-块"><a href="#HDFS-块" class="headerlink" title="HDFS- 块"></a>HDFS- 块</h3><h3 id="HDFS-NameNode"><a href="#HDFS-NameNode" class="headerlink" title="HDFS-NameNode"></a>HDFS-NameNode</h3><h3 id="HDFS-SecondaryNameNode"><a href="#HDFS-SecondaryNameNode" class="headerlink" title="HDFS-SecondaryNameNode"></a>HDFS-SecondaryNameNode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">定期的合并edits和fsimage文件</span><br><span class="line">Checkpiont：合并的时间点，默认3600秒，或editlog文件达到64M。</span><br></pre></td></tr></table></figure><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2%5C202304171810344.png" alt="image-20230417181053843"></p><h3 id="HDFS-DataNode"><a href="#HDFS-DataNode" class="headerlink" title="HDFS-DataNode"></a>HDFS-DataNode</h3><h3 id="HDFS读文件流程"><a href="#HDFS读文件流程" class="headerlink" title="HDFS读文件流程"></a>HDFS读文件流程</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171814207.png" alt="image-20230417181429835"></p><h4 id="HDFS读写机制-读文件机制"><a href="#HDFS读写机制-读文件机制" class="headerlink" title="HDFS读写机制-读文件机制"></a>HDFS读写机制-读文件机制</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171815977.png" alt="image-20230417181523597"></p><h4 id="HDFS读写机制-写文件机制"><a href="#HDFS读写机制-写文件机制" class="headerlink" title="HDFS读写机制-写文件机制"></a>HDFS读写机制-写文件机制</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171816393.png" alt="image-20230417181609459"></p><h3 id="HDFS副本机制"><a href="#HDFS副本机制" class="headerlink" title="HDFS副本机制"></a>HDFS副本机制</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171819478.png" alt="image-20230417181934890"></p><h3 id="HDFS容错"><a href="#HDFS容错" class="headerlink" title="HDFS容错"></a>HDFS容错</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171821864.png" alt="image-20230417182129386"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、NameNode出错：用Secondary NameNode备份的fsimage恢复</span><br><span class="line">2、DataNode出错：DataNode与NameNode通过“心跳”报告状态，当DataNode失效后，副本数减少，而NameNode会定期检查各节点的副本数量， 检查出问题后会启动数据冗余机制。</span><br><span class="line">3、数据出错：数据写入同时保存总和校验码，读取时校验。</span><br></pre></td></tr></table></figure><h2 id="03-Hbase组件和功能"><a href="#03-Hbase组件和功能" class="headerlink" title="03 Hbase组件和功能"></a>03 Hbase组件和功能</h2><h3 id="HBase-架构"><a href="#HBase-架构" class="headerlink" title="HBase 架构"></a>HBase 架构</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172115566.png" alt="image-20230417211518368"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172116455.png" alt="image-20230417211612969"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Client</span><br><span class="line">包含访问HBase的接口(用户通过client来访问hbase)并维护cache(region的位置)来加快对HBase的访问</span><br><span class="line"></span><br><span class="line">Zookeeper</span><br><span class="line">保证任何时候，集群中只有一个活跃master</span><br><span class="line">存贮所有Region的寻址入口。</span><br><span class="line">实时监控Region server的上线和下线信息。并实时通知Master</span><br><span class="line">存储HBase的schema和table元数据</span><br><span class="line"></span><br><span class="line">Master</span><br><span class="line">为Region server分配region</span><br><span class="line">负责Region server的负载均衡</span><br><span class="line">发现失效的Region server并重新分配其上的region</span><br><span class="line">管理用户对table的增删改操作</span><br><span class="line"></span><br><span class="line">RegionServer</span><br><span class="line">Region server维护region，处理对这些region的IO请求</span><br><span class="line">Region server负责切分在运行过程中变得过大的region</span><br><span class="line"></span><br><span class="line">Region</span><br><span class="line">HBase自动把表水平划分成多个区域(region)，每个region会保存一个表里面某段连续的数据</span><br><span class="line"></span><br><span class="line">每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变）</span><br><span class="line"></span><br><span class="line">当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上。</span><br><span class="line"></span><br><span class="line">Memstore与storefile</span><br><span class="line">一个region由多个store组成，一个store对应一个CF（列族）</span><br><span class="line"></span><br><span class="line">store包括位于内存中的memstore和位于磁盘的storefile写操作先写入memstore，当memstore中的数据达到某个阈值，hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile</span><br><span class="line"></span><br><span class="line">当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile</span><br><span class="line"></span><br><span class="line">当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡</span><br><span class="line"></span><br><span class="line">客户端检索数据，先在memstore找，找不到再找storefile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。</span><br><span class="line">HRegion由一个或者多个Store组成，每个store保存一个columns family。</span><br><span class="line">每个Strore又由一个memStore和0至多个StoreFile组成。如图：StoreFile以HFile格式保存在HDFS上。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172142998.png" alt="image-20230417214228696"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172144442.png" alt="image-20230417214420908"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-1</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-1.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-1.html</id>
    <published>2023-04-17T08:46:47.000Z</published>
    <updated>2023-04-19T10:07:34.371Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><h2 id="数据库基本知识"><a href="#数据库基本知识" class="headerlink" title="数据库基本知识"></a>数据库基本知识</h2><h3 id="什么是数据库？"><a href="#什么是数据库？" class="headerlink" title="什么是数据库？"></a>什么是数据库？</h3><h3 id="什么是数据模型？"><a href="#什么是数据模型？" class="headerlink" title="什么是数据模型？"></a>什么是数据模型？</h3><h4 id="有哪些数据模型？"><a href="#有哪些数据模型？" class="headerlink" title="有哪些数据模型？"></a>有哪些数据模型？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库技术发展至今，传统数据库根据不同的数据模型，主要有以下几种：层次型、网状型和关系型。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171657536.png" alt="image-20230417165719863"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171700245.png" alt="image-20230417170008924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">关系模型要点回顾   </span><br><span class="line">1. 数据结构：</span><br><span class="line">现实世界的实体以及实体之间的各种联系均用关系来表示</span><br><span class="line">数据逻辑结构：二维表</span><br><span class="line">   </span><br><span class="line">   2. 完整性约束条件</span><br><span class="line">域完整性，实体完整性，参照完整性</span><br><span class="line"></span><br><span class="line">    3. 关系操作</span><br><span class="line">选择，投影，连接 等等关系运算；操作对象和结果都是集合</span><br></pre></td></tr></table></figure><h3 id="关系型数据库的优点"><a href="#关系型数据库的优点" class="headerlink" title="关系型数据库的优点"></a>关系型数据库的优点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库的特点</span><br><span class="line">（1）容易理解：用二维表表示</span><br><span class="line">（2）使用方便：通用的SQL语言。</span><br><span class="line">（3）易于维护：丰富的完整性约束大大减低了数据冗余和数据不一致的可能性。</span><br></pre></td></tr></table></figure><h3 id="关系型数据库的不足"><a href="#关系型数据库的不足" class="headerlink" title="关系型数据库的不足"></a>关系型数据库的不足</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对海量数据的读写效率低</span><br><span class="line">    表中有大量数据时，数据的读写速率非常的缓慢</span><br><span class="line">无法适应多变的数据结构</span><br><span class="line">    现代网络中存在大量的半结构化、非结构化数据，针对结构化数据而设计的关系型数据库系统来说，对这些不断变化的数据结构，很难进行高效的处理。</span><br><span class="line">高并发读写的瓶颈</span><br><span class="line">     当数据量达到一定规模时由于关系型数据库的系统逻辑非常复杂，使得在并发处理时非常容易发生死锁，导致其读写速度下滑严重。</span><br><span class="line">可扩展性的限制</span><br><span class="line">由于关系型数据库存在类似的join操作，使得数据库在扩展方面很困难。</span><br></pre></td></tr></table></figure><h2 id="NOSQL数据库理论基础"><a href="#NOSQL数据库理论基础" class="headerlink" title="NOSQL数据库理论基础"></a>NOSQL数据库理论基础</h2><h3 id="什么是NoSQL"><a href="#什么是NoSQL" class="headerlink" title="什么是NoSQL"></a>什么是NoSQL</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171707315.png" alt="image-20230417170659240"></p><h3 id="分布式数据库的特征"><a href="#分布式数据库的特征" class="headerlink" title="分布式数据库的特征"></a>分布式数据库的特征</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">分布式数据库必须具有如下特征，才能应对不断增长的海量数据。</span><br><span class="line">● 高可扩展性：分布式数据库必须具有高可扩展性，能够动态地增添存储节点以实现存储容量的线性扩展</span><br><span class="line">● 高并发性：分布式数据库必须及时响应大规模用户的读&#x2F;写请求，能对海量数据进行随机读写</span><br><span class="line">● 高可用性：分布式数据库必须提供容错机制，能够实现对数据的冗余备份，保证数据和服务的高度可靠性</span><br></pre></td></tr></table></figure><h3 id="NoSQL的特点"><a href="#NoSQL的特点" class="headerlink" title="NoSQL的特点"></a>NoSQL的特点</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171713669.png" alt="image-20230417171305105"></p><h3 id="分布式数据库的数据管理"><a href="#分布式数据库的数据管理" class="headerlink" title="分布式数据库的数据管理"></a>分布式数据库的数据管理</h3><h4 id="什么是数据库系统？"><a href="#什么是数据库系统？" class="headerlink" title="什么是数据库系统？"></a>什么是数据库系统？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库系统  &#x3D;  数据库管理系统     +     数据库</span><br></pre></td></tr></table></figure><h4 id="什么是数据库管理系统？"><a href="#什么是数据库管理系统？" class="headerlink" title="什么是数据库管理系统？"></a>什么是数据库管理系统？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库管理系统(Database Management System)是一种操纵和管理数据库的大型软件，用于建立、使用和维护数据库，简称DBMS。主要任务就是对外提供数据，对内要管理数据。</span><br></pre></td></tr></table></figure><h4 id="数据处理方式：集中式VS分布式"><a href="#数据处理方式：集中式VS分布式" class="headerlink" title="数据处理方式：集中式VS分布式"></a>数据处理方式：集中式VS分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">集中式数据库是指数据库中的数据集中存储在一台计算机上，数据的处理也集中在一台机器上完成。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分布式数据库是指利用高速计算机网络将物理上分散的多个数据存储单元连接起来组成一个逻辑上统一的数据库。</span><br></pre></td></tr></table></figure><h4 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">C:一致性（consistency）（强一致性）</span><br><span class="line">它是指任何一个读操作总是能够读到之前完成的写操作的结果。所有节点在同一时间具有相同的数据。</span><br><span class="line"></span><br><span class="line">A:可用性（Availability）（高可用性）</span><br><span class="line">每个请求都能在确定时间内返回一个响应，无论请求是成功或失败。</span><br><span class="line"></span><br><span class="line">P:分区容忍性（Partition Tolerance）</span><br><span class="line">它是指在一个集群，即系统中的一部分节点无法和其他节点进行通信，系统也能正常运行。也就是说，系统中部分信息的丢失或失败不会影响系统的继续运作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">当处理CAP的问题时，可以有几个明显的选择：</span><br><span class="line">CA：也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最简单的做法是把所有与事务相关的内容都放到同一台机器上。</span><br><span class="line">CP：也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务</span><br><span class="line">AP：也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允许系统返回不一致的数据</span><br></pre></td></tr></table></figure><h5 id="设计原则：在C、A、P之中取舍"><a href="#设计原则：在C、A、P之中取舍" class="headerlink" title="设计原则：在C、A、P之中取舍"></a>设计原则：在C、A、P之中取舍</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171721678.png" alt="image-20230417172155421"></p><h2 id="ACID、BASE与一致性"><a href="#ACID、BASE与一致性" class="headerlink" title="ACID、BASE与一致性"></a>ACID、BASE与一致性</h2><h3 id="ACID与BASE"><a href="#ACID与BASE" class="headerlink" title="ACID与BASE"></a>ACID与BASE</h3><h4 id="为什么会出现ACID、BASE-？"><a href="#为什么会出现ACID、BASE-？" class="headerlink" title="为什么会出现ACID、BASE ？"></a>为什么会出现ACID、BASE ？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CAP理论定义了分布式存储的根本问题，但并没有指出一致性和可用性之间到底应该如何权衡。于是出现了ACID、BASE ，给出了权衡A与C的一种可行方案。</span><br><span class="line">ACID和BASE代表了在一致性-可用性两点之间进行选择的设计哲学</span><br><span class="line">ACID强调一致性被关系数据库使用，BASE强调可用性被大多数Nosql使用</span><br></pre></td></tr></table></figure><h4 id="ACID是什么？"><a href="#ACID是什么？" class="headerlink" title="ACID是什么？"></a>ACID是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">指数据库事务正确执行的四个基本要素的缩写。包含：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">原子性：一个事务的所有系列操作步骤被看成是一个动作，所有的步骤要么全部完成要么都不会完成。</span><br><span class="line"></span><br><span class="line">一致性：事务执行前后，数据库的状态都满足所有的完整性约束。不能发生表与表之间存在外键约束，但是有数据却违背这种约束性。</span><br><span class="line"></span><br><span class="line">隔离性：并发执行的事务是隔离的，保证多个事务互不影响，隔离能够确保并发执行的事务能够顺序一个接一个执行，通过隔离，一个未完成事务不会影响另外一个未完成事务。</span><br><span class="line"></span><br><span class="line">持久性：一个事务一旦提交，它对数据库中数据的改变就应该是永久性的，不会因为和其他操作冲突而取消这个事务。</span><br></pre></td></tr></table></figure><h4 id="BASE原则又是什么？"><a href="#BASE原则又是什么？" class="headerlink" title="BASE原则又是什么？"></a>BASE原则又是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BASE原则 &#x3D; 基本可用性（Basically Available）+软状态（Soft state）+最终一致性（Eventuallyconsistent）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基本可用性：分布式系统在出现故障的时候，允许损失部分可用性，即保证核心功能或者当前最重要功能可用，但是其他功能会被削弱。</span><br><span class="line"></span><br><span class="line">软状态：允许系统数据存在中间状态，但不会影响到系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步时存在延时。</span><br><span class="line"></span><br><span class="line">最终一致性：要求系统数据副本最终能够一致，而不需要实时保证数据副本一致。最终一致性是弱一致性的一种特殊情况。</span><br></pre></td></tr></table></figure><h2 id="NoSQL数据库分类"><a href="#NoSQL数据库分类" class="headerlink" title="NoSQL数据库分类"></a>NoSQL数据库分类</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171731393.png" alt="image-20230417173132275"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-5</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-5.html</id>
    <published>2023-04-15T17:08:43.000Z</published>
    <updated>2023-04-17T07:37:40.431Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-5"><a href="#第十四周-消息队列之Kafka从入门到小牛-5" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-5"></a>第十四周 消息队列之Kafka从入门到小牛-5</h1><h2 id="实战：Flume集成Kafka"><a href="#实战：Flume集成Kafka" class="headerlink" title="实战：Flume集成Kafka"></a>实战：Flume集成Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在实际工作中flume和kafka会深度结合使用</span><br><span class="line">1：flume采集数据，将数据实时写入kafka</span><br><span class="line">2：flume从kafka中消费数据，保存到hdfs，做数据备份</span><br><span class="line"></span><br><span class="line">下面我们就来看一个综合案例</span><br><span class="line">使用flume采集日志文件中产生的实时数据，写入到kafka中，然后再使用flume从kafka中将数据消费出来，保存到hdfs上面</span><br><span class="line">那为什么不直接使用flume将采集到的日志数据保存到hdfs上面呢？</span><br><span class="line">因为中间使用kafka进行缓冲之后，后面既可以实现实时计算，又可以实现离线数据备份，最终实现离线计算，所以这一份数据就可以实现两种需求，使用起来很方便，所以在工作中一般都会这样做。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171406839.png" alt="image-20230417140517700"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">下面我们来实现一下这个功能</span><br><span class="line">其实在Flume中，针对Kafka提供的有KafkaSource和KafkaSink</span><br><span class="line">KafkaSource是从kafka中读取数据</span><br><span class="line">KafkaSink是向kafka中写入数据</span><br><span class="line"></span><br><span class="line">所以针对我们目前这个架构，主要就是配置Flume的Agent。</span><br><span class="line">需要配置两个Agent：</span><br><span class="line">第一个Agent负责实时采集日志文件，将采集到的数据写入Kafka中</span><br><span class="line">第二个Agent负责从Kafka中读取数据，将数据写入HDFS中进行备份(落盘)</span><br><span class="line">针对第一个Agent：</span><br><span class="line">source：ExecSource，使用tail -F监控日志文件即可</span><br><span class="line">channel：MemoryChannel</span><br><span class="line">sink：KafkaSink</span><br><span class="line"></span><br><span class="line">针对第二个Agent</span><br><span class="line">Source：KafkaSource</span><br><span class="line">channel：MemoryChannel</span><br><span class="line">sink：HdfsSink</span><br><span class="line"></span><br><span class="line">这里面这些组件其实只有KafkaSource和KafkaSink我们没有使用过，其它的组件都已经用过了。</span><br></pre></td></tr></table></figure><h3 id="配置Agent"><a href="#配置Agent" class="headerlink" title="配置Agent"></a>配置Agent</h3><h4 id="file-to-kafka-conf"><a href="#file-to-kafka-conf" class="headerlink" title="file-to-kafka.conf"></a>file-to-kafka.conf</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来配置第一个Agent：</span><br><span class="line">文件名为： file-to-kafka.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;test.log</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"></span><br><span class="line"># 指定topic名称</span><br><span class="line">a1.sinks.k1.kafka.topic &#x3D; test_r2p5</span><br><span class="line"># 指定kafka地址，多个节点地址使用逗号分割</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03</span><br><span class="line"># 一次向kafka中写多少条数据，默认值为100，在这里为了演示方便，改为1</span><br><span class="line"># 在实际工作中这个值具体设置多少需要在传输效率和数据延迟上进行取舍</span><br><span class="line"># 如果kafka后面的实时计算程序对数据的要求是低延迟，那么这个值小一点比较好</span><br><span class="line"># 如果kafka后面的实时计算程序对数据延迟没什么要求，那么就考虑传输性能，一次多传输一些</span><br><span class="line"># 建议这个值的大小和ExecSource每秒钟采集的数据量大致相等，这样不会频繁向kafka中写数</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize &#x3D; 1</span><br><span class="line">a1.sinks.k1.kafka.producer.acks &#x3D; 1</span><br><span class="line"># 一个Batch被创建之后，最多过多久，不管这个Batch有没有写满，都必须发送出去</span><br><span class="line"># linger.ms和flumeBatchSize(不积到设置的条数，则一直不写入到topic)，哪个先满足先按哪个规则执行，这个值默认是0，在这设置为1</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms &#x3D; 1</span><br><span class="line"># 指定数据传输时的压缩格式，对数据进行压缩，提高传输效率</span><br><span class="line">a1.sinks.k1.kafka.producer.compression.type &#x3D; snappy</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka的producer的相关参数，可以直接在这里设置：a1.sinks.k1.kafka.producer.+。。。</span><br></pre></td></tr></table></figure><h4 id="kafka-to-hdfs-conf"><a href="#kafka-to-hdfs-conf" class="headerlink" title="kafka-to-hdfs.conf"></a>kafka-to-hdfs.conf</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来配置第二个Agent：</span><br><span class="line">文件名为： kafka-to-hdfs.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 一次性向channel中写入的最大数据量，在这为了演示方便，设置为1</span><br><span class="line"># 这个参数的值不要大于MemoryChannel中transactionCapacity的值</span><br><span class="line">a1.sources.r1.batchSize &#x3D; 1</span><br><span class="line"># 最大多长时间向channel写一次数据</span><br><span class="line">a1.sources.r1.batchDurationMillis &#x3D; 2000</span><br><span class="line"># kafka地址</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata</span><br><span class="line"># topic名称，可以指定一个或者多个，多个topic之间使用逗号隔开</span><br><span class="line"># 也可以使用正则表达式指定一个topic名称规则</span><br><span class="line">a1.sources.r1.kafka.topics &#x3D; test_r2p5</span><br><span class="line"># 指定消费者组id</span><br><span class="line">a1.sources.r1.kafka.consumer.group.id &#x3D; flume-con1</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;kafkaout</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data-</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04机器的flume目录下复制两个目录</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cp -r conf conf-file-to-kafka</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cp -r conf conf-kafka-to-hdfs</span><br><span class="line"></span><br><span class="line">修改 conf_file_to_kafka和conf_kafka_to_hdfs中log4j的配置</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-file-to-kafka</span><br><span class="line">[root@bigdata04 conf_file_to_kafka]# vi log4j.properties </span><br><span class="line">flume.root.logger&#x3D;ERROR,LOGFILE</span><br><span class="line">flume.log.file&#x3D;flume-file-to-kafka.log</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-kafka-to-hdfs</span><br><span class="line">[root@bigdata04 conf_kafka_to_hdfs]# vi log4j.properties </span><br><span class="line">flume.root.logger&#x3D;ERROR,LOGFILE</span><br><span class="line">flume.log.file&#x3D;flume-kafka-to-hdfs.log</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">把刚才配置的两个Agent的配置文件复制到这两个目录下</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-file-to-kafka</span><br><span class="line">[root@bigdata04 conf-file-to-kafka]# vi file-to-kafka.conf</span><br><span class="line">.....把file-to-kafka.conf文件中的内容复制进来即可</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-kafka-to-hdfs&#x2F;</span><br><span class="line">[root@bigdata04 conf-kafka-to-hdfs]# vi kafka-to-hdfs.conf</span><br><span class="line">.....把kafka-to-hdfs.conf文件中的内容复制进来即可</span><br></pre></td></tr></table></figure><h3 id="启动Flume-Agent"><a href="#启动Flume-Agent" class="headerlink" title="启动Flume Agent"></a>启动Flume Agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">启动这两个Flume Agent</span><br><span class="line">确保zookeeper集群、kafka集群和Hadoop集群是正常运行的</span><br><span class="line">以及Kafka中的topic需要提前创建好</span><br><span class="line"></span><br><span class="line">创建topic</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 -partions 5 --replication-factor 2 --topic test_r2p5</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">先启动第二个Agent，再启动第一个Agent</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf-kafka-to-hdfs --conf-file conf-kafka-to-hdfs&#x2F;kafka-to-hdfs.conf</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf-file-to-kafka --conf-file conf-file-to-kafka&#x2F;file-to-kafka.conf</span><br><span class="line"></span><br><span class="line">模拟产生日志数据</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;log&#x2F;</span><br><span class="line">[root@bigdata04 log]# echo hello world &gt;&gt; &#x2F;data&#x2F;log&#x2F;test.log</span><br><span class="line"></span><br><span class="line">到HDFS上查看数据，验证结果：</span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -ls &#x2F;kafkaout</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r-- 2 root supergroup 12 2020-06-09 22:59 &#x2F;kafkaout&#x2F;data-.15</span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -cat &#x2F;kafkaout&#x2F;data-.1591714755267.tmp</span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line">此时Flume可以通过tail -F命令实时监控文件中的新增数据，发现有新数据就写入kafka，然后kafka后面的flume落盘程序，以及kafka后面的实时计算程序就可以使用这份数据了。</span><br></pre></td></tr></table></figure><h2 id="实战：Kafka集群平滑升级"><a href="#实战：Kafka集群平滑升级" class="headerlink" title="实战：Kafka集群平滑升级"></a>实战：Kafka集群平滑升级</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">之前我们在使用 Kafka 0.9.0.0版本的时候，遇到一个比较诡异的问题</span><br><span class="line">（背景：这个版本他们遇到一个问题，官方通过升级kafka版本解决了，但他们之前的版本工作中运用于直播平台，所以不可能将集群停了重新部署一套）</span><br><span class="line">针对消费者组增加消费者的时候可能会导致rebalance，进而导致部分consumer不能再消费分区数据</span><br><span class="line">意思就是之前针对这个topic的5个分区只有2个消费者消费数据，后期我动态的把消费者调整为了5个，这样可能会导致部分消费者无法消费分区中的数据。</span><br><span class="line"></span><br><span class="line">针对这个bug这里有一份详细描述：</span><br><span class="line">https:&#x2F;&#x2F;issues.apache.org&#x2F;jira&#x2F;browse&#x2F;KAFKA-2978</span><br><span class="line">此bug官方在0.9.0.1版本中进行了修复</span><br><span class="line">当时我们的线上集群使用的就是0.9.0.0的版本。</span><br><span class="line"></span><br><span class="line">所以我们需要对线上集群在不影响线上业务的情况下进行升级，称为平滑升级(滚动升级)，也就是升级的时候不影响线上的正常业务运行(但还是要选择在业务低峰期时进行升级)。</span><br><span class="line"></span><br><span class="line">接下来我们就查看了官网文档(0.9.0.0)，上面有针对集群平滑升级的一些信息</span><br><span class="line">http:&#x2F;&#x2F;kafka.apache.org&#x2F;090&#x2F;documentation.html#upgrade</span><br><span class="line">在验证这个升级流程的时候我们是在测试环境下，先模拟线上的集群环境，进行充分测试，可千万不能简单测试一下就直接搞到测试环境去做，这样是很危险的。</span><br><span class="line">由于当时这个kafka集群我们还没有移交给运维负责，并且运维当时对这个框架也不是很熟悉，所以才由我们开发人员来进行平滑升级，否则这种框架升级的事情肯定是交给运维去做的。</span><br><span class="line"></span><br><span class="line">那接下来看一下具体的平滑升级步骤</span><br><span class="line">小版本之间集群升级不需要额外修改集群的配置文件。只需要按照下面步骤去执行即可。</span><br><span class="line">假设kafka0.9.0.0集群在三台服务器上，需要把这三台服务器上的kafka集群升级到0.9.0.1版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：提前在集群的三台机器上把0.9.0.1的安装包，解压、配置好。</span><br><span class="line">主要是log.dirs这个参数，0.9.0.1中的这个参数和0.9.0.0的这个参数一定要保持一致，这样新版本的kafka才可以识别之前的kakfa中的数据。</span><br><span class="line">在集群升级的过程当中建议通过CMAK(kafkamanager)查看集群的状态信息，比较方便</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171529822.png" alt="image-20230417152957587"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1：先stop掉0.9.0.0集群中的第一个节点，然后去CMAK上查看集群的broker信息，确认节点确实已停掉。并且再查看一下，节点的副本下线状态。确认集群是否识别到副本下线状态。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171531285.png" alt="image-20230417153133889"></p><p> <img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171533475.png" alt="image-20230417153328093"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后在当前节点把kafka0.9.0.1启动起来。再回到CMAK中查看broker信息，确认刚启动的节点是否已正确显示，并且还要确认这个节点是否可以正常接收和发送数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171534542.png" alt="image-20230417153455185"></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2：按照第一步的流程去依次操作剩余节点即可，就是先把0.9.0.0版本的kafka停掉，再把0.9.0.1版本的kafka启动即可。</span><br><span class="line"></span><br><span class="line">注意：每操作一个节点，需要稍等一下，确认这个节点可以正常接收和发送数据之后，再处理下一个节点。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.html</id>
    <published>2023-04-15T16:08:29.000Z</published>
    <updated>2023-04-19T16:39:50.981Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="Kafka技巧篇"><a href="#Kafka技巧篇" class="headerlink" title="Kafka技巧篇"></a>Kafka技巧篇</h1><h2 id="Kafka集群参数调忧"><a href="#Kafka集群参数调忧" class="headerlink" title="Kafka集群参数调忧"></a>Kafka集群参数调忧</h2><h3 id="JVM参数调忧"><a href="#JVM参数调忧" class="headerlink" title="JVM参数调忧"></a>JVM参数调忧</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">默认启动的Broker进程只会使用1G内存，在实际使用中会导致进程频繁GC，影响Kafka集群的性能和稳</span><br><span class="line">定性</span><br><span class="line">通过 jstat -gcutil &lt;pid&gt; 1000 查看到kafka进程GC情况</span><br><span class="line">主要看 YGC,YGCT,FGC,FGCT 这几个参数，如果这几个值不是很大，就没什么问题</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">YGC：young gc发生的次数</span><br><span class="line">YGCT：young gc消耗的时间</span><br><span class="line">FGC：full gc发生的次数</span><br><span class="line">FGCT：full gc消耗的时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">13248 Kafka</span><br><span class="line">18087 Jps</span><br><span class="line">1679 QuorumPeerMain</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jstat -gcutil 13248 1000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162304926.png" alt="image-20230416230418172"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果你发现YGC很频繁，或者FGC很频繁，就说明内存分配的少了</span><br><span class="line">此时需要修改kafka-server-start.sh中的KAFKA_HEAP_OPTS</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export KAFKA_HEAP_OPTS&#x3D;&quot;-Xmx10g -Xms10g -XX:MetaspaceSize&#x3D;96m -XX:+UseG1GC -XX</span><br><span class="line"></span><br><span class="line">xms:初始化内存</span><br><span class="line">xmx:最大内存</span><br><span class="line">建议设置成一样大，否则可能进行内存交换</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个配置表示给kafka分配了10G内存</span><br></pre></td></tr></table></figure><h3 id="Replication参数调忧"><a href="#Replication参数调忧" class="headerlink" title="Replication参数调忧"></a>Replication参数调忧</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">replica.socket.timeout.ms&#x3D;60000</span><br><span class="line">这个参数的默认值是30秒，它是控制partiton副本之间socket通信的超时时间，如果设置的太小，有可能会由于网络原因导致造成误判，认为某一个partition副本连不上了。</span><br><span class="line"></span><br><span class="line">replica.lag.time.max.ms&#x3D;50000</span><br><span class="line">如果一个副本在指定的时间内没有向leader节点发送任何请求，或者在指定的时间内没有同步完leader中的数据，则leader会将这个节点从Isr列表中移除。</span><br><span class="line"></span><br><span class="line">这个参数的值默认为10秒</span><br><span class="line">如果网络不好，或者kafka压力较大，建议调大该值，否则可能会频繁出现副本丢失，进而导致集群需要频繁复制副本，导致集群压力更大，会陷入一个恶性循环</span><br></pre></td></tr></table></figure><h3 id="Log参数调优"><a href="#Log参数调优" class="headerlink" title="Log参数调优"></a>Log参数调优</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这块是针对Kafka中数据文件的删除时机进行设置，不是对kafka本身的日志参数配置</span><br><span class="line">log.retention.hours&#x3D;24</span><br><span class="line">这个参数默认值为168，单位是小时，就是7天，默认对数据保存7天，可以在这调整数据保存的时间，我们在实际工作中改为了只保存1天，因为kafka中的数据我们会在hdfs中进行备份，保存一份，所以就没有必要在kafka中保留太长时间了。</span><br><span class="line"></span><br><span class="line">在kafka中保留只是为了能够让你在指定的时间内恢复数据，或者重新消费数据，如果没有这种需求，那就没有必要设置太长时间。</span><br><span class="line"></span><br><span class="line">这里分析的Replication的参数和Log参数都是在server.properties文件中进行配置</span><br><span class="line"></span><br><span class="line">JVM参数是在kafka-server-start.sh脚本中配置</span><br><span class="line"></span><br><span class="line">broker参数调优更多在开发文档里有</span><br></pre></td></tr></table></figure><h2 id="Kafka-Topic命名小技巧"><a href="#Kafka-Topic命名小技巧" class="headerlink" title="Kafka Topic命名小技巧"></a>Kafka Topic命名小技巧</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">针对Kafka中Topic命名的小技巧</span><br><span class="line">建议在给topic命名的时候在后面跟上r2p10之类的内容</span><br><span class="line">r2：表示Partition的副本因子是2</span><br><span class="line">p10：表示这个Topic的分区数是10</span><br><span class="line"></span><br><span class="line">这样的好处是后期我们如果要写消费者消费指定topic的数据，通过topic的名称我们就知道应该设置多少个消费者消费数据效率最高。</span><br><span class="line">因为一个partition同时只能被一个消费者消费，所以效率最高的情况就是消费者的数量和topic的分区数量保持一致。在这里通过topic的名称就可以直接看到，一目了然。</span><br><span class="line"></span><br><span class="line">但是也有一个缺点，就是后期如果我们动态调整了topic的partiton，那么这个topic名称上的partition数量就不准了，针对这个topic，建议大家一开始的时候就提前预估一下，可以多设置一些partition，我们</span><br><span class="line">在工作中的时候针对一些数据量比较大的topic一般会设置40-50个partition，数据量少的topic一般设置5-10个partition，这样后期调整topic partiton数量的场景就比较少了。</span><br></pre></td></tr></table></figure><h2 id="Kafka集群监控管理工具"><a href="#Kafka集群监控管理工具" class="headerlink" title="Kafka集群监控管理工具"></a>Kafka集群监控管理工具</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">现在我们操作Kafka都是在命令行界面中通过脚本操作的，后面需要传很多参数，用起来还是比较麻烦的，那kafka没有提供web界面的支持吗？</span><br><span class="line">很遗憾的告诉你，Apache官方并没有提供，不过好消息是有一个由雅虎开源的一个工具，目前用起来还是不错的。</span><br><span class="line"></span><br><span class="line">它之前的名字叫KafkaManager，后来改名字了，叫CMAK</span><br><span class="line">CMAK是目前最受欢迎的Kafka集群管理工具，最早由雅虎开源，用户可以在Web界面上操作Kafka集群</span><br><span class="line">可以轻松检查集群状态(Topic、Consumer、Offset、Brokers、Replica、Partition)</span><br><span class="line"></span><br><span class="line">那下面我们先去下载这个CMAK</span><br><span class="line">需要到github上面去下载</span><br><span class="line">在github里面搜索CMAK即可</span><br></pre></td></tr></table></figure><h3 id="下载CMAK"><a href="#下载CMAK" class="headerlink" title="下载CMAK"></a>下载CMAK</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162330282.png" alt="image-20230416233039880"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162330225.png" alt="image-20230416233050053"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162331098.png" alt="image-20230416233123771"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162331745.png" alt="image-20230416233141274"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：由于cmak-3.0.0.4.zip版本是在java11这个版本下编译的，所以在运行的时候也需要使用java11这个版本，我们目前服务器上使用的是java8这个版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">我们为什么不使用java11版本呢？因为自2019年1月1日1起，java8之后的更新版本在商业用途的时候就需要收费授权了。</span><br><span class="line">在这针对cmak-3.0.0.4这个版本，如果我们想要使用的话有两种解决办法</span><br><span class="line">1：下载cmak的源码，使用jdk8编译</span><br><span class="line">2：额外安装一个jdk11(自己用不属于商业用途，现实公司很少有用java8以后的)</span><br><span class="line">如果想要编译的话需要安装sbt这个工具对源码进行编译，sbt是Scala 的构建工具, 类似于Maven。</span><br><span class="line"></span><br><span class="line">由于我们在这使用不属于商业用途，所以使用jdk11是没有问题的，那就不用重新编译了。</span><br><span class="line">下载jdk11，jdk-11.0.7_linux-x64_bin.tar.gz</span><br><span class="line">将jdk11的安装包上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">只需要解压即可，不需要配置环境变量，因为只有cmak这个工具才需要使用jdk11</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# tar -zxvf jdk-11.0.7_linux-x64_bin.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来把 cmak-3.0.0.4.zip 上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">1：解压</span><br><span class="line">[root@bigdata01 soft]# unzip cmak-3.0.0.4.zip </span><br><span class="line">-bash: unzip: command not found</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：如果提示-bash: unzip: command not found，则说明目前不支持unzip命令，可以使用yum在线安装</span><br><span class="line">建议先清空一下yum缓存，否则使用yum可能无法安装unzip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# yum clean all </span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Cleaning repos: base extras updates</span><br><span class="line">Cleaning up list of fastest mirrors</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# yum install -y unzip</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">.....</span><br><span class="line">Running transaction</span><br><span class="line"> Installing : unzip-6.0-21.el7.x86_64 1&#x2F;1 </span><br><span class="line"> Verifying : unzip-6.0-21.el7.x86_64 1&#x2F;1 </span><br><span class="line">Installed:</span><br><span class="line"> unzip.x86_64 0:6.0-21.el7 </span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">再重新解压</span><br><span class="line">[root@bigdata01 soft]# unzip cmak-3.0.0.4.zip</span><br></pre></td></tr></table></figure><h3 id="配置CMAK"><a href="#配置CMAK" class="headerlink" title="配置CMAK"></a>配置CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2：修改CMAK配置</span><br><span class="line">首先修改bin目录下的cmak脚本</span><br><span class="line">在里面配置JAVA_HOME指向jdk11的安装目录，否则默认会使用jdk8</span><br><span class="line">[root@bigdata01 soft]# cd cmak-3.0.0.4</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# cd bin&#x2F;</span><br><span class="line">[root@bigdata01 bin]# vi cmak</span><br><span class="line">....</span><br><span class="line">JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk-11.0.7</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">然后修改conf目录下的application.conf文件</span><br><span class="line">只需要在里面增加一行cmak.zkhosts参数的配置即可，指定zookeeper的地址</span><br><span class="line"></span><br><span class="line">注意：在这里指定zookeeper地址主要是为了让CMAK在里面保存数据，这个zookeeper地址不一定是kafka集群使用的那个zookeeper集群，随便哪个zookeeper集群都可以。(cmak需要报错它自己的东西)</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# cd conf&#x2F;</span><br><span class="line">[root@bigdata01 conf]# vi application.conf </span><br><span class="line">....</span><br><span class="line">cmak.zkhosts&#x3D;&quot;bigdata01:2181,bigdata02:2181,bigdata03:2181&quot;</span><br><span class="line">....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3：修改kafka启动配置</span><br><span class="line">想要在CMAK中查看kafka的一些指标信息，在启动kafka的时候需要指定JMX_PORT</span><br><span class="line"></span><br><span class="line">停止kafka集群</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh </span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh </span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">重新启动kafka集群，指定JXM_PORT</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br></pre></td></tr></table></figure><h3 id="启动CMAK"><a href="#启动CMAK" class="headerlink" title="启动CMAK"></a>启动CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">4：启动cmak</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# bin&#x2F;cmak -Dconfig.file&#x3D;conf&#x2F;application.conf -</span><br><span class="line"></span><br><span class="line">如果想把cmak放在后台执行的话需要添加上nohup和&amp;</span><br><span class="line">1 [root@bigdata01 cmak-3.0.0.4]# nohup bin&#x2F;cmak -Dconfig.file&#x3D;conf&#x2F;application.conf -Dhttp.port&#x3D;9001 &amp;</span><br><span class="line"></span><br><span class="line">cmak默认监听端口9000，但这样和hdfs的端口重复了</span><br></pre></td></tr></table></figure><h3 id="访问CMAK"><a href="#访问CMAK" class="headerlink" title="访问CMAK"></a>访问CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5：访问cmak</span><br><span class="line">http:&#x2F;&#x2F;bigdata01:9001&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162356804.png" alt="image-20230416235630637"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6：操作CMAK</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4%5C202304162355045.png" alt="image-20230416235520700"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162355502.png" alt="image-20230416235548395"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这几个参数配置好了以后还需要配置以下几个线程池相关的参数，这几个参数默认值是1，在保存的时候会提示需要大于1，所以可以都改为10</span><br><span class="line">最后点击Save按钮保存即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brokerViewThreadPoolSize：10</span><br><span class="line">offsetCacheThreadPoolSize：10</span><br><span class="line">kafkaAdminClientThreadPoolSize：10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后进来是这样的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170003610.png" alt="image-20230417000304032"></p><h4 id="查看kafak集群的所有broker信息"><a href="#查看kafak集群的所有broker信息" class="headerlink" title="查看kafak集群的所有broker信息"></a>查看kafak集群的所有broker信息</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170003829.png" alt="image-20230417000358325"></p><h4 id="查看kafak集群的所有topic信息"><a href="#查看kafak集群的所有topic信息" class="headerlink" title="查看kafak集群的所有topic信息"></a>查看kafak集群的所有topic信息</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170008382.png" alt="image-20230417000847024"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170007471.png" alt="image-20230417000718104"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170007023.png" alt="image-20230417000734624"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击topic的消费者信息是可以进来查看的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170016794.png" alt="image-20230417001617419"></p><h4 id="创建一个topic"><a href="#创建一个topic" class="headerlink" title="创建一个topic"></a>创建一个topic</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170016835.png" alt="image-20230417001657421"></p><h4 id="给topic增加分区"><a href="#给topic增加分区" class="headerlink" title="给topic增加分区"></a>给topic增加分区</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170017969.png" alt="image-20230417001716328"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是CMAK中常见的功能，当然了这里面还要一些我们没有说到的功能就留给大家以后来发掘了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.html</id>
    <published>2023-04-15T14:14:17.000Z</published>
    <updated>2023-04-19T15:35:18.244Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-3"><a href="#第十四周-消息队列之Kafka从入门到小牛-3" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-3"></a>第十四周 消息队列之Kafka从入门到小牛-3</h1><h2 id="Kafka核心之存储和容错机制"><a href="#Kafka核心之存储和容错机制" class="headerlink" title="Kafka核心之存储和容错机制"></a>Kafka核心之存储和容错机制</h2><h3 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在kafka中每个topic包含1到多个partition，每个partition存储一部分Message。每条Message包含三个属性，其中有一个是offset。</span><br><span class="line"></span><br><span class="line">问题来了：offset相当于partition中这个message的唯一id，那么如何通过id高效的找到message？</span><br><span class="line">两大法宝：分段+索引(分段表示一个partition会存储多个文件)</span><br><span class="line"></span><br><span class="line">kafak中数据的存储方式是这样的：</span><br><span class="line">1、每个partition由多个segment【片段】组成，每个segment文件中存储多条消息，</span><br><span class="line">2、每个partition在内存中对应一个index，记录每个segment文件中的第一条消息偏移量。</span><br><span class="line"></span><br><span class="line">Kafka中数据的存储流程是这样的：</span><br><span class="line">生产者生产的消息会被发送到topic的多个partition上，topic收到消息后往对应partition的最后一个segment上添加该消息，segment达到一定的大小后会创建新的segment。</span><br><span class="line">来看这个图，可以认为是针对topic中某个partition的描述</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160035193.png" alt="image-20230416003228787"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">图中左侧就是索引，右边是segment文件，左边的索引里面会存储每一个segment文件中第一条消息的偏移量，由于消息的偏移量都是递增的，这样后期查找起来就方便了，先到索引中判断数据在哪个</span><br><span class="line">segment文件中，然后就可以直接定位到具体的segment文件了，这样再找具体的那一条数据就很快了，因为都是有序的。</span><br></pre></td></tr></table></figure><h3 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h3><h4 id="Broker节点宕机"><a href="#Broker节点宕机" class="headerlink" title="Broker节点宕机"></a>Broker节点宕机</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当Kafka集群中的一个Broker节点宕机，会出现什么现象？</span><br><span class="line"></span><br><span class="line">下面来演示一下</span><br><span class="line">使用kill -9 杀掉bigdata01中的broker进程测试</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">7522 Jps</span><br><span class="line">2054 Kafka</span><br><span class="line">1679 QuorumPeerMain</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# kill 2054</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">我们可以先通过zookeeper来查看一下，因为当kafka集群中的broker节点启动之后，会自动向zookeeper中进行注册，保存当前节点信息</span><br><span class="line">....]# bin&#x2F;zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers</span><br><span class="line">[ids, seqid, topics]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[1, 2]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160046086.png" alt="image-20230416004647924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时发现zookeeper的&#x2F;brokers&#x2F;ids下面只有2个节点信息</span><br><span class="line">可以通过get命令查看节点信息，这里面会显示对应的主机名和端口号</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] get &#x2F;brokers&#x2F;ids&#x2F;1</span><br><span class="line">&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLA</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160050443.png" alt="image-20230416005045245"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">然后再使用describe查询topic的详细信息，会发现此时的分区的leader全部变成了目前存活的另外两个节点</span><br><span class="line"></span><br><span class="line">此时可以发现Isr中的内容和Replicas中的不一样了，因为Isr中显示的是目前正常运行的节点</span><br><span class="line"></span><br><span class="line">所以当Kafka集群中的一个Broker节点宕机之后，对整个集群而言没有什么特别的大影响，此时集群会给partition重新选出来一些新的Leader节点</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160053702.png" alt="image-20230416005338215"></p><h4 id="新增一个Broker节点"><a href="#新增一个Broker节点" class="headerlink" title="新增一个Broker节点"></a>新增一个Broker节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当Kafka集群中新增一个Broker节点，会出现什么现象？</span><br><span class="line">新加入一个broker节点，zookeeper会自动识别并在适当的机会选择此节点提供服务</span><br><span class="line"></span><br><span class="line">再次启动bigdata01节点中的broker进程测试</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">此时到zookeeper中查看一下</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line">.....</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers</span><br><span class="line">[ids, seqid, topics]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[0, 1, 2]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160058241.png" alt="image-20230416005822131"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">发现broker.id为0的这个节点信息也有了</span><br><span class="line"></span><br><span class="line">在通过describe查看topic的描述信息，Isr中的信息和Replicas中的内容是一样的了</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper localhost:2181 --topic hello</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160059016.png" alt="image-20230416005947958"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">但是启动后有个问题：发现新启动的这个节点不会是任何分区的leader？怎么重新均匀分配呢？</span><br><span class="line">1、Broker中的自动均衡策略（默认已经有）</span><br><span class="line">auto.leader.rebalance.enable&#x3D;true</span><br><span class="line">leader.imbalance.check.interval.seconds 默认值：300</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2、手动执行：</span><br><span class="line">bin&#x2F;kafka-leader-election.sh --bootstrap-server localhost:9092 --election-type pareferred --all-topic-partitions</span><br><span class="line"></span><br><span class="line">Successfully completed leader election (PREFERRED) for partitions hello-4, he</span><br><span class="line"></span><br><span class="line">执行后的效果如下，这样就实现了均匀分配</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160105050.png" alt="image-20230416010500020"></p><h2 id="Kafka生产消费者实战"><a href="#Kafka生产消费者实战" class="headerlink" title="Kafka生产消费者实战"></a>Kafka生产消费者实战</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们使用基于console的生产者和消费者对topic实现了数据的生产和消费，，这个基于控制台的生产者和消费者主要是让我们做测试用的</span><br><span class="line">在实际工作中，我们有时候需要将生产者和消费者功能集成到我们已有的系统中，此时就需要写代码实现生产者和消费者的逻辑了。</span><br><span class="line">在这我们使用java代码来实现生产者和消费者的功能</span><br></pre></td></tr></table></figure><h3 id="Kafka-Java代码编程"><a href="#Kafka-Java代码编程" class="headerlink" title="Kafka Java代码编程"></a>Kafka Java代码编程</h3><h4 id="Java代码实现生产者代码"><a href="#Java代码实现生产者代码" class="headerlink" title="Java代码实现生产者代码"></a>Java代码实现生产者代码</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160114267.png" alt="image-20230416011434393"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先创建maven项目， db_kafka</span><br><span class="line"></span><br><span class="line">添加kafka的maven依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.kafka&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;kafka-clients&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">开发生产者代码</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.kafka;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Java代码实现生产者代码</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerDemo</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">         <span class="comment">//指定kafka的broker地址</span></span><br><span class="line">         prop.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">         <span class="comment">//指定key-value数据的序列化格式(key就是之前讲的，如果指定了数据有key，则可以根据它来将数据放入哪一个partition，一般用不到；但这里要知道不然要报错)</span></span><br><span class="line">         prop.put(<span class="string">"key.serializer"</span>, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         prop.put(<span class="string">"value.serializer"</span>, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定topic</span></span><br><span class="line">         String topic = <span class="string">"hello"</span>; </span><br><span class="line">         <span class="comment">//创建kafka生产者</span></span><br><span class="line">         KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String,String&gt;(prop);</span><br><span class="line">         <span class="comment">//向topic中生产数据(这里也没有传入key，只传入了value)</span></span><br><span class="line">         producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="string">"hello kafka"</span>))</span><br><span class="line">         <span class="comment">//关闭链接</span></span><br><span class="line">         producer.close();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Java代码实现消费者代码"><a href="#Java代码实现消费者代码" class="headerlink" title="Java代码实现消费者代码"></a>Java代码实现消费者代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.kafka;</span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Collection;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Java代码实现消费者代码</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerDemo</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">         <span class="comment">//指定kafka的broker地址(之前控制台那里server没s)</span></span><br><span class="line">         prop.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">         <span class="comment">//指定key-value的反序列化类型</span></span><br><span class="line">         prop.put(<span class="string">"key.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         prop.put(<span class="string">"value.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定消费者组(之前控制台那里，会自动生成)</span></span><br><span class="line">         prop.put(<span class="string">"group.id"</span>, <span class="string">"con-1"</span>);</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//创建消费者</span></span><br><span class="line">         KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String,String&gt;(prop);</span><br><span class="line">        Collection&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">         topics.add(<span class="string">"hello"</span>);</span><br><span class="line">         <span class="comment">//订阅指定的topic</span></span><br><span class="line">         consumer.subscribe(topics);</span><br><span class="line">         <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">             <span class="comment">//消费数据【注意：需要修改jdk编译级别为1.8，否则Duration.ofSeconds(1)会语法报错</span></span><br><span class="line">             ConsumerRecords&lt;String, String&gt; poll = consumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">             <span class="keyword">for</span> (ConsumerRecord&lt;String,String&gt; consumerRecord : poll) &#123;</span><br><span class="line">             System.out.println(consumerRecord);</span><br><span class="line">             &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1. 关闭kafka服务器的防火墙</span><br><span class="line">2. 配置windows的hosts文件 添加kafka节点的hostname和ip的映射关系。[如果我们的hosts文件中没有对kafka节点的hostnam和ip的映射关系做配置，在这经过多次尝试连接不上就会报错]</span><br><span class="line"></span><br><span class="line">先开启消费者。</span><br><span class="line">发现没有消费到数据，这个topic中是有数据的，为什么之前的数据没有消费出来呢？(就是前面讲的，默认会从consumer生成后生成的数据读取)不要着急，先带着这个问题往下面看</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3%5Cimage-20230416014022148.png" alt="image-20230416014022148"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再开启生产者，生产者会生产一条数据，然后就结束</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160143273.png" alt="image-20230416014143241"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时回到kafka的消费者端就可以看到消费出来的数据了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160142342.png" alt="image-20230416014214263"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以这个时候我们发现，新产生的数据我们是可以消费到的，但是之前的数据我们就无法消费了，那下面我们来分析一下这个问题</span><br></pre></td></tr></table></figure><h4 id="消费者代码扩展"><a href="#消费者代码扩展" class="headerlink" title="消费者代码扩展"></a>消费者代码扩展</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x2F;&#x2F;开启消费者自动提交offset功能，默认就是开启的</span><br><span class="line">prop.put(&quot;enable.auto.commit&quot;,&quot;true&quot;);</span><br><span class="line">&#x2F;&#x2F;自动提交offset的时间间隔，单位是毫秒(在开启自动提交时，它默认开启，且默认值是5000)</span><br><span class="line">prop.put(&quot;auto.commit.interval.ms&quot;,&quot;5000&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">注意：正常情况下，kafka消费数据的流程是这样的</span><br><span class="line">先根据group.id指定的消费者组到kafka中查找之前保存的offset信息</span><br><span class="line"></span><br><span class="line">如果查找到了，说明之前使用这个消费者组消费过数据，则根据之前保存的offset继续进行消费</span><br><span class="line"></span><br><span class="line">如果没查找到(说明第一次消费)，或者查找到了，但是查找到的那个offset对应的数据已经不存</span><br><span class="line"></span><br><span class="line">这个时候消费者该如何消费数据？</span><br><span class="line">(因为kafka默认只会保存7天的数据，超过时间数据会被删除)</span><br><span class="line"></span><br><span class="line">此时会根据auto.offset.reset的值执行不同的消费逻辑</span><br><span class="line"></span><br><span class="line">这个参数的值有三种:[earliest,latest,none]</span><br><span class="line">earliest：表示从最早的数据开始消费(从头消费)</span><br><span class="line">latest【默认】：表示从最新的数据开始消费</span><br><span class="line">none：如果根据指定的group.id没有找到之前消费的offset信息，就会抛异常</span><br><span class="line"></span><br><span class="line">(工作中earliest和latest常用)</span><br><span class="line"></span><br><span class="line">解释：【查找到了，但是查找到的那个offset对应的数据已经不存在了】 </span><br><span class="line">假设你第一天使用一个消费者去消费了一条数据，然后就把消费者停掉了，等了7天之后，你又使用这个消费者去消费数据</span><br><span class="line">这个时候，这个消费者启动的时候会到kafka里面查询它之前保存的offset信息</span><br><span class="line">但是那个offset对应的数据已经被删了，所以此时再根据这个offset去消费是消费不到数据的</span><br><span class="line"></span><br><span class="line">总结，一般在实时计算的场景下，这个参数的值建议设置为latest，消费最新的数据</span><br><span class="line"></span><br><span class="line">这个参数只有在消费者第一次消费数据，或者之前保存的offset信息已过期的情况下才会生效</span><br><span class="line"> *&#x2F;</span><br><span class="line"></span><br><span class="line">prop.put(&quot;auto.offset.reset&quot;,&quot;latest&quot;);</span><br><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时我们来验证一下，</span><br><span class="line">先启动一次生产者，再启动一次消费者，看看消费者能不能消费到这条数据，如果能消费到，就说明此时是根据上次保存的offset信息进行消费了。结果发现是可以消费到的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：消费者消费到数据之后，不要立刻关闭程序，要至少等5秒，因为自动提交offset的时机是5秒提交一次</span><br><span class="line"></span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 4, leaderEpoch &#x3D; 5, offset &#x3D; 0, Cre</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">将auto.offset.reset置为earliest，修改一下group.id的值，相当于使用一个新的消费者，验证一下，看是否能把这个topic中的所有数据都取出来，因为新的消费者第一次肯定是获取不到offset信息的，</span><br><span class="line">所以就会根据auto.offset.reset的值来消费数据</span><br><span class="line"></span><br><span class="line">prop.put(&quot;group.id&quot;, &quot;con-2&quot;);</span><br><span class="line">prop.put(&quot;auto.offset.reset&quot;,&quot;earliest&quot;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 2, leaderEpoch &#x3D; 0, offset &#x3D; 0, Cre</span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 3, leaderEpoch &#x3D; 3, offset &#x3D; 0, Cre</span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 4, leaderEpoch &#x3D; 5, offset &#x3D; 0, Cre</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162130163.png" alt="image-20230416213026828"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时，关闭消费者(需要等待5秒，这样才会提交offset)，再重新启动，发现没有消费到数据，说明此时就</span><br><span class="line">根据上次保存的offset来消费数据了，因为没有新数据产生，所以就消费不到了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最后来处理一下程序输出的日志警告信息，这里其实示因为缺少依赖日志依赖</span><br><span class="line">在pom文件中添加log4j的依赖，然后将 log4j.properties 添加到 resources目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;info,stdout</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout &#x3D; org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Target &#x3D; System.out</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br></pre></td></tr></table></figure><h3 id="Consumer消费offset查询"><a href="#Consumer消费offset查询" class="headerlink" title="Consumer消费offset查询"></a>Consumer消费offset查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka0.9版本以前，消费者的offset信息保存在zookeeper中</span><br><span class="line">从kafka0.9开始，使用了新的消费API，消费者的信息会保存在kafka里面的__consumer_offsets这个topic中</span><br><span class="line"></span><br><span class="line">因为频繁操作zookeeper性能不高，所以kafka在自己的topic中负责维护消费者的offset信息。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162135924.png" alt="image-20230416213511507"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如何查询保存在kafka中的Consumer的offset信息呢？</span><br><span class="line">使用kafka-consumer-groups.sh这个脚本可以查看目前所有的consumer group</span><br><span class="line"></span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-consumer-groups.sh --list --bootstrap-server localhost:9092</span><br><span class="line"></span><br><span class="line">con-1</span><br><span class="line">con-2 (前面视频里修改过)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">具体查看某一个consumer group的信息</span><br><span class="line">GROUP：当前消费者组，通过group.id指定的值</span><br><span class="line">TOPIC：当前消费的topic</span><br><span class="line">PARTITION：消费的分区</span><br><span class="line">CURRENT-OFFSET：消费者消费到这个分区的offset</span><br><span class="line">LOG-END-OFFSET：当前分区中数据的最大offset</span><br><span class="line">LAG：当前分区未消费数据量</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group con-1</span><br><span class="line">GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG</span><br><span class="line">con-1 hello 4 1 1 0 </span><br><span class="line">con-1 hello 2 1 1 0 </span><br><span class="line">con-1 hello 3 1 1 0</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162141088.png" alt="image-20230416214127550"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">partition:是指消费者消费了哪些分区</span><br><span class="line">current-offset:当前消费了的数据的offset</span><br><span class="line">log-end-offset:最新数据的offset</span><br><span class="line">lag:还有多少条数据没消费</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时再执行一次生产者代码，生产一条数据，重新查看一下这个消费者的offset情况</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162148346.png" alt="image-20230416214839816"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如何分析生产的数据能不能及时消费掉：查看lag</span><br><span class="line">如果lag值比较大：就需要增加消费者个数，同一个代码执行多次(但group.id不能变)</span><br></pre></td></tr></table></figure><h3 id="Consumer消费顺序"><a href="#Consumer消费顺序" class="headerlink" title="Consumer消费顺序"></a>Consumer消费顺序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当一个消费者消费一个partition时候，消费的数据顺序和此partition数据的生产顺序是一致的</span><br><span class="line"></span><br><span class="line">当一个消费者消费多个partition时候，消费者按照partition的顺序，首先消费一个partition，当消费完一个partition最新的数据后再消费其它partition中的数据</span><br><span class="line"></span><br><span class="line">总之：如果一个消费者消费多个partiton，只能保证消费的数据顺序在一个partition内是有序的</span><br><span class="line"></span><br><span class="line">也就是说消费kafka中的数据只能保证消费partition内的数据是有序的，多个partition之间是无序的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162201198.png" alt="image-20230416220156464"></p><h3 id="Kafka的三种语义"><a href="#Kafka的三种语义" class="headerlink" title="Kafka的三种语义"></a>Kafka的三种语义</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka可以实现以下三种语义，这三种语义是针对消费者而言的：</span><br></pre></td></tr></table></figure><h4 id="至少一次：at-least-once"><a href="#至少一次：at-least-once" class="headerlink" title="至少一次：at-least-once"></a>至少一次：at-least-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这种语义有可能会对数据重复处理</span><br><span class="line">实现至少一次消费语义的消费者也很简单。</span><br><span class="line">1: 设置enable.auto.commit为false，禁用自动提交offset</span><br><span class="line">2: 消息处理完之后手动调用consumer.commitSync()提交offset</span><br><span class="line">这种方式是在消费数据之后，手动调用函数consumer.commitSync()异步提交offset，有可能处理多次的场景是消费者的消息处理完并输出到结果库，但是offset还没提交，这个时候消费者挂掉了，再重启的时候会重新消费并处理消息，所以至少会处理一次</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162213700.png" alt="image-20230416221328144"></p><h4 id="至多一次：at-most-once"><a href="#至多一次：at-most-once" class="headerlink" title="至多一次：at-most-once"></a>至多一次：at-most-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">这种语义有可能会丢失数据</span><br><span class="line">至多一次消费语义是kafka消费者的默认实现。配置这种消费者最简单的方式是</span><br><span class="line">1: enable.auto.commit设置为true。</span><br><span class="line">2: auto.commit.interval.ms设置为一个较低的时间范围。</span><br><span class="line">由于上面的配置，此时kafka会有一个独立的线程负责按照指定间隔提交offset。</span><br><span class="line"></span><br><span class="line">消费者的offset已经提交，但是消息还在处理中(还没有处理完)，这个时候程序挂了，导致数据没有被成功处理，再重启的时候会从上次提交的offset处消费，导致上次没有被成功处理的消息就丢失了。</span><br></pre></td></tr></table></figure><h4 id="仅一次：exactly-once"><a href="#仅一次：exactly-once" class="headerlink" title="仅一次：exactly-once"></a>仅一次：exactly-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这种语义可以保证数据只被消费处理一次。</span><br><span class="line">实现仅一次语义的思路如下：</span><br><span class="line">1: 将enable.auto.commit设置为false，禁用自动提交offset</span><br><span class="line">2: 使用consumer.seek(topicPartition，offset)来指定offset</span><br><span class="line">3: 在处理消息的时候，要同时保存住每个消息的offset。以原子事务的方式保存offset和处理的消息结果，这个时候相当于自己保存offset信息了，把offset和具体的数据绑定到一块，数据真正处理成功的时候才会保存offset信息</span><br><span class="line">这样就可以保证数据仅被处理一次了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.html</id>
    <published>2023-04-15T14:13:26.000Z</published>
    <updated>2023-04-16T03:01:48.465Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="大数据开发工程师-第十四周-消息队列之Kafka从入门到小牛-2"><a href="#大数据开发工程师-第十四周-消息队列之Kafka从入门到小牛-2" class="headerlink" title="大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2"></a>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2</h1><h2 id="Kafka使用初体验"><a href="#Kafka使用初体验" class="headerlink" title="Kafka使用初体验"></a>Kafka使用初体验</h2><h3 id="Kafka中Topic的操作"><a href="#Kafka中Topic的操作" class="headerlink" title="Kafka中Topic的操作"></a>Kafka中Topic的操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka集群安装好了以后我们就想向kafka中添加一些数据</span><br><span class="line">想要添加数据首先需要创建topic</span><br><span class="line">那接下来看一下针对topic的一些操作</span><br></pre></td></tr></table></figure><h4 id="新增Topic"><a href="#新增Topic" class="headerlink" title="新增Topic"></a>新增Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">指定2个分区，2个副本，注意：副本数不能大于集群中Broker的数量</span><br><span class="line"></span><br><span class="line">因为每个partition的副本必须保存在不同的broker，否则没有意义，如果partition的副本都保存在同一个broker，那么这个broker挂了，则partition数据依然会丢失</span><br><span class="line"></span><br><span class="line">在这里我使用的是3个节点的kafka集群，所以副本数我就暂时设置为2，最大可以设置为3</span><br><span class="line"></span><br><span class="line">如果你们用的是单机kafka的话，这里的副本数就只能设置为1了，这个需要注意一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 2 --replication-factor 2</span><br><span class="line">--topic hello</span><br><span class="line">Created topic hello.</span><br></pre></td></tr></table></figure><h4 id="查询Topic"><a href="#查询Topic" class="headerlink" title="查询Topic"></a>查询Topic</h4><h5 id="查询Kafka中的所有Topic列表"><a href="#查询Kafka中的所有Topic列表" class="headerlink" title="查询Kafka中的所有Topic列表"></a>查询Kafka中的所有Topic列表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">查询Kafka中的所有Topic列表以及查看指定Topic的详细信息</span><br><span class="line">查询kafka中所有的topic列表</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line">hello</span><br><span class="line"></span><br><span class="line">topic数据是存在zookeeper中，所以直接指定zookeeper地址就可以了(有些地方需要指定kafka地址)</span><br></pre></td></tr></table></figure><h5 id="查看指定Topic的详细信息"><a href="#查看指定Topic的详细信息" class="headerlink" title="查看指定Topic的详细信息"></a>查看指定Topic的详细信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看指定topic的详细信息</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper localhost:2181 --topic hello</span><br><span class="line">Topic: hello PartitionCount: 2 ReplicationFactor: 2 Configs: </span><br><span class="line">Topic: hello Partition: 0 Leader: 2 Replicas: 2,0 Is</span><br><span class="line">Topic: hello Partition: 1 Leader: 0 Replicas: 0,1 Is</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152231741.png" alt="image-20230415223043384"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">第一个行显示指定topic所有partitions的一个总结</span><br><span class="line">PartitionCount：表示这个Topic一共有多少个partition</span><br><span class="line">ReplicationFactor：表示这个topic中partition的副本因子是几个</span><br><span class="line">Config：这个表示创建Topic时动态指定的配置信息，在这我们没有额外指定配置信息</span><br><span class="line"></span><br><span class="line">下面每一行给出的是一个partition的信息，如果只有一个partition，则只显示一行。</span><br><span class="line">Topic：显示当前的topic名称</span><br><span class="line">Partition：显示当前topic的partition编号</span><br><span class="line">Leader：Leader partition所在的节点编号，这个编号其实就是broker.id的值，</span><br><span class="line">来看这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152234654.png" alt="image-20230415223420514"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这个图里面的hello这个topic有两个partition，其中partition1的leader所在的节点是broker1，partition2的leader所在的节点是broker2</span><br><span class="line"></span><br><span class="line">Replicas：当前partition所有副本所在的节点编号【包含Leader所在的节点】，如果设置多个副本的话，这里会显示多个，不管该节点是否是Leader以及是否存活。</span><br><span class="line"></span><br><span class="line">Isr：当前partition处于同步状态的所有节点，这里显示的所有节点都是存活状态的，并且跟Leader同步的(包含Leader所在的节点)</span><br><span class="line"></span><br><span class="line">所以说Replicas和Isr的区别就是</span><br><span class="line">如果某个partition的副本所在的节点宕机了，在Replicas中还是会显示那个节点，但是在Isr中就不会显示了，Isr中显示的都是处于正常状态的节点。</span><br></pre></td></tr></table></figure><h4 id="修改Topic"><a href="#修改Topic" class="headerlink" title="修改Topic"></a>修改Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">修改一般是修改Topic的partition数量，只能增加</span><br><span class="line"></span><br><span class="line">为什么partition只能增加？</span><br><span class="line">因为数据是存储在partition中的，如果可以减少partition的话，那么partition中的数据就丢了</span><br><span class="line"></span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --alter --zookeeper localhost:2181 --partitions 5 --topic hello </span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partitio</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line">修改之后再来查看一下topic的详细信息</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper </span><br><span class="line">Topic: hello PartitionCount: 5 ReplicationFactor: 2 Configs: </span><br><span class="line"> Topic: hello Partition: 0 Leader: 2 Replicas: 2,0 I</span><br><span class="line"> Topic: hello Partition: 1 Leader: 0 Replicas: 0,1 I</span><br><span class="line"> Topic: hello Partition: 2 Leader: 1 Replicas: 1,2 I</span><br><span class="line"> Topic: hello Partition: 3 Leader: 2 Replicas: 2,1 I</span><br><span class="line"> Topic: hello Partition: 4 Leader: 0 Replicas: 0,2</span><br></pre></td></tr></table></figure><h4 id="删除Topic"><a href="#删除Topic" class="headerlink" title="删除Topic"></a>删除Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">删除Kafka中的指定Topic</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --delete --zookeeper localhost 2181 --topic hello</span><br><span class="line">Topic hello is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br><span class="line"></span><br><span class="line">删除操作是不可逆的，删除Topic会删除它里面的所有数据</span><br><span class="line"></span><br><span class="line">注意：Kafka从1.0.0开始默认开启了删除操作，之前的版本只会把Topic标记为删除状态，需要设置delete.topic.enable为true才可以真正删除</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果不想开启删除功能，可以设置delete.topic.enable为false，这样删除topic的时候只会把它标记为删除状态，此时这个topic依然可以正常使用。</span><br><span class="line">delete.topic.enable可以配置在server.properties文件中</span><br></pre></td></tr></table></figure><h3 id="Kafka中的生产者和消费者"><a href="#Kafka中的生产者和消费者" class="headerlink" title="Kafka中的生产者和消费者"></a>Kafka中的生产者和消费者</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了Kafka中的topic的创建方式，下面我们可以向topic中生产数据以及消费数据了</span><br><span class="line">生产数据需要用到生产者</span><br><span class="line">消费数据需要用到消费者</span><br><span class="line"></span><br><span class="line">kafka默认提供了基于控制台的生产者和消费者，方便测试使用</span><br><span class="line">生产者： bin&#x2F;kafka-console-producer.sh</span><br><span class="line">消费者： bin&#x2F;kafka-console-consumer.sh</span><br></pre></td></tr></table></figure><h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">先来看一下如何向里面生产数据</span><br><span class="line">直接使用kafka提供的基于控制台的生产者</span><br><span class="line">先创建一个topic【5个分区，2个副本】：</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost 2181 --partitions 5 --replication-factor 2 --topic hello</span><br><span class="line">Created topic hello.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">向这个topic中生产数据</span><br><span class="line">broker-list：kafka的服务地址[多个用逗号隔开](这里需要用到kafka地址，上面创建topic时指定的是zookeeper，这里用本地地址和使用bigdata01:9092,bigdata02:9092,bigdata03:9092是一样的)</span><br><span class="line">topic：topic名称</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-producer.sh --broker-list localhost:9092 --topic hello</span><br><span class="line">&gt;hehe</span><br></pre></td></tr></table></figure><h4 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">下面来创建一个消费者消费topic中的数据</span><br><span class="line">bootstrap-server：kafka的服务地址</span><br><span class="line">topic:具体的topic下面来创建一个消费者消费topic中的数据</span><br><span class="line">1 [root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hello</span><br><span class="line"></span><br><span class="line">发现消费不到刚才生产的数据，为什么呢？</span><br><span class="line">因为kafka的消费者默认是消费最新生产的数据，如果想消费之前生产的数据需要添加一个参数--from-beginning，表示从头消费的意思</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hello --from-beginning</span><br><span class="line"></span><br><span class="line">hehe</span><br><span class="line"></span><br><span class="line">这里创建消费者的机器01、02、03都可以</span><br></pre></td></tr></table></figure><h4 id="案例：QQ群聊天"><a href="#案例：QQ群聊天" class="headerlink" title="案例：QQ群聊天"></a>案例：QQ群聊天</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">通过kafka可以模拟QQ群聊天的功能，我们来看一下</span><br><span class="line">首先在kafka中创建一个新的topic，可以认为是我们在QQ里面创建了一个群，群号是88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost 2181 --partitions 5 --replication-factor 2 --topic 88888888</span><br><span class="line">Created topic 88888888.</span><br><span class="line"></span><br><span class="line">然后我把你们都拉到这个群里面，这样我在群里面发消息你们就都能收到了</span><br><span class="line">在bigdata02和bigdata03上开启消费者，可以认为是把这两个人拉到群里面了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic 88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic 88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">然后我在bigdata01上开启生产者发消息，这样bigdata02和bigdata03都是可以收到的。</span><br><span class="line">这样就可以认为在群里的人都能收到我发的消息，类似于发广播。</span><br><span class="line">这个其实主要利用了kafka中的多消费者的特性，每个消费者都可以消费到相同的数据</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-producer.sh --broker-list localhost:9092 --topic 88888888 </span><br><span class="line">&gt;hello everyone</span><br></pre></td></tr></table></figure><h2 id="Kafka核心扩展内容"><a href="#Kafka核心扩展内容" class="headerlink" title="Kafka核心扩展内容"></a>Kafka核心扩展内容</h2><h3 id="Broker扩展"><a href="#Broker扩展" class="headerlink" title="Broker扩展"></a>Broker扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Broker的参数可以配置在server.properties这个配置文件中，Broker中支持的完整参数在官方文档中有体现</span><br><span class="line">具体链接为：</span><br><span class="line">http:&#x2F;&#x2F;kafka.apache.org&#x2F;24&#x2F;documentation.html#brokerconfigs</span><br><span class="line">针对Broker的参数，我们主要分析两块</span><br></pre></td></tr></table></figure><h4 id="Log-Flush-Policy"><a href="#Log-Flush-Policy" class="headerlink" title="Log Flush Policy"></a>Log Flush Policy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1：Log Flush Policy：设置数据flush到磁盘的时机</span><br><span class="line">为了减少磁盘写入的次数,broker会将消息暂时缓存起来,当消息的个数达到一定阀值或者过了一定的时间间隔后,再flush到磁盘,这样可以减少磁盘IO调用的次数。</span><br><span class="line"></span><br><span class="line">这块主要通过两个参数控制</span><br><span class="line">log.flush.interval.messages 一个分区的消息数阀值，达到该阈值则将该分区的数据flush到磁盘，注意这里是针对分区，因为topic是一个逻辑概念，分区是真实存在的，每个分区会在磁盘上产生一个目录</span><br><span class="line">[root@bigdata01 kafka-logs]# ll</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:12 88888888-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:12 88888888-3</span><br><span class="line">-rw-r--r--. 1 root root 4 Jun 8 15:23 cleaner-offset-checkpoint</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-12</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-15</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-18</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-21</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-24</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-27</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-3</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-30</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-33</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-36</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-39</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-42</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-45</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-48</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-6</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-9</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-1</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-4 </span><br><span class="line">hello topic有5个分区，但这里只有2个目录的原因是：没写几条数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160022785.png" alt="image-20230416002248398"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个参数的默认值为9223372036854775807，long的最大值</span><br><span class="line">默认值太大了，所以建议修改，可以使用server.properties中针对这个参数指定的值10000，需要去掉注释之后这个参数才生效。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">log.flush.interval.ms间隔指定时间</span><br><span class="line">默认间隔指定的时间将内存中缓存的数据flush到磁盘中，由文档可知，这个参数的默认值为null，此时会使用log.flush.scheduler.interval.ms参数的值，log.flush.scheduler.interval.ms参数的值默认是 9223372036854775807，long的最大值</span><br><span class="line"></span><br><span class="line">所以这个值也建议修改，可以使用server.properties中针对这个参数指定的值1000，单位是毫秒，表示每1秒写一次磁盘，这个参数也需要去掉注释之后才生效</span><br></pre></td></tr></table></figure><h4 id="Log-Retention-Policy"><a href="#Log-Retention-Policy" class="headerlink" title="Log Retention Policy"></a>Log Retention Policy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">设置数据保存周期，默认7天</span><br><span class="line">kafka中的数据默认会保存7天，如果kafka每天接收的数据量过大，这样是很占磁盘空间的，建议修改数据保存周期，我们之前在实际工作中是将数据保存周期改为了1天。</span><br><span class="line"></span><br><span class="line">数据保存周期主要通过这几个参数控制</span><br><span class="line">log.retention.hours，这个参数默认值为168，单位是小时，就是7天，可以在这调整数据保存的时间，超过这个时间数据会被自动删除</span><br><span class="line">log.retention.bytes，这个参数表示当分区的文件达到一定大小的时候会删除它，如果设置了按照指定周期删除数据文件，这个参数不设置也可以，这个参数默认是没有开启的</span><br><span class="line">log.retention.check.interval.ms，这个参数表示检测的间隔时间，单位是毫秒，默认值是300000，就是5分钟，表示每5分钟检测一次文件看是否满足删除的时机</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">默认根据时间就行了，后期在时间范围内还可以从kafka中恢复数据</span><br></pre></td></tr></table></figure><h3 id="Producer扩展"><a href="#Producer扩展" class="headerlink" title="Producer扩展"></a>Producer扩展</h3><h4 id="producer发送数据到partition的方式"><a href="#producer发送数据到partition的方式" class="headerlink" title="producer发送数据到partition的方式"></a>producer发送数据到partition的方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Producer默认是随机将数据发送到topic的不同分区中，也可以根据用户设置的算法来根据消息的key来计算输入到哪个partition里面</span><br><span class="line"></span><br><span class="line">此时需要通过partitioner来控制，这个知道就行了，因为在实际工作中一般在向kafka中生产数据的都是不带key的，只有数据内容，所以一般都是使用随机的方式发送数据</span><br></pre></td></tr></table></figure><h4 id="producer的数据通讯方式"><a href="#producer的数据通讯方式" class="headerlink" title="producer的数据通讯方式"></a>producer的数据通讯方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">在这里有一个需要注意的内容就是</span><br><span class="line">针对producer的数据通讯方式：同步发送和异步发送</span><br><span class="line"></span><br><span class="line">同步是指：生产者发出数据后，等接收方发回响应以后再发送下个数据的通讯方式。</span><br><span class="line">异步是指：生产者发出数据后，不等接收方发回响应，接着发送下个数据的通讯方式。</span><br><span class="line"></span><br><span class="line">具体的数据通讯策略是由acks参数控制的</span><br><span class="line">acks默认为1，表示需要Leader节点回复收到消息，这样生产者才会发送下一条数据</span><br><span class="line">acks：all，表示需要所有Leader+副本节点回复收到消息（acks&#x3D;-1），这样生产者才会发送下一条数据</span><br><span class="line">acks：0，表示不需要任何节点回复，生产者会继续发送下一条数据</span><br><span class="line">再来看一下这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152350669.png" alt="image-20230415235025189"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">我们在向hello这个topic生产数据的时候，可以在生产者中设置acks参数，</span><br><span class="line">acks设置为1，表示我们在向hello这个topic的partition1这个分区写数据的时候，只需要让leader所在的broker1这个节点回复确认收到的消息就可以了，这样生产者就可以发送下一条数据了</span><br><span class="line"></span><br><span class="line">如果acks设置为all，则需要partition1的这两个副本所在的节点(包含Leader)都回复收到消息，生产者才会发送下一条数据</span><br><span class="line"></span><br><span class="line">如果acks设置为0，表示生产者不会等待任何partition所在节点的回复，它只管发送数据，不管你有没有收到，所以这种情况丢失数据的概率比较高。</span><br></pre></td></tr></table></figure><h5 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对这块在面试的时候会有一个面试题：Kafka如何保证数据不丢？</span><br><span class="line">其实就是通过acks机制保证的，如果设置acks为all，则可以保证数据不丢，因为此时把数据发送给kafka之后，会等待对应partition所在的所有leader和副本节点都确认收到消息之后才会认为数据发送成功了，所以在这种策略下，只要把数据发送给kafka之后就不会丢了。</span><br><span class="line"></span><br><span class="line">如果acks设置为1，则当我们把数据发送给partition之后，partition的leader节点也确认收到了，但是leader回复完确认消息之后，leader对应的节点就宕机了，副本partition还没来得及将数据同步过去，所以会存在丢失的可能性。</span><br><span class="line">不过如果宕机的是副本partition所在的节点，则数据是不会丢的</span><br><span class="line"></span><br><span class="line">如果acks设置为0的话就表示是顺其自然了，只管发送，不管kafka有没有收到，这种情况表示对数据丢不丢都无所谓了。</span><br></pre></td></tr></table></figure><h3 id="Consumer扩展"><a href="#Consumer扩展" class="headerlink" title="Consumer扩展"></a>Consumer扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">在消费者中还有一个消费者组的概念</span><br><span class="line">每个consumer属于一个消费者组，通过group.id指定消费者组</span><br><span class="line"></span><br><span class="line">那组内消费和组间消费有什么区别吗？</span><br><span class="line">组内：消费者组内的所有消费者消费同一份数据；</span><br><span class="line"></span><br><span class="line">注意：在同一个消费者组中，一个partition同时只能有一个消费者消费数据</span><br><span class="line">如果消费者的个数小于分区的个数，一个消费者会消费多个分区的数据。</span><br><span class="line">如果消费者的个数大于分区的个数，则多余的消费者不消费数据</span><br><span class="line">所以，对于一个topic,同一个消费者组中推荐不能有多于分区个数的消费者,否则将意味着某些消费者将无法获得消息。</span><br><span class="line"></span><br><span class="line">组间：多个消费者组消费相同的数据，互不影响。</span><br><span class="line">来看下面这个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160000840.png" alt="image-20230416000001364"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Kafka集群有两个节点，Broker1和Broker2</span><br><span class="line">集群内有一个topic，这个topic有4个分区，P0,P1,P2,P3</span><br><span class="line"></span><br><span class="line">下面有两个消费者组</span><br><span class="line">Consumer Group A和Consumer Group B</span><br><span class="line">其中Consumer Group A中有两个消费者C1和C2，由于这个topic有4个分区，所以，C1负责消费两个分区的数据，C2负责消费两个分区的数据，这个属于组内消费</span><br><span class="line">Consumer Group B有5个消费者，C3~C7，其中C3,C4,C5,C6分别消费一个分区的数据，而C7就是多余出来的了，因为现在这个消费者组内的消费者的数量比对应的topic的分区数量还多，但是一个分区同时只能被一个消费者消费，所以就会有一个消费者处于空闲状态。这个也属于组内消费</span><br><span class="line">Consumer Group A和Consumer Group B这两个消费者组属于组间消费，互不影响。</span><br></pre></td></tr></table></figure><h3 id="Topic、Partition扩展"><a href="#Topic、Partition扩展" class="headerlink" title="Topic、Partition扩展"></a>Topic、Partition扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">每个partition在存储层面是append log文件。</span><br><span class="line">新消息都会被直接追加到log文件的尾部，每条消息在log文件中的位置称为offset(偏移量)。</span><br><span class="line">越多partitions可以容纳更多的consumer,有效提升并发消费的能力。</span><br><span class="line"></span><br><span class="line">具体什么时候增加topic的数量？什么时候增加partition的数量呢？</span><br><span class="line"></span><br><span class="line">业务类型增加需要增加topic、数据量大需要增加partition</span><br></pre></td></tr></table></figure><h3 id="Message扩展"><a href="#Message扩展" class="headerlink" title="Message扩展"></a>Message扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每条Message包含了以下三个属性：</span><br><span class="line">1. offset对应类型：long，表示此消息在一个partition中的起始的位置。可以认为offset是partition中Message的id，自增的</span><br><span class="line">2. MessageSize 对应类型：int32 此消息的字节大小。</span><br><span class="line">3. data，类型为bytes,是message的具体内容。</span><br><span class="line">看这个图，加深对Topic、Partition、Message的理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160016235.png" alt="image-20230416001618719"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里也体现了kafka高吞吐量的原因：磁盘顺序读写由于内存随机访问</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2%5C202304160022785.png" alt></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BTableAPI%E5%92%8CSQL-5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BTableAPI%E5%92%8CSQL-5.html</id>
    <published>2023-04-08T15:22:12.000Z</published>
    <updated>2023-04-20T08:46:55.682Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-Flink核心API之TableAPI和SQL-5"><a href="#第十六周-Flink极速上手篇-Flink核心API之TableAPI和SQL-5" class="headerlink" title="第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5"></a>第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5</h1><h2 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意:Table API和SQL现在还处于活跃开发阶段，还没有完全实现Flink中所有的特性。不是所有的[Table API，SQL]和[流，批]的组合都是支持的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL的由来：</span><br><span class="line">Flink针对标准的流处理和批处理提供了两种关系型API，Table API和SQL。Table API允许用户以一种很直观的方式进行select、filter和join操作。Flink SQL基于Apache Calcite实现标准SQL。针对批处理和流处理可以提供相同的处理语义和结果。</span><br><span class="line">Flink Table API、SQL和Flink的DataStream API、DataSet API是紧密联系在一起的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL是一种关系型API，用户可以像操作Mysql数据库表一样的操作数据，而不需要写代码，更不需要手工的对代码进行调优。另外，SQL作为一个非程序员可操作的语言，学习成本很低，如果一个系统提供 SQL支持，将很容易被用户接受。</span><br><span class="line"></span><br><span class="line">如果你想要使用Table API和SQL的话，需要添加下面的依赖</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-api-scala-bridge_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如果你想在 本地 IDE中运行程序，还需要添加下面的依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-planner-blink_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如果你用到了老的执行引擎，还需要添加下面这个依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-planner_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注意：由于部分table相关的代码是用Scala实现的，所以，这个依赖也是必须的。【这个依赖我们在前面开发DataStream程序的时候已经添加过了】</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL通过join API集成在一起，这个join API的核心概念是Table，Table可以作为查询的输入和输出。</span><br><span class="line">针对Table API和SQL我们主要讲解以下内容</span><br><span class="line">1：Table API和SQL的使用</span><br><span class="line">2：DataStream、DataSet和Table之间的互相转换</span><br></pre></td></tr></table></figure><h2 id="Table-API-和SQL的使用"><a href="#Table-API-和SQL的使用" class="headerlink" title="Table API 和SQL的使用"></a>Table API 和SQL的使用</h2><h3 id="创建TableEnvironment对象"><a href="#创建TableEnvironment对象" class="headerlink" title="创建TableEnvironment对象"></a>创建TableEnvironment对象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">想要使用Table API和SQL，首先要创建一个TableEnvironment对象。</span><br><span class="line">下面我们来创建一个TableEnvironment对象</span><br></pre></td></tr></table></figure><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.&#123;<span class="type">BatchTableEnvironment</span>, <span class="type">Stream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">EnvironmentSettings</span>, <span class="type">TableEnvironment</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建TableEnvironment对象</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateTableEnvironmentScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL不需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 则针对stream和batch都可以使用TableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定底层使用Blink引擎，以及数据处理模式-stream</span></span><br><span class="line">         <span class="comment">//从1.11版本开始，Blink引擎成为Table API和SQL的默认执行引擎，在生产环境下面，推</span></span><br><span class="line">         <span class="keyword">val</span> sSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build()</span><br><span class="line">         <span class="comment">//创建TableEnvironment对象</span></span><br><span class="line">         <span class="keyword">val</span> sTableEnv = <span class="type">TableEnvironment</span>.create(sSettings)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定底层使用Blink引擎，以及数据处理模式-batch</span></span><br><span class="line">         <span class="keyword">val</span> bSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inBatchMode().build()</span><br><span class="line">         <span class="comment">//创建TableEnvironment对象</span></span><br><span class="line">         <span class="keyword">val</span> bTableEnv = <span class="type">TableEnvironment</span>.create(bSettings)</span><br><span class="line">         </span><br><span class="line">         </span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 针对stream需要使用StreamTableEnvironment</span></span><br><span class="line"><span class="comment">         * 针对batch需要使用BatchTableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="comment">//创建StreamTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> ssEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> ssSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build()</span><br><span class="line">         <span class="keyword">val</span> ssTableEnv = <span class="type">StreamTableEnvironment</span>.create(ssEnv, ssSettings)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//创建BatchTableEnvironment</span></span><br><span class="line">         <span class="comment">//注意：此时只能使用旧的执行引擎，新的Blink执行引擎不支持和DataSet转换</span></span><br><span class="line">         <span class="keyword">val</span> bbEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> bbTableEnv = <span class="type">BatchTableEnvironment</span>.create(bbEnv)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建TableEnvironment对象</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CreateTableEnvironmentJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL不需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 则针对stream和batch都可以使用TableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="comment">//创建TableEnvironment对象-stream</span></span><br><span class="line">         EnvironmentSettings sSettings = EnvironmentSettings.newInstance().use</span><br><span class="line">         TableEnvironment sTableEnv = TableEnvironment.create(sSettings);</span><br><span class="line">         <span class="comment">//创建TableEnvironment对象-batch</span></span><br><span class="line">         EnvironmentSettings bSettings = EnvironmentSettings.newInstance().use</span><br><span class="line">         TableEnvironment bTableEnv = TableEnvironment.create(bSettings);</span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 针对stream需要使用StreamTableEnvironment</span></span><br><span class="line"><span class="comment">         * 针对batch需要使用BatchTableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="comment">//创建StreamTableEnvironment</span></span><br><span class="line">         StreamExecutionEnvironment ssEnv = StreamExecutionEnvironment.getExec</span><br><span class="line">         EnvironmentSettings ssSettings = EnvironmentSettings.newInstance().us</span><br><span class="line">         StreamTableEnvironment ssTableEnv = StreamTableEnvironment.create(ssE</span><br><span class="line">         <span class="comment">//创建BatchTableEnvironment</span></span><br><span class="line">         <span class="comment">//注意：此时只能使用旧的执行引擎，新的Blink执行引擎不支持和DataSet转换</span></span><br><span class="line">         ExecutionEnvironment bbEnv = ExecutionEnvironment.getExecutionEnviron</span><br><span class="line">         BatchTableEnvironment bbTableEnv = BatchTableEnvironment.create(bbEnv</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Table-API和-SQL的使用"><a href="#Table-API和-SQL的使用" class="headerlink" title="Table API和 SQL的使用"></a>Table API和 SQL的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来演示一下Table API和SQL的使用</span><br><span class="line">目前创建Table的很多方法都过时了，都不推荐使用了，例如：registerTableSource、connect等方法</span><br><span class="line">目前官方推荐使用executeSql的方式，executeSql里面支持</span><br><span class="line">1DDL&#x2F;DML&#x2F;DQL&#x2F;SHOW&#x2F;DESCRIBE&#x2F;EXPLAIN&#x2F;USE等语法</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121052838.png" alt="image-20230412105247826"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">EnvironmentSettings</span>, <span class="type">TableEnvironment</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TableAPI 和 SQL的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableAPIAndSQLOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取TableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> sSettings = <span class="type">EnvironmentSettings</span>.newInstance.useBlinkPlanner.inStreamingMode().build()</span><br><span class="line">         <span class="keyword">val</span> sTableEnv = <span class="type">TableEnvironment</span>.create(sSettings)</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * connector.type：指定connector的类型</span></span><br><span class="line"><span class="comment">         * connector.path：指定文件或者目录地址</span></span><br><span class="line"><span class="comment">         * format.type：文件数据格式化类型</span></span><br><span class="line"><span class="comment">         * 注意：SQL语句如果出现了换行，行的末尾可以添加空格或者\n都可以，最后一行不用添</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         sTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//使用Table API实现数据查询和过滤等操作</span></span><br><span class="line">         <span class="comment">/*import org.apache.flink.table.api._</span></span><br><span class="line"><span class="comment">         val result = sTableEnv.from("myTable")</span></span><br><span class="line"><span class="comment">         .select($"id",$"name")</span></span><br><span class="line"><span class="comment">         .filter($"id" &gt; 1)*/</span></span><br><span class="line">        </span><br><span class="line">         <span class="comment">//使用SQL实现数据查询和过滤等操作</span></span><br><span class="line">         <span class="keyword">val</span> result = sTableEnv.sqlQuery(<span class="string">"select id,name from myTable where id &gt; 1"</span> )</span><br><span class="line">         <span class="comment">//输出结果到控制台</span></span><br><span class="line">         result.execute.print()</span><br><span class="line">          <span class="comment">//创建输出表</span></span><br><span class="line">         sTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table newTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\res',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">         <span class="comment">//输出结果到表newTable中</span></span><br><span class="line">         result.executeInsert(<span class="string">"newTable"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121053478.png" alt="image-20230412105345232"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">注意：针对SQL建表语句的写法还有一种比较清晰的写法</span><br><span class="line"></span><br><span class="line">sTableEnv.executeSql(</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line"> |create table myTable(</span><br><span class="line"> |id int,</span><br><span class="line"> |name string</span><br><span class="line"> |) with (</span><br><span class="line"> |&#39;connector.type&#39; &#x3D; &#39;filesystem&#39;,</span><br><span class="line"> |&#39;connector.path&#39; &#x3D; &#39;D:\data\source&#39;,</span><br><span class="line"> |&#39;format.type&#39; &#x3D; &#39;csv&#39;</span><br><span class="line"> |)</span><br><span class="line"> |&quot;&quot;&quot;.stripMargin)</span><br></pre></td></tr></table></figure><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TableAPI 和 SQL的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableAPIAndSQLOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//获取TableEnvironment</span></span><br><span class="line">         EnvironmentSettings sSettings = EnvironmentSettings.newInstance().use</span><br><span class="line">         TableEnvironment sTableEnv = TableEnvironment.create(sSettings);</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         sTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>);</span><br><span class="line">         <span class="comment">//使用Table API实现数据查询和过滤等操作</span></span><br><span class="line">         <span class="comment">/*Table result = sTableEnv.from("myTable")</span></span><br><span class="line"><span class="comment">         .select($("id"), $("name"))</span></span><br><span class="line"><span class="comment">         .filter($("id").isGreater(1));*/</span></span><br><span class="line">             <span class="comment">//使用SQL实现数据查询和过滤等操作</span></span><br><span class="line">         Table result = sTableEnv.sqlQuery(<span class="string">"select id,name from myTable where </span></span><br><span class="line"><span class="string">         //输出结果到控制台</span></span><br><span class="line"><span class="string">         result.execute().print();</span></span><br><span class="line"><span class="string">         //创建输出表</span></span><br><span class="line"><span class="string">         sTableEnv.executeSql("</span><span class="string">" +</span></span><br><span class="line"><span class="string">         "</span><span class="function">create table <span class="title">newTable</span><span class="params">(\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>id <span class="keyword">int</span>,\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>name string\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>)</span> <span class="title">with</span> <span class="params">(\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span><span class="string">'connector.type'</span> = <span class="string">'filesystem'</span>,\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span><span class="string">'connector.path'</span> = <span class="string">'D:\\data\\res'</span>,\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span><span class="string">'format.type'</span> = <span class="string">'csv'</span>\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>)</span>")</span>;</span><br><span class="line">         <span class="comment">//输出结果到表newTable中</span></span><br><span class="line">         result.executeInsert(<span class="string">"newTable"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DataStream、DataSet和Table之间的互相转换"><a href="#DataStream、DataSet和Table之间的互相转换" class="headerlink" title="DataStream、DataSet和Table之间的互相转换"></a>DataStream、DataSet和Table之间的互相转换</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL可以很容易的和DataStream和DataSet程序集成到一块。通过TableEnvironment，可以把DataStream或者DataSet注册为Table，这样就可以使用Table API和SQL查询了。通过</span><br><span class="line">TableEnvironment也可以把Table对象转换为DataStream或者DataSet，这样就可以使用DataStream或者DataSet中的相关API了。</span><br></pre></td></tr></table></figure><h3 id="使用DataStream创建表"><a href="#使用DataStream创建表" class="headerlink" title="使用DataStream创建表"></a>使用DataStream创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1：使用DataStream创建表，主要包含下面这两种情况</span><br><span class="line">使用DataStream创建view视图</span><br><span class="line">使用DataStream创建table对象</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataStream转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataStreamToTableScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> ssEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> ssSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build()</span><br><span class="line">         <span class="keyword">val</span> ssTableEnv = <span class="type">StreamTableEnvironment</span>.create(ssEnv, ssSettings)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//获取DataStream</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> stream = ssEnv.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mack"</span>)))</span><br><span class="line">         <span class="comment">//第一种：将DataStream转换为view视图</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line">   ssTableEnv.createTemporaryView(<span class="string">"myTable"</span>,stream,<span class="symbol">'id</span>,<span class="symbol">'name</span>)</span><br><span class="line">         ssTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().print()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//第二种：将DataStream转换为table对象</span></span><br><span class="line">         <span class="keyword">val</span> table = ssTableEnv.fromDataStream(stream, $<span class="string">"id"</span>, $<span class="string">"name"</span>)</span><br><span class="line">         table.select($<span class="string">"id"</span>,$<span class="string">"name"</span>)</span><br><span class="line">         .filter($<span class="string">"id"</span> &gt; <span class="number">1</span>)</span><br><span class="line">         .execute()</span><br><span class="line">         .print()</span><br><span class="line">         <span class="comment">//注意：'id,'name 和 $"id", $"name" 这两种写法是一样的效果</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataStream转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataStreamToTableJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         StreamExecutionEnvironment ssEnv = StreamExecutionEnvironment.getExec</span><br><span class="line">         EnvironmentSettings ssSettings = EnvironmentSettings.newInstance().us</span><br><span class="line">         StreamTableEnvironment ssTableEnv = StreamTableEnvironment.create(ssE</span><br><span class="line">         <span class="comment">//获取DataStream</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataStreamSource&lt;Tuple2&lt;Integer, String&gt;&gt; stream = ssEnv.fromCollecti</span><br><span class="line">         <span class="comment">//第一种：将DataStream转换为view视图</span></span><br><span class="line">         ssTableEnv.createTemporaryView(<span class="string">"myTable"</span>,stream,$(<span class="string">"id"</span>),$(<span class="string">"name"</span>));</span><br><span class="line">         ssTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().p</span><br><span class="line">         <span class="comment">//第二种：将DataStream转换为table对象</span></span><br><span class="line">         Table table = ssTableEnv.fromDataStream(stream, $(<span class="string">"id"</span>), $(<span class="string">"name"</span>));</span><br><span class="line">         table.select($(<span class="string">"id"</span>), $(<span class="string">"name"</span>))</span><br><span class="line">         .filter($(<span class="string">"id"</span>).isGreater(<span class="number">1</span>))</span><br><span class="line">         .execute()</span><br><span class="line">         .print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用DataSet创建表"><a href="#使用DataSet创建表" class="headerlink" title="使用DataSet创建表"></a>使用DataSet创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：此时只能使用旧的执行引擎，新的Blink执行引擎不支持和DataSet转换</span><br></pre></td></tr></table></figure><h4 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.&#123;<span class="type">BatchTableEnvironment</span>, <span class="type">Stream</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataSet转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSetToTableScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> bbEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> bbTableEnv = <span class="type">BatchTableEnvironment</span>.create(bbEnv)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//获取DataSet</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> set = bbEnv.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mack"</span>)))</span><br><span class="line">         <span class="comment">//第一种：将DataSet转换为view视图</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line">         bbTableEnv.createTemporaryView(<span class="string">"myTable"</span>,set,<span class="symbol">'id</span>,<span class="symbol">'name</span>)</span><br><span class="line">         bbTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().print</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//第二种：将DataSet转换为table对象</span></span><br><span class="line">         <span class="keyword">val</span> table = bbTableEnv.fromDataSet(set, $<span class="string">"id"</span>, $<span class="string">"name"</span>)</span><br><span class="line">         table.select($<span class="string">"id"</span>,$<span class="string">"name"</span>)</span><br><span class="line">         .filter($<span class="string">"id"</span> &gt; <span class="number">1</span>)</span><br><span class="line">         .execute()</span><br><span class="line">         .print()</span><br><span class="line">         <span class="comment">//注意：'id,'name 和 $"id", $"name" 这两种写法是一样的效果</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataSet转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataSetToTableJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         ExecutionEnvironment bbEnv = ExecutionEnvironment.getExecutionEnviron</span><br><span class="line">         BatchTableEnvironment bbTableEnv = BatchTableEnvironment.create(bbEnv</span><br><span class="line">         <span class="comment">//获取DataSet</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; set = bbEnv.fromCollection(data);</span><br><span class="line">         <span class="comment">//第一种：将DataSet转换为view视图</span></span><br><span class="line">         bbTableEnv.createTemporaryView(<span class="string">"myTable"</span>,set,$(<span class="string">"id"</span>),$(<span class="string">"name"</span>));</span><br><span class="line">         bbTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().p</span><br><span class="line">         <span class="comment">//第二种：将DataSet转换为table对象</span></span><br><span class="line">         Table table = bbTableEnv.fromDataSet(set, $(<span class="string">"id"</span>), $(<span class="string">"name"</span>));</span><br><span class="line">         table.select($(<span class="string">"id"</span>), $(<span class="string">"name"</span>))</span><br><span class="line">         .filter($(<span class="string">"id"</span>).isGreater(<span class="number">1</span>))</span><br><span class="line">         .execute()</span><br><span class="line">         .print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">将Table转换为DataStream或者DataSet时，你需要指定生成的 DataStream或者DataSet的数据类型，即，Table 的每行数据要转换成的数据类型。通常最方便的选择是转换成Row。以下列表概述了不同选项的功能：</span><br><span class="line">Row: 通过角标映射字段，支持任意数量的字段，支持null值，无类型安全（type-safe）检查。</span><br><span class="line">POJO: Java中的实体类，这个实体类中的字段名称需要和Table中的字段名称保持一致，支持任意数量的字段，支持null值，有类型安全检查。</span><br><span class="line">Case Class: 通过角标映射字段，不支持null值，有类型安全检查。</span><br><span class="line">Tuple: 通过角标映射字段，Scala中限制22个字段，Java中限制25个字段，不支持null值，有类型安全检查。</span><br><span class="line">Atomic Type: Table必须有一个字段，不支持null值，有类型安全检查。</span><br></pre></td></tr></table></figure><h3 id="将Table转换为DataStream"><a href="#将Table转换为DataStream" class="headerlink" title="将Table转换为DataStream"></a>将Table转换为DataStream</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3：将表转换成DataStream流式查询的结果Table会被动态地更新，即每个新的记录到达输入流时结果就会发生变化。因此，转换此动态查询的DataStream需要对表的更新进行编码。</span><br><span class="line"></span><br><span class="line">有几种模式可以将Table转换为DataStream。</span><br><span class="line">Append Mode:这种模式只适用于当动态表仅由INSERT更改修改时(仅附加)，之前添加的数据不会被更新。</span><br><span class="line">Retract Mode:可以始终使用此模式，它使用一个Boolean标识来编码INSERT和DELETE更改。</span><br></pre></td></tr></table></figure><h4 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将table转换成 DataStream</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableToDataStreamScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> ssEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> ssSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inSt</span><br><span class="line">         <span class="keyword">val</span> ssTableEnv = <span class="type">StreamTableEnvironment</span>.create(ssEnv, ssSettings)</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         ssTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         <span class="keyword">val</span> table = ssTableEnv.from(<span class="string">"myTable"</span>)</span><br><span class="line">         <span class="comment">//将table转换为DataStream</span></span><br><span class="line">         <span class="comment">//如果只有新增(追加)操作，可以使用toAppendStream</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> appStream = ssTableEnv.toAppendStream[<span class="type">Row</span>](table)</span><br><span class="line">         appStream.map(row=&gt;(row.getField(<span class="number">0</span>).toString.toInt,row.getField(<span class="number">1</span>).toString))</span><br><span class="line">         .print()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//如果有增加操作，还有删除操作，则使用toRetractStream</span></span><br><span class="line">         <span class="keyword">val</span> retStream = ssTableEnv.toRetractStream[<span class="type">Row</span>](table)</span><br><span class="line">         retStream.map(tup=&gt;&#123;</span><br><span class="line">         <span class="keyword">val</span> flag = tup._1</span><br><span class="line">         <span class="keyword">val</span> row = tup._2</span><br><span class="line">         <span class="keyword">val</span> id = row.getField(<span class="number">0</span>).toString.toInt</span><br><span class="line">         <span class="keyword">val</span> name = row.getField(<span class="number">1</span>).toString</span><br><span class="line">         (flag,id,name)</span><br><span class="line">         &#125;).print()</span><br><span class="line">         <span class="comment">//注意：将table对象转换为DataStream之后，就需要调用StreamExecutionEnvironment</span></span><br><span class="line">         ssEnv.execute(<span class="string">"TableToDataStreamScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121150371.png" alt="image-20230412115003343"></p><h4 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将table转换成 DataStream</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableToDataStreamJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         StreamExecutionEnvironment ssEnv = StreamExecutionEnvironment.getExec</span><br><span class="line">         EnvironmentSettings ssSettings = EnvironmentSettings.newInstance().us</span><br><span class="line">         StreamTableEnvironment ssTableEnv = StreamTableEnvironment.create(ssE</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         ssTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>);</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         Table table = ssTableEnv.from(<span class="string">"myTable"</span>);</span><br><span class="line">         <span class="comment">//将table转换为DataStream</span></span><br><span class="line">         <span class="comment">//如果只有新增(追加)操作，可以使用toAppendStream</span></span><br><span class="line">                                                                 DataStream&lt;Row&gt; appStream = ssTableEnv.toAppendStream(table, Row.clas</span><br><span class="line">         appStream.map(<span class="keyword">new</span> MapFunction&lt;Row, Tuple2&lt;Integer,String&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, String&gt; <span class="title">map</span><span class="params">(Row row)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">int</span> id = Integer.parseInt(row.getField(<span class="number">0</span>).toString());</span><br><span class="line">         String name = row.getField(<span class="number">1</span>).toString();</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, String&gt;(id, name);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         <span class="comment">//如果有增加操作，还有删除操作，则使用toRetractStream</span></span><br><span class="line">         DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retStream = ssTableEnv.toRetractStre</span><br><span class="line">         retStream.map(<span class="keyword">new</span> MapFunction&lt;Tuple2&lt;Boolean, Row&gt;, Tuple3&lt;Boolean,Int</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Boolean, Integer, String&gt; <span class="title">map</span><span class="params">(Tuple2&lt;Boolean, Row&gt; t</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         Boolean flag = tup.f0;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">int</span> id = Integer.parseInt(tup.f1.getField(<span class="number">0</span>)</span>.<span class="title">toString</span><span class="params">()</span>)</span>;</span><br><span class="line">         String name = tup.f1.getField(<span class="number">1</span>).toString();</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Boolean, Integer, String&gt;(flag, id, name);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         ssEnv.execute(<span class="string">"TableToDataStreamJava"</span>);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="将表转换成DataSet"><a href="#将表转换成DataSet" class="headerlink" title="将表转换成DataSet"></a>将表转换成DataSet</h3><h4 id="scala-5"><a href="#scala-5" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">BatchTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将table转换成 DataSet</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableToDataSetScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> bbEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> bbTableEnv = <span class="type">BatchTableEnvironment</span>.create(bbEnv)</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         bbTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         <span class="keyword">val</span> table = bbTableEnv.from(<span class="string">"myTable"</span>)</span><br><span class="line">         <span class="comment">//将table转换为DataSet</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> set = bbTableEnv.toDataSet[<span class="type">Row</span>](table)</span><br><span class="line">         set.map(row=&gt;(row.getField(<span class="number">0</span>).toString.toInt,row.getField(<span class="number">1</span>).toString))</span><br><span class="line">         .print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121153273.png" alt="image-20230412115320971"></p><h4 id="java-5"><a href="#java-5" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 将table转换成 DataSet</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableToDataSetJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         ExecutionEnvironment bbEnv = ExecutionEnvironment.getExecutionEnviron</span><br><span class="line">         BatchTableEnvironment bbTableEnv = BatchTableEnvironment.create(bbEnv</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         bbTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>);</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         Table table = bbTableEnv.from(<span class="string">"myTable"</span>);</span><br><span class="line">         <span class="comment">//将table转换为DataSet</span></span><br><span class="line">         DataSet&lt;Row&gt; set = bbTableEnv.toDataSet(table, Row<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">         set.map(<span class="keyword">new</span> MapFunction&lt;Row, Tuple2&lt;Integer,String&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, String&gt; <span class="title">map</span><span class="params">(Row row)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">int</span> id = Integer.parseInt(row.getField(<span class="number">0</span>).toString());</span><br><span class="line">         String name = row.getField(<span class="number">1</span>).toString();</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, String&gt;(id, name);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataSetAPI-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataSetAPI-4.html</id>
    <published>2023-04-08T15:21:40.000Z</published>
    <updated>2023-04-20T06:52:42.408Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-Flink核心API之DataSetAPI-4"><a href="#第十六周-Flink极速上手篇-Flink核心API之DataSetAPI-4" class="headerlink" title="第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4"></a>第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4</h1><h2 id="DataSet-API"><a href="#DataSet-API" class="headerlink" title="DataSet API"></a>DataSet API</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataSet API主要可以分为3块来分析：DataSource、Transformation、Sink。</span><br><span class="line"></span><br><span class="line">DataSource是程序的数据源输入。</span><br><span class="line">Transformation是具体的操作，它对一个或多个输入数据源进行计算处理，例如map、flatMap、filter等操作。</span><br><span class="line"></span><br><span class="line">DataSink是程序的输出，它可以把Transformation处理之后的数据输出到指定的存储介质中。</span><br></pre></td></tr></table></figure><h3 id="DataSet-API之DataSource"><a href="#DataSet-API之DataSource" class="headerlink" title="DataSet API之DataSource"></a>DataSet API之DataSource</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对DataSet批处理而言，其实最多的就是读取HDFS中的文件数据，所以在这里我们主要介绍两个DataSource组件。</span><br><span class="line"></span><br><span class="line">基于集合</span><br><span class="line">fromCollection(Collection)，主要是为了方便测试使用。它的用法和DataStreamAPI中的用法一样，我们已经用过很多次了。</span><br><span class="line"></span><br><span class="line">基于文件</span><br><span class="line">readTextFile(path)，读取hdfs中的数据文件。这个前面我们也使用过了。</span><br></pre></td></tr></table></figure><h3 id="DataSet-API之Transformation"><a href="#DataSet-API之Transformation" class="headerlink" title="DataSet API之Transformation"></a>DataSet API之Transformation</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">算子 解释</span><br><span class="line">map 输入一个元素进行处理，返回一个元素</span><br><span class="line">mapPartition 类似map，一次处理一个分区的数据</span><br><span class="line">flatMap 输入一个元素进行处理，可以返回多个元素</span><br><span class="line">filter 对数据进行过滤，符合条件的数据会被留下</span><br><span class="line">reduce 对当前元素和上一次的结果进行聚合操作</span><br><span class="line">aggregate sum(),min(),max()等</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里面的算子我们都是比较熟悉的，在前面DatatreamAPI中都用过，用法都是一样的，所以在这就不再演示了</span><br><span class="line">mapPartition这个算子我们在Flink中还没用过，不过在Spark中是用过的，用法也是一样的</span><br><span class="line"></span><br><span class="line">其实mapPartition就是一次处理一批数据，如果在处理数据的时候想要获取第三方资源连接，建议使用mapPartition，这样可以一批数据获取一次连接，提高性能。</span><br><span class="line">下面来演示一下Flink中mapPartition的使用</span><br></pre></td></tr></table></figure><h4 id="mapPartition"><a href="#mapPartition" class="headerlink" title="mapPartition"></a>mapPartition</h4><h5 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MapPartition的使用：一次处理一个分区的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchMapPartitionScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//生成数据源数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="string">"hello you"</span>, <span class="string">"hello me"</span>))</span><br><span class="line">         <span class="comment">//每次处理一个分区的数据</span></span><br><span class="line">         text.mapPartition(it=&gt;&#123;</span><br><span class="line">         <span class="comment">//可以在此处创建数据库连接，建议把这块代码放到try-catch代码块中</span></span><br><span class="line">         <span class="comment">//注意：此时是每个分区获取一个数据库连接，不需要每处理一条数据就获取一次连接，</span></span><br><span class="line">         <span class="keyword">val</span> res = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">         it.foreach(line=&gt;&#123;</span><br><span class="line">         <span class="keyword">val</span> words = line.split(<span class="string">" "</span>)</span><br><span class="line">         <span class="keyword">for</span>(word &lt;- words)&#123;</span><br><span class="line">         res.append(word)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         res</span><br><span class="line">         <span class="comment">//关闭数据库连接</span></span><br><span class="line">         &#125;).print()</span><br><span class="line">         <span class="comment">//No new data sinks have been defined since the last execution.</span></span><br><span class="line">         <span class="comment">//The last execution refers to the latest call to 'execute()', 'count()', </span></span><br><span class="line">         <span class="comment">//注意：针对DataSetAPI，如果在后面调用的是count、collect、print，则最后不需要指定execute即可</span></span><br><span class="line">         <span class="comment">//env.execute("BatchMapPartitionScala")</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapPartitionFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MapPartition的使用：一次处理一个分区的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchMapPartitionJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//生成数据源数据</span></span><br><span class="line">         DataSource&lt;String&gt; text = env.fromCollection(Arrays.asList(<span class="string">"hello you"</span>,<span class="string">"hello me"</span>));</span><br><span class="line">         <span class="comment">//每次处理一个分区的数据</span></span><br><span class="line">         text.mapPartition(<span class="keyword">new</span> MapPartitionFunction&lt;String, String&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mapPartition</span><span class="params">(Iterable&lt;String&gt; iterable, Collector&lt;String&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">//可以在此处创建数据库连接，建议把这块代码放到try-catch代码块中</span></span><br><span class="line">             Iterator&lt;String&gt; it = iterable.iterator();</span><br><span class="line">         <span class="keyword">while</span>(it.hasNext())&#123;</span><br><span class="line">             String line = it.next();</span><br><span class="line">             String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">             <span class="keyword">for</span>(String word: words)&#123;</span><br><span class="line">             out.collect(word);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//关闭数据库连接</span></span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">下面还有一些transformation算子</span><br><span class="line">算子 解释</span><br><span class="line">distinct 返回数据集中去重之后的元素</span><br><span class="line">join 内连接</span><br><span class="line">outerJoin 外连接</span><br><span class="line">cross 获取两个数据集的笛卡尔积</span><br><span class="line">union 返回多个数据集的总和，数据类型需要一致</span><br><span class="line">first-n 获取集合中的前N个元素</span><br><span class="line">distinct算子比较简单，就是对数据进行全局去重。</span><br><span class="line">join：内连接，可以连接两份数据集</span><br></pre></td></tr></table></figure><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><h5 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * join：内连接</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchJoinScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mick"</span>)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"bj"</span>), (<span class="number">2</span>, <span class="string">"sh"</span>), (<span class="number">4</span>, <span class="string">"gz"</span>)))</span><br><span class="line">         <span class="comment">//对两份数据集执行join操作</span></span><br><span class="line">         text1.join(text2)</span><br><span class="line">         <span class="comment">//注意：这里的where和equalTo实现了类似于on fieldA=fieldB的效果</span></span><br><span class="line">         <span class="comment">//where：指定左边数据集中参与比较的元素角标</span></span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         <span class="comment">//equalTo指定右边数据集中参与比较的元素角标</span></span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;(first,second)=&gt;&#123;</span><br><span class="line">             (first._1,first._2,second._2)</span><br><span class="line">         &#125;&#125;.print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101353385.png" alt="image-20230410135332014"></p><h5 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.JoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * join：内连接</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchJoinJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data1 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text1 = env.fromCollection(data1)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data2 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"bj"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"sh"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">4</span>,<span class="string">"gz"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text2 = env.fromCollection(data2)</span><br><span class="line">         <span class="comment">//对两份数据集执行join操作</span></span><br><span class="line">         text1.join(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         <span class="comment">//三个输入参数：</span></span><br><span class="line">         <span class="comment">//第一个tuple2是左边数据集的类型，</span></span><br><span class="line">         <span class="comment">//第二个tuple2是右边数据集的类型，</span></span><br><span class="line">         <span class="comment">//第三个tuple3是此函数返回的数据集类型</span></span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Integer,String&gt;,Tuple3&lt;Integer,String,String&gt;&gt;()</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Integer,String&gt; secondfirst,Tuple2&lt;Integer,String</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         return new Tuple3&lt;Integer, String, String&gt;(first.f0,f</span></span></span><br><span class="line"><span class="function"><span class="params">         &#125;</span></span></span><br><span class="line"><span class="function"><span class="params">         &#125;)</span>.<span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outerJoin：外连接</span><br></pre></td></tr></table></figure><h4 id="outerJoin"><a href="#outerJoin" class="headerlink" title="outerJoin"></a>outerJoin</h4><h5 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * outerJoin：外连接</span></span><br><span class="line"><span class="comment"> * 一共有三种情况</span></span><br><span class="line"><span class="comment"> * 1：leftOuterJoin</span></span><br><span class="line"><span class="comment"> * 2：rightOuterJoin</span></span><br><span class="line"><span class="comment"> * 3：fullOuterJoin</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchOuterJoinScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mick"</span>)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"bj"</span>), (<span class="number">2</span>, <span class="string">"sh"</span>), (<span class="number">4</span>, <span class="string">"gz"</span>)))</span><br><span class="line">         <span class="comment">//对两份数据集执行leftOuterJoin操作</span></span><br><span class="line">         text1.leftOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;</span><br><span class="line">         (first,second)=&gt;&#123;</span><br><span class="line">         <span class="comment">//注意：second中的元素可能为null</span></span><br><span class="line">         <span class="keyword">if</span>(second==<span class="literal">null</span>)&#123;</span><br><span class="line">         (first._1,first._2,<span class="string">"null"</span>)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         (first._1,first._2,second._2)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;.print()</span><br><span class="line">         println(<span class="string">"========================================"</span>)</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.rightOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;</span><br><span class="line">         (first,second)=&gt;&#123;</span><br><span class="line">         <span class="comment">//注意：first中的元素可能为null</span></span><br><span class="line">         <span class="keyword">if</span>(first==<span class="literal">null</span>)&#123;</span><br><span class="line">             (second._1,<span class="string">"null"</span>,second._2)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         (first._1,first._2,second._2)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;.print()</span><br><span class="line">         println(<span class="string">"========================================"</span>)</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.fullOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;</span><br><span class="line">         (first,second)=&gt;&#123;</span><br><span class="line">         <span class="comment">//注意：first和second中的元素都有可能为null</span></span><br><span class="line">             <span class="keyword">if</span>(first==<span class="literal">null</span>)&#123;</span><br><span class="line">         (second._1,<span class="string">"null"</span>,second._2)</span><br><span class="line">         &#125;<span class="keyword">else</span> <span class="keyword">if</span>(second==<span class="literal">null</span>)&#123;</span><br><span class="line">         (first._1,first._2,<span class="string">"null"</span>)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         (first._1,first._2,second._2)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;.print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101516270.png" alt="image-20230410151623671"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101516094.png" alt="image-20230410151648029"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101517219.png" alt="image-20230410151719016"></p><h5 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.JoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * outerJoin：外连接</span></span><br><span class="line"><span class="comment"> * 一共有三种情况</span></span><br><span class="line"><span class="comment"> * 1：leftOuterJoin</span></span><br><span class="line"><span class="comment"> * 2：rightOuterJoin</span></span><br><span class="line"><span class="comment"> * 3：fullOuterJoin</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchOuterJoinJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironme</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data1 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text1 = env.fromCollection(data1)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data2 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"bj"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"sh"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">4</span>,<span class="string">"gz"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text2 = env.fromCollection(data2)</span><br><span class="line">         <span class="comment">//对两份数据集执行leftOuterJoin操作</span></span><br><span class="line">         text1.leftOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Intege</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Intege</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">if</span>(second==<span class="keyword">null</span>)</span></span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">                                                    &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         System.out.println(<span class="string">"=============================================="</span>);</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.rightOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Intege</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Intege</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">if</span>(first==<span class="keyword">null</span>)</span></span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(second</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         System.out.println(<span class="string">"=============================================="</span>);</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.fullOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Intege</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Intege</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">if</span>(first==<span class="keyword">null</span>)</span></span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(second</span><br><span class="line">         &#125;<span class="keyword">else</span> <span class="keyword">if</span>(second==<span class="keyword">null</span>)&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="cross"><a href="#cross" class="headerlink" title="cross"></a>cross</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross：获取两个数据集的笛卡尔积</span><br></pre></td></tr></table></figure><h5 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * cross：获取两个数据集的笛卡尔积</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchCrossScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化第一份数据</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">         <span class="comment">//初始化第二份数据</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>(<span class="string">"a"</span>, <span class="string">"b"</span>))</span><br><span class="line">         <span class="comment">//执行cross操作</span></span><br><span class="line">             text1.cross(text2).print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101509307.png" alt="image-20230410150858841"></p><h5 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * cross：获取两个数据集的笛卡尔积</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchCrossJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironme</span><br><span class="line">         <span class="comment">//初始化第一份数据</span></span><br><span class="line">         DataSource&lt;Integer&gt; text1 = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>));</span><br><span class="line">         <span class="comment">//初始化第二份数据</span></span><br><span class="line">         DataSource&lt;String&gt; text2 = env.fromCollection(Arrays.asList(<span class="string">"a"</span>, <span class="string">"b"</span>)</span><br><span class="line">         <span class="comment">//执行cross操作</span></span><br><span class="line">         text1.cross(text2).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">union：返回两个数据集的总和，数据类型需要一致</span><br><span class="line">和DataStreamAPI中的union操作功能一样</span><br><span class="line">first-n：获取集合中的前N个元素</span><br></pre></td></tr></table></figure><h4 id="first-n"><a href="#first-n" class="headerlink" title="first-n"></a>first-n</h4><h5 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.<span class="type">Order</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * first-n：获取集合中的前N个元素</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchFirstNScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> data = <span class="type">ListBuffer</span>[<span class="type">Tuple2</span>[<span class="type">Int</span>,<span class="type">String</span>]]()</span><br><span class="line">         data.append((<span class="number">2</span>,<span class="string">"zs"</span>))</span><br><span class="line">         data.append((<span class="number">4</span>,<span class="string">"ls"</span>))</span><br><span class="line">         data.append((<span class="number">3</span>,<span class="string">"ww"</span>))</span><br><span class="line">         data.append((<span class="number">1</span>,<span class="string">"aw"</span>))</span><br><span class="line">         data.append((<span class="number">1</span>,<span class="string">"xw"</span>))</span><br><span class="line">         data.append((<span class="number">1</span>,<span class="string">"mw"</span>))</span><br><span class="line">        <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(data)</span><br><span class="line">         <span class="comment">//获取前3条数据，按照数据插入的顺序</span></span><br><span class="line">         text.first(<span class="number">3</span>).print()</span><br><span class="line">         println(<span class="string">"=================================="</span>)</span><br><span class="line">         <span class="comment">//根据数据中的第一列进行分组，获取每组的前2个元素</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).first(<span class="number">2</span>).print()</span><br><span class="line">         println(<span class="string">"=================================="</span>)</span><br><span class="line">         <span class="comment">//根据数据中的第一列分组，再根据第二列进行组内排序[倒序],获取每组的前2个元素</span></span><br><span class="line">         <span class="comment">//分组排序取TopN</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).sortGroup(<span class="number">1</span>,<span class="type">Order</span>.<span class="type">DESCENDING</span>).first(<span class="number">2</span>).print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.Order;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * first-n：获取集合中的前N个元素</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchFirstNJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironme</span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"zs"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">4</span>,<span class="string">"ls"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"ww"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"aw"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"xw"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"mw"</span>));</span><br><span class="line">         <span class="comment">//初始化数据</span></span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text = env.fromCollection(data);</span><br><span class="line">         <span class="comment">//获取前3条数据，按照数据插入的顺序</span></span><br><span class="line">         text.first(<span class="number">3</span>).print();</span><br><span class="line">         System.out.println(<span class="string">"===================================="</span>);</span><br><span class="line">         <span class="comment">//根据数据中的第一列进行分组，获取每组的前2个元素</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).first(<span class="number">2</span>).print();</span><br><span class="line">         System.out.println(<span class="string">"===================================="</span>);</span><br><span class="line">         <span class="comment">//根据数据中的第一列分组，再根据第二列进行组内排序[倒序],获取每组的前2个元素</span></span><br><span class="line">         <span class="comment">//分组排序取TopN</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).sortGroup(<span class="number">1</span>, Order.DESCENDING).first(<span class="number">2</span>).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="DataSet-API之DataSink"><a href="#DataSet-API之DataSink" class="headerlink" title="DataSet API之DataSink"></a>DataSet API之DataSink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Flink针对DataSet提供了一些已经实现好的数据目的地</span><br><span class="line">其中最常见的是向HDFS中写入数据</span><br><span class="line"></span><br><span class="line">writeAsText()：将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取</span><br><span class="line">writeAsCsv()：将元组以逗号分隔写入文件中，行及字段之间的分隔是可配置的，每个字段的值来自对象的toString()方法</span><br><span class="line"></span><br><span class="line">还有一个是print：打印每个元素的toString()方法的值</span><br><span class="line">这个print是测试的时候使用的。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataStreamAPI-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataStreamAPI-3.html</id>
    <published>2023-04-08T15:20:20.000Z</published>
    <updated>2023-04-20T07:11:33.646Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-Flink核心API之DataStreamAPI-3"><a href="#第十六周-Flink极速上手篇-Flink核心API之DataStreamAPI-3" class="headerlink" title="第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3"></a>第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3</h1><h2 id="Flink核心API"><a href="#Flink核心API" class="headerlink" title="Flink核心API"></a>Flink核心API</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091728382.png" alt="image-20230409172217160"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Flink中提供了4种不同层次的API，每种API在简洁和易表达之间有自己的权衡，适用于不同的场景。目前上面3个会用得比较多。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">低级API(Stateful Stream Processing)：提供了对时间和状态的细粒度控制，简洁性和易用性较差，主要应用在一些复杂事件处理逻辑上。</span><br><span class="line"></span><br><span class="line">核心API(DataStream&#x2F;DataSet API)：主要提供了针对流数据和批数据的处理，是对低级API进行了一些封装，提供了filter、sum、max、min等高级函数，简单易用，所以这些API在工作中应用还是比较广泛的。</span><br><span class="line"></span><br><span class="line">Table API：一般与DataSet或者DataStream紧密关联，可以通过一个DataSet或DataStream创建出一个Table，然后再使用类似于filter, join,或者select这种操作。最后还可以将一个Table对象转成</span><br><span class="line">DataSet或DataStream。</span><br><span class="line"></span><br><span class="line">SQL：Flink的SQL底层是基于Apache Calcite，Apache Calcite实现了标准的SQL，使用起来比其他API更加灵活，因为可以直接使用SQL语句。Table API和SQL可以很容易地结合在一块使用，因为它们都返回Table对象。</span><br><span class="line"></span><br><span class="line">针对这些API我们主要学习下面这些</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091729767.png" alt="image-20230409172651151"></p><h3 id="DataStream-API"><a href="#DataStream-API" class="headerlink" title="DataStream API"></a>DataStream API</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream API主要分为3块：DataSource、Transformation、DataSink。</span><br><span class="line"></span><br><span class="line">DataSource是程序的输入数据源。</span><br><span class="line">Transformation是具体的操作，它对一个或多个输入数据源进行计算处理，例如map、flatMap和filter等操作。</span><br><span class="line"></span><br><span class="line">DataSink是程序的输出，它可以把Transformation处理之后的数据输出到指定的存储介质中。</span><br></pre></td></tr></table></figure><h4 id="DataStream-API之DataSoure"><a href="#DataStream-API之DataSoure" class="headerlink" title="DataStream API之DataSoure"></a>DataStream API之DataSoure</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataSource是程序的输入数据源，Flink提供了大量内置的DataSource，也支持自定义DataSource，不过目前Flink提供的这些已经足够我们正常使用了。</span><br><span class="line">Flink提供的内置输入数据源：包括基于socket、基于Collection</span><br><span class="line">还有就是Flink还提供了一批Connectors，可以实现读取第三方数据源，</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091734753.png" alt="image-20230409173429387"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Flink内置：表示Flink中默认自带的。</span><br><span class="line"></span><br><span class="line">Apache Bahir：表示需要添加这个依赖包之后才能使用的。</span><br><span class="line"></span><br><span class="line">针对source的这些Connector，我们在实际工作中最常用的就是Kafka</span><br><span class="line">当程序出现错误的时候，Flink的容错机制能恢复并继续运行程序，这种错误包括机器故障、网络故障、程序故障等</span><br><span class="line"></span><br><span class="line">针对Flink提供的常用数据源接口，如果程序开启了checkpoint快照机制，Flink可以提供这些容错性保证</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091735114.png" alt="image-20230409173548905"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对这些常用的DataSouce，基于socket的我们之前已经用过了，下面我们来看一下基于Collection集合的。</span><br><span class="line"></span><br><span class="line">针对Kafka的这个我们在后面会详细分析，在这里先不讲。</span><br><span class="line"></span><br><span class="line">由于我们后面还会学到批处理的功能，所以在项目里面创建几个包，把流处理和批处理的代码分开，后期看起来比较清晰。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091750998.png" alt="image-20230409174935218"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来在 com.imooc.scala.stream 里面创建一个包：source，将代码放到source包里面</span><br></pre></td></tr></table></figure><h5 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.source</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于collection的source的使用</span></span><br><span class="line"><span class="comment"> * 注意：这个source的主要应用场景是模拟测试代码流程的时候使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamCollectionSourceScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//使用collection集合生成DataStream</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">         text.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamCollectionSource"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.source;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于collection的source的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamCollectionSourceJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">             StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//使用collection集合生成DataStream</span></span><br><span class="line">         DataStreamSource&lt;Integer&gt; text = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>));</span><br><span class="line">         text.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">     env.execute(<span class="string">"StreamCollectionSourceJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="DataStream-API之Transformation"><a href="#DataStream-API之Transformation" class="headerlink" title="DataStream API之Transformation"></a>DataStream API之Transformation</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transformation是Flink程序的计算算子，负责对数据进行处理，Flink提供了大量的算子，其实Flink中的大部分算子的使用和spark中算子的使用是一样的，下面我们来看一下：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">算子 解释</span><br><span class="line">map 输入一个元素进行处理，返回一个元素</span><br><span class="line">flatMap 输入一个元素进行处理，可以返回多个元素</span><br><span class="line">filter 对数据进行过滤，符合条件的数据会被留下</span><br><span class="line">keyBy 根据key分组，相同key的数据会进入同一个分区</span><br><span class="line">reduce 对当前元素和上一次的结果进行聚合操作</span><br><span class="line">aggregations sum(),min(),max()等</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这里面的算子的用法其实和spark中对应算子的用法是一致的，这里面的map、flatmap、keyBy、reduce、sum这些算子我们都用过了。</span><br><span class="line">所以这里面的算子就不再单独演示了。</span><br><span class="line">往下面看。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">算子 解释</span><br><span class="line">union 合并多个流，多个流的数据类型必须一致</span><br><span class="line">connect 只能连接两个流，两个流的数据类型可以不同</span><br><span class="line">split 根据规则把一个数据流切分为多个流</span><br><span class="line">shuffle 随机分区</span><br><span class="line">rebalance 对数据集进行再平衡，重分区，消除数据倾斜</span><br><span class="line">rescale 重分区</span><br><span class="line">partitionCustom 自定义分区</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这里面的算子我们需要分析一下。</span><br><span class="line">union：表示合并多个流，但是多个流的数据类型必须一致</span><br><span class="line">多个流join之后，就变成了一个流</span><br><span class="line">应用场景：多种数据源的数据类型一致，数据处理规则也一致</span><br></pre></td></tr></table></figure><h5 id="union"><a href="#union" class="headerlink" title="union"></a>union</h5><h6 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 合并多个流，多个流的数据类型必须一致</span></span><br><span class="line"><span class="comment"> * 应用场景：多种数据源的数据类型一致，数据处理规则也一致</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamUnionScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>(<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br><span class="line">         <span class="comment">//合并流</span></span><br><span class="line">         <span class="keyword">val</span> unionStream = text1.union(text2)</span><br><span class="line">         <span class="comment">//打印流中的数据</span></span><br><span class="line">         unionStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamUnionScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 合并多个流，多个流的数据类型必须一致</span></span><br><span class="line"><span class="comment"> * 应用场景：多种数据源的数据类型一致，数据处理规则也一致</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamUnionJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         DataStreamSource&lt;Integer&gt; text1 = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         DataStreamSource&lt;Integer&gt; text2 = env.fromCollection(Arrays.asList(<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>));</span><br><span class="line">         <span class="comment">//合并流</span></span><br><span class="line">         DataStream&lt;Integer&gt; unionStream = text1.union(text2);</span><br><span class="line">         <span class="comment">//打印流中的数据</span></span><br><span class="line">         unionStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamUnionJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">connect：只能连接两个流，两个流的数据类型可以不同</span><br><span class="line"></span><br><span class="line">两个流被connect之后，只是被放到了同一个流中，它们内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。</span><br><span class="line"></span><br><span class="line">connect方法会返回connectedStream，在connectedStream中需要使用CoMap、CoFlatMap这种函数，类似于map和flatmap</span><br></pre></td></tr></table></figure><h6 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.<span class="type">CoMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 只能连接两个流，两个流的数据类型可以不同</span></span><br><span class="line"><span class="comment"> * 应用：可以将两种不同格式的数据统一成一种格式</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamConnectScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromElements(<span class="string">"user:tom,age:18"</span>)</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromElements(<span class="string">"user:jack_age:20"</span>)</span><br><span class="line">         <span class="comment">//连接两个流</span></span><br><span class="line">         <span class="keyword">val</span> connectStream = text1.connect(text2)</span><br><span class="line">         connectStream.map(<span class="keyword">new</span> <span class="type">CoMapFunction</span>[<span class="type">String</span>,<span class="type">String</span>,<span class="type">String</span>] &#123;</span><br><span class="line">         <span class="comment">//处理第1份数据流中的数据</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map1</span></span>(value: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">         value.replace(<span class="string">","</span>,<span class="string">"-"</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//处理第2份数据流中的数据</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map2</span></span>(value: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">         value.replace(<span class="string">"_"</span>,<span class="string">"-"</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamConnectScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091821183.png" alt="image-20230409182145286"></p><h6 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.ConnectedStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.CoMapFunction;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 只能连接两个流，两个流的数据类型可以不同</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamConnectJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text1 = env.fromElements(<span class="string">"user:tom,age:18"</span>);</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text2 = env.fromElements(<span class="string">"user:jack_age:20"</span>);</span><br><span class="line">         <span class="comment">//连接两个流</span></span><br><span class="line">         ConnectedStreams&lt;String, String&gt; connectStream = text1.connect(text2);</span><br><span class="line">         connectStream.map(<span class="keyword">new</span> CoMapFunction&lt;String, String, String&gt;() &#123;</span><br><span class="line">         <span class="comment">//处理第1份数据流中的数据</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">map1</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> value.replace(<span class="string">","</span>,<span class="string">"-"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//处理第2份数据流中的数据</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">map2</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> value.replace(<span class="string">"_"</span>,<span class="string">"-"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamConnectJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="split"><a href="#split" class="headerlink" title="split"></a>split</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">split：根据规则把一个数据流切分为多个流</span><br><span class="line"></span><br><span class="line">注意：split只能分一次流，切分出来的流不能继续分流</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">split需要和select配合使用，选择切分后的流</span><br><span class="line">应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑</span><br></pre></td></tr></table></figure><h6 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation</span><br><span class="line"><span class="keyword">import</span> java.&#123;lang, util&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.collector.selector.<span class="type">OutputSelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据规则把一个数据流切分为多个流</span></span><br><span class="line"><span class="comment"> * 注意：split只能分一次流，切分出来的流不能继续分流</span></span><br><span class="line"><span class="comment"> * split需要和select配合使用，选择切分后的流</span></span><br><span class="line"><span class="comment"> * 应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamSplitScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         <span class="keyword">val</span> splitStream = text.split(<span class="keyword">new</span> <span class="type">OutputSelector</span>[<span class="type">Int</span>] &#123;</span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(value: <span class="type">Int</span>): lang.<span class="type">Iterable</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">         <span class="keyword">val</span> list = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">String</span>]()</span><br><span class="line">         <span class="keyword">if</span>(value % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">         list.add(<span class="string">"even"</span>)<span class="comment">//偶数</span></span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         list.add(<span class="string">"odd"</span>)<span class="comment">//奇数</span></span><br><span class="line">         &#125;</span><br><span class="line">         list</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//选择流</span></span><br><span class="line">         <span class="keyword">val</span> evenStream = splitStream.select(<span class="string">"even"</span>)</span><br><span class="line">         evenStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//二次切分流会报错</span></span><br><span class="line">         <span class="comment">//Consecutive multiple splits are not supported. Splits are deprecated. Pl</span></span><br><span class="line">         <span class="comment">/*val lowHighStream = evenStream.split(new OutputSelector[Int] &#123;</span></span><br><span class="line"><span class="comment">         override def select(value: Int): lang.Iterable[String] = &#123;</span></span><br><span class="line"><span class="comment">         val list = new util.ArrayList[String]()</span></span><br><span class="line"><span class="comment">         if(value &lt;= 5)&#123;</span></span><br><span class="line"><span class="comment">         list.add("low");</span></span><br><span class="line"><span class="comment">         &#125;else&#123;</span></span><br><span class="line"><span class="comment">         list.add("high")</span></span><br><span class="line"><span class="comment">         &#125;</span></span><br><span class="line"><span class="comment">         list</span></span><br><span class="line"><span class="comment">         &#125;</span></span><br><span class="line"><span class="comment">         &#125;)</span></span><br><span class="line"><span class="comment">         val lowStream = lowHighStream.select("low")</span></span><br><span class="line"><span class="comment">         lowStream.print().setParallelism(1)*/</span></span><br><span class="line">         env.execute(<span class="string">"StreamSplitScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.collector.selector.OutputSelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SplitStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据规则把一个数据流切分为多个流</span></span><br><span class="line"><span class="comment"> * 注意：split只能分一次流，切分出来的流不能继续分流</span></span><br><span class="line"><span class="comment"> * split需要和select配合使用，选择切分后的流</span></span><br><span class="line"><span class="comment"> * 应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamSplitJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">             DataStreamSource&lt;Integer&gt; text = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>));</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         SplitStream&lt;Integer&gt; splitStream = text.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">         ArrayList&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         <span class="keyword">if</span> (value % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">         list.add(<span class="string">"even"</span>);<span class="comment">//偶数</span></span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         list.add(<span class="string">"odd"</span>);<span class="comment">//奇数</span></span><br><span class="line">         &#125;</span><br><span class="line">         <span class="keyword">return</span> list;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//选择流</span></span><br><span class="line">         DataStream&lt;Integer&gt; evenStream = splitStream.select(<span class="string">"even"</span>);</span><br><span class="line">         evenStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamSplitJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">目前split切分的流无法进行二次切分，并且split方法已经标记为过时了，官方不推荐使用，现在官方推荐使用side output的方式实现。</span><br></pre></td></tr></table></figure><h5 id="side-output"><a href="#side-output" class="headerlink" title="side output"></a>side output</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我来看一下使用side output如何实现流的多次切分</span><br></pre></td></tr></table></figure><h6 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation</span><br><span class="line"><span class="keyword">import</span> java.&#123;lang, util&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.collector.selector.<span class="type">OutputSelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">ProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">OutputTag</span>, <span class="type">StreamExecutionEnviron</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用sideoutput切分流</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamSideOutputScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         <span class="comment">//首先定义两个sideoutput来准备保存切分出来的数据</span></span><br><span class="line">         <span class="keyword">val</span> outputTag1 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"even"</span>)&#123;&#125;<span class="comment">//保存偶数</span></span><br><span class="line">         <span class="keyword">val</span> outputTag2 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"odd"</span>)&#123;&#125;<span class="comment">//保存奇数</span></span><br><span class="line">         <span class="comment">//注意：process属于Flink中的低级api</span></span><br><span class="line">         <span class="keyword">val</span> outputStream = text.process(<span class="keyword">new</span> <span class="type">ProcessFunction</span>[<span class="type">Int</span>,<span class="type">Int</span>] &#123;</span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">Int</span>, ctx: <span class="type">ProcessFunction</span>[<span class="type">Int</span>, <span class="type">Int</span>]#<span class="type">Context</span>,out:<span class="type">Collector</span>[<span class="type">Int</span>]):</span><br><span class="line">         <span class="keyword">if</span>(value % <span class="number">2</span> == <span class="number">0</span> )&#123;</span><br><span class="line">         ctx.output(outputTag1,value)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         ctx.output(outputTag2,value)</span><br><span class="line">         &#125; </span><br><span class="line">                                     &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//获取偶数数据流</span></span><br><span class="line">         <span class="keyword">val</span> evenStream = outputStream.getSideOutput(outputTag1)</span><br><span class="line">         <span class="comment">//获取奇数数据流</span></span><br><span class="line">         <span class="keyword">val</span> oddStream = outputStream.getSideOutput(outputTag2)</span><br><span class="line">         <span class="comment">//evenStream.print().setParallelism(1)</span></span><br><span class="line">         <span class="comment">//对evenStream流进行二次切分</span></span><br><span class="line">         <span class="keyword">val</span> outputTag11 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"low"</span>)&#123;&#125;<span class="comment">//保存小于等五5的数字</span></span><br><span class="line">         <span class="keyword">val</span> outputTag12 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"high"</span>)&#123;&#125;<span class="comment">//保存大于5的数字</span></span><br><span class="line">         <span class="keyword">val</span> subOutputStream = evenStream.process(<span class="keyword">new</span> <span class="type">ProcessFunction</span>[<span class="type">Int</span>,<span class="type">Int</span>] &#123;</span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">Int</span>, ctx: <span class="type">ProcessFunction</span>[<span class="type">Int</span>, <span class="type">Int</span>]#<span class="type">Context</span>,<span class="type">Collector</span>[<span class="type">Int</span>]):</span><br><span class="line">         <span class="keyword">if</span>(value&lt;=<span class="number">5</span>)&#123;</span><br><span class="line">          ctx.output(outputTag11,value)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         ctx.output(outputTag12,value)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//获取小于等于5的数据流</span></span><br><span class="line">         <span class="keyword">val</span> lowStream = subOutputStream.getSideOutput(outputTag11)</span><br><span class="line">         <span class="comment">//获取大于5的数据流</span></span><br><span class="line">         <span class="keyword">val</span> highStream = subOutputStream.getSideOutput(outputTag12)</span><br><span class="line">         lowStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamSideOutputScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092310468.png" alt="image-20230409231015815"></p><h6 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.OutputTag;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用sideoutput切分流</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamSideoutputJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecuti</span><br><span class="line">         DataStreamSource&lt;Integer&gt; text = env.fromCollection(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>));</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         <span class="comment">//首先定义两个sideoutput来准备保存切分出来的数据</span></span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag1 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"even"</span>) &#123;&#125;;</span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag2 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"odd"</span>) &#123;&#125;;</span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; outputStream = text.process(<span class="keyword">new</span> ProcessFunction&lt;Integer,Integer&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer value, Context ctx, Collector&lt;Integer&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (value % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">         ctx.output(outputTag1, value);</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         ctx.output(outputTag2, value);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//获取偶数数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; evenStream = outputStream.getSideOutput(outputTag1);</span><br><span class="line">         <span class="comment">//获取奇数数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; oddStream = outputStream.getSideOutput(outputTag2);</span><br><span class="line">         <span class="comment">//对evenStream流进行二次切分</span></span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag11 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"low"</span>) &#123;&#125;;</span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag12 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"high"</span>) &#123;&#125;;</span><br><span class="line">         SingleOutputStreamOperator&lt;Integer&gt; subOutputStream = evenStream.process(<span class="keyword">new</span> ProcessFunction&lt;Integer,Integer&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer value, Context ctx, Collector&lt;Integer&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (value &lt;= <span class="number">5</span>) &#123;</span><br><span class="line">         ctx.output(outputTag11, value);</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         ctx.output(outputTag12, value);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//获取小于等于5的数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; lowStream = subOutputStream.getSideOutput(outputTag11);</span><br><span class="line">         <span class="comment">//获取大于5的数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; highStream = subOutputStream.getSideOutput(outputTag12);</span><br><span class="line">         lowStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamSideoutputJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">其实想要实现多级流切分，使用filter算子也是可以实现的，给大家留一个作业，大家可以下去实验一下。</span><br><span class="line">最后针对这几个算子总结一下：</span><br><span class="line">首先是union和connect的区别，如图所示：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092319871.png" alt="image-20230409231907397"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">union可以连接多个流，最后汇总成一个流，流里面的数据使用相同的计算规则</span><br><span class="line">connect值可以连接2个流，最后汇总成一个流，但是流里面的两份数据相互还是独立的，每一份数据使用一个计算规则</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后是流切分</span><br><span class="line">如果是只需要切分一次的话使用split或者side output都可以</span><br><span class="line">如果想要切分多次，就不能使用split了，需要使用side output</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092319304.png" alt="image-20230409231939157"></p><h5 id="分区相关算子"><a href="#分区相关算子" class="headerlink" title="分区相关算子"></a>分区相关算子</h5><h6 id="random"><a href="#random" class="headerlink" title="random"></a>random</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下这几个和分区相关的算子</span><br><span class="line">算子 解释</span><br><span class="line">random 随机分区</span><br><span class="line">rebalance对数据集进行再平衡，重分区，消除数据倾斜|</span><br><span class="line">rescale重分区</span><br><span class="line">custom partition 自定义分区</span><br><span class="line"></span><br><span class="line">random：随机分区，它表示将上游数据随机分发到下游算子实例的每个分区中，在代码层面体现是调用shuffle()函数</span><br><span class="line"></span><br><span class="line">查看源码，shuffle底层对应的是ShufflePartitioner这个类</span><br><span class="line">这个类里面有一个selectChannel函数，这个函数会计算数据将会被发送给哪个分区，里面使用的是random.nextInt，所以说是随机的。</span><br><span class="line"></span><br><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line"> return random.nextInt(numberOfChannels);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="rebalance"><a href="#rebalance" class="headerlink" title="rebalance"></a>rebalance</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rebalance：重新平衡分区(循环分区)，我觉得叫循环分区更好理解，它表示对数据集进行再平衡，消除数据倾斜，为每个分区创建相同的负载，其实就是通过循环的方式给下游算子实例的每个分区分配数据，在代码层面体现是调用rebalance()函数</span><br><span class="line"></span><br><span class="line">查看源码，rebalance底层对应的是RebalancePartitioner这个类</span><br><span class="line">这个类里面有一个setup和selectChannel函数，setup函数会根据分 区数初始化一个随机值nextChannelToSendTo，然后selectChannel函数会使用nextChannelToSendTo加1和分区数取模，把计算的值再赋给nextChannelToSendTo，后面以此类推，其实就可以实现向下游算子实例的多个分区循环发送数据了，这样每个分区获取到的数据基本一致。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public void setup(int numberOfChannels) &#123;</span><br><span class="line"> super.setup(numberOfChannels);</span><br><span class="line"> nextChannelToSendTo &#x3D; ThreadLocalRandom.current().nextInt(numberOfChannels)</span><br><span class="line">&#125;</span><br><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line">     nextChannelToSendTo &#x3D; (nextChannelToSendTo + 1) % numberOfChannels;</span><br><span class="line">     return nextChannelToSendTo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="rescale"><a href="#rescale" class="headerlink" title="rescale"></a>rescale</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rescale：重分区</span><br><span class="line">查看源码，rescale底层对应的是RescalePartitioner这个类</span><br><span class="line">这个类里面有一个selectChannel函数，这里面的numberOfChannels是分区数量，其实也可以认为是我们所说的算子的并行度，因为一个分区是由一个线程负责处理的，它们两个是一一对应的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line"> if (++nextChannelToSendTo &gt;&#x3D; numberOfChannels) &#123;</span><br><span class="line">  nextChannelToSendTo &#x3D; 0;</span><br><span class="line"> &#125;</span><br><span class="line"> return nextChannelToSendTo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">详细的解释在这个类的注释中也是有的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">* The subset of downstream operations to which the upstream operation sends</span><br><span class="line">* elements depends on the degree of parallelism of both the upstream and downs</span><br><span class="line">* For example, if the upstream operation has parallelism 2 and the downstream </span><br><span class="line">* has parallelism 4, then one upstream operation would distribute elements to </span><br><span class="line">* downstream operations while the other upstream operation would distribute to</span><br><span class="line">* two downstream operations. If, on the other hand, the downstream operation h</span><br><span class="line">* 2 while the upstream operation has parallelism 4 then two upstream operation</span><br><span class="line">* distribute to one downstream operation while the other two upstream operatio</span><br><span class="line">* distribute to the other downstream operations.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果上游操作有2个并发，而下游操作有4个并发，那么上游的1个并发结果循环分配给下游的2个并发操作，上游的另外1个并发结果循环分配给下游的另外2个并发操作。</span><br><span class="line">另一种情况，如果上游有4个并发操作，而下游有2个并发操作，那么上游的其中2个并发操作的结果会分配给下游的一个并发操作，而上游的另外2个并发操作的结果则分配给下游的另外1个并发操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：rescale与rebalance的区别是rebalance会产生全量重分区，而rescale不会。</span><br></pre></td></tr></table></figure><h6 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">broadcast：广播分区，将上游算子实例中的数据输出到下游算子实例的每个分区中，适合用于大数据集Join小数据集的场景，可以提高性能。</span><br><span class="line"></span><br><span class="line">查看源码，broadcast底层对应的是BroadcastPartitioner这个类</span><br><span class="line">看这个类中的selectChannel函数代码的注释，提示广播分区不支持选择Channel,因为会输出数据到下游的每个Channel中，就是发送到下游算子实例的每个分区中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Note: Broadcast mode could be handled directly for all the output channels</span><br><span class="line"> * in record writer, so it is no need to select channels via this method.</span><br><span class="line"> *&#x2F;</span><br><span class="line">@Override</span><br><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line"> throw new UnsupportedOperationException(&quot;Broadcast partitioner does not sup</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="custom-partition"><a href="#custom-partition" class="headerlink" title="custom partition"></a>custom partition</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">custom partition：自定义分区，可以按照自定义规则实现</span><br><span class="line">自定义分区需要实现Partitioner接口</span><br><span class="line"></span><br><span class="line">这是针对这几种分区的解释，下面来通过这个图总结一下，加深理解</span><br></pre></td></tr></table></figure><h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092345507.png" alt="image-20230409234549859"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092348975.png" alt="image-20230409234806351"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092350119.png" alt="image-20230409235046659"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后使用代码演示一下它们具体的用法</span><br></pre></td></tr></table></figure><h6 id="scala-5"><a href="#scala-5" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnviro</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 分区规则的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamPartitionOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="comment">//注意：默认情况下Fink任务中算子的并行度会读取当前机器的CPU个数</span></span><br><span class="line">         <span class="comment">//但fromCollection的并行度为1，由源码可知</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//注意：在这里建议将这个隐式转换代码放到类上面</span></span><br><span class="line">         <span class="comment">//因为默认它只在main函数生效，针对下面提取的shuffleOp是无效的，否则也需要在shuffleOp添加这行代码</span></span><br><span class="line">         <span class="comment">//import org.apache.flink.api.scala._</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用shuffle分区规则</span></span><br><span class="line">         <span class="comment">//shuffleOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用rebalance分区规则</span></span><br><span class="line">         <span class="comment">//rebalanceOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用rescale分区规则</span></span><br><span class="line">         <span class="comment">//rescaleOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用broadcast分区规则，此代码一共会打印40条数据，因为print的并行度为4</span></span><br><span class="line">         <span class="comment">//broadcastOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//自定义分区规则：根据数据的奇偶性进行分区</span></span><br><span class="line">         <span class="comment">//注意：此时虽然print算子的并行度是4，但是自定义的分区规则只会把数据分发给2个并行度，所以有两个不干活</span></span><br><span class="line">         <span class="comment">//custormPartitionOp(text)</span></span><br><span class="line">         env.execute(<span class="string">"StreamPartitionOpScala"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">custormPartitionOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         <span class="comment">//.partitionCustom(new MyPartitionerScala,0)//这种写法已经过期</span></span><br><span class="line">         .partitionCustom(<span class="keyword">new</span> <span class="type">MyPartitionerScala</span>, num =&gt; num)<span class="comment">//官方建议使用keyselector</span></span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">broadcastOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .broadcast</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">rescaleOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .rescale</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">rebalanceOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .rebalance</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">shuffleOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         <span class="comment">//由于fromCollection已经设置了并行度为1，所以需要再接一个算子之后才能修改并行度</span></span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .shuffle  </span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">Partitioner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义分区规则：按照数字的奇偶性进行分区</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitionerScala</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>[<span class="type">Int</span>]</span>&#123;</span><br><span class="line">     <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">partition</span></span>(key: <span class="type">Int</span>, numPartitions: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">         println(<span class="string">"分区总数："</span>+numPartitions)</span><br><span class="line">         <span class="keyword">if</span>(key % <span class="number">2</span> == <span class="number">0</span>)&#123;<span class="comment">//偶数分到0号分区</span></span><br><span class="line">         <span class="number">0</span></span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;<span class="comment">//奇数分到1号分区</span></span><br><span class="line">         <span class="number">1</span></span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">custom partition</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100026329.png" alt="image-20230410002655126"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">broadcast 此时每个数据都有四次输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100014718.png" alt="image-20230410001436246"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rescale  四个分区都有输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100013065.png" alt="image-20230410001313667"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rebalance 四个分区都有数据输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100010193.png" alt="image-20230410000959762"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shuffle  不一定每个分区都有数据输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100010853.png" alt="image-20230410001045259"></p><h6 id="java-5"><a href="#java-5" class="headerlink" title="java"></a>java</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.java.stream.transformation;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 分区规则的使用</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class StreamPartitionOpJava &#123;</span><br><span class="line">     public static void main(String[] args) throws Exception&#123;</span><br><span class="line">         StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecuti</span><br><span class="line">         DataStreamSource&lt;Integer&gt; text &#x3D; env.fromCollection(Arrays.asList(1, 2</span><br><span class="line">         &#x2F;&#x2F;使用shuffle分区规则</span><br><span class="line">         &#x2F;&#x2F;shuffleOp(text);</span><br><span class="line">         &#x2F;&#x2F;使用rebalance分区规则</span><br><span class="line">         &#x2F;&#x2F;rebalanceOp(text);</span><br><span class="line">         &#x2F;&#x2F;使用rescale分区规则</span><br><span class="line">         &#x2F;&#x2F;rescaleOp(text);</span><br><span class="line">         &#x2F;&#x2F;使用broadcast分区规则</span><br><span class="line">         &#x2F;&#x2F;broadcastOp(text);</span><br><span class="line">         &#x2F;&#x2F;自定义分区规则</span><br><span class="line">         &#x2F;&#x2F;custormPartitionOp(text);</span><br><span class="line">         env.execute(&quot;StreamPartitionOpJava&quot;);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void custormPartitionOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .partitionCustom(new MyPartitionerJava(), new KeySelector&lt;Inte</span><br><span class="line">         @Override</span><br><span class="line">         public Integer getKey(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void broadcastOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .broadcast()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void rescaleOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .rescale()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void rebalanceOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .rebalance()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void shuffleOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .shuffle()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="DataStream-API之DataSink"><a href="#DataStream-API之DataSink" class="headerlink" title="DataStream API之DataSink"></a>DataStream API之DataSink</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataSink是 输出组件，负责把计算好的数据输出到其它存储介质中</span><br><span class="line">Flink支持把流数据输出到文件中，不过在实际工作中这种场景不多，因为流数据处理之后一般会存储到一些消息队列里面，或者数据库里面，很少会保存到文件中的。</span><br><span class="line"></span><br><span class="line">还有就是print，直接打印，这个其实我们已经用了很多次了，这种用法主要是在测试的时候使用的，方便查看输出的结果信息</span><br><span class="line"></span><br><span class="line">Flink提供了一批Connectors，可以实现输出到第三方目的地</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Flink内置 Apache Bahir</span><br><span class="line">Kafka ActiveMQ</span><br><span class="line">Cassandra Flume</span><br><span class="line">Kinesis Streams Redis</span><br><span class="line">Elasticsearch Akka</span><br><span class="line">Hadoop FileSysterm</span><br><span class="line">RabbitMQ</span><br><span class="line">NiFi</span><br><span class="line">JDBC</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对sink的这些connector，我们在实际工作中最常用的是kafka、redis</span><br><span class="line">针对Flink提供的常用sink组件，可以提供这些容错性保证</span><br><span class="line"></span><br><span class="line">DataSink 容错保证 备注</span><br><span class="line">Redis at least once</span><br><span class="line">Kafka    at least once&#x2F;exactlyonceKafka0.9和0.10提供at least once，Kafka0.11及以上提供</span><br><span class="line">exactly once</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101205370.png" alt="image-20230410120538984"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">针对kafka这个sink组件的使用，我们在后面会统一分析，现在我们来使用一下redis这个sink组件</span><br><span class="line">需求：接收Socket传输过来的数据，把数据保存到Redis的list队列中。</span><br></pre></td></tr></table></figure><h5 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">先到Flink官网，文档，connector,datasteam,redis,添加对应的依赖(一般不是正确的依赖，把名字复制到maven官网，查找)</span><br></pre></td></tr></table></figure><h6 id="scala-6"><a href="#scala-6" class="headerlink" title="scala"></a>scala</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：redis sink是在Bahir这个依赖包中，所以在pom.xml中需要添加对应的依赖</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.bahir&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;flink-connector-redis_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;1.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.sink</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.<span class="type">RedisSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.config.<span class="type">FlinkJedisPoo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.&#123;<span class="type">RedisCommand</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：接收Socket传输过来的数据，把数据保存到Redis的list队列中。</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamRedisSinkScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//组装数据，这里组装的是tuple2类型</span></span><br><span class="line">         <span class="comment">//第一个元素是指list队列的key名称</span></span><br><span class="line">         <span class="comment">//第二个元素是指需要向list队列中添加的元素</span></span><br><span class="line">         <span class="keyword">val</span> listData = text.map(word =&gt; (<span class="string">"l_words_scala"</span>, word))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定redisSink</span></span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">FlinkJedisPoolConfig</span>.<span class="type">Builder</span>().setHost(<span class="string">"bigdata04"</span>).setPort(<span class="number">6379</span>).build()</span><br><span class="line">         <span class="keyword">val</span> redisSink = <span class="keyword">new</span> <span class="type">RedisSink</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">String</span>]](conf, <span class="keyword">new</span> <span class="type">MyRedisMapper</span>)</span><br><span class="line">                                                                listData.addSink(redisSink)</span><br><span class="line">         env.execute(<span class="string">"StreamRedisSinkScala"</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">    </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">extends</span> <span class="title">RedisMapper</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">String</span>]]</span>&#123;</span><br><span class="line">         <span class="comment">//指定具体的操作命令</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCommandDescription</span></span>: <span class="type">RedisCommandDescription</span> = &#123;</span><br><span class="line">         <span class="keyword">new</span> <span class="type">RedisCommandDescription</span>(<span class="type">RedisCommand</span>.<span class="type">LPUSH</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取key</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKeyFromData</span></span>(data: (<span class="type">String</span>, <span class="type">String</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">         data._1</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取value</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValueFromData</span></span>(data: (<span class="type">String</span>, <span class="type">String</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">         data._2</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注意：针对List数据类型，我们在定义getCommandDescription方法的时候，使用new</span><br><span class="line">RedisCommandDescription(RedisCommand.LPUSH);。</span><br><span class="line"></span><br><span class="line">如果是Hash数据类型，在定义getCommandDescription方法的时候，需要使用new</span><br><span class="line">RedisCommandDescription(RedisCommand.HSET,“hashKey”);，在构造函数中需要直接指定Hash数据类型的key的名称。</span><br><span class="line"></span><br><span class="line">注意：执行代码之前，需要先开启socket和redis服务</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">通过socket传递单词</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line"></span><br><span class="line">最终到redis中查看结果</span><br><span class="line">[root@bigdata04 redis-5.0.9]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; lrange l_words_scala 0 -1</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101233738.png" alt="image-20230410123339761"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101234121.png" alt="image-20230410123420552"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最终到redis中查看结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101234377.png" alt="image-20230410123448085"></p><h6 id="java-6"><a href="#java-6" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.sink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.RedisSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoo</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommand;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommandD</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.RedisMapper;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：接收Socket传输过来的数据，把数据保存到Redis的list队列中。</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamRedisSinkJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line">         <span class="comment">//组装数据</span></span><br><span class="line">         SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; listData = text.map(<span class="keyword">new</span> MapFunction&lt;String,String&gt;&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">map</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="string">"l_words_java"</span>, word);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//指定redisSink</span></span><br><span class="line">         FlinkJedisPoolConfig conf = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder().setHost(<span class="string">"bigdata04"</span>).setPort(<span class="number">6379</span>).build();</span><br><span class="line">        </span><br><span class="line">         RedisSink&lt;Tuple2&lt;String, String&gt;&gt; redisSink = <span class="keyword">new</span> RedisSink&lt;&gt;(conf, <span class="keyword">new</span> MyRedisMapper()) </span><br><span class="line">         listData.addSink(redisSink);</span><br><span class="line">         </span><br><span class="line">                                                                             env.execute(<span class="string">"StreamRedisSinkJava"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">        </span><br><span class="line">                                                      <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">implements</span> <span class="title">RedisMapper</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>,<span class="title">String</span>&gt;&gt;</span>&#123;</span><br><span class="line">         <span class="comment">//指定具体的操作命令</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> RedisCommandDescription(RedisCommand.LPUSH);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取key</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getKeyFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> data.f0;</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取value</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">getValueFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> data.f1;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
</feed>
