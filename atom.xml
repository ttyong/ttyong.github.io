<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2023-04-08T15:22:12.197Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BTableAPI%E5%92%8CSQL-5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BTableAPI%E5%92%8CSQL-5.html</id>
    <published>2023-04-08T15:22:12.000Z</published>
    <updated>2023-04-08T15:22:12.197Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataSetAPI-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataSetAPI-4.html</id>
    <published>2023-04-08T15:21:40.000Z</published>
    <updated>2023-04-08T15:21:40.393Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataStreamAPI-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataStreamAPI-3.html</id>
    <published>2023-04-08T15:20:20.000Z</published>
    <updated>2023-04-08T15:20:20.424Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-实战：流处理和批处理程序开发-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%AE%9E%E6%88%98%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%92%8C%E6%89%B9%E5%A4%84%E7%90%86%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%AE%9E%E6%88%98%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%92%8C%E6%89%B9%E5%A4%84%E7%90%86%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91-2.html</id>
    <published>2023-04-07T17:41:10.000Z</published>
    <updated>2023-04-08T16:44:33.313Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-实战：流处理和批处理程序开发-2"><a href="#第十六周-Flink极速上手篇-实战：流处理和批处理程序开发-2" class="headerlink" title="第十六周 Flink极速上手篇-实战：流处理和批处理程序开发-2"></a>第十六周 Flink极速上手篇-实战：流处理和批处理程序开发-2</h1><h2 id="Flink快速上手使用"><a href="#Flink快速上手使用" class="headerlink" title="Flink快速上手使用"></a>Flink快速上手使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">创建maven项目，因为要使用scala编写代码，在src里main里除了java目录，还要创建scala目录，再创建包</span><br><span class="line"></span><br><span class="line">setting里的module里的scala sdk要导入</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304081548060.png" alt="image-20230408154825118"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来在 pom.xml 中引入flink相关依赖，前面两个是针对java代码的，后面两个是针对scala代码的，最后一个依赖是这对flink1.11这个版本需要添加的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-streaming-java_2.12&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="Flink-Job开发步骤"><a href="#Flink-Job开发步骤" class="headerlink" title="Flink Job开发步骤"></a>Flink Job开发步骤</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在开发Flink程序之前，我们先来梳理一下开发一个Flink程序的步骤</span><br><span class="line">1：获得一个执行环境</span><br><span class="line">2：加载&#x2F;创建 初始化数据</span><br><span class="line">3：指定操作数据的transaction算子</span><br><span class="line">4：指定数据目的地</span><br><span class="line">5：调用execute()触发执行程序</span><br><span class="line"></span><br><span class="line">注意：Flink程序是延迟计算的，只有最后调用execute()方法的时候才会真正触发执行程序和Spark类似，Spark中是必须要有action算子才会真正执行。</span><br></pre></td></tr></table></figure><h3 id="Streaming-WordCount"><a href="#Streaming-WordCount" class="headerlink" title="Streaming WordCount"></a>Streaming WordCount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">需求</span><br><span class="line">通过socket实时产生一些单词，使用flink实时接收数据，对指定时间窗口内(例如：2秒)的数据进行聚合统计，并且把时间窗口内计算的结果打印出来</span><br><span class="line">代码开发</span><br><span class="line">下面我们就来开发第一个Flink程序。</span><br><span class="line">先使用scala代码开发</span><br></pre></td></tr></table></figure><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.<span class="type">KeySelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过Socket实时产生一些单词，</span></span><br><span class="line"><span class="comment"> * 使用Flink实时接收数据</span></span><br><span class="line"><span class="comment"> * 对指定时间窗口内(例如：2秒)的数据进行聚合统计</span></span><br><span class="line"><span class="comment"> * 并且把时间窗口内计算的结果打印出来</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SocketWindowWordCountScala</span> </span>&#123;</span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意：在执行代码之前，需要先在bigdata04机器上开启socket，端口为9001</span></span><br><span class="line"><span class="comment">     * @param args</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取运行环境</span></span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">         <span class="comment">//处理数据</span></span><br><span class="line">         <span class="comment">//注意：必须要添加这一行隐式转换的代码，否则下面的flatMap方法会报错</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> wordCount = text.flatMap(_.split(<span class="string">" "</span>))<span class="comment">//将每一行数据根据空格切分单词</span></span><br><span class="line">         .map((_,<span class="number">1</span>))<span class="comment">//每一个单词转换为tuple2的形式(单词,1)</span></span><br><span class="line">         <span class="comment">//.keyBy(0)//根据tuple2中的第一列进行分组</span></span><br><span class="line">         .keyBy(tup=&gt;tup._1)<span class="comment">//官方推荐使用keyselector选择器选择数据</span></span><br><span class="line">         .timeWindow(<span class="type">Time</span>.seconds(<span class="number">2</span>))<span class="comment">//时间窗口为2秒，表示每隔2秒钟计算一次接收到的数据</span></span><br><span class="line">         .sum(<span class="number">1</span>)<span class="comment">// 使用sum或者reduce都可以</span></span><br><span class="line">         <span class="comment">//.reduce((t1,t2)=&gt;(t1._1,t1._2+t2._2))</span></span><br><span class="line">             <span class="comment">//使用一个线程执行打印操作</span></span><br><span class="line">         wordCount.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         <span class="comment">//执行程序</span></span><br><span class="line">         env.execute(<span class="string">"SocketWindowWordCountScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在idea等开发工具里面运行代码的时候需要把pom.xml中的scope配置注释掉</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082223638.png" alt="image-20230408222314468"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04上面开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">hello you</span><br><span class="line">hello me</span><br><span class="line">hello you hello me</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082225596.png" alt="image-20230408222535380"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">注意：此时代码执行的时候下面会显示一些红色的log4j的警告信息，提示缺少相关依赖和配置</span><br><span class="line"></span><br><span class="line">将log4j.properties配置文件和log4j的相关maven配置添加到pom.xml文件中</span><br><span class="line"></span><br><span class="line">&lt;!-- log4j的依赖 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082228288.png" alt="image-20230408222823964"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时再执行就没有红色的警告信息了，但是使用info日志级别打印的信息太多了，所以将log4j中的日志级别配置改为error级别</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082229674.png" alt="image-20230408222903540"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket实时产生一些单词</span></span><br><span class="line"><span class="comment"> * 使用Flink实时接收数据</span></span><br><span class="line"><span class="comment"> * 对指定时间窗口内(例如：2秒)的数据进行聚合统计</span></span><br><span class="line"><span class="comment"> * 并且把时间窗口内计算的结果打印出来</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketWindowWordCountJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取运行环境</span></span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line">         <span class="comment">//处理数据</span></span><br><span class="line">         SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordCount = text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String,String&gt;()&#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">         <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">         out.collect(word);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).map(<span class="keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new Tuple2&lt;String, Integer&gt;<span class="params">(word, <span class="number">1</span>)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> tup.f0;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)<span class="comment">//.keyBy(0)</span></span><br><span class="line">         .timeWindow(Time.seconds(<span class="number">2</span>))</span><br><span class="line">         .sum(<span class="number">1</span>);</span><br><span class="line">         <span class="comment">//使用一个线程执行打印操作</span></span><br><span class="line">         wordCount.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         <span class="comment">//执行程序</span></span><br><span class="line">         env.execute(<span class="string">"SocketWindowWordCountJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Batch-WordCount"><a href="#Batch-WordCount" class="headerlink" title="Batch WordCount"></a>Batch WordCount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需求：统计指定文件中单词出现的总次数</span><br><span class="line">下面来开发Flink的批处理代码</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：统计指定文件中单词出现的总次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchWordCountScala</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"> <span class="comment">//获取执行环境</span></span><br><span class="line"> <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"> <span class="keyword">val</span> inputPath = <span class="string">"hdfs://bigdata01:9000/hello.txt"</span></span><br><span class="line"> <span class="keyword">val</span> outPath = <span class="string">"hdfs://bigdata01:9000/out"</span></span><br><span class="line"> <span class="comment">//读取文件中的数据</span></span><br><span class="line"> <span class="keyword">val</span> text = env.readTextFile(inputPath)</span><br><span class="line"> <span class="comment">//处理数据</span></span><br><span class="line"> <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"> <span class="keyword">val</span> wordCount = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"> .map((_, <span class="number">1</span>))</span><br><span class="line"> .groupBy(<span class="number">0</span>)</span><br><span class="line"> .sum(<span class="number">1</span>)</span><br><span class="line"> .setParallelism(<span class="number">1</span>) <span class="comment">// 这里设置并行度是为了将所有数据写到一个文件里，查看比较方便</span></span><br><span class="line"> <span class="comment">//将结果数据保存到文件中</span></span><br><span class="line"> wordCount.writeAsCsv(outPath,<span class="string">"\n"</span>,<span class="string">" "</span>)</span><br><span class="line"> <span class="comment">//执行程序</span></span><br><span class="line"> env.execute(<span class="string">"BatchWordCountScala"</span>)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：这里面执行setParallelism(1)设置并行度为1是为了将所有数据写到一个文件里面，我们查看结果的时候比较方便(此时输出路径会变成一个文件)</span><br><span class="line">还有就是flink在windows中执行代码，使用到hadoop的时候，需要将hadoop-client的依赖添加到项目中，否则会提示不支持hdfs这种文件系统。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在pom.xml文件中增加</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;3.2.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">此时执行代码就可以正常执行了。</span><br><span class="line">执行成功之后到hdfs上查看结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082256026.png" alt="image-20230408225649895"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.AggregateOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：统计指定文件中单词出现的总次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchWordCountJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取执行环境</span></span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">         String inputPath = <span class="string">"hdfs://bigdata01:9000/hello.txt"</span>;</span><br><span class="line">         String outPath = <span class="string">"hdfs://bigdata01:9000/out2"</span>;</span><br><span class="line">         <span class="comment">//读取文件中的数据</span></span><br><span class="line">         DataSource&lt;String&gt; text = env.readTextFile(inputPath);</span><br><span class="line">         <span class="comment">//处理数据(这里flatmap之后，没写map，直接一步到位)</span></span><br><span class="line">         DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; wordCount = text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String,Tuple2&lt;String,Integer&gt;&gt;()&#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">         <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">         out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, <span class="number">1</span>));</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).groupBy(<span class="number">0</span>)</span><br><span class="line">         .sum(<span class="number">1</span>)</span><br><span class="line">         .setParallelism(<span class="number">1</span>);</span><br><span class="line">         <span class="comment">//将结果数据保存到文件中</span></span><br><span class="line">         wordCount.writeAsCsv(outPath,<span class="string">"\n"</span>,<span class="string">" "</span>);</span><br><span class="line">         <span class="comment">//执行程序</span></span><br><span class="line">         env.execute(<span class="string">"BatchWordCountJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-Streaming和Batch的区别"><a href="#Flink-Streaming和Batch的区别" class="headerlink" title="Flink Streaming和Batch的区别"></a>Flink Streaming和Batch的区别</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">流处理Streaming</span><br><span class="line">执行环境：StreamExecutionEnvironment</span><br><span class="line">数据类型：DataStream</span><br><span class="line"></span><br><span class="line">批处理Batch</span><br><span class="line">执行环境：ExecutionEnvironment</span><br><span class="line">数据类型：DataSet</span><br></pre></td></tr></table></figure><h2 id="Flink集群安装部署"><a href="#Flink集群安装部署" class="headerlink" title="Flink集群安装部署"></a>Flink集群安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Flink支持多种安装部署方式</span><br><span class="line">Standalone</span><br><span class="line">ON YARN</span><br><span class="line">Mesos、Kubernetes、AWS…</span><br><span class="line">这些安装方式我们主要讲一下standalone和on yarn。</span><br><span class="line">如果是一个独立环境的话，可能会用到standalone集群模式。</span><br><span class="line">在生产环境下一般还是用on yarn 这种模式比较多，因为这样可以综合利用集群资源。和我们之前讲的spark on yarn是一样的效果</span><br><span class="line">这个时候我们的Hadoop集群上面既可以运行MapReduce任务，Spark任务，还可以运行Flink任务，一举三得。</span><br></pre></td></tr></table></figure><h3 id="standalone"><a href="#standalone" class="headerlink" title="standalone"></a>standalone</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下standalone模式</span><br><span class="line">它的架构是这样的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082323175.png" alt="image-20230408232348677"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">依赖环境</span><br><span class="line">jdk1.8及以上【配置JAVA_HOME环境变量】</span><br><span class="line">ssh免密码登录</span><br><span class="line">在这我们使用bigdata01、02、03这三台机器，这几台机器的基础环境都是ok的，可以直接使用。</span><br><span class="line"></span><br><span class="line">集群规划如下：</span><br><span class="line">master：bigdata01</span><br><span class="line">slave：bigdata02、bigdata03</span><br><span class="line">接下来我们需要先去下载Flink的安装包。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082326209.png" alt="image-20230408232658079"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于目前Flink各个版本之间差异比较大，属于快速迭代阶段，所以在这我们就使用最新版本了，使用Flink1.11.1版本。</span><br><span class="line">安装包下载好以后上传到bigdata01的&#x2F;data&#x2F;soft目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">下面开始安装Flink集群</span><br><span class="line">1：解压</span><br><span class="line"></span><br><span class="line">2：修改配置</span><br><span class="line">[root@bigdata01 soft]# cd flink-1.11.1</span><br><span class="line">[root@bigdata01 flink-1.11.1]# cd conf&#x2F;</span><br><span class="line">[root@bigdata01 conf]# vi flink-conf.yaml </span><br><span class="line">......</span><br><span class="line">jobmanager.rpc.address: bigdata01</span><br><span class="line">......</span><br><span class="line">[root@bigdata01 conf]# vi masters </span><br><span class="line">bigdata01:8081</span><br><span class="line">[root@bigdata01 conf]# vi workers</span><br><span class="line">bigdata02</span><br><span class="line">bigdata03</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">3：将修改完配置的flink目录拷贝到其它两个从节点</span><br><span class="line">[root@bigdata01 soft]# scp -rq flink-1.11.1 bigdata02:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">[root@bigdata01 soft]# scp -rq flink-1.11.1 bigdata03:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line"></span><br><span class="line">4：启动Flink集群</span><br><span class="line">[root@bigdata01 soft]# cd flink-1.11.1</span><br><span class="line">[root@bigdata01 flink-1.11.1]# bin&#x2F;start-cluster.sh </span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host bigdata01.</span><br><span class="line">Starting taskexecutor daemon on host bigdata02.</span><br><span class="line">Starting taskexecutor daemon on host bigdata03.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5：验证一下进程</span><br><span class="line">在bigdata01上执行jps</span><br><span class="line">[root@bigdata01 flink-1.11.1]# jps</span><br><span class="line">3986 StandaloneSessionClusterEntrypoint</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在bigdata02上执行jps</span><br><span class="line">在bigdata03上执行jps</span><br><span class="line">[root@bigdata02 ~]# jps</span><br><span class="line">2149 TaskManagerRunner</span><br><span class="line">[root@bigdata03 ~]# jps</span><br><span class="line">2150 TaskManagerRunner</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">6：访问Flink的web界面</span><br><span class="line">http:&#x2F;&#x2F;bigdata01:8081</span><br><span class="line">7：停止集群，在主节点上执行停止集群脚本</span><br><span class="line">[root@bigdata01 flink-1.11.1]# bin&#x2F;stop-cluster.sh </span><br><span class="line">Stopping taskexecutor daemon (pid: 2149) on host bigdata02.</span><br><span class="line">Stopping taskexecutor daemon (pid: 2150) on host bigdata03.</span><br><span class="line">Stopping standalonesession daemon (pid: 3986) on host bigdata01.</span><br></pre></td></tr></table></figure><h4 id="Standalone集群核心参数"><a href="#Standalone集群核心参数" class="headerlink" title="Standalone集群核心参数"></a>Standalone集群核心参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">参数 解释</span><br><span class="line">jobmanager.memory .process.size 主节点可用内存大小</span><br><span class="line">taskmanager.memory.process.size 从节点可用内存大小</span><br><span class="line">taskmanager.numberOfTaskSlots 从节点可以启动的进程数量，建议设置为从节可用的cpu数量</span><br><span class="line">parallelism.default Flink任务的默认并行度</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1：slot是静态的概念，是指taskmanager具有的并发执行能力</span><br><span class="line">2：parallelism是动态的概念，是指程序运行时实际使用的并发能力</span><br><span class="line">3：设置合适的parallelism能提高程序计算效率，太多了和太少了都不好</span><br></pre></td></tr></table></figure><h3 id="Flink-ON-YARN"><a href="#Flink-ON-YARN" class="headerlink" title="Flink ON YARN"></a>Flink ON YARN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink ON YARN模式就是使用客户端的方式，直接向Hadoop集群提交任务即可。不需要单独启动Flink进程。</span><br><span class="line">注意：</span><br><span class="line">1：Flink ON YARN 模式依赖Hadoop 2.4.1及以上版本</span><br><span class="line">2：Flink ON YARN支持两种使用方式</span><br></pre></td></tr></table></figure><h4 id="Flink-ON-YARN两种使用方式"><a href="#Flink-ON-YARN两种使用方式" class="headerlink" title="Flink ON YARN两种使用方式"></a>Flink ON YARN两种使用方式</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082341963.png" alt="image-20230408234110607"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在工作中建议使用第二种方式。</span><br></pre></td></tr></table></figure><h5 id="Flink-ON-YARN第一种方式"><a href="#Flink-ON-YARN第一种方式" class="headerlink" title="Flink ON YARN第一种方式"></a>Flink ON YARN第一种方式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下第一种方式</span><br><span class="line">第一步：在集群中初始化一个长时间运行的Flink集群</span><br><span class="line">使用yarn-session.sh脚本</span><br><span class="line">第二步：使用flink run命令向Flink集群中提交任务</span><br><span class="line"></span><br><span class="line">注意：使用flink on yarn需要确保hadoop集群已经启动成功</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">下面来具体演示一下</span><br><span class="line">首先在bigdata04机器上安装一个Flink客户端，其实就是把Flink的安装包上传上去解压即可，不需要启动</span><br><span class="line"></span><br><span class="line">接下来在执行yarn-session.sh 脚本之前我们需要先设置 HADOOP_CLASSPATH这个环境变量，否则，执行</span><br><span class="line">yarn-session.sh 是会报错的，提示找不到hadoop的一些依赖。</span><br><span class="line"></span><br><span class="line">在&#x2F;etc&#x2F;profile中配置HADOOP_CLASSPATH</span><br><span class="line">[root@bigdata04 flink-1.11.1]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin</span><br><span class="line">export SPARK_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;spark-2.4.3-bin-hadoop2.7</span><br><span class="line">export SQOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;sqoop-1.4.7.bin__hadoop-2.6.0</span><br><span class="line">export HADOOP_CLASSPATH&#x3D;&#96;$&#123;HADOOP_HOME&#125;&#x2F;bin&#x2F;hadoop classpath&#96;</span><br><span class="line">export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$HIVE_HOME&#x2F;bin:$SPARK_HO</span><br><span class="line">ME&#x2F;bin:$SQOOP_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082349229.png" alt="image-20230408234907861"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来，使用 yarn-session.sh在YARN中创建一个长时间运行的Flink集群</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;yarn-session.sh -jm 1024m -tm 1024m -d</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个表示创建一个Flink集群，-jm 是指定主节点的内存，-tm是指定从节点的内存，-d是表示把这个进程放到后台去执行。</span><br><span class="line">启动之后，会看到类似这样的日志信息，这里面会显示flink web界面的地址，以及这个flink集群在yarn中对应的applicationid。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082347701.png" alt="image-20230408234705575"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时到YARN的web界面中确实可以看到这个flink集群。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082353556.png" alt="image-20230408235222041"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082354623.png" alt="image-20230408235427696"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以使用屏幕中显示的flink的web地址或者yarn中这个链接都是可以进入这个flink的web界面的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082353738.png" alt="image-20230408235310889"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来向这个Flink集群中提交任务，此时使用Flink中的内置案例</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink run .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这个时候我们使用flink run的时候，它会默认找这个文件，然后根据这个文件找到刚才我们创建的那个永久的Flink集群，这个文件里面保存的就是刚才启动的那个Flink集群在YARN中对应的applicationid。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082357866.png" alt="image-20230408235700425"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082357811.png" alt="image-20230408235714610"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">任务提交上去执行完成之后，再来看flink的web界面，发现这里面有一个已经执行结束的任务了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082359725.png" alt="image-20230408235924462"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这个任务在执行的时候，会动态申请一些资源执行任务，任务执行完毕之后，对应的资源会自动释放掉。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">最后把这个Flink集群停掉，使用yarn的kill命令</span><br><span class="line">[root@bigdata04 flink-1.11.1]# yarn application -kill application_17689063095</span><br><span class="line">2026-01-20 23:25:22,548 INFO client.RMProxy: Connecting to ResourceManager at </span><br><span class="line">Killing application application_1768906309581_0005</span><br><span class="line">2026-01-20 23:25:23,239 INFO impl.YarnClientImpl: Killed application_17689063</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对 yarn-session命令，它后面还支持一些其它参数，可以在后面传一个-help参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090002237.png" alt="image-20230409000251119"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这里的-j 是指定Flink任务的jar包，此参数可以省略不写也可以</span><br></pre></td></tr></table></figure><h5 id="Flink-ON-YARN第二种方式"><a href="#Flink-ON-YARN第二种方式" class="headerlink" title="Flink ON YARN第二种方式"></a>Flink ON YARN第二种方式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">flink run -m yarn-cluster(创建Flink集群+提交任务)</span><br><span class="line">使用flink run直接创建一个临时的Flink集群，并且提交任务</span><br><span class="line">此时这里面的参数前面加上了一个y参数</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink run -m yarn-cluster -yjm 1024 -ytm 1024 .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar</span><br><span class="line"></span><br><span class="line">提交上去之后，会先创建一个Flink集群，然后在这个Flink集群中执行任务。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090009823.png" alt="image-20230409000904789"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对Flink命令的一些用法汇总</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090010917.png" alt="image-20230409001026836"></p><h4 id="Flink-ON-YARN的好处"><a href="#Flink-ON-YARN的好处" class="headerlink" title="Flink ON YARN的好处"></a>Flink ON YARN的好处</h4> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1：提高大数据集群机器的利用率</span><br><span class="line">2：一套集群，可以执行MR任务，Spark任务，Flink任务等</span><br></pre></td></tr></table></figure><h3 id="向集群中提交Flink任务"><a href="#向集群中提交Flink任务" class="headerlink" title="向集群中提交Flink任务"></a>向集群中提交Flink任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来我们希望把前面我们自己开发的Flink任务提交到集群上面，在这我就使用flink on yarn的第二种方式来向集群提交一个Flink任务。</span><br><span class="line"></span><br><span class="line">第一步：在pom.xml中添加打包配置</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;!-- 编译插件 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;!-- scala编译插件 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;descriptorRefs&gt;</span><br><span class="line">                    &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                &lt;archive&gt;</span><br><span class="line">                    &lt;manifest&gt;</span><br><span class="line">                        &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                        &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                    &lt;&#x2F;manifest&gt;</span><br><span class="line">                &lt;&#x2F;archive&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">    &lt;&#x2F;plugins&gt;</span><br><span class="line">&lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：需要将Flink和Hadoop的相关依赖的score属性设置为provided，这些依赖不需要打进jar包里面。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">第二步：生成jar包： mvn clean package -DskipTests</span><br><span class="line">第三步：将 db_flink-1.0-SNAPSHOT-jar-with-dependencies.jar 上传到bigdata04机器上的 &#x2F;data&#x2F;sof</span><br><span class="line">t&#x2F;flink-1.11.1 目录中(上传到哪个目录都可以)</span><br><span class="line"></span><br><span class="line">第四步：提交Flink任务</span><br><span class="line">注意：提交任务之前，先开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001 </span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink run -m yarn-cluster -c com.imooc.scala.SocketWindowWordCountScala -yjm 1024 -ytm 1024 db_flink-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时到yarn上面可以看到确实新增了一个任务，点击进去可以看到flink的web界面</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090028085.png" alt="image-20230409002811696"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通过socket输入一串内容</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090028348.png" alt="image-20230409002838306"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090029943.png" alt="image-20230409002909775"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090029419.png" alt="image-20230409002934711"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090030195.png" alt="image-20230409003014134"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090030583.png" alt="image-20230409003040435"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090031669.png" alt="image-20230409003059283"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们希望把这个任务停掉，因为这个任务是一个流处理的任务，提交成功之后，它会一直运行。</span><br><span class="line">注意：此时如果我们使用ctrl+c关掉之前提交任务的那个进程，这里的flink任务是不会有任何影响的，可以一直运行，因为flink任务已经提交到hadoop集群里面了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090032922.png" alt="image-20230409003221865"></p><h4 id="停止任务"><a href="#停止任务" class="headerlink" title="停止任务"></a>停止任务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时如果想要停止Flink任务，有两种方式</span><br><span class="line"></span><br><span class="line">1：停止yarn中任务(这种是直接停止yarn中flink集群，任务也就停止了)</span><br><span class="line">[root@bigdata04 flink-1.11.1]# yarn application -kill application_17689629561</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2：停止flink任务</span><br><span class="line">可以在界面上点击这个按钮，或者在命令行中执行flink cancel停止都可以</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090035939.png" alt="image-20230409003528258"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">或者</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink cancel -yid application_176896295613 d7bo35cf4co10xxxxxxxxxxx(具体任务ID)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090039108.png" alt="image-20230409003917743"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">这个flink任务停止之后，对应的那个yarn-session（Flink集群）也就停止了。</span><br><span class="line"></span><br><span class="line">注意：此时flink任务停止之后就无法再查看flink的web界面了，如果想看查看历史任务的执行信息就看不了了，怎么办呢？</span><br><span class="line">咱们之前在学习spark的时候其实也遇到过这种问题，当时是通过启动spark的historyserver进程解决</span><br><span class="line">的。</span><br><span class="line">flink也有historyserver进程，也是可以解决这个问题的。</span><br><span class="line">historyserver进程可以在任意一台机器上启动，在这我们选择在bigdata04机器上启动</span><br><span class="line">在启动historyserver进程之前，需要先修改bigdata04中的flink-conf.yaml配置文件</span><br><span class="line"></span><br><span class="line">[root@bigdata04 flink-1.11.1]# vi conf&#x2F;flink-conf.yaml </span><br><span class="line">......</span><br><span class="line">jobmanager.archive.fs.dir: hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;completed-jobs&#x2F;</span><br><span class="line">historyserver.web.address: 192.168.182.103</span><br><span class="line">historyserver.web.port: 8082</span><br><span class="line">historyserver.archive.fs.dir: hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;completed-jobs&#x2F;</span><br><span class="line">historyserver.archive.fs.refresh-interval: 10000</span><br><span class="line">......</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">然后启动flink的historyserver进程</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;historyserver.sh start</span><br><span class="line"></span><br><span class="line">验证进程</span><br><span class="line">[root@bigdata04 flink-1.11.1]# jps</span><br><span class="line">5894 HistoryServer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：hadoop集群中的historyserver进程也需要启动</span><br><span class="line">在bigdata01、bigdata02、bigdata03节点上启动hadoop的historyserver进程</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br><span class="line">[root@bigdata02 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br><span class="line">[root@bigdata03 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时Flink任务停止之后也是可以访问flink的web界面的。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-初识Flink</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%88%9D%E8%AF%86Flink-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%88%9D%E8%AF%86Flink-1.html</id>
    <published>2023-04-07T16:52:17.000Z</published>
    <updated>2023-04-08T13:46:48.682Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-初识Flink"><a href="#第十六周-Flink极速上手篇-初识Flink" class="headerlink" title="第十六周 Flink极速上手篇-初识Flink"></a>第十六周 Flink极速上手篇-初识Flink</h1><h2 id="初识Flink"><a href="#初识Flink" class="headerlink" title="初识Flink"></a>初识Flink</h2><h3 id="什么是Flink"><a href="#什么是Flink" class="headerlink" title="什么是Flink"></a>什么是Flink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Apache Flink 是一个开源的分布式，高性能，高可用，准确的流处理框架。</span><br><span class="line">分布式：表示flink程序可以运行在很多台机器上，</span><br><span class="line">高性能：表示Flink处理性能比较高</span><br><span class="line">高可用：表示flink支持程序的自动重启机制。</span><br><span class="line">准确的：表示flink可以保证处理数据的准确性。</span><br><span class="line">Flink支持流处理和批处理，虽然我们刚才说了flink是一个流处理框架，但是它也支持批处理。</span><br><span class="line">其实对于flink而言，它是一个流处理框架，批处理只是流处理的一个极限特例而已。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080056558.png" alt="image-20230408005641755"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">左边是数据源，从这里面可以看出来，这些数据是实时产生的一些日志，或者是数据库、文件系统、kv存储系统中的数据。</span><br><span class="line">中间是Flink，负责对数据进行处理。</span><br><span class="line">右边是目的地，Flink可以将计算好的数据输出到其它应用中，或者存储系统中。</span><br></pre></td></tr></table></figure><h3 id="Flink架构图"><a href="#Flink架构图" class="headerlink" title="Flink架构图"></a>Flink架构图</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080059947.png" alt="image-20230408005900310"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">首先图片最下面表示是flink的一些部署模式，支持local，和集群(standalone，yarn)，也支持在云上部署。</span><br><span class="line"></span><br><span class="line">往上一层是flink的核心，分布式的流处理引擎。</span><br><span class="line"></span><br><span class="line">再往上面是flink的API和类库</span><br><span class="line">主要有两大块API，DataStream API和DataSet API，分别做流处理和批处理。</span><br><span class="line">针对DataStream API这块，支持复杂事件处理，和table操作，其实也是支持SQL操作的。</span><br><span class="line">针对Dataset API这块，支持flinkML机器学习，Gelly图计算，table操作，这块也是支持SQL操作的。</span><br><span class="line"></span><br><span class="line">其实从这可以看出来，Flink也是有自己的生态圈的，里面包含了实时计算、离线计算、机器学习、图计算、Table和SQL计算等等</span><br><span class="line">所以说它和Spark还是有点像的，不过它们两个的底层计算引擎是有本质区别的，一会我们会详细分析。</span><br></pre></td></tr></table></figure><h3 id="Flink三大核心组件"><a href="#Flink三大核心组件" class="headerlink" title="Flink三大核心组件"></a>Flink三大核心组件</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080113253.png" alt="image-20230408011332076"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink包含三大核心组件</span><br><span class="line">Data Source，数据源(负责接收数据)，</span><br><span class="line">Transformations 算子(负责对数据进行处理)</span><br><span class="line">Data Sink 输出组件(负责把计算好的数据输出到其它存储介质中)</span><br></pre></td></tr></table></figure><h3 id="Flink的流处理与批处理"><a href="#Flink的流处理与批处理" class="headerlink" title="Flink的流处理与批处理"></a>Flink的流处理与批处理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来分析一下Flink这个计算引擎的核心内容</span><br><span class="line"></span><br><span class="line">在大数据处理领域，批处理和流处理一般被认为是两种不同的任务，一个大数据框架一般会被设计为只能处理其中一种任务</span><br><span class="line"></span><br><span class="line">例如Storm只支持流处理任务，而MapReduce、Spark只支持批处理任务。Spark Streaming是Spark之上支持流处理任务的子系统，看似是一个特例，其实并不是——Spark Streaming采用了一种micro-batch的架构，就是把输入的数据流切分成细粒度的batch，并为每一个batch提交一个批处理的Spark任务，所以Spark Streaming本质上执行的还是批处理任务，和Storm这种流式的数据处理方式是完全不同的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Flink通过灵活的执行引擎，能够同时支持批处理和流处理</span><br><span class="line"></span><br><span class="line">在执行引擎这一层，流处理系统与批处理系统最大的不同在于节点之间的数据传输方式。</span><br><span class="line">对于一个流处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理</span><br><span class="line">这就是典型的一条一条处理</span><br><span class="line">而对于一个批处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满的时候，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点</span><br><span class="line">这两种数据传输模式是两个极端，对应的是流处理系统对低延迟的要求和批处理系统对高吞吐量的要求</span><br><span class="line">Flink的执行引擎采用了一种十分灵活的方式，同时支持了这两种数据传输模型</span><br><span class="line">Flink以固定的缓存块为单位进行网络数据传输，用户可以通过缓存块超时值指定缓存块的传输时机。如果缓存块的超时值为0，则Flink的数据传输方式类似前面所说的流处理系统的标准模型，此时系统可以获得最低的处理延迟</span><br><span class="line"></span><br><span class="line">如果缓存块的超时值为无限大，则Flink的数据传输方式类似前面所说的批处理系统的标准模型，此时系统可以获得最高的吞吐量</span><br><span class="line"></span><br><span class="line">这样就比较灵活了，其实底层还是流式计算模型，批处理只是一个极限特例而已。</span><br><span class="line">看一下这个图中显示的三种数据传输模型</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080127620.png" alt="image-20230408012754579"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个：一条一条处理</span><br><span class="line">第二个：一批一批处理</span><br><span class="line">第三个：按照缓存块进行处理，缓存块可以无限小，也可以无限大，这样就可以同时支持流处理和批处理了。</span><br></pre></td></tr></table></figure><h3 id="Storm-vs-SparkStreaming-vs-Flink"><a href="#Storm-vs-SparkStreaming-vs-Flink" class="headerlink" title="Storm vs SparkStreaming vs Flink"></a>Storm vs SparkStreaming vs Flink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来对比一下目前大数据领域中的三种实时计算引擎</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080133033.png" alt="image-20230408013308451"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">Native：表示来一条数据处理一条数据</span><br><span class="line">Mirco-Batch：表示划分小批，一小批一小批的处理数据</span><br><span class="line">组合式：表示是基础API，例如实现一个求和操作都需要写代码实现，比较麻烦，代码量会比较多。</span><br><span class="line">声明式：表示提供的是封装后的高阶函数，例如filter、count等函数，可以直接使用，比较方便，代码量比较少。</span><br></pre></td></tr></table></figure><h3 id="实时计算框架如何选择"><a href="#实时计算框架如何选择" class="headerlink" title="实时计算框架如何选择"></a>实时计算框架如何选择</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：需要关注流数据是否需要进行状态管理</span><br><span class="line">2：消息语义是否有特殊要求At-least-once或者Exectly-once</span><br><span class="line">3：小型独立的项目，需要低延迟的场景，建议使用Storm</span><br><span class="line">4：如果项目中已经使用了Spark，并且秒级别的实时处理可以满足需求，建议使用SparkStreaming</span><br><span class="line">5：要求消息语义为Exectly-once，数据量较大，要求高吞吐低延迟，需要进行状态管理，建议选择Flink</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-1.html</id>
    <published>2023-04-07T16:50:41.000Z</published>
    <updated>2023-04-07T16:50:41.816Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十三周 综合项目:电商数据仓库之商品订单数仓2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.html</id>
    <published>2023-04-05T14:30:33.000Z</published>
    <updated>2023-04-06T16:35:47.609Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十三周-综合项目-电商数据仓库之商品订单数仓2"><a href="#第十三周-综合项目-电商数据仓库之商品订单数仓2" class="headerlink" title="第十三周 综合项目:电商数据仓库之商品订单数仓2"></a>第十三周 综合项目:电商数据仓库之商品订单数仓2</h1><h2 id="拉链表"><a href="#拉链表" class="headerlink" title="拉链表"></a>拉链表</h2><h3 id="什么是拉链表"><a href="#什么是拉链表" class="headerlink" title="什么是拉链表"></a>什么是拉链表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对订单表、订单商品表，流水表，这些表中的数据是比较多的，如果使用全量的方式，会造成大量的数据冗余，浪费磁盘空间。</span><br><span class="line">所以这种表，一般使用增量的方式，每日采集新增的数据。</span><br><span class="line"></span><br><span class="line">在这注意一点：针对订单表，如果单纯的按照订单产生时间增量采集数据，是有问题的，因为用户可能今天下单，明天才支付，但是Hive是不支持数据更新的，这样虽然MySQL中订单的状态改变了，但是Hive中订单的状态还是之前的状态。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">想要解决这个问题，一般有这么几种方案：</span><br><span class="line">第一种：每天全量导入订单表的数据，这种方案在项目启动初期是没有多大问题的，因为前期数据量不大，但是随着项目的运营，订单量暴增，假设每天新增1亿订单，之前已经累积了100亿订单，如果每天都是全量导入的话，那也就意味着每天都需要把数据库中的100多亿订单数据导入到HDFS中保存一份，这样会极大的造成数据冗余，太浪费磁盘空间了。</span><br><span class="line">第二种：只保存当天的全量订单表数据，每次在导入之前，删除前一天保存的全量订单数据，这种方式虽然不会造成数据冗余，但是无法查询订单的历史状态，只有当前的最新状态，也不太好。</span><br><span class="line">第三种：拉链表，这种方式在普通增量导入方式的基础之上进行完善，把变化的数据也导入进来，这样既不会造成大量的数据冗余，还可以查询订单的历史状态。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">拉链表是针对数据仓库设计中表存储数据的方式而定义的，顾名思义，所谓拉链，就是记录历史。记录一个事物从开始，一直到当前状态的所有历史变化的信息。</span><br><span class="line"></span><br><span class="line">下面就是一张拉链表，存储的是用户的最基本信息以及每条记录的生命周期。我们可以使用这张表拿到当天的最新数据以及之前的历史数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061515178.png" alt="image-20230406151532551"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">说明：</span><br><span class="line">start_time表示该条记录的生命周期开始时间，end_time 表示该条记录的生命周期结束时间；</span><br><span class="line">end_time &#x3D; &#39;9999-12-31&#39;表示该条记录目前处于有效状态；</span><br><span class="line"></span><br><span class="line">如果查询当前所有有效的记录，则使用 SQL</span><br><span class="line">select * from user where end_time &#x3D;&#39;9999-12-31&#39;</span><br><span class="line"></span><br><span class="line">如果查询2026-01-02的历史快照【获取指定时间内的有效数据】，则使用SQL</span><br><span class="line">select * from user where start_time &lt;&#x3D; &#39;2026-01-02&#39; and end_time &gt;&#x3D; &#39;2026-01-02&#39;</span><br><span class="line"></span><br><span class="line">这就是拉链表。</span><br></pre></td></tr></table></figure><h3 id="如何制作拉链表"><a href="#如何制作拉链表" class="headerlink" title="如何制作拉链表"></a>如何制作拉链表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">那针对我们前面分析的订单表，希望使用拉链表的方式实现数据采集，因为每天都保存全量订单数据比较浪费磁盘空间，但是只采集增量的话无法反应订单的状态变化。</span><br><span class="line">所以需要既采集增量，还要采集订单状态变化了的数据。</span><br><span class="line">针对订单表中的订单状态字段有这么几个阶段</span><br><span class="line">未支付</span><br><span class="line">已支付</span><br><span class="line">未发货</span><br><span class="line">已发货</span><br><span class="line">在这我们先分析两种状态：未支付和已支付。</span><br><span class="line"></span><br><span class="line">我们先举个例子：</span><br><span class="line">假设我们的系统是2026年3月1日开始运营的</span><br><span class="line">那么到3月1日结束订单表所有数据如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061527784.png" alt="image-20230406152722507"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061528829.png" alt="image-20230406152831907"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">基于订单表中的这些数据如何制作拉链表？</span><br><span class="line"></span><br><span class="line">实现思路</span><br><span class="line">1：首先针对3月1号中的订单数据构建初始的拉链表，拉链表中需要有一个start_time(数据生效开始时间)和end_time(数据生效结束时间)，默认情况下start_time等于表中的创建时间，end_time初始化为一个无限大的日期9999-12-31</span><br><span class="line">将3月1号的订单数据导入到拉链表中。</span><br><span class="line">此时拉链表中数据如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061531144.png" alt="image-20230406153103346"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2：在3月2号的时候，需要将订单表中发生了变化的数据和新增的订单数据 整合到之前的拉链表中</span><br><span class="line">此时需要先创建一个每日更新表，将每日新增和变化了的数据保存到里面</span><br><span class="line"></span><br><span class="line">然后基于拉链表和这个每日更新表进行left join，根据订单id进行关联，如果可以关联上，就说明这个订单的状态发生了变化，然后将订单状态发生了变化的数据的end_time改为2026-03-01(当天日期-1天)</span><br><span class="line">然后再和每日更新表中的数据执行union all操作，将结果重新insert到拉链表中</span><br><span class="line"></span><br><span class="line">最终拉链表中的数据如下：</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932%5Cimage-20230406153453757.png" alt="image-20230406153453757"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">因为在3月2号的时候，订单id为001的数据的订单状态发生了变化，所以拉链表中订单id为001的原始数据的end_time需要修改为2026-03-01，</span><br><span class="line">然后需要新增一条订单id为001的数据，订单状态为已支付，start_time为2026-03-02，end_time为9999-12-31。</span><br><span class="line">还需要将3月2号新增的订单id为003的数据也添加进来。</span><br></pre></td></tr></table></figure><h3 id="【实战】基于订单表的拉链表实现"><a href="#【实战】基于订单表的拉链表实现" class="headerlink" title="【实战】基于订单表的拉链表实现"></a>【实战】基于订单表的拉链表实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面我们开始实现：</span><br><span class="line">1：首先初始化2026-03-01、2026-03-02和2026-03-03的订单表新增和变化的数据，ods_user_order(直接将数据初始化到HDFS中)，这个表其实就是前面我们所说的每日更新表</span><br><span class="line"></span><br><span class="line">注意：这里模拟使用sqoop从mysql中抽取新增和变化的数据，根据order_date和update_time这两个字段获取这些数据，所以此时ods_user_order中的数据就是每日的新增和变化了的数据。</span><br><span class="line"></span><br><span class="line">执行代码生成数据：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ods_user_order在前面已经使用过，所以在这只需要将2026-03-01、2026-03-02和2026-03-03的数据加载进去即可</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061542631.png" alt="image-20230406154210239"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061543916.png" alt="image-20230406154346557"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061544315.png" alt="image-20230406154427193"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_order <span class="keyword">partition</span>(dt=<span class="string">'20260301'</span>)  <span class="keyword">select</span> </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time</span><br><span class="line"><span class="keyword">from</span> ods_mall.ods_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260301'</span> <span class="keyword">and</span> order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_order <span class="keyword">partition</span>(dt=<span class="string">'20260302'</span>)  <span class="keyword">select</span> </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time</span><br><span class="line"><span class="keyword">from</span> ods_mall.ods_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260302'</span> <span class="keyword">and</span> order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_order <span class="keyword">partition</span>(dt=<span class="string">'20260303'</span>)  <span class="keyword">select</span> </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time</span><br><span class="line"><span class="keyword">from</span> ods_mall.ods_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260303'</span> <span class="keyword">and</span> order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061552760.png" alt="image-20230406155251457"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061553441.png" alt="image-20230406155334317"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061553044.png" alt="image-20230406155350251"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2：创建拉链表，基于每日更新订单表构建拉链表中的数据</span><br><span class="line">创建拉链表：dws_user_order_zip</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_order_zip(</span><br><span class="line">   order_id    <span class="built_in">bigint</span>,</span><br><span class="line">   order_date    <span class="keyword">string</span>,</span><br><span class="line">   user_id    <span class="built_in">bigint</span>,</span><br><span class="line">   order_money    <span class="keyword">double</span>,</span><br><span class="line">   order_type    <span class="built_in">int</span>,</span><br><span class="line">   order_status    <span class="built_in">int</span>,</span><br><span class="line">   pay_id    <span class="built_in">bigint</span>,</span><br><span class="line">   update_time    <span class="keyword">string</span>,</span><br><span class="line">   start_time    <span class="keyword">string</span>,</span><br><span class="line">   end_time    <span class="keyword">string</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_order_zip/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_order_zip</span><br><span class="line"><span class="keyword">select</span> *</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">       duoz.order_id,</span><br><span class="line">       duoz.order_date,</span><br><span class="line">       duoz.user_id,</span><br><span class="line">       duoz.order_money,</span><br><span class="line">       duoz.order_type,</span><br><span class="line">       duoz.order_status,</span><br><span class="line">       duoz.pay_id,</span><br><span class="line">       duoz.update_time,</span><br><span class="line">       duoz.start_time,</span><br><span class="line">       <span class="keyword">case</span></span><br><span class="line">           <span class="keyword">when</span> duoz.end_time = <span class="string">'9999-12-31'</span> <span class="keyword">and</span> duo.order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">date_add</span>(<span class="string">'2026-03-01'</span>,<span class="number">-1</span>)</span><br><span class="line">           <span class="keyword">else</span> duoz.end_time</span><br><span class="line">       <span class="keyword">end</span> <span class="keyword">as</span> end_time</span><br><span class="line">    <span class="keyword">from</span> dws_mall.dws_user_order_zip <span class="keyword">as</span> duoz</span><br><span class="line">    <span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span> order_id <span class="keyword">from</span> dwd_mall.dwd_user_order</span><br><span class="line">        <span class="keyword">where</span> dt = <span class="string">'20260301'</span></span><br><span class="line">    ) <span class="keyword">as</span> duo</span><br><span class="line">    <span class="keyword">on</span> duoz.order_id = duo.order_id</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">   <span class="keyword">select</span></span><br><span class="line">       duo.order_id,</span><br><span class="line">       duo.order_date,</span><br><span class="line">       duo.user_id,</span><br><span class="line">       duo.order_money,</span><br><span class="line">       duo.order_type,</span><br><span class="line">       duo.order_status,</span><br><span class="line">       duo.pay_id,</span><br><span class="line">       duo.update_time,</span><br><span class="line">       <span class="string">'2026-03-01'</span> <span class="keyword">as</span> start_time,</span><br><span class="line">       <span class="string">'9999-12-31'</span> <span class="keyword">as</span> end_time</span><br><span class="line">   <span class="keyword">from</span> dwd_mall.dwd_user_order <span class="keyword">as</span> duo</span><br><span class="line">   <span class="keyword">where</span> duo.dt = <span class="string">'20260301'</span></span><br><span class="line">) <span class="keyword">as</span> t;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在windows中编写sql语句，回车时可能会有tab制表符，这样的sql复制到hive中执行会报错；对于复制到hive中连成一行无法解析的情况，可以在复制前给每一行的最后添加一个空格</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932%5Cimage-20230406163700653.png" alt="image-20230406163700653"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061642146.png" alt="image-20230406164241911"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql语句里的时间改成20260302后再执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061641683.png" alt="image-20230406164129176"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061642440.png" alt="image-20230406164251390"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061645394.png" alt="image-20230406164509353"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql语句里的时间改成20260303后再执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061648636.png" alt="image-20230406164804417"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061649776.png" alt="image-20230406164904719"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061710378.png" alt="image-20230406171011072"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061710744.png" alt="image-20230406171046707"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061711261.png" alt="image-20230406171146204"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查询有效数据</span><br><span class="line">查询切片数据</span><br><span class="line">查询一条订单历史记录</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061658975.png" alt="image-20230406165823692"></p><h3 id="拉链表的性能问题-面试爱问"><a href="#拉链表的性能问题-面试爱问" class="headerlink" title="拉链表的性能问题(面试爱问)"></a>拉链表的性能问题(面试爱问)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">拉链表也会遇到查询性能的问题，假设我们存放了5年的拉链数据，那么这张表势必会比较大，当查询的时候性能就比较低了</span><br><span class="line">可以用以下思路来解决：</span><br><span class="line">1. 可以尝试对start_time和end_time做索引，这样可以提高一些性能。</span><br><span class="line">2. 保留部分历史数据，我们可以在一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">还有什么是拉链表，如何制作拉链表也爱问</span><br></pre></td></tr></table></figure><h2 id="商品订单数据数仓总结"><a href="#商品订单数据数仓总结" class="headerlink" title="商品订单数据数仓总结"></a>商品订单数据数仓总结</h2><h3 id="数据库和表梳理"><a href="#数据库和表梳理" class="headerlink" title="数据库和表梳理"></a>数据库和表梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062102932.png" alt="image-20230406210206233"></p><h3 id="任务脚本梳理"><a href="#任务脚本梳理" class="headerlink" title="任务脚本梳理"></a>任务脚本梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062103230.png" alt="image-20230406210336400"></p><h2 id="数据可视化和任务调度实现"><a href="#数据可视化和任务调度实现" class="headerlink" title="数据可视化和任务调度实现"></a>数据可视化和任务调度实现</h2><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">数据可视化这块不是项目的重点，不过为了让大家能有一个更加直观的感受，我们可以选择一些现成的数据可视化工具实现。</span><br><span class="line">咱们前面分析过，想要查询hive中的数据可以使用hue，不过hue无法自动生成图表。</span><br><span class="line">所以我们可以考虑使用Zeppelin(还可以操作spark,flink)</span><br><span class="line">针对一些复杂的图表，可以选择定制开发，使用echarts等组件实现</span><br></pre></td></tr></table></figure><h4 id="Zeppelin安装部署"><a href="#Zeppelin安装部署" class="headerlink" title="Zeppelin安装部署"></a>Zeppelin安装部署</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：不要使用Zeppelin0.8.2版本，这个版本有bug，无法使用图形展现数据。</span><br><span class="line">在这我们使用zeppelin-0.9.0-preview1这个版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">把下载好的安装包上传到bigdata04的&#x2F;data&#x2F;soft目录中</span><br><span class="line">1：解压</span><br><span class="line"></span><br><span class="line">2：修改配置</span><br><span class="line">[root@bigdata04 soft]# cd zeppelin-0.9.0-preview1-bin-all&#x2F;conf</span><br><span class="line">[root@bigdata04 conf]# mv zeppelin-env.sh.template zeppelin-env.sh</span><br><span class="line">[root@bigdata04 conf]# mv zeppelin-site.xml.template zeppelin-site.xml</span><br><span class="line">[root@bigdata04 conf]# vi zeppelin-site.xml</span><br><span class="line"># 将默认的127.0.0.1改为0.0.0.0 否则默认情况下只能在本机访问zeppline</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;zeppelin.server.addr&lt;&#x2F;name&gt;</span><br><span class="line"> &lt;value&gt;0.0.0.0&lt;&#x2F;value&gt;</span><br><span class="line"> &lt;description&gt;Server binding address&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3：增加Hive依赖jar包</span><br><span class="line">由于我们需要使用Zepplien连接hive，它里面默认没有集成Hive的依赖jar包，所以最简单的方式就是将Hive的lib目录中的所有jar包全复制到Zeppline中的interpreter&#x2F;jdbc目录中</span><br><span class="line">[root@bigdata04 zeppelin-0.9.0-preview1-bin-all]# cd interpreter&#x2F;jdbc</span><br><span class="line">[root@bigdata04 jdbc]# cp &#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;lib&#x2F;*.jar .</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4：启动</span><br><span class="line">[root@bigdata04 zeppelin-0.9.0-preview1-bin-all]# bin&#x2F;zeppelin-daemon.sh start</span><br><span class="line">5：停止【需要停止的时候传递stop命令即可】</span><br><span class="line">[root@bigdata04 zeppelin-0.9.0-preview1-bin-all]# bin&#x2F;zeppelin-daemon.sh stop</span><br></pre></td></tr></table></figure><h4 id="Zepplin的界面参数配置"><a href="#Zepplin的界面参数配置" class="headerlink" title="Zepplin的界面参数配置"></a>Zepplin的界面参数配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Zepplin启动之后可以通过8080端口进行访问</span><br><span class="line">http:&#x2F;&#x2F;bigdata04:8080&#x2F;</span><br><span class="line"></span><br><span class="line">在使用之前需要先配置hive的基本信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062115317.png" alt="image-20230406211509288"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">搜索jdbc</span><br><span class="line"></span><br><span class="line">点击edit修改里面的参数信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062115096.png" alt="image-20230406211538750"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">修改这四项的内容即可，这里的内容其实就是我们之前学习hive的jdbc操作时指定的参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062118908.png" alt="image-20230406211809935"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：需要在192.168.182.103这台机器上启动hiveserver2服务，否则在zeppline中连不上hive</span><br><span class="line">最后点save按钮即可Zepplin的使用</span><br><span class="line">创建一个note，类似于工作台的概念</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin</span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;hiveserver2</span><br></pre></td></tr></table></figure><h4 id="Zepplin的使用"><a href="#Zepplin的使用" class="headerlink" title="Zepplin的使用"></a>Zepplin的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建一个note，类似于工作台的概念</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062119442.png" alt="image-20230406211917306"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时就可以在里面写SQL了。</span><br><span class="line">如果以图形的形式展示结果，则点击这里面对应图形的图标即可。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062121862.png" alt="image-20230406212125211"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062124210.png" alt="image-20230406212432043"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于想制作一些复杂的图表，可以用sqoop导出到mysql，让前端的人去制作。这里当然也很方便直接与hive交互</span><br></pre></td></tr></table></figure><h3 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">针对数据仓库中的任务脚本我们前面已经整理过了，任务脚本还是比较多的，针对初始化表的脚本只需要</span><br><span class="line">执行一次即可，其它的脚本需要每天都执行一次，这个时候就需要涉及到任务定时调度了。</span><br></pre></td></tr></table></figure><h4 id="Crontab调度器的使用"><a href="#Crontab调度器的使用" class="headerlink" title="Crontab调度器的使用"></a>Crontab调度器的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">咱们前面在学习Linux的时候学过一个crontab调度器，通过它可以实现定时执行指定的脚本。</span><br><span class="line">针对我们这个数据仓库中的这些脚本使用crontab进行调度是可以的</span><br><span class="line">但是需要注意一点：这些任务之间是有一些依赖关系的，从大的层面上来说，dwd层的任务需要等ods层的任务执行成功之后才能开始执行</span><br><span class="line"></span><br><span class="line">那crontab如何知道任务之间的依赖关系呢？</span><br><span class="line">crontab是无法知道任务之间的依赖关系的，我们只能间接实现</span><br><span class="line">举个例子：针对MapReduce任务和Spark任务，任务执行成功之后，在输出目录中会有一个success标记文件，这个文件表示这个任务成功的执行结束了。</span><br><span class="line"></span><br><span class="line">此时如果我们使用crontab调度两个job，一个jobA，一个jobB，先执行jobA，jobA成功执行结束之后才能执行jobB，这个时候我们就需要在脚本中添加一个判断，判断jobA的结果输出目录中是否存在success文件，如果存在则继续执行jobB，否则不执行，并且告警，提示jobA任务执行失败。</span><br><span class="line"></span><br><span class="line">那我们现在执行的是hive的sql任务，sql任务最终不会在输出目录中产生success文件，所以没有办法使用这个标记文件进行判断，不过sql任务会产生数据文件，数据文件的文件名是类似000000_0这样的，可能会有多个，具体的个数是由任务中的reduce的个数决定的，我们也可以选择判断这个数据文件是否存在来判断任务是否成功执行结束。</span><br><span class="line"></span><br><span class="line">注意了：针对SQL任务虽然可以通过判断数据文件来判定任务是否执行成功，不过这种方式不能保证100%的准确率，有可能会存在这种情况，任务确实执行成功了，但是在向结果目录中写数据文件的时候，写了一半，由于网络原因导致数据没有写完，但是我们过来判断000000_0这个文件肯定是存在的，那我们就误以为这个任务执行成功了，其实它的数据是缺失一部分的，不过这种情况的概率极低，可以忽略不计。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时使用crontab调度两个有依赖关系的任务，脚本该如何实现呢？</span><br><span class="line">在bigdata04机器中创建 &#x2F;data&#x2F;soft&#x2F;warehouse_job 目录</span><br><span class="line">创建脚本： crontab_job_schedule.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行jobA</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 先删除jobA的输出目录</span></span><br><span class="line">hdfs dfs -rm -r hdfs://bigdata01:9000/data/dwd/user_addr/dt=$&#123;dt&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dwd_mall.dwd_user_addr partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   addr_id,</span><br><span class="line">   user_id,</span><br><span class="line">   addr_name,</span><br><span class="line">   order_flag,</span><br><span class="line">   user_name,</span><br><span class="line">   mobile</span><br><span class="line">from ods_mall.ods_user_addr</span><br><span class="line">where dt = '$&#123;dt&#125;' and addr_id is not null;</span><br><span class="line">"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">如果jobA执行成功，这条查询命令就可以成功执行，否则在执行的时候会报错</span></span><br><span class="line">hdfs dfs -ls hdfs://bigdata01:9000/data/dwd/user_addr/dt=$&#123;dt&#125;/000000_0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在这里通过$?来判断上一条命令是否成功执行，如果$?的返回值为0，则表示jobA执行成功，否则表示jobA执行失败</span></span><br><span class="line">if [ $? = 0 ]</span><br><span class="line">then</span><br><span class="line">    echo "执行jobB"</span><br><span class="line">else</span><br><span class="line">    # 可以在这里发短息或者发邮件</span><br><span class="line">    echo "jobA执行失败，请处理..."</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果jobA成功执行，则jobB也可以执行，如果jobA执行失败，则jobB不会执行，脚本会生成告警信息。</span><br><span class="line"></span><br><span class="line">这就是使用crontab调度器如何实现任务依赖的功能。</span><br><span class="line">如果项目中的定时任务有很多，使用crontab虽然可以实现任务依赖的功能，但是管理起来不方便，crontab没有提供图形化的界面，使用起来比较麻烦</span><br><span class="line">针对一些简单的定时任务的配置，并且任务比较少的情况下使用crontab是比较方便的。</span><br></pre></td></tr></table></figure><h4 id="Azkaban调度器的使用"><a href="#Azkaban调度器的使用" class="headerlink" title="Azkaban调度器的使用"></a>Azkaban调度器的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于定时执行的脚本很多，可以使用可视化管理界面的任务调度工具进行管理：azkaban(轻量级),Ooize(重量级)</span><br></pre></td></tr></table></figure><h5 id="Azkaban介绍"><a href="#Azkaban介绍" class="headerlink" title="Azkaban介绍"></a>Azkaban介绍</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在实际工作中如果需要配置很多的定时任务，一般会采用一些支持界面操作的调度器，例如：Azkaban、Ooize</span><br><span class="line">Azkaban是一个轻量级的调度器，使用起来比较简单，容易上手。</span><br><span class="line">Ooize是一个重量级的调度器，使用起来相对比较复杂。</span><br><span class="line">在这里我们主要考虑易用性，所以我们选择使用Azkaban。</span><br><span class="line">下面我们来快速了解一下Azkaban，以及它的用法。</span><br><span class="line">Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。</span><br><span class="line">Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">那我们首先把Azkaban安装部署起来</span><br><span class="line">官网下载的Azkaban是源码，需要编译，编译过程会非常慢，主要是由于国外网络原因导致的。</span><br><span class="line">在这我给大家直接提供一个编译好的Azkaban。</span><br><span class="line">为了简化Azakan的安装过程，不让大家在工具的安装上花费太多时间，在这里我们就使用Azkaban的solo模式部署。</span><br><span class="line"></span><br><span class="line">solo模式属于单机模式，那对应的Azkaban也支持在多台机器上运行。</span><br><span class="line">solo模式的优点：</span><br><span class="line">易于安装：无需MySQL示例。它将H2打包为主要的持久存储。</span><br><span class="line">易于启动：Web服务器和执行程序服务器都在同一个进程中运行。</span><br><span class="line">全功能：它包含所有Azkaban功能。</span><br></pre></td></tr></table></figure><h5 id="Azkaban安装部署"><a href="#Azkaban安装部署" class="headerlink" title="Azkaban安装部署"></a>Azkaban安装部署</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">把编译后的Azkaban安装包 azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz 上传到bigdata04的&#x2F;data&#x2F;soft目录下</span><br><span class="line">1：解压</span><br><span class="line"></span><br><span class="line">2：修改Azkaban的时区为上海时区</span><br><span class="line">我们的bigdata04机器在创建的时候已经指定了时区为上海时区</span><br><span class="line">要保证bigdata04机器的时区和Azkaban的时区是一致的。</span><br><span class="line">[root@bigdata04 azkaban-solo-server-0.1.0-SNAPSHOT]# cd conf&#x2F;</span><br><span class="line">[root@bigdata04 conf]# vi azkaban.properties</span><br><span class="line">......</span><br><span class="line">default.timezone.id&#x3D;Asia&#x2F;Shanghai</span><br><span class="line">......</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3：启动</span><br><span class="line">[root@bigdata04 azkaban-solo-server-0.1.0-SNAPSHOT]# bin&#x2F;azkaban-solo-start.sh</span><br><span class="line"></span><br><span class="line">4：停止</span><br><span class="line">[root@bigdata04 azkaban-solo-server-0.1.0-SNAPSHOT]# bin&#x2F;azkaban-solo-stop.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启动成功之后，azkaban会启动一个web界面，监听的端口是8081</span><br><span class="line">http:&#x2F;&#x2F;bigdata04:8081&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062150906.png" alt="image-20230406215038265"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户名和密码默认都是 azkaban</span><br></pre></td></tr></table></figure><h5 id="提交独立任务"><a href="#提交独立任务" class="headerlink" title="提交独立任务"></a>提交独立任务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">如何向Azkaban中提交任务呢？</span><br><span class="line">1：先创建一个Project</span><br><span class="line"></span><br><span class="line">2：向test项目中提交任务</span><br><span class="line">先演示一个独立的任务</span><br><span class="line">创建一个文件hello.job，文件内容如下：</span><br><span class="line"># hello.job</span><br><span class="line">type&#x3D;command</span><br><span class="line">command&#x3D;echo &quot;Hello World!</span><br><span class="line"></span><br><span class="line">这里面的#号开头表示是注释</span><br><span class="line">type：任务类型，这里的command表示这个任务执行的是一个命令</span><br><span class="line">command：这里的command是指具体的命令</span><br><span class="line">将hello.job文件添加到一个zip压缩文件中，hello.zip。</span><br><span class="line"></span><br><span class="line">将hello.zip压缩包提交到刚才创建的test项目中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062155170.png" alt="image-20230406215538918"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时test项目中就包含hello这个任务，点击execute flow执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062156281.png" alt="image-20230406215604662"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062156394.png" alt="image-20230406215635048"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062156493.png" alt="image-20230406215648313"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时这个任务执行一次就结束了，如果想要让任务定时执行，可以在这配置。</span><br><span class="line">先进入test项目中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062200802.png" alt="image-20230406220045801"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062201588.png" alt="image-20230406220101356"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击Schedule，进入配置定时信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062201092.png" alt="image-20230406220133705"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062201869.png" alt="image-20230406220150848"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这就将test项目中的hello.job配置了定时任务。</span><br><span class="line">其实我们会发现这里面的定时任务的配置格式和crontab中的一样。</span><br><span class="line">后期如果想查看这个定时任务的执行情况可以点击hello</span><br><span class="line"></span><br><span class="line">点击executions查看任务的执行列表</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062202682.png" alt="image-20230406220233640"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062202551.png" alt="image-20230406220252870"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果想查看某一次任务的具体执行情况，可以点击execution id</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是一个基本的独立任务的执行，如果是有依赖的多个任务，如何配置呢？</span><br></pre></td></tr></table></figure><h5 id="提交依赖任务"><a href="#提交依赖任务" class="headerlink" title="提交依赖任务"></a>提交依赖任务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">下面再看一个例子：</span><br><span class="line">1：先创建一个project：depen_test</span><br><span class="line"></span><br><span class="line">2：向depen_test项目中提交任务</span><br><span class="line">先创建一个文件first.job</span><br><span class="line"># first.job</span><br><span class="line">type&#x3D;command</span><br><span class="line">command&#x3D;echo &quot;Hello First!&quot;</span><br><span class="line">再创建一个文件second.job</span><br><span class="line"># second.job</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;first</span><br><span class="line">command&#x3D;echo &quot;Hello Second!&quot;</span><br><span class="line"></span><br><span class="line">这里面通过dependencies属性指定了任务的依赖关系，后面的first表示依赖的任务的文件名称</span><br><span class="line"></span><br><span class="line">最后将这两个job文件打成一个zip压缩包</span><br><span class="line">将这个压缩包上传到项目里面</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062206761.png" alt="image-20230406220617754"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">点击Execute Flow，也可以看到任务之间的依赖关系</span><br><span class="line">点击Execute</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062206575.png" alt="image-20230406220640754"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后查看jobList，可以看到这个项目中的两个任务的执行顺序</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062208908.png" alt="image-20230406220807410"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这就是带依赖的任务的任务调度。</span><br></pre></td></tr></table></figure><h5 id="在数仓中使用Azkaban"><a href="#在数仓中使用Azkaban" class="headerlink" title="在数仓中使用Azkaban"></a>在数仓中使用Azkaban</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下针对数仓中的多级任务依赖，如何使用Azkaban实现</span><br><span class="line">以统计电商GMV为例：</span><br><span class="line">这个指标需要依赖于这个流程 MySQL--&gt;HDFS--&gt;ODS--&gt;DWD--&gt;APP</span><br><span class="line">MySQL–&gt;HDFS需要使用Sqoop脚本</span><br><span class="line">HDFS–&gt;ODS需要使用hive alter命令</span><br><span class="line">ODS–&gt;DWD需要使用hive的SQL</span><br><span class="line">DWD–&gt;APP需要使用hive的SQL</span><br><span class="line">接下来开发对应的azkaban job</span><br></pre></td></tr></table></figure><h6 id="collect-job"><a href="#collect-job" class="headerlink" title="collect.job"></a>collect.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># collect.job</span><br><span class="line"># 采集MySQL中的数据至HDFS</span><br><span class="line">type&#x3D;command</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;collect_mysql.sh</span><br></pre></td></tr></table></figure><h6 id="ods-job"><a href="#ods-job" class="headerlink" title="ods.job"></a>ods.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ods.job</span><br><span class="line"># 关联ods层的数据</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;collect</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;ods_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h6 id="dwd-job"><a href="#dwd-job" class="headerlink" title="dwd.job"></a>dwd.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># dwd.job</span><br><span class="line"># 生成dwd层的数据</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;ods</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;dwd_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h6 id="app-job"><a href="#app-job" class="headerlink" title="app.job"></a>app.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># app.job</span><br><span class="line"># 生成app层的数据</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;dwd</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;app_mall_add_partition_2.sh</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062218260.png" alt="image-20230406221808936"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">还要把job文件里涉及的sh脚本上传到linux上指定目录</span><br><span class="line"></span><br><span class="line">然后配置定时任务</span><br></pre></td></tr></table></figure><h2 id="项目优化"><a href="#项目优化" class="headerlink" title="项目优化"></a>项目优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Sqoop数据采集参数调优，它只会生成一个map任务，性能较低</span><br><span class="line"></span><br><span class="line">集群Queue队列优化</span><br><span class="line"></span><br><span class="line">避免任务启动时间过于集中</span><br><span class="line"></span><br><span class="line">Hive on Tez</span><br><span class="line"></span><br><span class="line">Impala提供快速交互查询</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十三周 综合项目:电商数据仓库之商品订单数仓</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%93.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%93.html</id>
    <published>2023-04-05T14:30:13.000Z</published>
    <updated>2023-04-05T17:36:04.999Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十三周-综合项目-电商数据仓库之商品订单数仓"><a href="#第十三周-综合项目-电商数据仓库之商品订单数仓" class="headerlink" title="第十三周 综合项目:电商数据仓库之商品订单数仓"></a>第十三周 综合项目:电商数据仓库之商品订单数仓</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">和之前用户行为数仓构建步骤一样，先对下面两层进行构建，上面两层基于业务需求来构建这两层的表</span><br><span class="line"></span><br><span class="line">服务端数据在mysql中的表如下：(已通过sqoop抽取到hdfs上，只需要创建表将数据关联起来就可以了)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052249257.png" alt="image-20230405224934100"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs上商品订单数据相关目录</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052309451.png" alt="image-20230405230954102"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052310213.png" alt="image-20230405231024155"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052312876.png" alt="image-20230405231205937"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052313424.png" alt="image-20230405231347893"></p><h2 id="ods层"><a href="#ods层" class="headerlink" title="ods层"></a>ods层</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052251060.png" alt="image-20230405225123910"></p><h3 id="建表脚本"><a href="#建表脚本" class="headerlink" title="建表脚本"></a>建表脚本</h3><h4 id="ods-mall-init-table-sh"><a href="#ods-mall-init-table-sh" class="headerlink" title="ods_mall_init_table.sh"></a>ods_mall_init_table.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ods层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists ods_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists ods_mall.ods_user(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   user_gender          tinyint,</span><br><span class="line">   user_birthday        string,</span><br><span class="line">   e_mail               string,</span><br><span class="line">   mobile               string,</span><br><span class="line">   register_time        string,</span><br><span class="line">   is_blacklist         tinyint</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/user/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_user_extend(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   is_pregnant_woman    tinyint,</span><br><span class="line">   is_have_children     tinyint,</span><br><span class="line">   is_have_car          tinyint,</span><br><span class="line">   phone_brand          string,</span><br><span class="line">   phone_cnt            int,</span><br><span class="line">   change_phone_cnt     int,</span><br><span class="line">   weight               int,</span><br><span class="line">   height               int</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/user_extend/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_user_addr(</span><br><span class="line">   addr_id              bigint,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   addr_name            string,</span><br><span class="line">   order_flag           tinyint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   mobile               string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/user_addr/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_goods_info(</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_no             string,</span><br><span class="line">   goods_name           string,</span><br><span class="line">   curr_price           double,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   goods_desc           string,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/goods_info/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_category_code(</span><br><span class="line">   first_category_id    int,</span><br><span class="line">   first_category_name  string,</span><br><span class="line">   second_category_id   int,</span><br><span class="line">   second_catery_name   string,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   third_category_name  string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/category_code/';</span><br><span class="line"></span><br><span class="line">create external table if not exists ods_mall.ods_user_order(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   order_date           string,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   order_money          double,</span><br><span class="line">   order_type           int,</span><br><span class="line">   order_status         int,</span><br><span class="line">   pay_id               bigint,</span><br><span class="line">   update_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/user_order/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_order_item(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_amount         int,</span><br><span class="line">   curr_price           double,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/order_item/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_order_delivery(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   addr_id              bigint,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   carriage_money       double,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/order_delivery/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_payment_flow(</span><br><span class="line">   pay_id               bigint,</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   trade_no             bigint,</span><br><span class="line">   pay_money            double,</span><br><span class="line">   pay_type             int,</span><br><span class="line">   pay_time             string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/payment_flow/';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h4 id="ods-mall-add-partition-sh"><a href="#ods-mall-add-partition-sh" class="headerlink" title="ods_mall_add_partition.sh"></a>ods_mall_add_partition.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 给ods层的表添加分区，这个脚本后期每天执行一次</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨，添加昨天的分区，添加完分区之后，再执行后面的计算脚本</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_user add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_user_extend add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_user_addr add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_goods_info add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_category_code add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_user_order add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_order_item add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_order_delivery add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_payment_flow add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里用了开发用户行为数仓时用的添加分区通用脚本</span></span><br><span class="line">sh add_partition.sh ods_mall.ods_user $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_user_extend $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_user_addr $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_goods_info $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_category_code $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_user_order $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_order_item $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_order_delivery $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_payment_flow $&#123;dt&#125; $&#123;dt&#125;</span><br></pre></td></tr></table></figure><h2 id="dwd层"><a href="#dwd层" class="headerlink" title="dwd层"></a>dwd层</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其实可以直接对ods中对应的表进行抽取字段，但为了保险起见，对各个表中的id字段(是各个表的主键)进行过滤，因为数据采集过程中可能id字段会出现问题</span><br></pre></td></tr></table></figure><h3 id="建表脚本-1"><a href="#建表脚本-1" class="headerlink" title="建表脚本"></a>建表脚本</h3><h4 id="dwd-mall-init-table-sh"><a href="#dwd-mall-init-table-sh" class="headerlink" title="dwd_mall_init_table.sh"></a>dwd_mall_init_table.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dwd层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dwd_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_user(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   user_gender          tinyint,</span><br><span class="line">   user_birthday        string,</span><br><span class="line">   e_mail               string,</span><br><span class="line">   mobile               string,</span><br><span class="line">   register_time        string,</span><br><span class="line">   is_blacklist         tinyint</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/user/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists dwd_mall.dwd_user_extend(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   is_pregnant_woman    tinyint,</span><br><span class="line">   is_have_children     tinyint,</span><br><span class="line">   is_have_car          tinyint,</span><br><span class="line">   phone_brand          string,</span><br><span class="line">   phone_cnt            int,</span><br><span class="line">   change_phone_cnt     int,</span><br><span class="line">   weight               int,</span><br><span class="line">   height               int</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/user_extend/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_user_addr(</span><br><span class="line">   addr_id              bigint,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   addr_name            string,</span><br><span class="line">   order_flag           tinyint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   mobile               string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/user_addr/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists dwd_mall.dwd_goods_info(</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_no             string,</span><br><span class="line">   goods_name           string,</span><br><span class="line">   curr_price           double,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   goods_desc           string,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/goods_info/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists dwd_mall.dwd_category_code(</span><br><span class="line">   first_category_id    int,</span><br><span class="line">   first_category_name  string,</span><br><span class="line">   second_category_id   int,</span><br><span class="line">   second_catery_name   string,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   third_category_name  string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/category_code/';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_user_order(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   order_date           string,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   order_money          double,</span><br><span class="line">   order_type           int,</span><br><span class="line">   order_status         int,</span><br><span class="line">   pay_id               bigint,</span><br><span class="line">   update_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/user_order/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_order_item(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_amount         int,</span><br><span class="line">   curr_price           double,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/order_item/';</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists dwd_mall.dwd_order_delivery(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   addr_id              bigint,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   carriage_money       double,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/order_delivery/';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_payment_flow(</span><br><span class="line">   pay_id               bigint,</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   trade_no             bigint,</span><br><span class="line">   pay_money            double,</span><br><span class="line">   pay_type             int,</span><br><span class="line">   pay_time             string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited   </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/payment_flow/';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h4 id="dwd-mall-add-partition-sh"><a href="#dwd-mall-add-partition-sh" class="headerlink" title="dwd_mall_add_partition.sh"></a>dwd_mall_add_partition.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 基于ods层的表进行清洗，将清洗之后的数据添加到dwd层对应表的对应分区中</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_user partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   user_id,</span><br><span class="line">   user_name,</span><br><span class="line">   user_gender,</span><br><span class="line">   user_birthday,</span><br><span class="line">   e_mail,</span><br><span class="line">   mobile,</span><br><span class="line">   register_time,</span><br><span class="line">   is_blacklist</span><br><span class="line">from ods_mall.ods_user</span><br><span class="line">where dt = '$&#123;dt&#125;' and user_id is not null;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_user_extend partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   user_id,</span><br><span class="line">   is_pregnant_woman,</span><br><span class="line">   is_have_children,</span><br><span class="line">   is_have_car,</span><br><span class="line">   phone_brand,</span><br><span class="line">   phone_cnt,</span><br><span class="line">   change_phone_cnt,</span><br><span class="line">   weight,</span><br><span class="line">   height</span><br><span class="line">from ods_mall.ods_user_extend</span><br><span class="line">where dt = '$&#123;dt&#125;' and user_id is not null;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_user_addr partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   addr_id,</span><br><span class="line">   user_id,</span><br><span class="line">   addr_name,</span><br><span class="line">   order_flag,</span><br><span class="line">   user_name,</span><br><span class="line">   mobile</span><br><span class="line">from ods_mall.ods_user_addr</span><br><span class="line">where dt = '$&#123;dt&#125;' and addr_id is not null;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_goods_info partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   goods_id,</span><br><span class="line">   goods_no,</span><br><span class="line">   goods_name,</span><br><span class="line">   curr_price,</span><br><span class="line">   third_category_id,</span><br><span class="line">   goods_desc,</span><br><span class="line">   create_time</span><br><span class="line">from ods_mall.ods_goods_info</span><br><span class="line">where dt = '$&#123;dt&#125;' and goods_id is not null;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_category_code partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   first_category_id,</span><br><span class="line">   first_category_name,</span><br><span class="line">   second_category_id,</span><br><span class="line">   second_catery_name,</span><br><span class="line">   third_category_id,</span><br><span class="line">   third_category_name</span><br><span class="line">from ods_mall.ods_category_code</span><br><span class="line">where dt = '$&#123;dt&#125;' and first_category_id is not null;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_user_order partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time </span><br><span class="line">from ods_mall.ods_user_order</span><br><span class="line">where dt = '$&#123;dt&#125;' and order_id is not null;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_order_item partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   order_id,</span><br><span class="line">   goods_id,</span><br><span class="line">   goods_amount,</span><br><span class="line">   curr_price,</span><br><span class="line">   create_time</span><br><span class="line">from ods_mall.ods_order_item</span><br><span class="line">where dt = '$&#123;dt&#125;' and order_id is not null;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite t able dwd_mall.dwd_order_delivery partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   order_id,</span><br><span class="line">   addr_id,</span><br><span class="line">   user_id,</span><br><span class="line">   carriage_money,</span><br><span class="line">   create_time</span><br><span class="line">from ods_mall.ods_order_delivery</span><br><span class="line">where dt = '$&#123;dt&#125;' and order_id is not null;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_payment_flow partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   pay_id,</span><br><span class="line">   order_id,</span><br><span class="line">   trade_no,</span><br><span class="line">   pay_money,</span><br><span class="line">   pay_type,</span><br><span class="line">   pay_time</span><br><span class="line">from ods_mall.ods_payment_flow</span><br><span class="line">where dt = '$&#123;dt&#125;' and order_id is not null;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052333331.png" alt="image-20230405233300223"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052335705.png" alt="image-20230405233549286"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052336364.png" alt="image-20230405233613163"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052337003.png" alt="image-20230405233743838"></p><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">需求一：用户信息宽表(后期查询用户信息时只需在一张表里查询，不需要关联多张表)</span><br><span class="line">需求二：电商GMV(电商领域很常见，表示一段时间内的交易总金额)</span><br><span class="line">需求三：商品相关指标(商品销售情况，比如哪些受用户喜欢)</span><br><span class="line">需求四：用户行为漏斗分析</span><br><span class="line"></span><br><span class="line">为了保证大家在练习的时候计算的需求结果和我的保持一致，所以针对后面的测试数据就不再随机生成了，而是生成固定的数据，一共1个月的数据</span><br><span class="line">从2026-02-01到2026-02-28的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052344522.png" alt="image-20230405234430452"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052346838.png" alt="image-20230405234656212"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052349025.png" alt="image-20230405234927830"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">执行上面的代码，此时数据就会被上传到HDFS上面。</span><br><span class="line">然后执行ods层和dwd层的脚本，重新加载计算2026-02月份的数据。</span><br><span class="line">1：执行ods层的脚本</span><br><span class="line">写一个临时脚本，在脚本中写一个for循环，循环加载数据</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 加载ods层的数据</span></span><br><span class="line"></span><br><span class="line">for((i=1;i&lt;=28;i++))</span><br><span class="line">do</span><br><span class="line">if [ $i -lt 10 ]</span><br><span class="line">then</span><br><span class="line">dt="2026020"$i</span><br><span class="line">else</span><br><span class="line">dt="202602"$i </span><br><span class="line">fi</span><br><span class="line">echo "ods_mall_add_partition.sh" $&#123;dt&#125;</span><br><span class="line">sh ods_mall_add_partition.sh $&#123;dt&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 计算dwd层的数据</span></span><br><span class="line"></span><br><span class="line">for((i=1;i&lt;=28;i++))</span><br><span class="line">do</span><br><span class="line">if [ $i -lt 10 ]</span><br><span class="line">then</span><br><span class="line">dt="2026020"$i</span><br><span class="line">else</span><br><span class="line">dt="202602"$i</span><br><span class="line">fi</span><br><span class="line">echo "dwd_mall_add_partition.sh" $&#123;dt&#125;</span><br><span class="line">sh  dwd_mall_add_partition.sh $&#123;dt&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">等脚本执行完毕之后，验证一下结果，随便找一个日期的数据验证即可，能查到数据就说明是OK的。最好是再查看一下 partition的信息。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052358185.png" alt="image-20230405235827525"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052353292.png" alt="image-20230405235349443"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052354916.png" alt="image-20230405235406590"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052355506.png" alt="image-20230405235510464"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052356274.png" alt="image-20230405235640110"></p><h3 id="需求一：用户信息宽表"><a href="#需求一：用户信息宽表" class="headerlink" title="需求一：用户信息宽表"></a>需求一：用户信息宽表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">宽表主要是便于使用，在使用的时候不至于每次都要关联很多张表</span><br><span class="line">用户信息宽表包括服务端中的user表、user_extend表</span><br><span class="line">如果有需求的话其实还可以把用户的一些其它维度的数据关联过来，例如：当日的下单数量、消费金额等等指标，</span><br><span class="line">实现思路如下：</span><br><span class="line">对dwd_user表和dwd_user_extend表执行left join操作，通过user_id进行关联即可，将结果数据保存到 dws_user_info_all表中</span><br></pre></td></tr></table></figure><h4 id="dws层"><a href="#dws层" class="headerlink" title="dws层"></a>dws层</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_info_all(</span><br><span class="line">   user_id              <span class="built_in">bigint</span>,</span><br><span class="line">   user_name            <span class="keyword">string</span>,</span><br><span class="line">   user_gender          <span class="built_in">tinyint</span>,</span><br><span class="line">   user_birthday        <span class="keyword">string</span>,</span><br><span class="line">   e_mail               <span class="keyword">string</span>,</span><br><span class="line">   mobile               <span class="keyword">string</span>,</span><br><span class="line">   register_time        <span class="keyword">string</span>,</span><br><span class="line">   is_blacklist         <span class="built_in">tinyint</span>,</span><br><span class="line">   is_pregnant_woman    <span class="built_in">tinyint</span>,</span><br><span class="line">   is_have_children     <span class="built_in">tinyint</span>,</span><br><span class="line">   is_have_car          <span class="built_in">tinyint</span>,</span><br><span class="line">   phone_brand          <span class="keyword">string</span>,</span><br><span class="line">   phone_cnt            <span class="built_in">int</span>,</span><br><span class="line">   change_phone_cnt     <span class="built_in">int</span>,</span><br><span class="line">   weight               <span class="built_in">int</span>,</span><br><span class="line">   height               <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_info_all/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_info_all <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>)  <span class="keyword">select</span> </span><br><span class="line">   du.user_id,</span><br><span class="line">   du.user_name,</span><br><span class="line">   du.user_gender,</span><br><span class="line">   du.user_birthday,</span><br><span class="line">   du.e_mail,</span><br><span class="line">   du.mobile,</span><br><span class="line">   du.register_time,</span><br><span class="line">   du.is_blacklist,</span><br><span class="line">   due.is_pregnant_woman,</span><br><span class="line">   due.is_have_children,</span><br><span class="line">   due.is_have_car,</span><br><span class="line">   due.phone_brand,</span><br><span class="line">   due.phone_cnt,</span><br><span class="line">   due.change_phone_cnt,</span><br><span class="line">   due.weight,</span><br><span class="line">   due.height</span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user <span class="keyword">as</span> du</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dwd_mall.dwd_user_extend <span class="keyword">as</span> due</span><br><span class="line"><span class="keyword">on</span> du.user_id = due.user_id</span><br><span class="line"><span class="keyword">where</span> du.dt = <span class="string">'20260201'</span> <span class="keyword">and</span> due.dt = <span class="string">'20260201'</span>;</span><br></pre></td></tr></table></figure><h5 id="建表脚本-2"><a href="#建表脚本-2" class="headerlink" title="建表脚本"></a>建表脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_1.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_1.sh</span><br></pre></td></tr></table></figure><h5 id="dws-mall-init-table-1-sh"><a href="#dws-mall-init-table-1-sh" class="headerlink" title="dws_mall_init_table_1.sh"></a>dws_mall_init_table_1.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：用户信息宽表</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_info_all(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   user_gender          tinyint,</span><br><span class="line">   user_birthday        string,</span><br><span class="line">   e_mail               string,</span><br><span class="line">   mobile               string,</span><br><span class="line">   register_time        string,</span><br><span class="line">   is_blacklist         tinyint,</span><br><span class="line">   is_pregnant_woman    tinyint,</span><br><span class="line">   is_have_children     tinyint,</span><br><span class="line">   is_have_car          tinyint,</span><br><span class="line">   phone_brand          string,</span><br><span class="line">   phone_cnt            int,</span><br><span class="line">   change_phone_cnt     int,</span><br><span class="line">   weight               int,</span><br><span class="line">   height               int</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_info_all/';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="dws-mall-add-partition-1-sh"><a href="#dws-mall-add-partition-1-sh" class="headerlink" title="dws_mall_add_partition_1.sh"></a>dws_mall_add_partition_1.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：用户信息宽表</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_user_info_all partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   du.user_id,</span><br><span class="line">   du.user_name,</span><br><span class="line">   du.user_gender,</span><br><span class="line">   du.user_birthday,</span><br><span class="line">   du.e_mail,</span><br><span class="line">   du.mobile,</span><br><span class="line">   du.register_time,</span><br><span class="line">   du.is_blacklist,</span><br><span class="line">   due.is_pregnant_woman,</span><br><span class="line">   due.is_have_children,</span><br><span class="line">   due.is_have_car,</span><br><span class="line">   due.phone_brand,</span><br><span class="line">   due.phone_cnt,</span><br><span class="line">   due.change_phone_cnt,</span><br><span class="line">   due.weight,</span><br><span class="line">   due.height</span><br><span class="line">from dwd_mall.dwd_user as du</span><br><span class="line">left join dwd_mall.dwd_user_extend as due</span><br><span class="line">on du.user_id = due.user_id</span><br><span class="line">where du.dt = '$&#123;dt&#125;' and due.dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060014790.png" alt="image-20230406001420794"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060014322.png" alt="image-20230406001456982"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060015364.png" alt="image-20230406001541136"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问题：如何将服务端的数据和客户端的数据通过用户维度关联起来？</span><br><span class="line">之前统计的客户端数据(用户行为数据)针对用户相关的指标都是使用的xaid(因为用户在使用app时，不一定登陆了，所以没有用user_id)。</span><br><span class="line">但是在服务端数据中用户信息只有user_id。</span><br><span class="line">这两份数据如果先要关联起来，还需要在用户行为数仓中提取一个表，表里面只需要有两列：</span><br><span class="line">user_id和xaid。这样就可以把客户端数据和服务端数据中的用户关联起来了。</span><br></pre></td></tr></table></figure><h3 id="需求二：电商-GMV"><a href="#需求二：电商-GMV" class="headerlink" title="需求二：电商 GMV"></a>需求二：电商 GMV</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GMV：Gross Merchandise Volume，是指一定时间段内的成交总金额</span><br><span class="line">GMV多用于电商行业，这个实际指的是拍下的订单总金额，包含付款和未付款的部分。</span><br><span class="line">我们在统计的时候就可以将订单表中的每天的所有订单金额全部都累加起来就可以获取到当天的GMV了</span><br><span class="line"></span><br><span class="line">实现思路：</span><br><span class="line">对dwd_user_order表中的数据进行统计即可，通过order_money字段可以计算出来GMV，将结果数据保存到表 app_gmv中</span><br></pre></td></tr></table></figure><h4 id="app层"><a href="#app层" class="headerlink" title="app层"></a>app层</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：gmv 字段的类型可以使用double或者decimal(10,2)都可以</span><br><span class="line">decimal(10,2)，可以更方便的控制小数位数，数据看起来更加清晰，所以建议使用decimal(10,2)。</span><br><span class="line">其实针对金钱相关的字段类型建议使用decimal(10,2)，使用double也不错，也是可以的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists app_mall.app_gmv(</span><br><span class="line">    gmv   decimal(10,2) </span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;app&#x2F;gmv&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_gmv partition(dt&#x3D;&#39;20260201&#39;)  select </span><br><span class="line">sum(order_money) as gmv</span><br><span class="line">from dwd_mall.dwd_user_order</span><br><span class="line">where dt &#x3D; &#39;20260201&#39;;</span><br></pre></td></tr></table></figure><h5 id="建表脚本-3"><a href="#建表脚本-3" class="headerlink" title="建表脚本"></a>建表脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_2.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_2.sh</span><br></pre></td></tr></table></figure><h5 id="app-mall-init-table-2-sh"><a href="#app-mall-init-table-2-sh" class="headerlink" title="app_mall_init_table_2.sh"></a>app_mall_init_table_2.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求二：电商GMV</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_gmv(</span><br><span class="line">    gmv   decimal(10,2) </span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/gmv/';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-mall-add-partition-2-sh"><a href="#app-mall-add-partition-2-sh" class="headerlink" title="app_mall_add_partition_2.sh"></a>app_mall_add_partition_2.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求二：电商GMV</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_gmv partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">sum(order_money) as gmv</span><br><span class="line">from dwd_mall.dwd_user_order</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">问题：有了每天的GMV，后期能不能计算月度GMV，季度GMV，以及年度GMV？以及GMV的同比，环比？</span><br><span class="line">可以的，有了每日的GMV之后，按照月、季度、年对GMV进行聚合即可。</span><br><span class="line">同比环比，我们在前面已经实现过类似的需求。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060037068.png" alt="image-20230406003729623"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060037204.png" alt="image-20230406003754317"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060038053.png" alt="image-20230406003824047"></p><h3 id="需求三：商品相关指标"><a href="#需求三：商品相关指标" class="headerlink" title="需求三：商品相关指标"></a>需求三：商品相关指标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">针对这个需求我们主要统计两个指标</span><br><span class="line">1：商品销售情况（商品名称，一级类目，订单总量，销售额）</span><br><span class="line">2：商品品类偏好Top10（商品一级类目，订单总量）</span><br><span class="line"></span><br><span class="line">首先看第一个指标：商品销售情况</span><br><span class="line">这个指标主要统计商品名称，一级类目，订单总量，销售额这些字段信息</span><br><span class="line">订单中的详细信息是在dwd_order_item表中，需要关联dwd_goods_info和</span><br><span class="line">dwd_category_code获取商品名称和商品一级类目信息</span><br><span class="line"></span><br><span class="line">在这最好是基于这些表先构建一个商品订单信息的宽表dws_order_goods_all_info，便于后期</span><br><span class="line">其它需求复用。</span><br><span class="line">然后基于这个宽表统计出来这个指标需要的信息，保存到表app_goods_sales_item</span><br><span class="line"></span><br><span class="line">接着看第二个指标：商品品类偏好Top10</span><br><span class="line">这个指标可以在第一个指标的基础之上，根据一级类目进行分组，按照类目下的订单总量排序，取Top10，保存到表 app_category_top10中</span><br></pre></td></tr></table></figure><h4 id="dws层-1"><a href="#dws层-1" class="headerlink" title="dws层"></a>dws层</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists dws_mall.dws_order_goods_all_info(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_amount         int,</span><br><span class="line">   order_curr_price           double,</span><br><span class="line">   order_create_time          string,</span><br><span class="line">   goods_no             string,</span><br><span class="line">   goods_name           string,</span><br><span class="line">   goods_curr_price           double,</span><br><span class="line">   goods_desc           string,</span><br><span class="line">   goods_create_time          string,</span><br><span class="line">   first_category_id    int,</span><br><span class="line">   first_category_name  string,</span><br><span class="line">   second_category_id   int,</span><br><span class="line">   second_catery_name   string,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   third_category_name  string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/order_goods_all_info/';</span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_order_goods_all_info partition(dt='20260201')  select </span><br><span class="line">   doi.order_id,</span><br><span class="line">   doi.goods_id,</span><br><span class="line">   doi.goods_amount,</span><br><span class="line">   doi.curr_price as order_curr_price,</span><br><span class="line">   doi.create_time as order_create_time,</span><br><span class="line">   dgi.goods_no,</span><br><span class="line">   dgi.goods_name,</span><br><span class="line">   dgi.curr_price as goods_curr_price,</span><br><span class="line">   dgi.goods_desc,</span><br><span class="line">   dgi.create_time as goods_create_time,</span><br><span class="line">   dcc.first_category_id,</span><br><span class="line">   dcc.first_category_name,</span><br><span class="line">   dcc.second_category_id,</span><br><span class="line">   dcc.second_catery_name,</span><br><span class="line">   dcc.third_category_id,</span><br><span class="line">   dcc.third_category_name</span><br><span class="line">from dwd_mall.dwd_order_item as doi</span><br><span class="line">left join dwd_mall.dwd_goods_info as dgi</span><br><span class="line">on doi.goods_id = dgi.goods_id</span><br><span class="line">left join dwd_mall.dwd_category_code as dcc</span><br><span class="line">on dgi.third_category_id = dcc.third_category_id</span><br><span class="line">where doi.dt = '20260201' and dgi.dt = '20260201' and dcc.dt = '20260201';</span><br></pre></td></tr></table></figure><h5 id="开发脚本"><a href="#开发脚本" class="headerlink" title="开发脚本"></a>开发脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_3.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_3.sh</span><br></pre></td></tr></table></figure><h6 id="dws-mall-init-table-3-sh"><a href="#dws-mall-init-table-3-sh" class="headerlink" title="dws_mall_init_table_3.sh"></a>dws_mall_init_table_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：商品相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_order_goods_all_info(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_amount         int,</span><br><span class="line">   order_curr_price           double,</span><br><span class="line">   order_create_time          string,</span><br><span class="line">   goods_no             string,</span><br><span class="line">   goods_name           string,</span><br><span class="line">   goods_curr_price           double,</span><br><span class="line">   goods_desc           string,</span><br><span class="line">   goods_create_time          string,</span><br><span class="line">   first_category_id    int,</span><br><span class="line">   first_category_name  string,</span><br><span class="line">   second_category_id   int,</span><br><span class="line">   second_catery_name   string,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   third_category_name  string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/order_goods_all_info/';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-3-sh"><a href="#dws-mall-add-partition-3-sh" class="headerlink" title="dws_mall_add_partition_3.sh"></a>dws_mall_add_partition_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：商品相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_order_goods_all_info partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   doi.order_id,</span><br><span class="line">   doi.goods_id,</span><br><span class="line">   doi.goods_amount,</span><br><span class="line">   doi.curr_price as order_curr_price,</span><br><span class="line">   doi.create_time as order_create_time,</span><br><span class="line">   dgi.goods_no,</span><br><span class="line">   dgi.goods_name,</span><br><span class="line">   dgi.curr_price as goods_curr_price,</span><br><span class="line">   dgi.goods_desc,</span><br><span class="line">   dgi.create_time as goods_create_time,</span><br><span class="line">   dcc.first_category_id,</span><br><span class="line">   dcc.first_category_name,</span><br><span class="line">   dcc.second_category_id,</span><br><span class="line">   dcc.second_catery_name,</span><br><span class="line">   dcc.third_category_id,</span><br><span class="line">   dcc.third_category_name</span><br><span class="line">from dwd_mall.dwd_order_item as doi</span><br><span class="line">left join dwd_mall.dwd_goods_info as dgi</span><br><span class="line">on doi.goods_id = dgi.goods_id</span><br><span class="line">left join dwd_mall.dwd_category_code as dcc</span><br><span class="line">on dgi.third_category_id = dcc.third_category_id</span><br><span class="line">where doi.dt = '$&#123;dt&#125;' and dgi.dt = '$&#123;dt&#125;' and dcc.dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h4 id="app层-1"><a href="#app层-1" class="headerlink" title="app层"></a>app层</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_goods_sales_item(</span><br><span class="line">    goods_name    <span class="keyword">string</span>,</span><br><span class="line">first_category_name     <span class="keyword">string</span>,</span><br><span class="line">order_total    <span class="built_in">bigint</span>,</span><br><span class="line">price_total    <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/goods_sales_item/'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_goods_sales_item <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>)  <span class="keyword">select</span> </span><br><span class="line">goods_name,</span><br><span class="line">first_category_name,</span><br><span class="line"><span class="keyword">count</span>(order_id) <span class="keyword">as</span> order_total,</span><br><span class="line"><span class="keyword">sum</span>(goods_amount * order_curr_price) <span class="keyword">as</span> price_total</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_order_goods_all_info</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> goods_name,first_category_name;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_category_top10(</span><br><span class="line">first_category_name     <span class="keyword">string</span>,</span><br><span class="line">order_total    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/category_top10/'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_category_top10 <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>)  <span class="keyword">select</span> </span><br><span class="line">first_category_name,</span><br><span class="line"><span class="keyword">sum</span>(order_total) <span class="keyword">as</span> order_total</span><br><span class="line"><span class="keyword">from</span> app_mall.app_goods_sales_item </span><br><span class="line"><span class="keyword">where</span> dt =<span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> first_category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> order_total <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><h5 id="开发脚本-1"><a href="#开发脚本-1" class="headerlink" title="开发脚本"></a>开发脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_3.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_3.sh</span><br></pre></td></tr></table></figure><h5 id="app-mall-init-table-3-sh"><a href="#app-mall-init-table-3-sh" class="headerlink" title="app_mall_init_table_3.sh"></a>app_mall_init_table_3.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：商品相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_goods_sales_item(</span><br><span class="line">    goods_name    string,</span><br><span class="line">first_category_name     string,</span><br><span class="line">order_total    bigint,</span><br><span class="line">price_total    decimal(10,2)</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/goods_sales_item/';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_category_top10(</span><br><span class="line">first_category_name     string,</span><br><span class="line">order_total    bigint</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/category_top10/';</span><br><span class="line"> </span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-mall-add-partition-3-sh"><a href="#app-mall-add-partition-3-sh" class="headerlink" title="app_mall_add_partition_3.sh"></a>app_mall_add_partition_3.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：商品相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_goods_sales_item partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">goods_name,</span><br><span class="line">first_category_name,</span><br><span class="line">count(order_id) as order_total,</span><br><span class="line">sum(goods_amount * order_curr_price) as price_total</span><br><span class="line">from dws_mall.dws_order_goods_all_info</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by goods_name,first_category_name;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_category_top10 partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">first_category_name,</span><br><span class="line">sum(order_total) as order_total</span><br><span class="line">from app_mall.app_goods_sales_item </span><br><span class="line">where dt ='$&#123;dt&#125;'</span><br><span class="line">group by first_category_name</span><br><span class="line">order by order_total desc</span><br><span class="line">limit 10;</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060107837.png" alt="image-20230406010738922"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060107081.png" alt="image-20230406010754961"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060108980.png" alt="image-20230406010814759"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060108252.png" alt="image-20230406010840827"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060109152.png" alt="image-20230406010927249"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060116924.png" alt="image-20230406011604503"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">问题：如果想要统计商品品类内最受用户喜欢的Top10商品，如何统计？</span><br><span class="line">基于dws_order_goods_all_info表中的数据，根据商品品类进行分区，根据商品的订单总量</span><br><span class="line">进行排序，获取每个品类中用户喜欢的Top10商品</span><br></pre></td></tr></table></figure><h3 id="需求四：漏斗分析"><a href="#需求四：漏斗分析" class="headerlink" title="需求四：漏斗分析"></a>需求四：漏斗分析</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060119866.png" alt="image-20230406011942433"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">其实就是根据用户的行为一层一层分析用户的转化率。</span><br><span class="line">活跃--&gt;商品详情页--&gt;下单--&gt;支付</span><br><span class="line">每一个指标对应的表：</span><br><span class="line">活跃：dws_user_active_history</span><br><span class="line">商品详情页：dwd_good_item</span><br><span class="line">下单：dwd_user_order</span><br><span class="line">支付：dwd_user_order</span><br><span class="line">实现思路：</span><br><span class="line">首先统计当天活跃用户数量</span><br><span class="line">接着统计当天进入了多少个商品详情页</span><br><span class="line">接着统计当天下单的数量</span><br><span class="line">最后统计当天支付的数量</span><br><span class="line">并且计算每一层的转化率。</span><br><span class="line">最终把结果数据保存到表 app_user_conver_funnel中</span><br></pre></td></tr></table></figure><h4 id="app-层"><a href="#app-层" class="headerlink" title="app 层"></a>app 层</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_conver_funnel(</span><br><span class="line">    active_num    <span class="built_in">int</span>,</span><br><span class="line">    item_num     <span class="built_in">int</span>,</span><br><span class="line">    order_num    <span class="built_in">int</span>,</span><br><span class="line">    pay_num    <span class="built_in">int</span>,</span><br><span class="line">    active_to_item_ratio    <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">    item_to_order_ratio    <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">    order_to_pay_ratio    <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_conver_funnel/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_conver_funnel <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>)  <span class="keyword">select</span> </span><br><span class="line">duah.active_num,</span><br><span class="line">dgi.item_num,</span><br><span class="line">duo.order_num,</span><br><span class="line">duo.pay_num,</span><br><span class="line">dgi.item_num/duah.active_num <span class="keyword">as</span> active_to_item_ratio,</span><br><span class="line">duo.order_num/dgi.item_num <span class="keyword">as</span> item_to_order_ratio,</span><br><span class="line">duo.pay_num/duo.order_num <span class="keyword">as</span> order_to_pay_ratio</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> </span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> active_num</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line">) <span class="keyword">as</span> duah</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> </span><br><span class="line"><span class="keyword">count</span>(<span class="keyword">distinct</span> goods_id) <span class="keyword">as</span> item_num</span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_good_item</span><br><span class="line">    <span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line">)<span class="keyword">as</span> dgi</span><br><span class="line"><span class="keyword">on</span> <span class="number">1</span>=<span class="number">1</span></span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> order_num,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">case</span> <span class="keyword">when</span> order_status != <span class="number">0</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> pay_num</span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line">) <span class="keyword">as</span> duo</span><br><span class="line"><span class="keyword">on</span> <span class="number">1</span>=<span class="number">1</span>;</span><br></pre></td></tr></table></figure><h5 id="开发脚本-2"><a href="#开发脚本-2" class="headerlink" title="开发脚本"></a>开发脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_4.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_4.sh</span><br></pre></td></tr></table></figure><h5 id="app-mall-init-table-4-sh"><a href="#app-mall-init-table-4-sh" class="headerlink" title="app_mall_init_table_4.sh"></a>app_mall_init_table_4.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求四：漏斗分析</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_conver_funnel(</span><br><span class="line">    active_num    int,</span><br><span class="line">    item_num     int,</span><br><span class="line">    order_num    int,</span><br><span class="line">    pay_num    int,</span><br><span class="line">    active_to_item_ratio    decimal(10,2),</span><br><span class="line">    item_to_order_ratio    decimal(10,2),</span><br><span class="line">    order_to_pay_ratio    decimal(10,2)</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_conver_funnel/';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-mall-add-partition-4-sh"><a href="#app-mall-add-partition-4-sh" class="headerlink" title="app_mall_add_partition_4.sh"></a>app_mall_add_partition_4.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求四：漏斗分析</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_conver_funnel partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">duah.active_num,</span><br><span class="line">dgi.item_num,</span><br><span class="line">duo.order_num,</span><br><span class="line">duo.pay_num,</span><br><span class="line">dgi.item_num/duah.active_num as active_to_item_ratio,</span><br><span class="line">duo.order_num/dgi.item_num as item_to_order_ratio,</span><br><span class="line">duo.pay_num/duo.order_num as order_to_pay_ratio</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select </span><br><span class="line">count(*) as active_num</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">) as duah</span><br><span class="line">join</span><br><span class="line">(</span><br><span class="line">    select </span><br><span class="line">count(distinct goods_id) as item_num</span><br><span class="line">from dwd_mall.dwd_good_item</span><br><span class="line">    where dt = '$&#123;dt&#125;'</span><br><span class="line">)as dgi</span><br><span class="line">on 1=1</span><br><span class="line">join</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">count(*) as order_num,</span><br><span class="line">sum(case when order_status != 0 then 1 else 0 end) as pay_num</span><br><span class="line">from dwd_mall.dwd_user_order</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">) as duo</span><br><span class="line">on 1=1;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060133819.png" alt="image-20230406013300841"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060133557.png" alt="image-20230406013314309"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060134761.png" alt="image-20230406013432629"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库之用户行为数仓4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%934.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%934.html</id>
    <published>2023-04-04T08:46:35.000Z</published>
    <updated>2023-04-05T14:30:03.789Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="综合项目-电商数据仓库之用户行为数仓4"><a href="#综合项目-电商数据仓库之用户行为数仓4" class="headerlink" title="综合项目:电商数据仓库之用户行为数仓4"></a>综合项目:电商数据仓库之用户行为数仓4</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">截止目前hdfs上，用户行为数据目录结构的样子</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304042036477.png" alt="image-20230404203633888"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304042038941.png" alt="image-20230404203822647"></p><h2 id="用户行为数据数仓开发"><a href="#用户行为数据数仓开发" class="headerlink" title="用户行为数据数仓开发"></a>用户行为数据数仓开发</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304042029511.png" alt="image-20230404202837476"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据仓库分为 4层：ods层、dwd层、dws层、app层，</span><br><span class="line">我们先来构建第一层：ods层</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1：由于在构建数据仓库的时候我们会创建多个数据库，所以在创建以及使用表的时候最好</span><br><span class="line">都在表名前面带上对应的数据库名称，否则可能会出现一些不必要的问题，可能会把ods层的表建到dwd层。</span><br><span class="line">2：考虑到SQL重跑的情况，需要在SQL语句中添加if not exists</span><br><span class="line">3：hive中可以用string、date和timestamp表示日期时间，date用yyyy-MM-dd的形式表示，timestamp用yyyy-MM-dd hh:mm:ss 的形式表示，string 可以表示 yyyy-MM-dd和yyyy-MM-dd hh:mm:ss</span><br><span class="line">这三种格式之间可以互相转换，不过在用的时候建议格式统一，String可以表示另外两种格式，并且也支持日期的大小比较，所以在这里针对时间统一使用String表示。</span><br></pre></td></tr></table></figure><h3 id="ods-层"><a href="#ods-层" class="headerlink" title="ods 层"></a>ods 层</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">创建ods层的表</span><br><span class="line">表名对应的建表语句</span><br><span class="line"></span><br><span class="line">注意：在使用这里的建表语句的时候注意里面的日期目录，需要和你 HDFS 中生成的日期目录保持一致</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_user_active( <span class="comment"># 都使用的数据库.表名，这样后期就不用来回切换当前数据库</span></span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_user_active <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/1'</span>; <span class="comment"># 这里写的相对路径，前面最后有/，则这里头不需要/，且相对路径，这里不能使用/开头；使用绝对路径一定正确</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_click_good(</span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_click_good <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/2'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_good_item(</span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_good_item <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/3'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_good_list(</span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_good_list <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/4'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_app_close(</span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_app_close <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/5'</span>;</span><br></pre></td></tr></table></figure><h4 id="ods层抽取脚本"><a href="#ods层抽取脚本" class="headerlink" title="ods层抽取脚本"></a>ods层抽取脚本</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">针对ods层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">ods_mall_init_table.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">add_partition.sh：这个脚本是通用的，所有添加分区的地方都可以使用。</span><br><span class="line">ods_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h5 id="ods-mall-init-table-sh"><a href="#ods-mall-init-table-sh" class="headerlink" title="ods_mall_init_table.sh"></a>ods_mall_init_table.sh</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># ods层数据库和表初始化脚本，只需要执行一次</span><br><span class="line"></span><br><span class="line">hive -e &quot;</span><br><span class="line">create database if not exists ods_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists ods_mall.ods_user_active(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_click_good(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_good_item(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_good_list(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists ods_mall.ods_app_close(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure><h5 id="add-partition-sh"><a href="#add-partition-sh" class="headerlink" title="add_partition.sh"></a>add_partition.sh</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 给外部分区表添加分区</span><br><span class="line"># 接收三个参数</span><br><span class="line">#1：表名</span><br><span class="line">#2：分区字段dt的值：格式20260101</span><br><span class="line">#3：分区路径(相对路径或者绝对路径都可以)</span><br><span class="line"></span><br><span class="line">if [ $# !&#x3D; 3 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;参数异常：add_partition.sh &lt;tabkle_name&gt; &lt;dt&gt; &lt;path&gt;&quot;</span><br><span class="line">exit 100</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">table_name&#x3D;$1</span><br><span class="line">dt&#x3D;$2</span><br><span class="line">path&#x3D;$3</span><br><span class="line"></span><br><span class="line">hive -e &quot;</span><br><span class="line">alter table $&#123;table_name&#125; add  if not exists partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;) location &#39;$&#123;path&#125;&#39;;</span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure><h5 id="ods-mall-add-partition-sh"><a href="#ods-mall-add-partition-sh" class="headerlink" title="ods_mall_add_partition.sh"></a>ods_mall_add_partition.sh</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 给ods层的表添加分区，这个脚本后期每天执行一次</span><br><span class="line"># 每天凌晨，添加昨天的分区，添加完分区之后，再执行后面的计算脚本</span><br><span class="line"></span><br><span class="line"># 默认获取昨天的日期，也支持传参指定一个日期</span><br><span class="line">if [ &quot;z$1&quot; &#x3D; &quot;z&quot; ]</span><br><span class="line">then</span><br><span class="line">dt&#x3D;&#96;date +%Y%m%d --date&#x3D;&quot;1 days ago&quot;&#96;</span><br><span class="line">else</span><br><span class="line">dt&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#alter table ods_mall.ods_user_active add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;1&#39;;</span><br><span class="line">#alter table ods_mall.ods_click_good add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;2&#39;;</span><br><span class="line">#alter table ods_mall.ods_good_item add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;3&#39;;</span><br><span class="line">#alter table ods_mall.ods_good_list add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;4&#39;;</span><br><span class="line">#alter table ods_mall.ods_app_close add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;5&#39;;</span><br><span class="line">sh add_partition.sh ods_mall.ods_user_active $&#123;dt&#125; $&#123;dt&#125;&#x2F;1</span><br><span class="line">sh add_partition.sh ods_mall.ods_click_good $&#123;dt&#125; $&#123;dt&#125;&#x2F;2</span><br><span class="line">sh add_partition.sh ods_mall.ods_good_item $&#123;dt&#125; $&#123;dt&#125;&#x2F;3</span><br><span class="line">sh add_partition.sh ods_mall.ods_good_list $&#123;dt&#125; $&#123;dt&#125;&#x2F;4</span><br><span class="line">sh add_partition.sh ods_mall.ods_app_close $&#123;dt&#125; $&#123;dt&#125;&#x2F;5</span><br></pre></td></tr></table></figure><h3 id="dwd层"><a href="#dwd层" class="headerlink" title="dwd层"></a>dwd层</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对 ods层表中的数据进行清洗，参考数据清洗规则，按照实际情况对数据进行清洗</span><br><span class="line">注意：如果清洗规则使用SQL可以实现，那么就使用SQL实现数据清洗，如果清洗的规则</span><br><span class="line">使用SQL实现起来非常麻烦，或者使用SQL压根无法实现，此时就可以考虑需要使用MapReduce代码或者 Spark代码对数据进行清洗了。</span><br><span class="line">由于我们这里采集的数据还是比较规整的，可以使用SQL实现，所以我们就直接使用SQL实现数据清洗了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">创建dwd层的表</span><br><span class="line">注意：</span><br><span class="line">1：原始json数据中的用户 id字段名称为 uid，但是在商品订单数据中用户id字段名称为user_id，这块需要注意一下，在实际工作中会有这种情况，客户端数据和服务端数据的个别字段名称不一致，所以我们在使用的时候最好是统一一下，后期使用起来比较方便，所以在这里我会通过 uid解析数据，解析之后，给字段起别名为 user_id</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2：hive中的timestamp只能解析yyyy-MM-dd HH:MM:SS格式的数据，所以针对这里面的acttime字段我们使用bigint类型</span><br><span class="line">3：为了考虑到SQL重跑的情况，在使用insert into table(追加)的时候最好改为insert overwrite table(覆盖)，否则SQL重复执行的时候会重复写入数据</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_user_active(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    ad_status    <span class="built_in">tinyint</span>,</span><br><span class="line">    loading_time    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/user_active/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_active <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>)  <span class="keyword">select</span></span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ad_status'</span>) <span class="keyword">as</span> ad_status,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.loading_time'</span>) <span class="keyword">as</span> loading_time</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_user_active <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span> <span class="comment"># group by在这里的作用是去重，因为flume采集数据可能会重复，distinct也可以但效率不高</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_click_good(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    goods_id    <span class="built_in">bigint</span>,</span><br><span class="line">    location    <span class="built_in">tinyint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/click_good/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_click_good <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>)  <span class="keyword">select</span> </span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.goods_id'</span>) <span class="keyword">as</span> goods_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.location'</span>) <span class="keyword">as</span> location</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_click_good <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>; <span class="comment"># 为空则是pc端生成的数据直接过滤掉，只保留app端生成的数据；且pc端数据实际生活中很少</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_good_item(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    goods_id    <span class="built_in">bigint</span>,</span><br><span class="line">    stay_time    <span class="built_in">bigint</span>,</span><br><span class="line">loading_time    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/good_item/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_good_item <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) <span class="keyword">select</span> </span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.goods_id'</span>) <span class="keyword">as</span> goods_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.stay_time'</span>) <span class="keyword">as</span> stay_time,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.loading_time'</span>) <span class="keyword">as</span> loading_time</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_good_item <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_good_list(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    loading_time    <span class="built_in">bigint</span>,</span><br><span class="line">    loading_type    <span class="built_in">tinyint</span>,</span><br><span class="line">goods_num    <span class="built_in">tinyint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/good_list/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_good_list <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) <span class="keyword">select</span> </span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.loading_time'</span>) <span class="keyword">as</span> loading_time,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.loading_type'</span>) <span class="keyword">as</span> loading_type,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.goods_num'</span>) <span class="keyword">as</span> goods_num</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_good_list <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_app_close(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/app_close/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_app_close <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) <span class="keyword">select</span> </span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_app_close <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>;</span><br></pre></td></tr></table></figure><h4 id="dwd层抽取脚本"><a href="#dwd层抽取脚本" class="headerlink" title="dwd层抽取脚本"></a>dwd层抽取脚本</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dwd_mall_init_table.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dwd_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h5 id="dwd-mall-init-table-sh"><a href="#dwd-mall-init-table-sh" class="headerlink" title="dwd_mall_init_table.sh"></a>dwd_mall_init_table.sh</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment"># dwd层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_user_active(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    ad_status    <span class="built_in">tinyint</span>,</span><br><span class="line">    loading_time    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/user_active/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_click_good(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    goods_id    <span class="built_in">bigint</span>,</span><br><span class="line">    location    <span class="built_in">tinyint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/click_good/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_good_item(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    goods_id    <span class="built_in">bigint</span>,</span><br><span class="line">    stay_time    <span class="built_in">bigint</span>,</span><br><span class="line">loading_time    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/good_item/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_good_list(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    loading_time    <span class="built_in">bigint</span>,</span><br><span class="line">    loading_type    <span class="built_in">tinyint</span>,</span><br><span class="line">goods_num    <span class="built_in">tinyint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/good_list/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_app_close(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/app_close/'</span>;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="dwd-mall-add-partition-sh"><a href="#dwd-mall-add-partition-sh" class="headerlink" title="dwd_mall_add_partition.sh"></a>dwd_mall_add_partition.sh</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 基于ods层的表进行清洗，将清洗之后的数据添加到dwd层对应表的对应分区中</span><br><span class="line"># 每天凌晨执行一次</span><br><span class="line"></span><br><span class="line"># 默认获取昨天的日期，也支持传参指定一个日期</span><br><span class="line">if [ &quot;z$1&quot; &#x3D; &quot;z&quot; ]</span><br><span class="line">then</span><br><span class="line">dt&#x3D;&#96;date +%Y%m%d --date&#x3D;&quot;1 days ago&quot;&#96;</span><br><span class="line">else</span><br><span class="line">dt&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e &quot;</span><br><span class="line">insert overwrite table dwd_mall.dwd_user_active partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;)  select</span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime,</span><br><span class="line">get_json_object(log,&#39;$.ad_status&#39;) as ad_status,</span><br><span class="line">get_json_object(log,&#39;$.loading_time&#39;) as loading_time</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_user_active where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_click_good partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;)  select </span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime,</span><br><span class="line">get_json_object(log,&#39;$.goods_id&#39;) as goods_id,</span><br><span class="line">get_json_object(log,&#39;$.location&#39;) as location</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_click_good where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_good_item partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;) select </span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime,</span><br><span class="line">get_json_object(log,&#39;$.goods_id&#39;) as goods_id,</span><br><span class="line">get_json_object(log,&#39;$.stay_time&#39;) as stay_time,</span><br><span class="line">get_json_object(log,&#39;$.loading_time&#39;) as loading_time</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_good_item where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_good_list partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;) select </span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime,</span><br><span class="line">get_json_object(log,&#39;$.loading_time&#39;) as loading_time,</span><br><span class="line">get_json_object(log,&#39;$.loading_type&#39;) as loading_type,</span><br><span class="line">get_json_object(log,&#39;$.goods_num&#39;) as goods_num</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_good_list where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_app_close partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;) select </span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_app_close where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">前面的两层中的表和需求一般没什么关系，就是把已有的数据接入进来，然后对数据进行清洗处理</span><br><span class="line">但是后面的dws层和app层是和业务有关联的，所以在构建这两层中的表的时候，我们需要根据一些典型的业务场景来进行分析，在根据具体业务建表的时候尽可能把表设计的更加通用，可以满足后期一些类似业务需求</span><br><span class="line">就是说我们在基于业务构建表的时候，不要直接一个SQL搞定，可以把一些复杂的SQL基于一些维度进行拆分，拆分出来一些中间表，再基于这些中间表统计最终的结果。</span><br><span class="line">这样这个中间表里面的数据，我们后期针对一些类似的业务需求还是可以服用的。</span><br><span class="line">需求一：每日新增用户相关指标</span><br><span class="line">需求二：每日活跃用户相关指标(主活)</span><br><span class="line">需求三：用户7日流失push提醒</span><br><span class="line">需求四：每日启动App次数相关指标</span><br><span class="line">需求五：操作系统活跃用户相关指标</span><br><span class="line">需求六：APP崩溃相关指标</span><br><span class="line"></span><br><span class="line">在计算这些需求的时候，为了保证大家在下面练习时计算的结果和我这边计算的结果保持一致，所以针对后面的测试数据就不再随机生成了，而是生成固定的数据，一共1个月的数据</span><br><span class="line">从2026-02-01到2026-02-2的8数据</span><br><span class="line"></span><br><span class="line">然后执行 ods层和 dwd层的脚本，重新加载计算 2026-02月份的数据。</span><br></pre></td></tr></table></figure><h4 id="tmp-load-ods-data-sh"><a href="#tmp-load-ods-data-sh" class="headerlink" title="tmp_load_ods_data.sh"></a>tmp_load_ods_data.sh</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1：执行 ods层的脚本</span><br><span class="line">写一个临时脚本，在脚本中写一个 for循环，循环加载数据</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 加载ods层的数据</span></span><br><span class="line"></span><br><span class="line">for((i=1;i&lt;=28;i++))</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ $i -lt <span class="number">10</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    dt=<span class="string">"2026020"</span>$i</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">        dt=<span class="string">"202602"</span>$i</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">"ods_mall_add_partition.sh"</span> $&#123;dt&#125;</span><br><span class="line">sh ods_mall_add_partition.sh $&#123;dt&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h4 id="tmp-load-dwd-data-sh"><a href="#tmp-load-dwd-data-sh" class="headerlink" title="tmp_load_dwd_data.sh"></a>tmp_load_dwd_data.sh</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 加载dwd层的数据</span></span><br><span class="line"></span><br><span class="line">for((i=1;i&lt;=28;i++))</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ $i -lt <span class="number">10</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    dt=<span class="string">"2026020"</span>$i</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">        dt=<span class="string">"202602"</span>$i</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">"dwd_mall_add_partition.sh"</span> $&#123;dt&#125;</span><br><span class="line">sh dwd_mall_add_partition.sh $&#123;dt&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051440087.png" alt="image-20230405144005173"></p><h4 id="需求一：每日新增用户相关指标"><a href="#需求一：每日新增用户相关指标" class="headerlink" title="需求一：每日新增用户相关指标"></a>需求一：每日新增用户相关指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在统计新增用户时，用户是以设备标识(xaid 字段)来判断的，每一个设备都有一个唯一设备码，因为会存在用户不登录的情况，以及多人共用一个账号的情况，所以根据用户 id 进行统计是不准确的。</span><br><span class="line">新增用户是指第一次安装并且使用 app 的用户，后期卸载之后再使用就不算新用户了</span><br><span class="line">这个新增用户其实也可以称为新增设备，一个设备对应一个用户。</span><br><span class="line">1：每日新增用户量</span><br><span class="line">2：每日新增用户量的日环比和周同比</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> 先详细分析第 1 个指标，每日新增用户量</span><br><span class="line">在实际工作中通过这个指标可以衡量我们产品的用户增长速度，如果每日新增用户量一直是</span><br><span class="line">上升的，说明我们的产品势头正好，如果在一段时间内增速减缓或者下降，这个时候需要考</span><br><span class="line">虑如何获取新用户。</span><br><span class="line">咱们前面分析了，新增用户是指第一次安装并且使用 APP 的用户，咱们有一个埋点会上报</span><br><span class="line">用户打开 APP 这个行为，所以计算新增用户量就使用这一份数据</span><br><span class="line">ods 层的表名为：ods_user_active</span><br><span class="line">dwd 层的表名为：dwd_user_active</span><br><span class="line">实现思路如下：</span><br><span class="line">1：我们基于清洗之后的打开 app 上报的数据创建一个历史表，这个表里面包含的有 xaid 字</span><br><span class="line">段，针对每天的数据基于 xaid 进行去重</span><br><span class="line">2：如果我们要计算 2026 年 02 月 1 日的新增用户量的话，就拿这一天上报的打开 app 的数</span><br><span class="line">据，和前面的历史表进行 left join，使用 xaid 进行关联，关联不上的数据则为新增数据。</span><br></pre></td></tr></table></figure><h5 id="每日新增用户量"><a href="#每日新增用户量" class="headerlink" title="每日新增用户量"></a>每日新增用户量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">举个例子：</span><br><span class="line">(1)：第一步会产生一个历史表：dws_user_active_history，这个表中有一个 xaid 字段</span><br><span class="line">dws_user_active_history</span><br><span class="line">xaid</span><br><span class="line">a1</span><br><span class="line">b1</span><br><span class="line">c1</span><br><span class="line">d1</span><br><span class="line">(2)：第二步会产生一个临时表，表里面包含的是那一天上报的打开 app的数据</span><br><span class="line">dws_user_active_20260201_tmp</span><br><span class="line">xaid</span><br><span class="line">a1</span><br><span class="line">b1</span><br><span class="line">x1</span><br><span class="line">y1</span><br><span class="line">z1</span><br><span class="line">(3)：对这两个表进行left join</span><br><span class="line">dws_user_active_20260201_tmp dws_user_active_history</span><br><span class="line">xaid xaid</span><br><span class="line">a1 a1</span><br><span class="line">b1 b1</span><br><span class="line">x1 null</span><br><span class="line">y1 null</span><br><span class="line">z1 null</span><br><span class="line">此时，dws_user_active_history.xaid为null的数据条数即为当日新增用户数</span><br><span class="line"></span><br><span class="line">3：将计算出来的每日新增用户信息保存到表 dws_user_new_item 表中，这个表按照天作为分区，便于后期其它需求使用这个表</span><br><span class="line"></span><br><span class="line">4：基于 dws_user_new_item 对数据进行聚合计算，将计算出来的新增用户数量保存到结果表 app_user_new_count 中。</span><br><span class="line"></span><br><span class="line">注意：在这里处理完之后，还需要将 dws_user_active_20260201_tmp 这个临时表中的数据</span><br><span class="line">insert 到 dws_user_active_history 这个历史表中。</span><br><span class="line">最后删除这个临时表即可</span><br></pre></td></tr></table></figure><h5 id="每日新增用户量的日环比和周同比"><a href="#每日新增用户量的日环比和周同比" class="headerlink" title="每日新增用户量的日环比和周同比"></a>每日新增用户量的日环比和周同比</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">接下来是第2个指标，每日新增用户量的日环比和周同比</span><br><span class="line">同比一般是指本期统计数据和往年的同时期的统计数据比较，例如 2026 年2月和2025年2月相比较；这个统计周期也可以是按月或者周</span><br><span class="line"></span><br><span class="line">环比一般是指本期统计数据和上一期的统计数据作比较，例如 2026 年 2月和2026年1月相比较；这个统计周期也可以是按周或者日</span><br><span class="line"></span><br><span class="line">在实际工作中通过同比和环比是可以衡量某一个指标的变化速度，供产品经理做一些决策的时候使用。</span><br><span class="line">日环比&#x3D;(本期的数据-上一期的数据)&#x2F;上一期的数据</span><br><span class="line">日环比中的时间单位是天</span><br><span class="line">周同比&#x3D;(本期的数据-上一期的数据)&#x2F;上一期的数据</span><br><span class="line">周同比中的时间单位是周(7天)</span><br><span class="line"></span><br><span class="line">实现思路</span><br><span class="line">直接基于 app_user_new_count 进行统计即可，可以统计出来某一天的日环比和周同比</span><br><span class="line">生成一个新表 app_user_new_count_ratio</span><br><span class="line">里面包含日期、新增用户量、日环比、周同比</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">针对前面对需求的分析，我们最终在dws层需要创建三个表</span><br><span class="line">dws_user_active_20260201_tmp</span><br><span class="line">和</span><br><span class="line">dws_user_new_item</span><br><span class="line">和</span><br><span class="line">dws_user_active_history</span><br><span class="line"></span><br><span class="line">在app层需要创建两个表</span><br><span class="line">app_user_new_count</span><br><span class="line">和</span><br><span class="line">app_user_new_count_ratio</span><br><span class="line">下面开始在数据仓库中构建这些表</span><br></pre></td></tr></table></figure><h5 id="dws"><a href="#dws" class="headerlink" title="dws"></a>dws</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_active_20260201_tmp(</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">times   <span class="built_in">int</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_active_20260201_tmp <span class="keyword">select</span></span><br><span class="line">xaid,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> times</span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> xaid;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_active_history(</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">times   <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_active_history'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_new_item(</span><br><span class="line">    xaid    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_new_item'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_new_item <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">duat.xaid</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_20260201_tmp duat</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> (<span class="keyword">select</span> xaid <span class="keyword">from</span> dws_mall.dws_user_active_history <span class="keyword">group</span> <span class="keyword">by</span> xaid) duah</span><br><span class="line"><span class="keyword">on</span> duat.xaid = duah.xaid</span><br><span class="line"><span class="keyword">where</span> duah.xaid <span class="keyword">is</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_active_history <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">xaid,</span><br><span class="line">times</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_20260201_tmp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_lost_item(</span><br><span class="line">    xaid    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_lost_item'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_lost_item <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">xaid</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt &gt;= regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> xaid</span><br><span class="line"><span class="keyword">having</span> <span class="keyword">max</span>(dt) = regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_platform_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_platform_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_android_osver_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">osver <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_ios_osver_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">osver <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_brand_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">brand <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> brand;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_model_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_model_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">model</span> <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">model</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_net_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> net</span><br><span class="line"><span class="keyword">when</span> <span class="number">0</span> <span class="keyword">then</span> <span class="string">'未知'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'WIFI'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'2G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">3</span> <span class="keyword">then</span> <span class="string">'3G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">4</span> <span class="keyword">then</span> <span class="string">'4G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">5</span> <span class="keyword">then</span> <span class="string">'5G'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> net;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_app_close_platform_vercode(</span><br><span class="line">    platform    <span class="keyword">string</span>,</span><br><span class="line">vercode    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/app_close_platform_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_app_close_platform_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">platform,</span><br><span class="line">vercode,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_app_close</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform,vercode;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_1.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_1.sh</span><br></pre></td></tr></table></figure><h6 id="dws-mall-init-table-1-sh"><a href="#dws-mall-init-table-1-sh" class="headerlink" title="dws_mall_init_table_1.sh"></a>dws_mall_init_table_1.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日新增用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 由于这个表需要每天创建一个，用完之后删除，所以选择把这个建表语句放到添加分区数据的脚本中</span></span><br><span class="line"><span class="meta">#</span><span class="bash">create table <span class="keyword">if</span> not exists dws_mall.dws_user_active_20260201_tmp(</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    xaid    string,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">times</span>   int</span></span><br><span class="line"><span class="meta">#</span><span class="bash">);</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_active_history(</span><br><span class="line">    xaid    string,</span><br><span class="line">times   int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_active_history';</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">create external table if not exists dws_mall.dws_user_new_item(</span><br><span class="line">    xaid    string</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_new_item';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-1-sh"><a href="#dws-mall-add-partition-1-sh" class="headerlink" title="dws_mall_add_partition_1.sh"></a>dws_mall_add_partition_1.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日新增用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create table if not exists dws_mall.dws_user_active_$&#123;dt&#125;_tmp(</span><br><span class="line">    xaid    string,</span><br><span class="line">    times   int</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_active_$&#123;dt&#125;_tmp select</span><br><span class="line">xaid,</span><br><span class="line">count(*) as times</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by xaid;</span><br><span class="line"></span><br><span class="line">--注意：考虑到脚本重跑的情况，所以在这开面每次执行的时候都会先删除dws_user_active_history表中指定分区的数据</span><br><span class="line">--因为在计算每日新增用户的时候需要和dws_user_active_history进行关联查询</span><br><span class="line">alter table dws_mall.dws_user_active_history drop partition(dt='$&#123;dt&#125;');</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_new_item partition(dt='$&#123;dt&#125;') select</span><br><span class="line">duat.xaid</span><br><span class="line">from dws_mall.dws_user_active_$&#123;dt&#125;_tmp duat</span><br><span class="line">left join (select xaid from dws_mall.dws_user_active_history group by xaid) duah</span><br><span class="line">on duat.xaid = duah.xaid</span><br><span class="line">where duah.xaid is null;</span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_active_history partition(dt='$&#123;dt&#125;') select</span><br><span class="line">xaid,</span><br><span class="line">times</span><br><span class="line">from dws_mall.dws_user_active_$&#123;dt&#125;_tmp;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app"><a href="#app" class="headerlink" title="app"></a>app</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_new_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_new_count'</span>;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_new_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_new_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_new_count_ratio(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span>,</span><br><span class="line">day_ratio    <span class="keyword">double</span>,</span><br><span class="line">week_ratio    <span class="keyword">double</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_new_count_ratio'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 日环比，周同比</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_new_count_ratio <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">num</span>,</span><br><span class="line">(<span class="keyword">num</span>-num_1)/num_1 <span class="keyword">as</span> day_ratio,</span><br><span class="line">(<span class="keyword">num</span>-num_7)/num_7 <span class="keyword">as</span> week_ratio</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">    dt,</span><br><span class="line">    <span class="keyword">num</span>,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_1,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">7</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_7</span><br><span class="line">    <span class="keyword">from</span> app_mall.app_user_new_count</span><br><span class="line"><span class="keyword">where</span> dt &gt;=regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line">) <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_active_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_active_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_active_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_active_count_ratio(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span>,</span><br><span class="line">day_ratio    <span class="keyword">double</span>,</span><br><span class="line">week_ratio    <span class="keyword">double</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_active_count_ratio'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_active_count_ratio <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">num</span>,</span><br><span class="line">(<span class="keyword">num</span>-num_1)/num_1 <span class="keyword">as</span> day_ratio,</span><br><span class="line">(<span class="keyword">num</span>-num_7)/num_7 <span class="keyword">as</span> week_ratio</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">    dt,</span><br><span class="line">    <span class="keyword">num</span>,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_1,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">7</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_7</span><br><span class="line">    <span class="keyword">from</span> app_mall.app_user_active_count</span><br><span class="line"><span class="keyword">where</span> dt &gt;=regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line">) <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_lost_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_lost_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_lost_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_lost_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_count(</span><br><span class="line">    pv    <span class="built_in">int</span>,</span><br><span class="line">uv    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_count'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>(times) <span class="keyword">as</span> pv,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_distrib(</span><br><span class="line">    ts_1    <span class="built_in">int</span>,</span><br><span class="line">ts_2    <span class="built_in">int</span>,</span><br><span class="line">ts_3_m    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">1</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_1,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">2</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_2,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times &gt;= <span class="number">3</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_3_m</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_platform_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_platform_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_platform_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_android_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_android_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_ios_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_ios_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_brand_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_brand_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_model_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_model_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_model_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_net_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_net_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_platform_all(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_platform_all'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_platform_all <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_android_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_android_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_android_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_ios_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_ios_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_ios_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br></pre></td></tr></table></figure><h6 id="app-mall-init-table-1-sh"><a href="#app-mall-init-table-1-sh" class="headerlink" title="app_mall_init_table_1.sh"></a>app_mall_init_table_1.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日新增用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_new_count(</span><br><span class="line">    num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_new_count';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_new_count_ratio(</span><br><span class="line">    num    int,</span><br><span class="line">day_ratio    double,</span><br><span class="line">week_ratio    double</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_new_count_ratio';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-1-sh"><a href="#app-mall-add-partition-1-sh" class="headerlink" title="app_mall_add_partition_1.sh"></a>app_mall_add_partition_1.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日新增用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 转换日期格式，20260201改为<span class="variable">$&#123;dt_new&#125;</span></span></span><br><span class="line">dt_new=`date +%Y-%m-%d --date="$&#123;dt&#125;"`</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_new_count partition(dt='$&#123;dt&#125;') select</span><br><span class="line">count(*) as num</span><br><span class="line">from dws_mall.dws_user_new_item</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_new_count_ratio partition(dt='$&#123;dt&#125;') select</span><br><span class="line">num,</span><br><span class="line">(num-num_1)/num_1 as day_ratio,</span><br><span class="line">(num-num_7)/num_7 as week_ratio</span><br><span class="line">from(</span><br><span class="line">    select</span><br><span class="line">    dt,</span><br><span class="line">    num,</span><br><span class="line">    lead(num,1) over(order by dt desc) as num_1,</span><br><span class="line">    lead(num,7) over(order by dt desc) as num_7</span><br><span class="line">    from app_mall.app_user_new_count</span><br><span class="line">where dt &gt;=regexp_replace(date_add('$&#123;dt_new&#125;',-7),'-','')</span><br><span class="line">) as t</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051622730.png" alt="image-20230405162216940"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051622364.png" alt="image-20230405162233516"></p><h4 id="需求二：每日活跃用户-主活-相关指标"><a href="#需求二：每日活跃用户-主活-相关指标" class="headerlink" title="需求二：每日活跃用户(主活)相关指标"></a>需求二：每日活跃用户(主活)相关指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">活跃用户的定义是指只要当天使用过 APP 就算是活跃用户，使用 APP 这种操作属于主动操作，所以这种活跃我们也会称为主动活跃，简称主活</span><br><span class="line">针对这个需求统计的指标和新增用户的指标类似</span><br><span class="line">1：每日主活用户量</span><br><span class="line">2：每日主活用户量的日环比和周同比</span><br><span class="line"></span><br><span class="line"> 首先看第一个指标：每日主活用户量</span><br><span class="line">主活的概念和定义我们知道了，其实就是统计每天使用过app的用户，所以我们可以直接使用dws层的dws_user_active_history这个表</span><br><span class="line">直接求和即可获取到当日的主活用户量,将最终的结果保存到app层的</span><br><span class="line">app_user_active_count表中</span><br><span class="line"></span><br><span class="line"> 接着看第二个指标：每日主活用户量的日环比和周同比</span><br><span class="line">这个指标直接基于每日主活用户量的表(app_user_active_coun)进行计算即可，把最终的结果保存到app层的app_user_active_count_ratio 表中</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_new_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_new_count'</span>;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_new_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_new_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_new_count_ratio(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span>,</span><br><span class="line">day_ratio    <span class="keyword">double</span>,</span><br><span class="line">week_ratio    <span class="keyword">double</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_new_count_ratio'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 日环比，周同比</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_new_count_ratio <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">num</span>,</span><br><span class="line">(<span class="keyword">num</span>-num_1)/num_1 <span class="keyword">as</span> day_ratio,</span><br><span class="line">(<span class="keyword">num</span>-num_7)/num_7 <span class="keyword">as</span> week_ratio</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">    dt,</span><br><span class="line">    <span class="keyword">num</span>,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_1,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">7</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_7</span><br><span class="line">    <span class="keyword">from</span> app_mall.app_user_new_count</span><br><span class="line"><span class="keyword">where</span> dt &gt;=regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line">) <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_active_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_active_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_active_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_active_count_ratio(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span>,</span><br><span class="line">day_ratio    <span class="keyword">double</span>,</span><br><span class="line">week_ratio    <span class="keyword">double</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_active_count_ratio'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_active_count_ratio <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">num</span>,</span><br><span class="line">(<span class="keyword">num</span>-num_1)/num_1 <span class="keyword">as</span> day_ratio,</span><br><span class="line">(<span class="keyword">num</span>-num_7)/num_7 <span class="keyword">as</span> week_ratio</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">    dt,</span><br><span class="line">    <span class="keyword">num</span>,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_1,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">7</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_7</span><br><span class="line">    <span class="keyword">from</span> app_mall.app_user_active_count</span><br><span class="line"><span class="keyword">where</span> dt &gt;=regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line">) <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_lost_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_lost_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_lost_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_lost_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_count(</span><br><span class="line">    pv    <span class="built_in">int</span>,</span><br><span class="line">uv    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_count'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>(times) <span class="keyword">as</span> pv,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_distrib(</span><br><span class="line">    ts_1    <span class="built_in">int</span>,</span><br><span class="line">ts_2    <span class="built_in">int</span>,</span><br><span class="line">ts_3_m    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">1</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_1,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">2</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_2,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times &gt;= <span class="number">3</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_3_m</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_platform_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_platform_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_platform_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_android_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_android_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_ios_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_ios_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_brand_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_brand_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_model_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_model_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_model_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_net_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_net_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_platform_all(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_platform_all'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_platform_all <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_android_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_android_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_android_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_ios_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_ios_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_ios_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_2.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_2.sh</span><br></pre></td></tr></table></figure><h5 id="app-mall-init-table-2-sh"><a href="#app-mall-init-table-2-sh" class="headerlink" title="app_mall_init_table_2.sh"></a>app_mall_init_table_2.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求二：每日活跃用户(主活)相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_active_count(</span><br><span class="line">    num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_active_count';</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_active_count_ratio(</span><br><span class="line">    num    int,</span><br><span class="line">day_ratio    double,</span><br><span class="line">week_ratio    double</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_active_count_ratio';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-mall-add-partition-2-sh"><a href="#app-mall-add-partition-2-sh" class="headerlink" title="app_mall_add_partition_2.sh"></a>app_mall_add_partition_2.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日活跃用户(主活)相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 转换日期格式，<span class="variable">$&#123;dt&#125;</span>改为<span class="variable">$&#123;dt_new&#125;</span></span></span><br><span class="line">dt_new=`date +%Y-%m-%d --date="$&#123;dt&#125;"`</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_active_count partition(dt='$&#123;dt&#125;') select</span><br><span class="line">count(*) as num</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_active_count_ratio partition(dt='$&#123;dt&#125;') select</span><br><span class="line">num,</span><br><span class="line">(num-num_1)/num_1 as day_ratio,</span><br><span class="line">(num-num_7)/num_7 as week_ratio</span><br><span class="line">from(</span><br><span class="line">    select</span><br><span class="line">    dt,</span><br><span class="line">    num,</span><br><span class="line">    lead(num,1) over(order by dt desc) as num_1,</span><br><span class="line">    lead(num,7) over(order by dt desc) as num_7</span><br><span class="line">    from app_mall.app_user_active_count</span><br><span class="line">where dt &gt;=regexp_replace(date_add('$&#123;dt_new&#125;',-7),'-','')</span><br><span class="line">) as t</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051642004.png" alt="image-20230405164201393"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051642482.png" alt="image-20230405164229164"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051642965.png" alt="image-20230405164252247"></p><h6 id="需求扩展"><a href="#需求扩展" class="headerlink" title="需求扩展"></a>需求扩展</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如何统计每周主活？每月主活？</span><br><span class="line">周：按照自然周，每周一凌晨计算上一周的主活</span><br><span class="line">月：按照自然月，每月 1 号计算上一个月的主活</span><br></pre></td></tr></table></figure><h4 id="需求三：用户-7-日流失-push-提醒"><a href="#需求三：用户-7-日流失-push-提醒" class="headerlink" title="需求三：用户 7 日流失 push 提醒"></a>需求三：用户 7 日流失 push 提醒</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">什么是流失呢？</span><br><span class="line">假设这个用户在 2026年 2月 2 日是新增用户，如果他在后续的 7 天内，也就是在 2 月 9日</span><br><span class="line">内没有再使用 app，则认为是流失用户，具体多少天属于流失用户，这个是需要产品经理根</span><br><span class="line">据对应产品的特点来定的，一般业内使用比较多的是 7天这个时间点。</span><br><span class="line">push是什么意思呢</span><br><span class="line">大家平时是不是深受各种 app的提醒轰炸，我针对大部分的 app都禁用了消息推送，要不</span><br><span class="line">然每天手机上会有各种各样的推送消息，很烦，这个其实就是软件给你 push的消息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">什么是流失呢？</span><br><span class="line">假设这个用户在 2026年 2月 2 日是新增用户，如果他在后续的 7 天内，也就是在 2 月 9日</span><br><span class="line">内没有再使用 app，则认为是流失用户，具体多少天属于流失用户，这个是需要产品经理根</span><br><span class="line">据对应产品的特点来定的，一般业内使用比较多的是 7天这个时间点。</span><br><span class="line">push是什么意思呢</span><br><span class="line">大家平时是不是深受各种 app的提醒轰炸，我针对大部分的 app都禁用了消息推送，要不</span><br><span class="line">然每天手机上会有各种各样的推送消息，很烦，这个其实就是软件给你 push的消息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">实现思路：</span><br><span class="line">1：基于 dws_user_active_histo表ry，获取表中最近 8 天的数据，根据 xaid进行分组，这样</span><br><span class="line">可以获取 xaid以及 xaid对应的多个日期(dt)</span><br><span class="line">2：接着需要对 xaid对应的 dt进行过滤，获取 xaid中最大的 dt，判断这个 dt是否等于(当天</span><br><span class="line">日期-7)，如果满足条件，则说明这个用户最近 7日内没有使用 app，就认为他属于 7 日流失</span><br><span class="line">用户</span><br><span class="line">例如：dws_user_active_histo表ry中有以下几条数据</span><br><span class="line">xaid dt</span><br><span class="line">a1 2026-02-01</span><br><span class="line">a1 2026-02-05</span><br><span class="line">b1 2026-02-01</span><br><span class="line">b1 2026-02-02</span><br><span class="line">c1 2026-02-03</span><br><span class="line">针对这份数据，我们想要在 02-09号统计用户 7 日流失量</span><br><span class="line">那也就意味着要统计表里面在 02-02使用过 APP，但是在之后的 7天内，一直到 02-09号没有再使用过app的用户</span><br><span class="line">根据xaid进行分组,获取里面最大的日期(最近一次使用app的时间)</span><br><span class="line">a1 2026-02-01,2026-02-05</span><br><span class="line">b1 2026-02-01,2026-02-02</span><br><span class="line">c1 2026-02-03</span><br><span class="line"></span><br><span class="line">判断这个时间是否等于 02-02，如果满足这个条件，就说明在 02-09 号之前的7天内没有使用过app，这里的b1满足条件，所以他就是7日流失用户了。</span><br><span class="line">依此类推，可以计算14日流失, 21日流失用户，针对流失的时间不同可以实现不同的策略给用户实现push提醒，告诉用户他关注的商品降价了，或者给用户推荐他经常浏览的类似商品，促进用户活跃，最终促进订单成交。</span><br><span class="line">3：将满足条件的 xaid 数据保存到dws层的dws_user_lost_item 表中</span><br><span class="line">4：对dws_user_lost_item表中的数据进行聚合统计，统计用户 7 日流失数据量，保存到app层的app_user_lost_count表中</span><br></pre></td></tr></table></figure><h5 id="dws-1"><a href="#dws-1" class="headerlink" title="dws"></a>dws</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_lost_item(</span><br><span class="line">    xaid    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_lost_item'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_lost_item <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">xaid</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt &gt;= regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> xaid</span><br><span class="line"><span class="keyword">having</span> <span class="keyword">max</span>(dt) = regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>);</span><br></pre></td></tr></table></figure><h6 id="提取脚本"><a href="#提取脚本" class="headerlink" title="提取脚本"></a>提取脚本</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_3.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_3.sh</span><br></pre></td></tr></table></figure><h6 id="dws-mall-init-table-3-sh"><a href="#dws-mall-init-table-3-sh" class="headerlink" title="dws_mall_init_table_3.sh"></a>dws_mall_init_table_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：用户7日流失push提醒</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_lost_item(</span><br><span class="line">    xaid    string</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_lost_item';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-3-sh"><a href="#dws-mall-add-partition-3-sh" class="headerlink" title="dws_mall_add_partition_3.sh"></a>dws_mall_add_partition_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：用户7日流失push提醒</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 转换日期格式，<span class="variable">$&#123;dt&#125;</span>改为<span class="variable">$&#123;dt_new&#125;</span></span></span><br><span class="line">dt_new=`date +%Y-%m-%d --date="$&#123;dt&#125;"`</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_user_lost_item partition(dt='$&#123;dt&#125;') select</span><br><span class="line">xaid</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt &gt;= regexp_replace(date_add('$&#123;dt_new&#125;',-7),'-','')</span><br><span class="line">group by xaid</span><br><span class="line">having max(dt) = regexp_replace(date_add('$&#123;dt_new&#125;',-7),'-','');</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-1"><a href="#app-1" class="headerlink" title="app"></a>app</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_lost_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_lost_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_lost_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_lost_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br></pre></td></tr></table></figure><h6 id="提取脚本-1"><a href="#提取脚本-1" class="headerlink" title="提取脚本"></a>提取脚本</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_3.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_3.sh</span><br></pre></td></tr></table></figure><h6 id="app-mall-init-table-3-sh"><a href="#app-mall-init-table-3-sh" class="headerlink" title="app_mall_init_table_3.sh"></a>app_mall_init_table_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：用户7日流失push提醒</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_lost_count(</span><br><span class="line">    num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_lost_count';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-3-sh"><a href="#app-mall-add-partition-3-sh" class="headerlink" title="app_mall_add_partition_3.sh"></a>app_mall_add_partition_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：用户7日流失push提醒</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_lost_count partition(dt='$&#123;dt&#125;') select</span><br><span class="line">count(*) as num</span><br><span class="line">from dws_mall.dws_user_lost_item</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051726034.png" alt="image-20230405172638690"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051730420.png" alt="image-20230405173019710"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051731280.png" alt="image-20230405173138361"></p><h4 id="需求四：每日启动-App-次数相关指标"><a href="#需求四：每日启动-App-次数相关指标" class="headerlink" title="需求四：每日启动 App 次数相关指标"></a>需求四：每日启动 App 次数相关指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">这个需求就是对每日打开 app上报的数据进行统计</span><br><span class="line">针对这个需求我们需要统计两个指标</span><br><span class="line">1：每日人均启动App次数</span><br><span class="line">2：每日APP启动次数分布（1次，2次，3次及以上）</span><br><span class="line"></span><br><span class="line"> 首先看第一个指标：每日人均启动App次数</span><br><span class="line">每日人均启动App次数&#x3D;当日所有用户启动APP总次数&#x2F;当日所有人数</span><br><span class="line">针对这种需求，我们在计算结果的时候最好是把这个指标的分子和分母保存起来，这样这份数据后期还有可能被复用，如果直接保存最终的结果，这个数据就没办法复用了。</span><br><span class="line"></span><br><span class="line">实现思路如下：</span><br><span class="line">1：基于dws_user_active_history表，统计当日的数据，根据times字段的值求pv和uv即可</span><br><span class="line">2：将计算的结果保存到app层的app_user_open_app_count表中</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> 接下来是第二个指标：每日APP启动次数分布（1次，2次，3次及以上）</span><br><span class="line">这个指标也需要基于dws_user_active_history表</span><br><span class="line"></span><br><span class="line">实现思路如下：</span><br><span class="line">对这里面的 times字段进行统计，计算 times&#x3D;1的数据条数、times&#x3D;2的数据条数以及times&gt;&#x3D;3的数据条数即可，将最终的结果保存到app层的 app_user_open_app_distrib中即可</span><br></pre></td></tr></table></figure><h5 id="app-2"><a href="#app-2" class="headerlink" title="app"></a>app</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_count(</span><br><span class="line">    pv    <span class="built_in">int</span>,</span><br><span class="line">uv    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_count'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>(times) <span class="keyword">as</span> pv,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_distrib(</span><br><span class="line">    ts_1    <span class="built_in">int</span>,</span><br><span class="line">ts_2    <span class="built_in">int</span>,</span><br><span class="line">ts_3_m    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">1</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_1,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">2</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_2,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times &gt;= <span class="number">3</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_3_m</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br></pre></td></tr></table></figure><h5 id="开发脚本"><a href="#开发脚本" class="headerlink" title="开发脚本"></a>开发脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_4.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_4.sh</span><br></pre></td></tr></table></figure><h6 id="app-mall-init-table-4-sh"><a href="#app-mall-init-table-4-sh" class="headerlink" title="app_mall_init_table_4.sh"></a>app_mall_init_table_4.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求四：每日启动APP次数相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_open_app_count(</span><br><span class="line">    pv    int,</span><br><span class="line">uv    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_open_app_count';</span><br><span class="line"> </span><br><span class="line">create external table if not exists app_mall.app_user_open_app_distrib(</span><br><span class="line">    ts_1    int,</span><br><span class="line">ts_2    int,</span><br><span class="line">ts_3_m    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_open_app_distrib';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-4-sh"><a href="#app-mall-add-partition-4-sh" class="headerlink" title="app_mall_add_partition_4.sh"></a>app_mall_add_partition_4.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求四：每日启动APP次数相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_open_app_count partition(dt='$&#123;dt&#125;') select</span><br><span class="line">sum(times) as pv,</span><br><span class="line">count(*) as uv</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_open_app_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">sum( case when times = 1 then 1 else 0 end) ts_1,</span><br><span class="line">sum( case when times = 2 then 1 else 0 end) ts_2,</span><br><span class="line">sum( case when times &gt;= 3 then 1 else 0 end) ts_3_m</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051755754.png" alt="image-20230405175545120"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051756799.png" alt="image-20230405175646747"></p><h4 id="需求五：操作系统活跃用户相关指标"><a href="#需求五：操作系统活跃用户相关指标" class="headerlink" title="需求五：操作系统活跃用户相关指标]"></a>需求五：操作系统活跃用户相关指标]</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">这个需求是统计一下我们产品的目前主要用户群体是使用什么类型的操作系统</span><br><span class="line">因为我们产品的app是有Android端和ios端的</span><br><span class="line">如果我们的用户80%以上使用的都是Android，那么我们肯定要针对Android端的 APP做更多的优化支持，这样可以保证大部分用户的使用体验。</span><br><span class="line"></span><br><span class="line">还有就是获取用户使用的手机型号，分辨率信息，这样可以更好的做适配。</span><br><span class="line">针对这个需求我们主要统计以下指标：</span><br><span class="line">1：操作系统活跃用户分布（安卓、IOS）</span><br><span class="line">2：安卓系统版本活跃用户分布</span><br><span class="line">3：IOS系统版本活跃用户分布</span><br><span class="line">4：设备品牌活跃用户分布</span><br><span class="line">5：设备型号活跃用户分布</span><br><span class="line">6：网络类型活跃用户分布</span><br><span class="line">针对这些指标统一分析，其实可以看出来，他们是有相似之处的。都是基于用户使用app时上报的数据相关的一些指标</span><br><span class="line">其实主要就是针对dwd_user_active表中的这些相关维度字段进行分组聚合统计</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">实现思路如下：</span><br><span class="line">1：利用咱们前面讲的维度建模的思想，使用星型模型，基于dwd_user_active表，在外层构建对应的维度表。</span><br><span class="line">2：在dws层基于以上6种维度创建对应的维度聚合表，按天建分区</span><br><span class="line">对应的表名为：</span><br><span class="line">dws_user_platform_distrib</span><br><span class="line">dws_user_andriod_osver_distrib</span><br><span class="line">dws_user_ios_osver_distrib</span><br><span class="line">dws_user_brand_distrib</span><br><span class="line">dws_user_model_distrib</span><br><span class="line">dws_user_net_distrib</span><br><span class="line">3：基于dws层的轻度聚合数据进行全局聚合，因为这些指标统计的时候需要统计所有数据，只统计某一天的没有多大意义，将最终聚合的结果保存到app层，这里面的表就是普通外部表了，里面也不需要日期字段，每天重新生成表里面的数据即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意了，咱们前面保存的有每天聚合的数据，如果后期有需求要统计一段时间内的这些维度的指标，那也很简单，直接基于dws层的表进行统计即可，从这也体现出来了数据分层的好处。</span><br><span class="line"></span><br><span class="line">在app层对应的表名为操作系统活跃用户分布（安卓、IOS）：app_user_platform_distrib</span><br><span class="line">安卓系统版本活跃用户分布：app_user_andriod_osver_distrib</span><br><span class="line">IOS 系统版本活跃用户分布：app_user_ios_osver_distrib</span><br><span class="line">设备品牌活跃用户分布：app_user_brand_distrib</span><br><span class="line">设备型号活跃用户分布：app_user_model_distrib</span><br><span class="line">网络类型活跃用户分布：app_user_net_distrib</span><br></pre></td></tr></table></figure><h5 id="dws-2"><a href="#dws-2" class="headerlink" title="dws"></a>dws</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052112561.png" alt="image-20230405211242014"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_platform_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_platform_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_android_osver_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">osver <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_ios_osver_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">osver <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_brand_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">brand <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> brand;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_model_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_model_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">model</span> <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">model</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_net_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> net</span><br><span class="line"><span class="keyword">when</span> <span class="number">0</span> <span class="keyword">then</span> <span class="string">'未知'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'WIFI'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'2G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">3</span> <span class="keyword">then</span> <span class="string">'3G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">4</span> <span class="keyword">then</span> <span class="string">'4G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">5</span> <span class="keyword">then</span> <span class="string">'5G'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> net;</span><br></pre></td></tr></table></figure><h6 id="开发脚本-1"><a href="#开发脚本-1" class="headerlink" title="开发脚本"></a>开发脚本</h6><h6 id="dws-mall-init-table-5-sh"><a href="#dws-mall-init-table-5-sh" class="headerlink" title="dws_mall_init_table_5.sh"></a>dws_mall_init_table_5.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求五：操作系统活跃用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_platform_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_platform_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_android_osver_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_android_osver_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_ios_osver_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_ios_osver_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_brand_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_brand_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_model_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_model_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_net_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_net_distrib';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-5-sh"><a href="#dws-mall-add-partition-5-sh" class="headerlink" title="dws_mall_add_partition_5.sh"></a>dws_mall_add_partition_5.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求五：操作系统活跃用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_user_platform_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">case platform</span><br><span class="line">when 1 then 'android'</span><br><span class="line">when 2 then 'ios'</span><br><span class="line">end ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform in (1,2)</span><br><span class="line">group by platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_android_osver_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">osver as ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform = 1</span><br><span class="line">group by osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_ios_osver_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">osver as ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform = 2</span><br><span class="line">group by osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_brand_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">brand as ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by brand;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_model_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">model as ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by model;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_net_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">case net</span><br><span class="line">when 0 then '未知'</span><br><span class="line">when 1 then 'WIFI'</span><br><span class="line">when 2 then '2G'</span><br><span class="line">when 3 then '3G'</span><br><span class="line">when 4 then '4G'</span><br><span class="line">when 5 then '5G'</span><br><span class="line">end ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by net;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052144416.png" alt="image-20230405214415743"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052144560.png" alt="image-20230405214454630"></p><h5 id="app-3"><a href="#app-3" class="headerlink" title="app"></a>app</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_platform_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_platform_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_platform_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_android_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_android_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_ios_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_ios_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_brand_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_brand_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_model_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_model_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_model_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_net_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_net_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br></pre></td></tr></table></figure><h6 id="开发脚本-2"><a href="#开发脚本-2" class="headerlink" title="开发脚本"></a>开发脚本</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_5.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_5.sh</span><br></pre></td></tr></table></figure><h6 id="app-mall-init-table-5-sh"><a href="#app-mall-init-table-5-sh" class="headerlink" title="app_mall_init_table_5.sh"></a>app_mall_init_table_5.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求五：操作系统活跃用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_platform_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_platform_distrib';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_android_osver_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_android_osver_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_ios_osver_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_ios_osver_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_brand_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_brand_distrib';</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_model_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_model_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_net_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_net_distrib';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-5-sh"><a href="#app-mall-add-partition-5-sh" class="headerlink" title="app_mall_add_partition_5.sh"></a>app_mall_add_partition_5.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求五：操作系统活跃用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_platform_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_platform_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_android_osver_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_android_osver_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_ios_osver_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_ios_osver_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_brand_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_brand_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_model_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_model_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_net_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_net_distrib</span><br><span class="line">group by ty;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052145058.png" alt="image-20230405214547266"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052146709.png" alt="image-20230405214646268"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052147527.png" alt="image-20230405214755005"></p><h4 id="需求六：APP-崩溃相关指标"><a href="#需求六：APP-崩溃相关指标" class="headerlink" title="需求六：APP 崩溃相关指标"></a>需求六：APP 崩溃相关指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">这个需求是统计在不同平台系统不同版本下APP崩溃的情况，统计这个数据可以方便排查定位问题，如果发现某一个版本的APP崩溃频繁，则需要及时修复问题，推送新版本，提升用户体验。</span><br><span class="line">针对这个需求主要统计下面几个指标</span><br><span class="line">1：每日操作系统崩溃总计（安卓、IOS）</span><br><span class="line">2：每日安卓系统-不同APP版本崩溃量</span><br><span class="line">3：每日IOS系统-不同APP版本崩溃量</span><br><span class="line"></span><br><span class="line">这里面这三个指标是有关联的，第一个是总的统计，第二个和第三个是不同维度的统计</span><br><span class="line">实现思路：</span><br><span class="line">针对第一个指标，使用dwd_app_close表中的数据，根据platform进行分组统计即可</span><br><span class="line"></span><br><span class="line">但是注意：第二个指标和第三个指标，也需要根据不同的platform进行统计，但是又多了一个操作系统的维度，如果按照我们刚才的分析，直接基于platform进行分组的话，针对后面两个指标还需要重新计算中间表，没有体现出来数据仓库的好处。</span><br><span class="line"></span><br><span class="line">所以我们可以这样做：</span><br><span class="line">针对dwd_app_close表中的数据，使用platform和vercode进行分组，做轻度聚合，将数据保存到dws层的dws_app_close_platform_vercode表中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于dws_app_close_platform_vercode表中的数据就可以计算出来这三个指标了。</span><br><span class="line">这三个指标的结果分别保存到app层的以下这些表中：</span><br><span class="line">每日操作系统崩溃总计（安卓、IOS）：app_app_close_platform_all</span><br><span class="line">每日安卓系统-不同 APP 版本崩溃量 app_app_close_android_vercode</span><br><span class="line">每日苹果系统-不同 APP 版本崩溃量 app_app_close_ios_vercode</span><br></pre></td></tr></table></figure><h5 id="dws-层"><a href="#dws-层" class="headerlink" title="dws 层"></a>dws 层</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_app_close_platform_vercode(</span><br><span class="line">    platform    <span class="keyword">string</span>,</span><br><span class="line">vercode    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/app_close_platform_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_app_close_platform_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">platform,</span><br><span class="line">vercode,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_app_close</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform,vercode;</span><br></pre></td></tr></table></figure><h6 id="开发脚本-3"><a href="#开发脚本-3" class="headerlink" title="开发脚本"></a>开发脚本</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_6.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_6.sh</span><br></pre></td></tr></table></figure><h6 id="dws-mall-init-table-6-sh"><a href="#dws-mall-init-table-6-sh" class="headerlink" title="dws_mall_init_table_6.sh"></a>dws_mall_init_table_6.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求六：APP崩溃相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_app_close_platform_vercode(</span><br><span class="line">    platform    string,</span><br><span class="line">vercode    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/app_close_platform_vercode';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-6-sh"><a href="#dws-mall-add-partition-6-sh" class="headerlink" title="dws_mall_add_partition_6.sh"></a>dws_mall_add_partition_6.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求六：APP崩溃相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_app_close_platform_vercode partition(dt='$&#123;dt&#125;') select</span><br><span class="line">platform,</span><br><span class="line">vercode,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_app_close</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform in (1,2)</span><br><span class="line">group by platform,vercode;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052215542.png" alt="image-20230405221537102"></p><h5 id="app层"><a href="#app层" class="headerlink" title="app层"></a>app层</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_platform_all(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_platform_all'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_platform_all <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_android_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_android_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_android_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_ios_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_ios_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_ios_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br></pre></td></tr></table></figure><h6 id="开发脚本-4"><a href="#开发脚本-4" class="headerlink" title="开发脚本"></a>开发脚本</h6><h6 id="app-mall-init-table-6-sh"><a href="#app-mall-init-table-6-sh" class="headerlink" title="app_mall_init_table_6.sh"></a>app_mall_init_table_6.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求六：APP崩溃相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_app_close_platform_all(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/app_close_platform_all';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_app_close_android_vercode(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/app_close_android_vercode';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_app_close_ios_vercode(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/app_close_ios_vercode';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-6-sh"><a href="#app-mall-add-partition-6-sh" class="headerlink" title="app_mall_add_partition_6.sh"></a>app_mall_add_partition_6.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求六：APP崩溃相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_app_close_platform_all partition(dt='$&#123;dt&#125;') select</span><br><span class="line">case platform</span><br><span class="line">when 1 then 'android'</span><br><span class="line">when 2 then 'ios'</span><br><span class="line">end ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_app_close_platform_vercode</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by platform;</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_app_close_android_vercode partition(dt='$&#123;dt&#125;') select</span><br><span class="line">vercode as ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_app_close_platform_vercode</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform = 1</span><br><span class="line">group by vercode;</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_app_close_ios_vercode partition(dt='$&#123;dt&#125;') select</span><br><span class="line">vercode as ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_app_close_platform_vercode</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform = 2</span><br><span class="line">group by vercode;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052216787.png" alt="image-20230405221619712"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052219532.png" alt="image-20230405221957019"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052221901.png" alt="image-20230405222129463"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052221386.png" alt="image-20230405222138929"></p><h2 id="用户行为数据数仓总结"><a href="#用户行为数据数仓总结" class="headerlink" title="用户行为数据数仓总结"></a>用户行为数据数仓总结</h2><h3 id="数据库和表梳理"><a href="#数据库和表梳理" class="headerlink" title="数据库和表梳理"></a>数据库和表梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052224849.png" alt="image-20230405222432071"></p><h3 id="任务脚本梳理"><a href="#任务脚本梳理" class="headerlink" title="任务脚本梳理"></a>任务脚本梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052226522.png" alt="image-20230405222643594"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库之用户行为数仓3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933.html</id>
    <published>2023-03-30T13:22:30.000Z</published>
    <updated>2023-04-06T16:59:51.760Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="电商数据仓库之用户行为数仓3-数据生成与采集"><a href="#电商数据仓库之用户行为数仓3-数据生成与采集" class="headerlink" title="电商数据仓库之用户行为数仓3-数据生成与采集"></a>电商数据仓库之用户行为数仓3-数据生成与采集</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们就来开发第一个模块：数据采集模块</span><br><span class="line">这一块内容在开发的时候，我们需要先生成测试数据，一份是服务端数据，还有一份是客户端数据</span><br></pre></td></tr></table></figure><h2 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h2><h3 id="【客户端数据】用户行为数据"><a href="#【客户端数据】用户行为数据" class="headerlink" title="【客户端数据】用户行为数据"></a>【客户端数据】用户行为数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先我们模拟生成用户行为数据，也就是客户端数据，主要包含用户打开APP、点击、浏览等行为数据</span><br><span class="line">用户行为数据：通过埋点上报，后端日志服务器(http)负责接收数据</span><br><span class="line">埋点上报数据基本格式：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;uid&quot;：1001, &#x2F;&#x2F;用户ID</span><br><span class="line">&quot;xaid&quot;：&quot;ab25617-c38910-m2991&quot;, &#x2F;&#x2F;手机设备ID</span><br><span class="line">&quot;platform&quot;：2, &#x2F;&#x2F;设备类型, 1:Android-APP, 2:IOS-APP, 3:PC </span><br><span class="line">&quot;ver&quot;：&quot;3.5.10&quot;, &#x2F;&#x2F;大版本号</span><br><span class="line">&quot;vercode&quot;：&quot;35100083&quot;, &#x2F;&#x2F;子版本号</span><br><span class="line">&quot;net&quot;：1, &#x2F;&#x2F;网络类型, 0:未知, 1:WIFI, 2:2G , 3:3G, 4:4G, 5:5G</span><br><span class="line">&quot;brand&quot;：&quot;iPhone&quot;, &#x2F;&#x2F;手机品牌</span><br><span class="line">&quot;model&quot;：&quot;iPhone8&quot;, &#x2F;&#x2F;机型</span><br><span class="line">&quot;display&quot;：&quot;1334x750&quot;, &#x2F;&#x2F;分辨率</span><br><span class="line">&quot;osver&quot;：&quot;ios13.5&quot;, &#x2F;&#x2F;操作系统版本号</span><br><span class="line">&quot;data&quot;：[ &#x2F;&#x2F;用户行为数据</span><br><span class="line">&#123;&quot;act&quot;：1,&quot;acttime&quot;：1592486549819,&quot;ad_status&quot;：1,&quot;loading_time&quot;:100&#125;,</span><br><span class="line">&#123;&quot;act&quot;：2,&quot;acttime&quot;：1592486549819,&quot;goods_id&quot;：&quot;2881992&quot;&#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个json串中的data是一个json数组，它里面包含了多种用户行为数据。</span><br><span class="line">json串中的其它字段属于公共字段</span><br><span class="line"></span><br><span class="line">注意：考虑到性能，一般数据上报都是批量上报，假设间隔10秒上报一次，这种数据延迟是可以接受的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以在每次上报的时候，公共字段只需要报一份就行，把不同的用户行为相关的业务字段放到data数组中，这样可以避免上报大量的重复数据，影响数据上报性能，我们只需要在后期解析的时候，把公共字段和data数组总的每一条业务字段进行拼装，就可以获取到每一个用户行为的所有字段信息。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">act代表具体的用户行为，在这列出来几种</span><br><span class="line">act&#x3D;1：打开APP</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型 </span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">ad_status 开屏广告展示状态, 1:成功 2:失败</span><br><span class="line">loading_time 开屏广告加载耗时(单位毫秒)</span><br><span class="line"></span><br><span class="line">act&#x3D;2：点击商品</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">goods_id 商品ID</span><br><span class="line">location 商品展示顺序：在列表页中排第几位，从0开始</span><br><span class="line"></span><br><span class="line">act&#x3D;3：商品详情页</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">goods_id 商品ID</span><br><span class="line">stay_time 页面停留时长(单位毫秒)</span><br><span class="line">loading_time 页面加载耗时(单位毫秒)</span><br><span class="line"></span><br><span class="line">act&#x3D;4：商品列表页</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">loading_time 页面加载耗时(单位毫秒)</span><br><span class="line">loading_type 加载类型：1:读缓存 2:请求接口</span><br><span class="line">goods_num 列表页加载商品数量</span><br><span class="line"></span><br><span class="line">act&#x3D;5：app崩溃数据</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br></pre></td></tr></table></figure><h4 id="生成用户行为测试数据"><a href="#生成用户行为测试数据" class="headerlink" title="生成用户行为测试数据"></a>生成用户行为测试数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里实现不了，课程提供的用户行为生成接口，需要提供uid、课程订单，然后执行提前编写好的测试数据生成代码。</span><br></pre></td></tr></table></figure><h4 id="部署日志采集服务"><a href="#部署日志采集服务" class="headerlink" title="部署日志采集服务"></a>部署日志采集服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">部署日志采集服务，模拟埋点上报数据的流程，代码在db_data_warehouse中的data_collect这个子项目中，将这个子项目打成jar包，部署到bigdata04服务器中，并且启动此HTTP服务。</span><br><span class="line">对data_collect执行打包操作，在cmd命令下执行 mvn clean package -DskipTests</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">D:\IdeaProjects\db_data_warehouse\data_collect&gt;mvn clean package -DskipTests</span><br><span class="line">[INFO] Scanning for projects...</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br><span class="line">[INFO] Building data_collect 1.0-SNAPSHOT</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br><span class="line">[INFO] Replacing main artifact with repackaged archive</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04的&#x2F;data&#x2F;soft&#x2F;目录下创建data_collect目录</span><br><span class="line"></span><br><span class="line">1 [root@bigdata04 soft]# mkdir data_collect</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">然后把target目录下的 data_collect-1.0-SNAPSHOT.jar 上传到bigdata04的 &#x2F;data&#x2F;soft&#x2F;data_collect</span><br><span class="line">里面</span><br><span class="line">接着就可以启动这个项目了，这个其实就是一个web项目。</span><br><span class="line">为了后面使用方便，我在这里面写一个启动脚本</span><br><span class="line"></span><br><span class="line">[root@bigdata04 data_collect]# vi start.sh</span><br><span class="line">nohup java -jar data_collect-1.0-SNAPSHOT.jar &gt;&gt; nohup.out &amp;</span><br><span class="line">[root@bigdata04 data_collect]# sh start.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">确认是否成功启动</span><br><span class="line">[root@bigdata04 data_collect]# jps -ml</span><br><span class="line">1601 sun.tools.jps.Jps -ml</span><br><span class="line">1563 data_collect-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">获取到的数据格式是这样的：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302339866.png" alt="image-20230330233905222"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302339704.png" alt="image-20230330233917373"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先解析data属性的值，里面包含了多个用户的行为数据</span><br><span class="line">并且每个用户的行为数据中还包含了多种具体的行为操作，因为客户端在上报数据的时候不是产生一条就上报一条，这样效率太低了，一般都会批量上报，所以内层json串中还有一个data参数，data参数的值是一个JSONArray，里面包含一个用户的多种行为数据</span><br><span class="line"></span><br><span class="line">然后通过接口模拟上报数据，data_collect接口接收到数据之后，会对数据进行拆分，将包含了多个用户行为的数据拆开，打平，输出多条日志数据</span><br><span class="line">日志数据格式是这样的：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302343266.png" alt="image-20230330234319650"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">最终的日志数据会保存在data_collect这个日志采集服务所在的机器上，通过log4j记录在&#x2F;data&#x2F;log目录下面。</span><br><span class="line">去确认一下：</span><br><span class="line">[root@bigdata04 log]# ll</span><br><span class="line">total 32</span><br><span class="line">-rw-r--r--. 1 root root 20881 Jun 30 18:00 user_action.log</span><br><span class="line">[root@bigdata04 log]# head -1 user_action.log </span><br><span class="line">&#123;&quot;ver&quot;:&quot;3.4.1&quot;,&quot;display&quot;:&quot;1920x1080&quot;,&quot;osver&quot;:&quot;7.1.1&quot;,&quot;platform&quot;:1,&quot;uid&quot;:&quot;1000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302347512.png" alt="image-20230330234726391"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到这为止，用户行为数据就生成好了。</span><br></pre></td></tr></table></figure><h3 id="【服务端数据】商品订单相关数据"><a href="#【服务端数据】商品订单相关数据" class="headerlink" title="【服务端数据】商品订单相关数据"></a>【服务端数据】商品订单相关数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">接下来需要生成商品订单相关数据，这些数据都是存储在mysql中的</span><br><span class="line">注意：MySQL在这里我使用的版本是8.x</span><br><span class="line"></span><br><span class="line">相关表名为：</span><br><span class="line">订单表：user_order </span><br><span class="line">商品信息表：goods_info</span><br><span class="line">订单商品表：order_item</span><br><span class="line">商品类目码表：category_code</span><br><span class="line">订单收货表：order_delivery </span><br><span class="line">支付流水表：payment_flow</span><br><span class="line">用户收货地址表：user_addr</span><br><span class="line">用户信息表：user</span><br><span class="line">用户扩展表：user_extend</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302357855.png" alt="image-20230330235704680"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">powerdesigner这个软件设计的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先在MySQL中初始化数据库和表。</span><br><span class="line">使用这个脚本进行初始化： init_mysql_tables.sql</span><br><span class="line">初始化成功之后的效果如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310002439.png" alt="image-20230331000219996"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接下来需要向表中初始化数据。</span><br><span class="line">使用generate_data项目中的这个类：GenerateGoodsOrderData</span><br><span class="line"></span><br><span class="line">在具体执行之前需要先修改GenerateGoodsOrderData中的几个参数值</span><br><span class="line">(1)code的值</span><br><span class="line">(2)date的值</span><br><span class="line">(3)user_num的值</span><br><span class="line">(4)order_num的值</span><br><span class="line">(5)修改项目的resources目录下的db.properties文件</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面就可以执行GenerateGoodsOrderData向MySQL中初始化数据了。</span><br></pre></td></tr></table></figure><h2 id="采集数据"><a href="#采集数据" class="headerlink" title="采集数据"></a>采集数据</h2><h3 id="采集用户行为数据"><a href="#采集用户行为数据" class="headerlink" title="采集用户行为数据"></a>采集用户行为数据</h3><h4 id="配置Flume的Agent"><a href="#配置Flume的Agent" class="headerlink" title="配置Flume的Agent"></a>配置Flume的Agent</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据接收到以后，需要使用flume采集数据，按照act值的不同，将数据分目录存储</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">flume Agent配置内容如下：</span><br><span class="line">useraction-to-hdfs.conf</span><br><span class="line"></span><br><span class="line"># agent的名称是a1</span><br><span class="line"># 指定source组件、channel组件和Sink组件的名称</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line"># 配置source组件</span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;user_action.log</span><br><span class="line"># 配置拦截器</span><br><span class="line">a1.sources.r1.interceptors &#x3D; i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type &#x3D; regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i1.regex &#x3D; &quot;act&quot;:(\\d)</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers &#x3D; s1</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.name &#x3D; act</span><br><span class="line"># 配置channel组件</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"># 配置sink组件</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;%Y%m%d&#x2F;%&#123;a</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#增加文件前缀和后缀</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix &#x3D; .log</span><br><span class="line"># 把组件连接起来</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h4 id="开始采集数据"><a href="#开始采集数据" class="headerlink" title="开始采集数据"></a>开始采集数据</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310100507.png" alt="image-20230331010029253"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310101359.png" alt="image-20230331010115320"></p><h3 id="采集商品订单相关数据"><a href="#采集商品订单相关数据" class="headerlink" title="采集商品订单相关数据"></a>采集商品订单相关数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们需要将商品订单数据采集到HDFS里面，咱们前面分析过，在这里针对关系型数据库数据的采集我们使用Sqoop</span><br><span class="line">使用sqoop的导入功能，将MySQL中的数据导入到HDFS上面</span><br><span class="line">那首先我们来看一下Sqoop的使用，因为Sqoop主要是一个工具，所以我们就快速的学习一下。</span><br></pre></td></tr></table></figure><h4 id="Sqoop的使用"><a href="#Sqoop的使用" class="headerlink" title="Sqoop的使用"></a>Sqoop的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下面我们需要将商品订单数据采集到HDFS里面，咱们前面分析过，在这里针对关系型数据库数据的采集</span><br><span class="line"></span><br><span class="line">我们使用Sqoop</span><br><span class="line">使用sqoop的导入功能，将MySQL中的数据导入到HDFS上面</span><br><span class="line">那首先我们来看一下Sqoop的使用，因为Sqoop主要是一个工具，所以我们就快速的学习一下。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Sqoop目前有两大版本，Sqoop1和Sqoop2，这两个版本都是一直在维护者的，所以使用哪个版本都可以。</span><br><span class="line">这两个版本我都用过，还是感觉Sqoop1用起来比较方便，使用Sqoop1的时候可以将具体的命令全部都</span><br><span class="line">写到脚本中，这样看起来是比较清晰的，但是有一个弊端，就是在操作MySQL的时候，MySQL数据库的</span><br><span class="line">用户名和密码会明文暴露在这些脚本中，不过一般也没有什么问题，因为在访问生产环境下的MySQL的</span><br><span class="line">时候，是需要申请权限的，就算你知道了MySQL的用户名和密码，但是你压根无法访问MySQL的那台机</span><br><span class="line">器，所以这样也是安全的，只要运维那边权限控制到位了就没问题。</span><br><span class="line">sqoop2中引入了sqoop server(服务)，集中管理connector(连接)，而sqoop1只是客户端工具。</span><br><span class="line">相对来说，Sqoop1更加简洁，轻量级。</span><br><span class="line">Sqoop1的最后更新时间是2018年</span><br><span class="line">Sqoop2的最后更新时间是2016年</span><br><span class="line">Sqoop2我之前在使用的时候发现里面bug还是比较多的，相对来说Sqoop1更加稳定一些。</span><br><span class="line">所以在这我们采用Sqoop1。</span><br><span class="line">想要使用Sqoop1，先去官网下载安装包</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：最终下载的sqoop1.4.7的安装是这个 sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个安装包表示里面包含了hadoop-2.6.0的依赖，我们目前使用的是hadoop3.2.0，不过是可以兼容的，这样就没有必要重新编辑sqoop了。</span><br><span class="line">Sqoop的安装部署很简单，因为Sqoop1只是一个客户端工具，直接解压，修改一下配置文件就行，不需要启动任何进程</span><br><span class="line">Sqoop在执行的时候底层会生成MapReduce任务，所以Sqoop需要部署在Hadoop客户端机器上，因为它是依赖于Hadoop的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">我们在bigdata04机器上安装部署Sqoop</span><br><span class="line">1：把Sqoop的安装包上传到bigdata04机器的&#x2F;data&#x2F;soft目录下</span><br><span class="line">2：解压</span><br><span class="line"></span><br><span class="line">3：修改配置文件的名称</span><br><span class="line">[root@bigdata04 soft]# cd sqoop-1.4.7.bin__hadoop-2.6.0&#x2F;conf</span><br><span class="line">[root@bigdata04 conf]# mv sqoop-env-template.sh sqoop-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">4：配置SQOOP_HOME环境变量</span><br><span class="line">[root@bigdata04 conf]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">...</span><br><span class="line">export SQOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;sqoop-1.4.7.bin__hadoop-2.6.0</span><br><span class="line">export PATH&#x3D;.......$SQOOP_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5：将MySQL 8的驱动jar包，添加到SQOOP_HOME的lib目录下，因为我们需要使用Sqoop操作MySQL</span><br><span class="line">查看验证一下是否成功添加MySQL的驱动jar包</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：使用hadoop 3.2.0版本的时候，需要在SQOOP_HOME的lib目录下增加commons-lang2.6.jar</span><br><span class="line">查看验证一下是否成功添加commons-lang-2.6.jar</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sqoop-1.4.7.bin__hadoop-2.6.0]# ll lib&#x2F;commons-lang-2.6.jar </span><br><span class="line">-rw-r--r--. 1 root root 284220 Nov 10 2015 lib&#x2F;commons-lang-2.6.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">6：开放MySQL远程访问权限【开放权限以后集群中的机器才可以连接windows上的MySQL服务,否则只能在windows本地访问】</span><br><span class="line">注意：我的MySQL的用户名为root，密码为admin，大家在执行下面命令的时候需要对应替换为自己的MySQL的真实用户名和密码。</span><br><span class="line"></span><br><span class="line">C:\Users\yehua&gt;mysql -uroot -padmin</span><br><span class="line">mysql&gt; USE mysql;</span><br><span class="line">mysql&gt; CREATE USER &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;admin&#39;;</span><br><span class="line">mysql&gt; GRANT ALL ON *.* TO &#39;root&#39;@&#39;%&#39;;</span><br><span class="line">mysql&gt; ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;admin&#39;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">到此为止，Sqoop相关的配置全部搞定。</span><br><span class="line">下面我们就来分析一下Sqoop中的两大功能。</span><br><span class="line">导入数据sqoop-import：从MySQL导入HDFS</span><br><span class="line">导出数据sqoop-export：从HDFS导出MySQL</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">来看一下Sqoop的文档</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041526800.png" alt="image-20230404152619779"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041526805.png" alt="image-20230404152632314"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041527063.png" alt="image-20230404152729853"></p><h5 id="通用参数"><a href="#通用参数" class="headerlink" title="通用参数"></a>通用参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">整理了文档中一些参数的解释：</span><br><span class="line">Sqoop通用参数：</span><br><span class="line"></span><br><span class="line">选项 含义说明</span><br><span class="line">--connect &lt;jdbc-uri&gt; 指定JDBC连接字符串</span><br><span class="line">--connection-manager &lt;class-name&gt; 指定要使用的连接管理器类</span><br><span class="line">--driver &lt;class-name&gt; 指定要使用的JDBC驱动类</span><br><span class="line">--hadoop-mapred-home &lt;dir&gt; 指定$HADOOP_MAPRED_HOME路径</span><br><span class="line">--help 万能帮助</span><br><span class="line">--password-file 设置用于存放认证的密码信息文件的路径</span><br><span class="line">-P 从控制台读取输入的密码</span><br><span class="line">--password &lt;password&gt; 设置认证密码</span><br><span class="line">--username &lt;username&gt; 设置认证用户名</span><br><span class="line">--verbose 打印详细的运行信息</span><br><span class="line">--connection-param-file &lt;filename&gt; 可选，指定存储数据库连接参数的属性文件</span><br></pre></td></tr></table></figure><h5 id="导入参数"><a href="#导入参数" class="headerlink" title="导入参数"></a>导入参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">导入功能相关参数</span><br><span class="line"></span><br><span class="line">选项 含义说明</span><br><span class="line">--append 将数据追加到HDFS上一个已存在的数据集上</span><br><span class="line">--as-avrodatafile 将数据导入到Avro数据文件</span><br><span class="line">--as-sequencefile 将数据导入到SequenceFile</span><br><span class="line">--as-textfile 将数据导入到普通文本文件（默认）</span><br><span class="line">--boundary-query &lt;statement&gt; 边界查询，用于创建分片（InputSplit）</span><br><span class="line">--columns &lt;col,col,col…&gt; 从表中导出指定的一组列的数据</span><br><span class="line">--delete-target-dir 如果指定目录存在，则先删除掉</span><br><span class="line">--direct 使用直接导入模式（优化导入速度）</span><br><span class="line">--direct-split-size &lt;n&gt; 分割输入stream的字节大小（在直接导入模式下）</span><br><span class="line">--fetch-size &lt;n&gt; 从数据库中批量读取记录数</span><br><span class="line">--inline-lob-limit &lt;n&gt; 设置内联的LOB对象的大小</span><br><span class="line">-m,--num-mappers &lt;n&gt; 使用n个map任务并行导入数据</span><br><span class="line">-e,--query &lt;statement&gt; 导入的查询语句</span><br><span class="line">--split-by &lt;column-name&gt; 指定按照哪个列去分割数据</span><br><span class="line">--table &lt;table-name&gt; 导入的源表表名</span><br><span class="line">--target-dir &lt;dir&gt; 导入HDFS的目标路径</span><br><span class="line">--warehouse-dir &lt;dir&gt; HDFS存放表的根路径</span><br><span class="line">--where &lt;where clause&gt; 指定导出时所使用的查询条件</span><br><span class="line">-z,--compress 启用压缩</span><br><span class="line">--compression-codec &lt;c&gt; 指定Hadoop的codec方式（默认gzip）</span><br><span class="line">--null-string &lt;null-string&gt; 如果指定列为字符串类型，使用指定字符串替换值为</span><br><span class="line">--null-non-string &lt;null-string&gt; 如果指定列为非字符串类型，使用指定字符串替换值</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下数据导入功能：</span><br><span class="line">数据导入可以分为全表导入和查询导入</span><br><span class="line">(1)全表导入：直接把一个表中的所有数据全部导入到HDFS里面</span><br><span class="line">先在MySQL中创建一个数据库和表</span><br><span class="line">C:\Users\yehua&gt;mysql -uroot -padmin</span><br><span class="line">mysql&gt; create database imooc;</span><br><span class="line">Query OK, 1 row affected (0.09 sec)</span><br><span class="line">mysql&gt; use imooc;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; create table user(id int(10),name varchar(64));</span><br><span class="line">Query OK, 0 rows affected (0.60 sec)</span><br><span class="line">mysql&gt; insert into table user(id,name) values(1,&#39;jack&#39;);</span><br><span class="line">Query OK, 1 row affected (0.16 sec)</span><br><span class="line">mysql&gt; insert into table user(id,name) values(2,&#39;tom&#39;);</span><br><span class="line">Query OK, 1 row affected (0.08 sec)</span><br><span class="line">mysql&gt; insert into table user(id,name) values(3,&#39;mike&#39;);</span><br><span class="line">Query OK, 1 row affected (0.05 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">使用Sqoop将imooc.user表中的数据导入到HDFS中</span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table user \</span><br><span class="line">--target-dir &#x2F;out1 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#39;\t&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：如果表中没有主键则会报错(因为mapper数默认是4，需要分4个Task。但是info表又没有主键，MapReduce不知道以哪个字段为准来分Task。)</span><br><span class="line">解决办法有三种：</span><br><span class="line">可以选择在表中设置主键，默认根据主键字段分task</span><br><span class="line">使用–num-mappers 1 ，表示将map任务个数设置为1，sqoop默认是4</span><br><span class="line">使用–split-by ，后面跟上一个数字类型的列，会根据这个列分task</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(2)查询导入：使用sql语句查询表中满足条件的数据导入到HDFS里面</span><br><span class="line">注意：在使用–query指定sql的时候，则必须包含$CONDITIONS</span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--target-dir &#x2F;out2 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--query &#39;select id,name from user where id &gt;1 and $CONDITIONS;&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意：–query和–table不能同时指定</span><br><span class="line">问题：sqoop在导入数据的时候针对空值如何处理？</span><br><span class="line">默认情况下MySQL中的null值(无论字段类型是字符串类型还是数字类型)，使用Sqoop导入到HDFS文件中之后，都会显示为字符串null。</span><br><span class="line">针对字符串null类型：通过 --null-string &#39;*&#39; 来指定，单引号中指定一个字符即可，这个字符不能是--，因为 -- 是保留关键字</span><br><span class="line">针对非字符串的null类型：通过 --null-non-string &#39;&#x3D;&#39; 来指定，单引号中指定一个字符即可，这个字符不能是 -- ，因为 -- 是保留关键字</span><br><span class="line">这两个参数可以同时设置，这样在导入数据的时候，针对空值字段，会替换为指定的内容。</span><br><span class="line">例如：可以使用 \N ，因为我们把数据导入到HDFS之后，最终是希望在Hive中查询的，Hive中针对NULL值在底层是使用 \N 存储的。</span><br><span class="line">当然了，我们也可以选择给NULL值指定一个默认的其它字符。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--target-dir &#x2F;out2 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--query &#39;select id,name from user where id &gt;1 and $CONDITIONS;&#39; \</span><br><span class="line">--null-string &#39;\\N&#39; \</span><br><span class="line">--null-non-string &#39;\\N&#39; \</span><br></pre></td></tr></table></figure><h5 id="导出参数"><a href="#导出参数" class="headerlink" title="导出参数"></a>导出参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">--validate &lt;class-name&gt; 启用数据副本验证功能，仅支持单表拷贝，可以</span><br><span class="line">--validation-threshold &lt;class-name&gt; 指定验证门限所使用的类</span><br><span class="line">--direct 使用直接导出模式（优化速度）</span><br><span class="line">--export-dir &lt;dir&gt; 导出过程中HDFS源路径</span><br><span class="line">--m,--num-mappers &lt;n&gt; 使用n个map任务并行导出</span><br><span class="line">--table &lt;table-name&gt; 导出的目的表名称</span><br><span class="line">--call &lt;stored-proc-name&gt; 导出数据调用的指定存储过程名</span><br><span class="line">--update-key &lt;col-name&gt; 更新参考的列名称，多个列名使用逗号分隔</span><br><span class="line">--update-mode &lt;mode&gt; 指定更新策略，包括：updateonly（默认）、</span><br><span class="line">--input-null-string &lt;null-string&gt; 使用指定字符串，替换字符串类型值为null的列</span><br><span class="line">--input-null-non-string &lt;null-string&gt; 使用指定字符串，替换非字符串类型值为null的</span><br><span class="line">--staging-table &lt;staging-table-name&gt; 在数据导出到数据库之前，数据临时存放的表名</span><br><span class="line">--clear-staging-table 清除工作区中临时存放的数据</span><br><span class="line">--batch 使用批量模式导出</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下Sqoop的导出功能</span><br><span class="line">从HDFS导出到MySQL，将刚才导入到HDFS中的数据再导出来。</span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table user2 \</span><br><span class="line">--export-dir &#x2F;out2 \</span><br><span class="line">--input-fields-terminated-by &#39;\t&#39;</span><br><span class="line"></span><br><span class="line">注意：这里 --table 指定的表名需要提前创建，sqoop不会自动创建此表。</span><br><span class="line"></span><br><span class="line">C:\Users\yehua&gt;mysql -uroot -padmin</span><br><span class="line">mysql&gt; use imooc;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; create table user2(id int(10),name varchar(64));</span><br><span class="line">Query OK, 0 rows affected (0.24 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">验证结果，查询MySQL中的数据</span><br><span class="line">mysql&gt; select * from user2;</span><br><span class="line">+------+------+</span><br><span class="line">| id | name |</span><br><span class="line">+------+------+</span><br><span class="line">| 3 | mike |</span><br><span class="line">| 2 | tom |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在导出的时候可以实现插入和更新功能</span><br><span class="line">如果存在就更新，不存在就插入</span><br><span class="line"></span><br><span class="line">注意：此时表中必须有一个主键字段</span><br><span class="line"></span><br><span class="line">将user2中的id字段设置为主键，</span><br><span class="line">然后修改user2中id为2那条数据的name字段的值为imooc</span><br><span class="line">删除id为3的那条数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041552875.png" alt="image-20230404155237504"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">修改之后的user2表中的数据如下：</span><br><span class="line">mysql&gt; select * from user2;</span><br><span class="line">+----+-------+</span><br><span class="line">| id | name |</span><br><span class="line">+----+-------+</span><br><span class="line">| 2 | imooc |</span><br><span class="line">+----+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">执行sqoop语句</span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table user2 \</span><br><span class="line">--export-dir &#x2F;out2 \</span><br><span class="line">--input-fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--update-key id \</span><br><span class="line">--update-mode allowinsert</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">再验证一下结果，会发现针对已有的数据更新，没有的数据新增。</span><br><span class="line">mysql&gt; select * from user2;</span><br><span class="line">+------+------+</span><br><span class="line">| id | name |</span><br><span class="line">+------+------+</span><br><span class="line">| 3 | mike |</span><br><span class="line">| 2 | tom |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这就是Sqoop的导入和导出功能。</span><br><span class="line">后期我们在使用Sqoop的时候，建议将Sqoop的命名写到shell脚本中，否则使用起来不方便。</span><br><span class="line">[root@bigdata04 soft]# vi sqoop-ex-user.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table user2 \</span><br><span class="line">--export-dir &#x2F;out2 \</span><br><span class="line">--input-fields-terminated-by &#39;\t&#39;</span><br></pre></td></tr></table></figure><h4 id="数据采集方式"><a href="#数据采集方式" class="headerlink" title="数据采集方式"></a>数据采集方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">全量采集(数据量不大，每天都改变：一天采集一次；数据量不大，几十年都不变：只做一次全量采集)</span><br><span class="line"></span><br><span class="line">增量采集(数据量大，每天采集新增数据) </span><br><span class="line"></span><br><span class="line">hive不能对数据进行修改(比如mysql中的表的订单信息已改变，但hive中不支持修改)-&gt;解决：拉链表</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933%5C202303310122674.png" alt="image-20230331012207048"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：手机号在采集的时候需要脱敏处理，因为数据进入到数据仓库之后会有很多人使用，为保护用户隐私，最好在采集的时候进行脱敏处理。</span><br><span class="line">所以在采集user和user_addr表中的数据时对手机号进行脱敏。</span><br><span class="line"></span><br><span class="line">18315138177</span><br><span class="line">183xxxxx177</span><br></pre></td></tr></table></figure><h4 id="数据采集脚本开发"><a href="#数据采集脚本开发" class="headerlink" title="数据采集脚本开发"></a>数据采集脚本开发</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面就开始进行数据采集，其实就是使用Sqoop实现的数据导入</span><br><span class="line">开发一个通用的sqoop数据采集脚本</span><br><span class="line">在bigdata04机器上创建目录 &#x2F;data&#x2F;soft&#x2F;warehouse_shell_good_order ，针对商品订单相关的脚本全部放在这里面</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 soft]# mkdir warehouse_shell_good_order</span><br><span class="line"></span><br><span class="line">创建脚本 sqoop_collect_data_util.sh</span><br><span class="line"></span><br><span class="line">[root@bigdata04 warehouse_shell_good_order]#vi sqoop_collect_data_util.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 采集MySQL中的数据导入到HDFS中</span><br><span class="line">if [ $# !&#x3D; 2 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;参数异常：sqoop_collect_data_util.sh &lt;sql&gt; &lt;hdfs_path&gt;&quot;</span><br><span class="line">exit 100</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># 数据SQL</span><br><span class="line"># 例如：select id,name from user where id &gt;1</span><br><span class="line">sql&#x3D;$1</span><br><span class="line"></span><br><span class="line"># 导入到HDFS的路径</span><br><span class="line">hdfs_path&#x3D;$2</span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;mall?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--target-dir &quot;$&#123;hdfs_path&#125;&quot; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--query &quot;$&#123;sql&#125;&quot;&#39; and $CONDITIONS&#39; \</span><br><span class="line">--null-string &#39;\\N&#39; \</span><br><span class="line">--null-non-string &#39;\\N&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：如果在windows中使用notepad++开发shell脚本的时候，需要将此参数设置为UNIX。</span><br><span class="line">这个我们之前在讲shell的时候已经讲过了，在这再重复一遍。</span><br><span class="line">否则在windows中开发的脚本直接上传到linux中执行会报错。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041610733.png" alt="image-20230404161003265"></p><h4 id="开始采集数据-1"><a href="#开始采集数据-1" class="headerlink" title="开始采集数据"></a>开始采集数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">针对全量数据采集和增量数据采集开发不同的脚本</span><br><span class="line">全量数据采集： collect_data_full.sh</span><br></pre></td></tr></table></figure><h5 id="全量采集"><a href="#全量采集" class="headerlink" title="全量采集"></a>全量采集</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 warehouse_shell_good_order]#vi collect_data_full.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 全量数据采集</span><br><span class="line"># 每天凌晨执行一次</span><br><span class="line"></span><br><span class="line"># 默认获取昨天的日期，也支持传参指定一个日期</span><br><span class="line">if [ &quot;z$1&quot; &#x3D; &quot;z&quot; ]</span><br><span class="line">then</span><br><span class="line">dt&#x3D;&#96;date +%Y%m%d --date&#x3D;&quot;1 days ago&quot;&#96;</span><br><span class="line">else</span><br><span class="line">dt&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># SQL语句</span><br><span class="line">user_sql&#x3D;&quot;select user_id,user_name,user_gender,user_birthday,e_mail,concat(left(mobile,3), &#39;****&#39; ,right(mobile,4)) as mobile,register_time,is_blacklist from user where 1&#x3D;1&quot;</span><br><span class="line">user_extend_sql&#x3D;&quot;select user_id,is_pregnant_woman,is_have_children,is_have_car,phone_brand,phone_cnt,change_phone_cnt,weight,height from user_extend where 1&#x3D;1&quot;</span><br><span class="line">user_addr_sql&#x3D;&quot;select addr_id,user_id,addr_name,order_flag,user_name,concat(left(mobile,3), &#39;****&#39; ,right(mobile,4)) as mobile from user_addr where 1&#x3D;1&quot;</span><br><span class="line">goods_info_sql&#x3D;&quot;select goods_id,goods_no,goods_name,curr_price,third_category_id,goods_desc,create_time from goods_info where 1&#x3D;1&quot;</span><br><span class="line">category_code_sql&#x3D;&quot;select first_category_id,first_category_name,second_category_id,second_catery_name,third_category_id,third_category_name from category_code where 1&#x3D;1&quot;</span><br><span class="line"></span><br><span class="line"># 路径后缀</span><br><span class="line">path_prefix&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&quot;</span><br><span class="line"></span><br><span class="line"># 输出路径</span><br><span class="line">user_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;user&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">user_extend_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;user_extend&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">user_addr_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;user_addr&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">goods_info_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;goods_info&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">category_code_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;category_code&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line"></span><br><span class="line"># 采集数据</span><br><span class="line">echo &quot;开始采集...&quot;</span><br><span class="line">echo &quot;采集表：user&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;user_sql&#125;&quot; &quot;$&#123;user_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：user_extend&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;user_extend_sql&#125;&quot; &quot;$&#123;user_extend_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：user_addr&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;user_addr_sql&#125;&quot; &quot;$&#123;user_addr_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：goods_info&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;goods_info_sql&#125;&quot; &quot;$&#123;goods_info_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：category_code&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;category_code_sql&#125;&quot; &quot;$&#123;category_code_path&#125;&quot;</span><br><span class="line">echo &quot;结束采集...&quot;</span><br></pre></td></tr></table></figure><h5 id="增量采集"><a href="#增量采集" class="headerlink" title="增量采集"></a>增量采集</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">增量数据采集： collect_data_incr.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 增量数据采集</span><br><span class="line"># 每天凌晨执行一次</span><br><span class="line"></span><br><span class="line"># 默认获取昨天的日期，也支持传参指定一个日期</span><br><span class="line">if [ &quot;z$1&quot; &#x3D; &quot;z&quot; ]</span><br><span class="line">then</span><br><span class="line">dt&#x3D;&#96;date +%Y%m%d --date&#x3D;&quot;1 days ago&quot;&#96;</span><br><span class="line">else</span><br><span class="line">dt&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># 转换日期格式，20260101 改为 2026-01-01</span><br><span class="line">dt_new&#x3D;&#96;date +%Y-%m-%d --date&#x3D;&quot;$&#123;dt&#125;&quot;&#96;</span><br><span class="line"></span><br><span class="line"># SQL语句</span><br><span class="line">user_order_sql&#x3D;&quot;select order_id,order_date,user_id,order_money,order_type,order_status,pay_id,update_time from user_order where order_date &gt;&#x3D; &#39;$&#123;dt_new&#125; 00:00:00&#39; and order_date &lt;&#x3D; &#39;$&#123;dt_new&#125; 23:59:59&#39;&quot;</span><br><span class="line">order_item_sql&#x3D;&quot;select order_id,goods_id,goods_amount,curr_price,create_time from order_item where create_time &gt;&#x3D; &#39;$&#123;dt_new&#125; 00:00:00&#39; and create_time &lt;&#x3D; &#39;$&#123;dt_new&#125; 23:59:59&#39;&quot;</span><br><span class="line">order_delivery_sql&#x3D;&quot;select order_id,addr_id,user_id,carriage_money,create_time from order_delivery where create_time &gt;&#x3D; &#39;$&#123;dt_new&#125; 00:00:00&#39; and create_time &lt;&#x3D; &#39;$&#123;dt_new&#125; 23:59:59&#39;&quot;</span><br><span class="line">payment_flow_sql&#x3D;&quot;select pay_id,order_id,trade_no,pay_money,pay_type,pay_time from payment_flow where pay_time &gt;&#x3D; &#39;$&#123;dt_new&#125; 00:00:00&#39; and pay_time &lt;&#x3D; &#39;$&#123;dt_new&#125; 23:59:59&#39;&quot;</span><br><span class="line"></span><br><span class="line"># 路径后缀</span><br><span class="line">path_prefix&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&quot;</span><br><span class="line"></span><br><span class="line"># 输出路径</span><br><span class="line">user_order_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;user_order&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">order_item_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;order_item&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">order_delivery_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;order_delivery&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">payment_flow_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;payment_flow&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line"></span><br><span class="line"># 采集数据</span><br><span class="line">echo &quot;开始采集...&quot;</span><br><span class="line">echo &quot;采集表：user_order&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;user_order_sql&#125;&quot; &quot;$&#123;user_order_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：order_item&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;order_item_sql&#125;&quot; &quot;$&#123;order_item_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：order_delivery&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;order_delivery_sql&#125;&quot; &quot;$&#123;order_delivery_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：payment_flow&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;payment_flow_sql&#125;&quot; &quot;$&#123;payment_flow_path&#125;&quot;</span><br><span class="line">echo &quot;结束采集...&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">执行脚本</span><br><span class="line">验证结果：</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F;data&#x2F;ods</span><br><span class="line">Found 10 items</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;category_</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;goods_inf</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;order_de</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;order_it</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:43 &#x2F;data&#x2F;ods&#x2F;payment_f</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:29 &#x2F;data&#x2F;ods&#x2F;user</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 11:04 &#x2F;data&#x2F;ods&#x2F;user_act</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;user_add</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:29 &#x2F;data&#x2F;ods&#x2F;user_ext</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:41 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F;data&#x2F;ods&#x2F;user_order&#x2F;20260101</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r-- 2 root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br><span class="line">-rw-r--r-- 2 root supergroup 74618 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">执行脚本</span><br><span class="line"></span><br><span class="line">[root@bigdata04 warehouse_shel_good_order]# sh collect_data_full.sh 20260101</span><br><span class="line">[root@bigdata04 warehouse_shel_good_order]# sh collect_data_incr.sh 20260101</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F;data&#x2F;ods</span><br><span class="line">Found 10 items</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;category_</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;goods_inf</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;order_de</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;order_it</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:43 &#x2F;data&#x2F;ods&#x2F;payment_f</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:29 &#x2F;data&#x2F;ods&#x2F;user</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 11:04 &#x2F;data&#x2F;ods&#x2F;user_act</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;user_add</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:29 &#x2F;data&#x2F;ods&#x2F;user_ext</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:41 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F;data&#x2F;ods&#x2F;user_order&#x2F;20260101</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r-- 2 root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br><span class="line">-rw-r--r-- 2 root supergroup 74618 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库之用户行为数仓2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%932.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%932.html</id>
    <published>2023-03-30T12:27:10.000Z</published>
    <updated>2023-03-30T13:17:21.292Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十二周-综合项目-电商数据仓库之用户行为数仓2"><a href="#第十二周-综合项目-电商数据仓库之用户行为数仓2" class="headerlink" title="第十二周 综合项目:电商数据仓库之用户行为数仓2"></a>第十二周 综合项目:电商数据仓库之用户行为数仓2</h1><h2 id="电商数仓技术选型"><a href="#电商数仓技术选型" class="headerlink" title="电商数仓技术选型"></a>电商数仓技术选型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">咱们前面对项目的需求进行了分析，整体上来说是需要三个大的功能模块，那下面我们就来分析一下，想要实现这些功能模块，具体使用哪些技术框架比较合适</span><br></pre></td></tr></table></figure><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">首先是数据采集：</span><br><span class="line">咱们前面学习了Flume这个数据采集工具</span><br><span class="line">其实还有一些类似的数据采集工具，Logstash、FileBeat，这两个也可以实现数据采集</span><br><span class="line"></span><br><span class="line">那这三个日志采集工具我们需要如何选择呢？</span><br><span class="line">首先从性能消耗上面来说，Flume和Logstash的性能消耗差不多，都是基于JVM执行的，都是重量级的组件，支持多种数据源和目的地。</span><br><span class="line"></span><br><span class="line">FileBeat是一个只支持文件数据采集的工具，是一个轻量级组件，性能消耗比价低，它不是基于JVM执行的，它是使用go语言开发的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们在采集数据的时候可以分为两种情况</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302033047.png" alt="image-20230330203300260"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">第一种是把采集工具部署到产生数据的服务器上面</span><br><span class="line">web项目产生的日志数据直接保存在服务器上面，并且这个服务器的性能比较高，可以允许我在上面部署Flume数据采集工具，这样也不会对上面的web项目的稳定性产生什么影响。</span><br><span class="line"></span><br><span class="line">第二种是把采集工具部署在一个独立的服务器上面</span><br><span class="line">web项目产生的日志数据直接保存在服务器上面，但是这个服务器的性能一般，并且对web项目的稳定性要求非常高，如果让你在上面部署一个其它服务，这样这个服务器的稳定性就没办法保证了，进而也就无法保证web项目的稳定性了，所以这个时候可以选择在产生日志的时候使用埋点上报的方式，通过http接</span><br><span class="line">口把日志数据传输到日志接收服务器中</span><br><span class="line"></span><br><span class="line">那针对第一种情况肯定是要选择一个性能消耗比较低的数据采集工具，优先选择FileBeat</span><br><span class="line">针对第二种情况的话就不需要考虑性能消耗了，因为采集工具是在独立的机器上，不会影响web项目，这个时候我们需要考虑的就是采集工具的功能是否完整，因为在采集数据的时候可能需要对数据进行一些简单的处理，以及后期可能会输出到不同的存储介质中。</span><br><span class="line"></span><br><span class="line">Flume和Logstash都是支持多种输入、多种输出、以及都可以在采集数据的时候对数据做一些处理，那这个时候该如何选择呢？</span><br><span class="line">注意了，这个时候我们就要考虑如果后期采集工具出现了问题，或者我们需要自定义一些功能，维护成本高不高，Flume是使用java开发的，而Logstash是使用ruby开发的，由于我们都是java出身，所以考虑到后期的维护成本，Flume是最优的选择。</span><br><span class="line"></span><br><span class="line">在采集数据的时候，除了日志数据，有时候还需要采集一些业务系统的数据，这些数据一般保存在关系型数据库中，例如：MySQL</span><br><span class="line">我们也需要把这些数据采集过来，如果MySQL开启了binlog，那我们可以使用Flume采集</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">什么是MySQL的binlog呢？</span><br><span class="line">可以这样理解，MySQL对数据库的任何修改都会记录在binlog中，所以如果开启了binlog，那我们就可以使用Flume采集这个日志数据，就可以获取到MySQL中数据的变化了。</span><br><span class="line"></span><br><span class="line">但是目前我们的MySQL没有开启binlog，并且也没有打算开启binlog，那怎么办？</span><br><span class="line">因为我们这个需求不需要实时采集MySQL中的数据，所以不开启binlog也是没有问题的，Flume默认不支持直接采集MySQL中的数据，如果想要实现的话需要自定义Source，这样就比较麻烦了</span><br><span class="line"></span><br><span class="line">其实采集MySQL中的数据有一个比较常用的方式是通过Sqoop实现。</span><br><span class="line">Sqoop中有两大功能，数据导入和数据导出</span><br><span class="line"></span><br><span class="line">数据导入是指把关系型数据库中的数据导入HDFS中</span><br><span class="line">数据导出是指把HDFS中的数据导出到关系型数据库中</span><br><span class="line"></span><br><span class="line">我们后期在做一些报表的时候其实也是需要把数据仓库中的数据(APP层)导出到MySQL中的，所以在这选择Sqoop也是非常实用的。</span><br><span class="line"></span><br><span class="line">所以针对数据采集这块，我们主要选择了Flume和Sqoop</span><br></pre></td></tr></table></figure><h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据采集过来以后，由于我们后面要构建数据仓库，数据仓库是使用Hive实现，Hive的数据是存储在HDFS中的，所以我们把采集到的数据也直接存储到HDFS里面</span><br><span class="line">还有一点是后期在做一些数据报表的时候，是需要把数据仓库中的数据导出到MySQL中的，所以数据存储也需要使用到MySQL。</span><br></pre></td></tr></table></figure><h3 id="数据计算"><a href="#数据计算" class="headerlink" title="数据计算"></a>数据计算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在构建数据仓库的时候，我们前面说了，是使用Hive构建数据仓库，一般的数据处理通过SQL是可以搞定的，如果遇到了比较复杂的处理逻辑，可能还需要和外部的数据进行交互的，这个时候使用SQL就比较麻烦了，内置的函数有时候搞不定，还需要开发自定义函数</span><br><span class="line"></span><br><span class="line">针对复杂的数据清洗任务我们也可以考虑使用Spark进行处理。</span><br></pre></td></tr></table></figure><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在数据可视化层面，我们可以使用Hue(无法出图表)数据查询</span><br><span class="line"></span><br><span class="line">如果想实现写SQL直接出图表(简单的图表)zeppelin</span><br><span class="line">如果想定制开发图表(复杂图表)的话可以使用Echarts(百度开源的)之类的图表库，这个时候是需要我们自己开发数据接口实现的。</span><br></pre></td></tr></table></figure><h2 id="整体架构设计"><a href="#整体架构设计" class="headerlink" title="整体架构设计"></a>整体架构设计</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">技术选型搞定后，下面我们来看一下整体的架构设计</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302116171.png" alt="image-20230330211658080"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">我们采集的数据主要分为服务端数据和客户端数据</span><br><span class="line">什么是服务端数据，就是网站上的商品详情数据以及你下的订单信息之类的数据，这些数据都是在服务端存储着的，一般是存储在类似于MySQL之类的关系型数据库中，这些数据对事务性要求比较严格，所以会存放在关系型数据库中。</span><br><span class="line"></span><br><span class="line">什么是客户端数据呢，就是用户在网站或者app上的一些滑动、点击、浏览、停留时间之类的用户行为数据，这些数据会通过埋点直接上报，这些其实就是一些日志类型的数据了，这种类型的数据没有事务性要求，并且对数据的完整性要求也不是太高，就算丢一些数据，对整体结果影响也不大。</span><br><span class="line"></span><br><span class="line">针对服务端数据，在采集的时候，主要是通过Sqoop进行采集，按天采集，每天凌晨的时候把昨天的数据采集过来，存储到HDFS上面。</span><br><span class="line">针对客户端数据，会通过埋点上报到日志接收服务器中，其实这里面就是一个Http服务，埋点上报就是调用了这个Http服务，把日志数据传输过来，日志接收服务收到数据之后，会把数据落盘，存储到本地，记录为日志文件，然后通过Flume进行采集，将数据采集到HDFS上面，按天分目录存储。</span><br><span class="line"></span><br><span class="line">服务端数据和客户端数据都进入到HDFS之后，就需要对数据进行ETL，构建数据仓库了。</span><br><span class="line">数据仓库构建好了以后可以选择把一些需要报表展现的数据导出到MySQL中，最终在页面进行展现。</span><br></pre></td></tr></table></figure><h2 id="服务器资源规划"><a href="#服务器资源规划" class="headerlink" title="服务器资源规划"></a>服务器资源规划</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">整体架构分析好了，下面我们来分析一下，想要实现这个架构，服务器资源应该如何划分</span><br><span class="line">针对我们的测试环境：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302105852.png" alt="image-20230330210548415"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对生产环境，至少需要这些机器：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302106891.png" alt="image-20230330210649799"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">说明</span><br><span class="line">1：由于NameNode开启了HA，所以SecondaryNameNode进程就不需要了</span><br><span class="line">2：NameNode需要使用单独的机器，并且此机器的内存配置要比较高，建议128G</span><br><span class="line">3：DataNode和NodeManager需要部署在相同的集群上，这样可以实现数据本地化计算</span><br><span class="line">4：Hadoop Client需要部署在需要和Hadoop交互的机器上 </span><br><span class="line">5：数据接口服务器需要使用至少两台，为了实现负载均衡及故障转移，保证数据接收服务的稳定性</span><br><span class="line">6：Flume部署在日志服务器上面，便于采集本机保存的用户行为日志信息；还需要有单独的Flume机器，便于处理其它的日志采集需求</span><br><span class="line">7：Hive需要部署在所有业务机器上</span><br><span class="line">8：MySQL建议单独部署，至少两台，一主一备</span><br><span class="line">9：Sqoop需要部署在所有业务机器上</span><br><span class="line">10：Zeppelin可以单独部署在一台普通配置的机器上即可</span><br><span class="line">11：Azkaban建议至少使用三台，一主两从，这样可以保证一个从节点挂掉之后不影响定时任务的调度</span><br><span class="line">针对Hadoop集群的搭建在线上环境需要使用CDH或者HDP</span><br><span class="line">具体Hadoop集群需要使用多少台集群需要根据当前的数据规模来预估</span><br><span class="line">假设集群中的机器配置为8T，64 Core，128G</span><br><span class="line">1：如果每天会产生1T的日志数据，需要保存半年的历史数据： 1T*180天&#x3D;180T</span><br><span class="line">2：集群中的数据默认是3副本： 180T*3&#x3D;540T</span><br><span class="line">3：预留20%左右的空间： 540T&#x2F;0.8&#x3D;675T</span><br><span class="line">这样计算的话就需要675T&#x2F;8T&#x3D;85台服务器</span><br><span class="line">如果我们在数据仓库中对数据进行分层存储，这样数据会出现冗余，存储空间会再扩容1~2倍</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：没有必要一开始就上线全部的机器，我们可以前期上线30台，后面随着业务数据量的增长再去动态扩容机器即可。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%93.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%93.html</id>
    <published>2023-03-30T09:08:53.000Z</published>
    <updated>2023-04-06T16:41:44.598Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十二周-综合项目-电商数据仓库之用户数据行为数仓"><a href="#第十二周-综合项目-电商数据仓库之用户数据行为数仓" class="headerlink" title="第十二周 综合项目:电商数据仓库之用户数据行为数仓"></a>第十二周 综合项目:电商数据仓库之用户数据行为数仓</h1><h2 id="电商数据仓库效果展示"><a href="#电商数据仓库效果展示" class="headerlink" title="电商数据仓库效果展示"></a>电商数据仓库效果展示</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面我们来学习一个电商行业的数据仓库项目</span><br><span class="line">首先看一下项目效果</span><br><span class="line"></span><br><span class="line">本身我们这个数据仓库项目其实是一个纯后台项目，不过为了让大家能够更加直观的感受项目的效果，我们可以基于数据仓库中的数据统计一些指标进行展现。</span><br><span class="line">我们这个项目要讲的重点不是这个大屏，这个大屏只是一个效果，为了让大家感受更加直观一些而已，我们主要讲的是这些指标对应的底层数据是如何在数据仓库中一层一层构建的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301725400.png" alt="image-20230330172526095"></p><h3 id="项目的由来"><a href="#项目的由来" class="headerlink" title="项目的由来"></a>项目的由来</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下这个项目的由来，我们为什么要做这个数据仓库项目呢？或者说做这个数据仓库项目有什么意义吗？</span><br><span class="line"></span><br><span class="line">在工作中我们经常会遇到这些情况</span><br><span class="line">产品经理过来说，老王啊，你来看一下，为什么这个数据指标在不同的报表中统计的结果不一样呢？</span><br><span class="line">老王听到后，心里拔凉拔凉的，今晚又得加班了，约好的女朋友看电影估计又得泡汤了。</span><br><span class="line"></span><br><span class="line">举个例子：针对平台里面的用户下单数据：我们会在客户端记录一份，就是当用户通过网页或者app下单的时候，会触发一个行为，官方名词叫“埋点”，这个埋点对应的其实就是一个接口，当用户通过网页或者app下单的时候，就会触发这个埋点，然后埋点会上报这个数据，最终会把用户下单行为的数据记录下来，这份数据我们就称之为是客户端记录的数据。</span><br><span class="line"></span><br><span class="line">同时服务端也会在数据库中维护一份用户的下单数据。</span><br><span class="line">最终在做报表统计的时候：</span><br><span class="line">如果想要按照分钟级别实时计算，做一个用户消费金额的曲线图，我们一般会使用客户端实时上报的数据，用起来比较方便，但是通过客户端埋点上报的数据可能会有一些问题，有可能会丢数据，以及用户在点击下单按钮的时候可能由于网络异常导致下单失败了，但是这条行为数据却发送过来了，以及用户下单之后还可能会退款，所以这里面统计的数据指标会有一些偏差，不过偏差倒不是很大，如果只是想看一下当天用户消费总金额的一个实时趋势，其实这样做是没有什么问题的。但是你要是使用这份数据统计每天用户的消费总金额，那肯定是有问题的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">如果想要按照天来汇总每天用户的消费总金额，这种数据指标对数据的实时性没什么要求，但是对数据的准确度有很高要求，所以一般会使用数据库中的数据，因为数据库是有事务的，并且在统计的时候也可以排除掉用户退款的数据，这样统计出来的才是这一天用户真正的消费总金额。</span><br><span class="line">所以刚才产品经理反馈的问题很大概率是这个原因，当然也有可能是由于计算的口径不一样，因为不同的报表可能是由不同的需求人员提出来的，指标的计算公式也是有所差别的，甚至有的指标是上一任产品经理提出来的，每个人想要统计的指标是有一些区别的，所以在使用数据的时候就会遇到各种各样的问题。</span><br><span class="line"></span><br><span class="line">其实，归根结底，就是因为数据不统一，计算流程不统一导致的结果出现偏差。</span><br><span class="line">后来，老王经过一路追踪，发现，原来在统计这个指标的时候，两个报表使用的底层数据不是同一份，所以导致统计的指标有偏差，然后又给产品经理一顿解释，给产品经理解释完又赶紧打电话给女朋友解释，然后就没有然后了。</span><br><span class="line"></span><br><span class="line">经过这件事情之后，老王觉得，数据仓库的构建必须提上日程了</span><br><span class="line">通过构建企业级数据仓库，对企业中的所有数据进行整合，为企业各个部门提供统一的，规范的数据出口</span><br><span class="line">这样大家在使用数据的时候不需要每次都到各种地方去找数据，所有人在使用的时候都是基于相同的基础数据，这样计算出来的指标肯定是相同的。</span><br><span class="line"></span><br><span class="line">一个完善合理的数据仓库对于企业整体的数据管理是意义重大的，而数据仓库也是整个大数据系统中的重要一环，更高层次的数据分析、数据挖掘等工作都会基于数据仓库进行</span><br><span class="line">如果你的底层数据都没有规划好，那么上层的数据分析以及数据挖掘都是会受影响的。</span><br><span class="line">就像是我们盖房子，如果地基没有打牢，盖出来的房子肯定也是摇摇欲坠。</span><br><span class="line">所以说数据仓库对于一个中大型企业而言是至关重要的。</span><br><span class="line"></span><br><span class="line">那话又说回来了，如果你们公司刚起步，产品也是刚上线，这个时候你花大量的时间去搞数据仓库也是没有意义的，这个时候讲究的是快速迭代。</span><br><span class="line"></span><br><span class="line">只有说数据规模上来之后，数据仓库才是有意义的，并且也是必不可少的。</span><br></pre></td></tr></table></figure><h2 id="数据仓库前置技术"><a href="#数据仓库前置技术" class="headerlink" title="数据仓库前置技术"></a>数据仓库前置技术</h2><h3 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">咱们前面说了要构建一个数据仓库，那严格意义上来说，到底什么是数据仓库呢？</span><br><span class="line">咱们前面学习过Hive，说Hive其实就是一个数据仓库，可以这样理解，就是把Hive认为是一种技术，通过Hive这种技术可以实现数据仓库的建设。</span><br><span class="line"></span><br><span class="line">咱们这个项目中的数据仓库就是使用Hive构建的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">来看一下针对数据仓库的官方解释：</span><br><span class="line">数据仓库(Data Warehouse)是一个面向主题的、集成的、稳定的且随时间变化的数据集合，用于支持管理人员的决策</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">注意它里面的这几个特性：</span><br><span class="line"></span><br><span class="line">面向主题</span><br><span class="line">主题就是类型的意思。</span><br><span class="line">传统数据库主要是为应用程序进行数据处理，未必会按照同一主题存储数据；</span><br><span class="line">数据仓库侧重于数据分析工作，是按照主题存储的。</span><br><span class="line"></span><br><span class="line">这一点，类似于传统农贸市场与超市的区别</span><br><span class="line">市场里面，针对一个商贩，他卖的萝卜、白菜这些蔬菜以及水果会在一个摊位上；</span><br><span class="line">而超市里，蔬菜和水果是分开的，并且在蔬菜里面也会进行分类，不同类型的蔬菜放到不同的地方。</span><br><span class="line">也就是说，农贸市场里的菜(数据)是按照商贩(应用程序)去归类(存储)的，而超市里面则是按照蔬菜、水果的类型(同主题)归类的。</span><br><span class="line"></span><br><span class="line">集成</span><br><span class="line">传统数据库通常与某些特定的应用相关，数据库之间相互独立。而数据仓库中的数据是在对原有分散的数据库数据抽取、清理的基础上经过系统加工、汇总和整理得到的，必须消除源数据中的不一致性，以保证数据仓库内的信息是关于整个企业的一致的全局信息。</span><br><span class="line"></span><br><span class="line">稳定</span><br><span class="line">稳定说的是相对稳定</span><br><span class="line">传统数据库中的数据通常实时更新，数据根据需要及时发生变化。数据仓库的数据主要供企业决策分析使用，所涉及的数据操作主要是数据查询，一旦某个数据进入数据仓库以后，一般情况下将被长期保留，也就是数据仓库中一般有大量的查询操作，但修改和删除操作很少，通常只需要定期的加载、刷新。</span><br><span class="line"></span><br><span class="line">变化</span><br><span class="line">这里的变化说的是反映历史变化</span><br><span class="line">传统数据库主要关心当前某一个时间段内的数据，而数据仓库中的数据通常包含历史信息，它里面记录了企业从过去某一时间点(如开始应用数据仓库的时间)到目前的各个阶段的信息，通过这些信息，可以对企业的发展历程和未来趋势做出分析和预测。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">企业数据仓库的建设，是以现有企业业务系统和大量业务数据的积累为基础。数据仓库不是静态的概念，只有把信息及时交给需要这些信息的使用者，供他们做出改善其业务经营的决策，信息才能发挥作用，信息才有意义。而把信息加以整理归纳和重组，并及时提供给相应的管理决策人员，是数据仓库的根本任务。</span><br></pre></td></tr></table></figure><h3 id="数据仓库基础知识"><a href="#数据仓库基础知识" class="headerlink" title="数据仓库基础知识"></a>数据仓库基础知识</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在学习数据仓库之前我们先来看一些数据仓库的基础知识</span><br><span class="line">1：事实表、维度表</span><br><span class="line">2：数据库三范式</span><br><span class="line">3：维度建模模型：雪花模型、星型模型</span><br></pre></td></tr></table></figure><h4 id="事实表、维度表"><a href="#事实表、维度表" class="headerlink" title="事实表、维度表"></a>事实表、维度表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">首先我们来看两个名词：事实表和维度表</span><br><span class="line">什么是事实表呢？</span><br><span class="line">事实表是指保存了大量业务数据的表，或者说保存了一些真实的行为数据的表</span><br><span class="line">例如：销售商品所产生的订单数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301738183.png" alt="image-20230330173811777"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">什么是维度表呢？</span><br><span class="line">首先说一下什么是维度</span><br><span class="line">维度其实指的就是一个对象的属性或者特征，例如：时间维度，地理区域维度，年龄维度</span><br><span class="line">这是维度的概念。</span><br><span class="line">维度表里面存放的其实就是刚才我们所说的那些维度相关的信息。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301739060.png" alt="image-20230330173934481"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是事实表和维度表的特点，大家最起码要能区分出来一个表是事实表还是维度表，否则在工作中别人提到这两个概念你还是一脸懵，那就尴尬了。</span><br></pre></td></tr></table></figure><h4 id="数据库三范式"><a href="#数据库三范式" class="headerlink" title="数据库三范式"></a>数据库三范式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接下来我们需要复习一下数据库中的三范式的特性，不知道大家还有没有印象，这个属于数据库相关的知识，如果大家系统的学习过类似于MySQL之类的关系型数据库的话，这块应该是有一些印象的。</span><br><span class="line">其实严格意义上来说，关系型数据库的范式是有多种的</span><br><span class="line">第一范式(1NF)</span><br><span class="line">第二范式(2NF)</span><br><span class="line">第三范式(3NF)</span><br><span class="line">巴斯-科德范式(BCNF)</span><br><span class="line">第四范式(4NF)</span><br><span class="line">第五范式(5NF)</span><br><span class="line">不过后面那几种不太常见，数据库设计一般满足第三范式就足够了，所以在这我们就分析一下前三种范式</span><br></pre></td></tr></table></figure><h5 id="第一范式-1NF"><a href="#第一范式-1NF" class="headerlink" title="第一范式(1NF)"></a>第一范式(1NF)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">它的意思是说数据库表的每一列都是不可分割的原子数据项</span><br><span class="line">来看下面这个案例：</span><br><span class="line">这里面存储的是学生信息</span><br><span class="line">但是这里面的地址字段显然是不符合第一范式的，因为这里面的地址信息是可以拆分为省份+城市+街道信息的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301743386.png" alt="image-20230330174354885"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以针对这个字段进行拆分，让这个表满足第一范式</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301744439.png" alt="image-20230330174431140"></p><h5 id="第二范式-2NF"><a href="#第二范式-2NF" class="headerlink" title="第二范式(2NF)"></a>第二范式(2NF)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第二范式(2NF)表示在1NF的基础上，数据库表中每一列都和主键相关，不能只和主键的某一部分相关(针对联合主键而言)</span><br><span class="line">也就是说一个表中只能保存一种类型的数据，不可以把多种类型数据保存在同一张表中</span><br><span class="line">来看下面这个案例：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301747252.png" alt="image-20230330174638394"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个表里面除了存储的有学生的班级信息，还有学生的考试成绩信息</span><br><span class="line">根据我们刚才的分析，它是满足第一范式的，但是违背了第二范式，数据库表中的每一列并不是都和主键相关</span><br><span class="line">所以我们为了让这个表满足第二范式，可以这样拆分：</span><br><span class="line">拆成两个表，一个表里面保存学生的班级信息，一个表里面保存学生的考试成绩信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301752515.png" alt="image-20230330175233201"></p><h5 id="第三范式-3NF"><a href="#第三范式-3NF" class="headerlink" title="第三范式(3NF)"></a>第三范式(3NF)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">注意：满足第三范式（3NF）必须先满足第二范式（2NF）。</span><br><span class="line"></span><br><span class="line">第三范式(3NF): 要求一个数据库表中不包含已在其它表中包含的非主键字段</span><br><span class="line">就是说，表中的某些字段信息，如果能够被推导出来，就不应该单独的设计一个字段来存放(能尽量外键join就用外键join)。</span><br><span class="line"></span><br><span class="line">很多时候，我们为了满足第三范式往往会把一张表拆分成多张表</span><br><span class="line"></span><br><span class="line">来看下面这个案例</span><br><span class="line">针对刚才满足了第二范式的表，其实还可以进行拆分</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301750405.png" alt="image-20230330175033146"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这样就满足数据库的第三范式了。</span><br><span class="line">我们在这分析这三种范式有什么意义吗？不要着急，往下面看</span><br></pre></td></tr></table></figure><h4 id="数据仓库建模方式"><a href="#数据仓库建模方式" class="headerlink" title="数据仓库建模方式"></a>数据仓库建模方式</h4><h5 id="ER实体模型"><a href="#ER实体模型" class="headerlink" title="ER实体模型"></a>ER实体模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">数据仓库建模可以使用多种方式</span><br><span class="line">1：ER实体模型，这种模型其实就是满足数据库第三范式的模型，这就是刚才我们为什么要分析数据库中的三范式了。</span><br><span class="line"></span><br><span class="line">ER模型是数据库设计的理论基础，当前几乎所有的OLTP系统设计都采用ER模型建模的方式Bill Inom提出的数仓理论，推荐采用ER关系模型进行建模，不过这种方式在实际工作中不推荐使用。</span><br></pre></td></tr></table></figure><h5 id="维度建模模型"><a href="#维度建模模型" class="headerlink" title="维度建模模型"></a>维度建模模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Ralph Kimball提出的数仓理论中，提出了维度建模，将数据仓库中的表划分为事实表和维度表。</span><br><span class="line">基于事实表和维度表进行维度建模。</span><br><span class="line">维度建模通常又分为星型模型和雪花模型，一会我们详细分析这两种维度建模模型。</span><br><span class="line">维度建模是我们在构建数据仓库中常用的方式。</span><br></pre></td></tr></table></figure><h5 id="Data-Vault模型"><a href="#Data-Vault模型" class="headerlink" title="Data Vault模型"></a>Data Vault模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Data Vault是在ER模型的基础上衍生而来，模型设计的初衷是有效的组织基础数据层，使之易扩展、灵活的应对业务的变化，同时强调历史性、可追溯性和原子性，不要求对数据进行过度的一致性处理；并非针对分析场景所设计。</span><br></pre></td></tr></table></figure><h5 id="Anchor模型"><a href="#Anchor模型" class="headerlink" title="Anchor模型"></a>Anchor模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Anchor是对Data Vault模型做了更近一步的规范化处理，初衷是为了设计高度可扩展的模型，核心思想是所有的扩张只添加而不修改，于是设计出的模型基本变成了k-v结构的模型。</span><br><span class="line">Data Vault模型和Anchor模型，这两种模型大家知道就行了，很少使用，如果大家感兴趣的话可以到网上查阅相关资料了解一下。</span><br></pre></td></tr></table></figure><h4 id="维度建模模型-1"><a href="#维度建模模型-1" class="headerlink" title="维度建模模型"></a>维度建模模型</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面详细分析一下维度建模模型</span><br><span class="line"></span><br><span class="line">星型模型和雪花模型</span><br><span class="line">星型模型和雪花模型主要区别就是对维度表的拆分，</span><br><span class="line">对于雪花模型，维度表的设计更加规范，一般符合3NF；</span><br><span class="line">而星型模型，一般采用降维的操作，利用冗余来避免模型过于复杂，提高易用性和分析效率</span><br><span class="line">先来看一下星型模型</span><br></pre></td></tr></table></figure><h5 id="星型模型"><a href="#星型模型" class="headerlink" title="星型模型"></a>星型模型</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301805131.png" alt="image-20230330180534808"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里面的中间的订单表是事实表，外面的四个是维度表。</span><br><span class="line">这几个维度表，其实严格意义上来说，只能满足第二范式，是不满足第三范式的。</span><br><span class="line">但是这样的好处是查询效率比较高，在查询的时候不需要关联很多张表。</span><br><span class="line">缺点就是数据有冗余。</span><br><span class="line">使用这个五角星代表星型模型还是比较形象的，因为针对事实表周边的这些维度表，外层就没有其它的表了。</span><br></pre></td></tr></table></figure><h5 id="雪花模型"><a href="#雪花模型" class="headerlink" title="雪花模型"></a>雪花模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下雪花模型</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301811839.png" alt="image-20230330181059516"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个里面订单表是一个事实表，其余的都是维度表。</span><br><span class="line">针对商品维度表外层又拆分出来了一个商品类目的维度表，这样拆分之后其实就满足第三范式了，但是这样就变的复杂了，后期在获取商品维度数据的时候，还需要关联这个商品类目维度表。</span><br><span class="line">这里使用这个雪花代表雪花模型也是比较形象的，事实表周边会有一层维度表，这些维度表外层还可能会有多层维度表</span><br><span class="line">那接下里我们针对这两种模型的优缺点进行一个总结</span><br></pre></td></tr></table></figure><h5 id="星型模型-VS-雪花模型"><a href="#星型模型-VS-雪花模型" class="headerlink" title="星型模型 VS 雪花模型"></a>星型模型 VS 雪花模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">冗余：雪花模型符合业务逻辑设计，采用3NF设计，有效降低数据冗余；星型模型的维度表设计不符合3NF，反规范化，维度表之间不会直接相关，牺牲部分存储空间</span><br><span class="line"></span><br><span class="line">性能：雪花模型由于存在维度间的关联，采用3NF降低冗余，通常在使用过程中，需要连接更多的维度表，导致性能偏低；星型模型违反三范式，采用降维的操作将维度整合，以存储空间为代价有效降低维度表连接数，性能比雪花模型高</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们在实际工作中一般会选择哪种呢？</span><br><span class="line">在实际工作中我们多采用星型模型，因为数据仓库主要是侧重于做数据分析，对数据的查询性能要求比较高，所以星型模型是比较好的选择，在实际工工作中我们会尽可能的多构建一些宽表，提前把多种有关联的维度整合到一张表中，后期使用时就不需要多表关联了，比较方便，并且性能也高。</span><br></pre></td></tr></table></figure><h3 id="数据仓库分层"><a href="#数据仓库分层" class="headerlink" title="数据仓库分层"></a>数据仓库分层</h3><h4 id="为什么要分层"><a href="#为什么要分层" class="headerlink" title="为什么要分层"></a>为什么要分层</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">数据仓库在构建过程中通常都需要进行分层处理。业务不同，分层的技术处理手段也不同。对数据进行分层的一个主要原因就是希望在管理数据的时候，能对数据有一个更加清晰的掌控</span><br><span class="line">详细来讲，主要有下面几个原因：</span><br><span class="line">1. 清晰的数据结构：每一个分层的数据都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。</span><br><span class="line">2. 数据血缘追踪：简单来讲可以这样理解，我们最终给业务方呈现的是一个能直接使用的业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围，分层之后就很好定位问题，以及可以清晰的知道它的危害范围。</span><br><span class="line">3. 减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少重复计算。</span><br><span class="line">4. 把复杂问题简单化：将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。</span><br><span class="line">5. 屏蔽业务的影响，不必改一次业务就重新接入数据。</span><br></pre></td></tr></table></figure><h4 id="数据仓库分层设计"><a href="#数据仓库分层设计" class="headerlink" title="数据仓库分层设计"></a>数据仓库分层设计</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那针对我们这里的数据仓库该如何分层呢？</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301823028.png" alt="image-20230330182319581"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据仓库一般会分为4层</span><br><span class="line">1. ODS层：原始数据层，数据源中的数据，采集过来之后，原样保存。</span><br><span class="line">2. DWD层：明细数据层：这一层是对ODS层的数据进行清洗，解决一些数据质量问题和数据的完整度问题。</span><br><span class="line">3. DWS层：这一层是对DWD层的数据进行轻度聚合汇总，生成一系列的中间表，提升公共指标的复用性，减少重复加工，并且构建出来一些宽表，用于提供后续的业务查询。</span><br><span class="line">4. APP层：根据业务需要，由前面三层的数据统计而出的结果(一般会使用数据明细层，数据汇总层的数据)，可以直接提供查询展现，一般会把APP层的数据导出到MySQL中供线上系统使用，提供报表展示、数据监控及其它功能。也有公司把这层称为DM层。虽然名字不一样，但是性质是一样的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：针对DWD层在对数据进行清洗的时候，一般需要遵循以下原则</span><br><span class="line">1. 数据唯一性校验(通过数据采集工具采集的数据会存在重复的可能性)</span><br><span class="line">2. 数据完整性校验(采集的数据中可能会出现缺失字段的情况，针对缺失字段的数据建议直接丢掉，如果可以确定是哪一列缺失也可以进行补全，可以用同一列上的前一个数据来填补或者同一列上的后一个数据来填补，或者默认值)</span><br><span class="line">3. 数据合法性校验-1(针对数字列中出现了null、或者-之类的异常值，全部替换为一个特殊值，例如0或者-1，这个需要根据具体的业务场景而定)</span><br><span class="line">4. 数据合法性校验-2(针对部分字段需要校验数据的合法性，例如：用户的年龄，不能是负数)</span><br></pre></td></tr></table></figure><h4 id="数据仓库命名规范"><a href="#数据仓库命名规范" class="headerlink" title="数据仓库命名规范"></a>数据仓库命名规范</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">我们在使用Hive实现数据仓库的时候该如何体现这些层次？</span><br><span class="line">1. 针对数据仓库的每一层都在Hive中创建一个数据库，数据库的命名包含每一层的标识符</span><br><span class="line">例如：针对ODS层可以在Hive中创建数据库 ods_mall，把同一层的表都放到一个数据库里面，方便管理</span><br><span class="line">2. 针对每一层中的表名，在创建的时候可以使用每一层的标识符开头</span><br><span class="line">例如：针对ODS层，创建的表名为：ods_user，这样方便后期使用，只要看到表名就可以知道这个表示哪一层的了。</span><br><span class="line"></span><br><span class="line">针对一些临时表，我们可以在对应的分层中创建表名的时候，以_tmp结尾。</span><br><span class="line">针对一些备份的表，可以在表名后面添加_bak。</span><br></pre></td></tr></table></figure><h3 id="典型的数据仓库系统架构"><a href="#典型的数据仓库系统架构" class="headerlink" title="典型的数据仓库系统架构"></a>典型的数据仓库系统架构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下图是一个典型的企业数据仓库系统，通常包含数据源、数据存储与管理、数据的访问三个部分</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302018736.png" alt="image-20230330201757040"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">数据源部分负责采集各种日志数据、业务数据，以及一些文档资料，将我们需要的这些数据加载到Hive中，构建数据仓库</span><br><span class="line">数据仓库构建好了以后可以为很多服务提供数据支撑</span><br><span class="line"></span><br><span class="line">例如：做数据报表，做OLAP数据分析，以及在做用户画像和数据挖掘的时候都是需要使用到数据仓库中的数据的</span><br><span class="line"></span><br><span class="line">在实际工作中，数据仓库分为离线数据仓库和实时数据仓库</span><br><span class="line">我们这个项目主要分析离线数据仓库，因为到现阶段为止我们主要学习了离线计算相关的技术框架。</span><br></pre></td></tr></table></figure><h3 id="项目需求分析"><a href="#项目需求分析" class="headerlink" title="项目需求分析"></a>项目需求分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">通过刚才对一个典型的企业数据仓库系统架构进行分析，我们发现，想要开发一个完整的数据仓库系统，至少需要以下这几个功能模块</span><br><span class="line">1：数据采集平台，这个模块主要负责采集各种数据源的数据</span><br><span class="line">2：数据仓库，这个模块负责数据存储和管理</span><br><span class="line">3：数据报表，这个模块其实就是数据可视化展示了</span><br><span class="line"></span><br><span class="line">通过这三个模块可以实现数据采集，构建数据仓库，最后基于数据仓库中的数据实现上层应用，体现数据仓库的价值。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-Java程序基础 数组类型</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.html</id>
    <published>2023-03-29T14:30:59.000Z</published>
    <updated>2023-03-29T17:52:24.800Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="数组类型"><a href="#数组类型" class="headerlink" title="数组类型"></a>数组类型</h1><h2 id="法一"><a href="#法一" class="headerlink" title="法一"></a>法一</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">使用数组来表示“一组”int类型。代码如下：</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // <span class="number">5</span>位同学的成绩:</span><br><span class="line">        int[] ns = new int[<span class="number">5</span>];</span><br><span class="line">        ns[<span class="number">0</span>] = <span class="number">68</span>;</span><br><span class="line">        ns[<span class="number">1</span>] = <span class="number">79</span>;</span><br><span class="line">        ns[<span class="number">2</span>] = <span class="number">91</span>;</span><br><span class="line">        ns[<span class="number">3</span>] = <span class="number">85</span>;</span><br><span class="line">        ns[<span class="number">4</span>] = <span class="number">62</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  定义一个数组类型的变量，使用数组类型“类型[]”，例如，int[]。和单个基本类型变量不同，数组变量初始化必须使用new int[<span class="number">5</span>]表示创建一个可容纳<span class="number">5</span>个int元素的数组。</span><br><span class="line">  </span><br><span class="line">Java的数组有几个特点：</span><br><span class="line">  数组所有元素初始化为默认值，整型都是<span class="number">0</span>，浮点型是<span class="number">0.0</span>，布尔型是false；</span><br><span class="line">  数组一旦创建后，大小就不可改变。</span><br><span class="line">  要访问数组中的某一个元素，需要使用索引。数组索引从<span class="number">0</span>开始，例如，<span class="number">5</span>个元素的数组，索引范围是<span class="number">0</span>~<span class="number">4</span>。</span><br><span class="line">  可以修改数组中的某一个元素，使用赋值语句，例如，ns[<span class="number">1</span>] = <span class="number">79</span>;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">可以用数组变量.length获取数组大小</span><br><span class="line">数组是引用类型，在使用索引访问数组元素时，如果索引超出范围，运行时将报错</span><br></pre></td></tr></table></figure><h2 id="法二"><a href="#法二" class="headerlink" title="法二"></a>法二</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">也可以在定义数组时直接指定初始化的元素，这样就不必写出数组大小，而是由编译器自动推算数组大小。例如：</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // <span class="number">5</span>位同学的成绩:</span><br><span class="line">        int[] ns = new int[] &#123; <span class="number">68</span>, <span class="number">79</span>, <span class="number">91</span>, <span class="number">85</span>, <span class="number">62</span> &#125;;</span><br><span class="line">        System.out.println(ns.length); // 编译器自动推算数组大小为<span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">还可以进一步简写为：</span><br><span class="line">int[] ns = &#123; <span class="number">68</span>, <span class="number">79</span>, <span class="number">91</span>, <span class="number">85</span>, <span class="number">62</span> &#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意数组是引用类型，并且数组大小不可变。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HUNHEt" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%5CHUNHEt.md.png" alt="HUNHEt.md.png"></a></p><h2 id="字符串数组"><a href="#字符串数组" class="headerlink" title="字符串数组"></a>字符串数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如果数组元素不是基本类型，而是一个引用类型，那么，修改数组元素会有哪些不同？</span><br><span class="line">字符串是引用类型，因此我们先定义一个字符串数组：</span><br><span class="line"></span><br><span class="line">String[] names = &#123;</span><br><span class="line">    <span class="string">"ABC"</span>, <span class="string">"XYZ"</span>, <span class="string">"zoo"</span></span><br><span class="line">&#125;;xxxxxxxxxx 如果数组元素不是基本类型，而是一个引用类型，那么，修改数组元素会有哪些不同？字符串是引用类型，因此我们先定义一个字符串数组：String[] names = &#123;    <span class="string">"ABC"</span>, <span class="string">"XYZ"</span>, <span class="string">"zoo"</span>&#125;;</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HUNHEt" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%5CHUNHEt.md-16801002864109.png" alt="HUNHEt.md.png"></a></p><p><a href="https://imgtu.com/i/HUUBPf" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%5CHUUBPf.md.png" alt="HUUBPf.md.png"></a></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数组是同一数据类型的集合，数组一旦创建后，大小就不可变；</span><br><span class="line"></span><br><span class="line">可以通过索引访问数组元素，但索引超出范围将报错；</span><br><span class="line"></span><br><span class="line">数组元素可以是值类型（如int）或引用类型（如String），但数组本身是引用类型；</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-Java程序基础 变量和数据类型</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html</id>
    <published>2023-03-29T14:24:52.000Z</published>
    <updated>2023-03-29T17:52:20.297Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="变量和数据类型"><a href="#变量和数据类型" class="headerlink" title="变量和数据类型"></a>变量和数据类型</h1><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Java中，变量分为两种：基本类型的变量和引用类型的变量。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">*先定义再应用*</span><br><span class="line">*可以一次性多个定义和赋值*</span><br><span class="line">*没有赋值，将自动赋默认值(基本数据类型)*</span><br><span class="line">*可以将一个基本数据类型变量赋值给另一个基本类型变量。不是指向同一个地址*</span><br></pre></td></tr></table></figure><h2 id="基本数据类型有"><a href="#基本数据类型有" class="headerlink" title="基本数据类型有"></a>基本数据类型有</h2><p><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5C202303292133989.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">~~~</span><br><span class="line">+ 整型 byte,short,int,long</span><br><span class="line">+ 浮点型 float,double</span><br><span class="line">   float要加上f或F，double可以省略</span><br><span class="line">+ 字符型 char</span><br><span class="line">  用单引号</span><br><span class="line">+ 布尔型 false,true</span><br><span class="line">~~~</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;不同的数据类型占用的字节数不一样。我们看一下Java基本数据类型占用的字节数</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HtXtqe" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5CHtXtqe.png" alt="HtXtqe.png"></a></p><h3 id="整型"><a href="#整型" class="headerlink" title="整型"></a>整型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对于整型类型，Java只定义了带符号的整型，因此，最高位的bit表示符号位（<span class="number">0</span>表示正数，<span class="number">1</span>表示负数）。各种整型能表示的最大范围如下</span><br><span class="line"></span><br><span class="line">byte：<span class="number">-128</span> ~ <span class="number">127</span></span><br><span class="line">short: <span class="number">-32768</span> ~ <span class="number">32767</span></span><br><span class="line">int: <span class="number">-2147483648</span> ~ <span class="number">2147483647</span></span><br><span class="line">long: <span class="number">-9223372036854775808</span> ~ <span class="number">9223372036854775807</span></span><br><span class="line"></span><br><span class="line">对于float类型，需要加上f后缀。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int i = <span class="number">2147483647</span>;</span><br><span class="line">        int i2 = <span class="number">-2147483648</span>;</span><br><span class="line">        int i3 = <span class="number">2</span>_000_000_000; // 加下划线更容易识别</span><br><span class="line">        int i4 = <span class="number">0xff0000</span>; // 十六进制表示的<span class="number">16711680</span></span><br><span class="line">        int i5 = <span class="number">0b1000000000</span>; // 二进制表示的<span class="number">512</span></span><br><span class="line">        long l = <span class="number">9000000000000000000L</span>; // long型的结尾需要加L</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">特别注意：同一个数的不同进制的表示是完全相同的，例如<span class="number">15</span>=<span class="number">0xf</span>＝<span class="number">0b1111</span></span><br></pre></td></tr></table></figure><h3 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a>浮点型</h3><p><code>因为小数用科学计数法表示的时候，小数点是可以“浮动”的,所以称为浮点数</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">float f1 = <span class="number">3.14</span>f;</span><br><span class="line">float f2 = <span class="number">3.14e38</span>f; // 科学计数法表示的<span class="number">3.14</span>x10^<span class="number">38</span></span><br><span class="line">double d = <span class="number">1.79e308</span>;</span><br><span class="line">double d2 = <span class="number">-1.79e308</span>;</span><br><span class="line">double d3 = <span class="number">4.9e-324</span>; // 科学计数法表示的<span class="number">4.9</span>x10^<span class="number">-324</span></span><br></pre></td></tr></table></figure><h3 id="布尔类型"><a href="#布尔类型" class="headerlink" title="布尔类型"></a>布尔类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">布尔类型boolean只有true和false两个值，布尔类型总是关系运算的计算结果</span><br><span class="line"></span><br><span class="line">Java语言对布尔类型的存储并没有做规定，因为理论上存储布尔类型只需要1 bit，但是通常JVM内部会把boolean表示为4字节整数</span><br></pre></td></tr></table></figure><h3 id="字符类型"><a href="#字符类型" class="headerlink" title="字符类型"></a>字符类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">字符类型char表示一个字符。Java的char类型除了可表示标准的ASCII外，还可以表示一个Unicode字符：</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        char a = <span class="string">'A'</span>;</span><br><span class="line">        char zh = <span class="string">'中'</span>;</span><br><span class="line">        System.out.println(a);</span><br><span class="line">        System.out.println(zh);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意char类型使用单引号&#39;，且仅有一个字符，要和双引号&quot;的字符串类型区分开。</span><br></pre></td></tr></table></figure><h2 id="引用类型"><a href="#引用类型" class="headerlink" title="引用类型"></a>引用类型</h2><p><em>除了上述基本类型的变量，剩下的都是引用类型</em></p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">引用类型最常用的就是String字符串：</span><br><span class="line"></span><br><span class="line">String s &#x3D; &quot;hello&quot;;</span><br><span class="line"></span><br><span class="line">引用类型的变量类似于C语言的指针，它内部存储一个“地址”，指向某个对象在内存的位置，后续我们介绍类的概念时会详细讨论。</span><br></pre></td></tr></table></figure><h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">定义变量的时候，如果加上final修饰符，这个变量就变成了常量</span><br><span class="line"></span><br><span class="line">final double PI = <span class="number">3.14</span>; // PI是一个常量</span><br><span class="line">double r = <span class="number">5.0</span>;</span><br><span class="line">double area = PI * r * r;</span><br><span class="line">PI = <span class="number">300</span>; // compile error!</span><br><span class="line"></span><br><span class="line">常量在定义时进行初始化后就不可再次赋值，再次赋值会导致编译错误。</span><br><span class="line">根据习惯，常量名通常全部大写。</span><br></pre></td></tr></table></figure><h3 id="var关键字"><a href="#var关键字" class="headerlink" title="var关键字"></a>var关键字</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">定义变量时，变量类型太长，可以用var</span><br><span class="line">StringBuilder sb = new StringBuilder();</span><br><span class="line"></span><br><span class="line">这个时候，如果想省略变量类型，可以使用var关键字:</span><br><span class="line">var sb = new StringBuilder();</span><br><span class="line">编译器会根据赋值语句自动推断出变量sb的类型是StringBuilder</span><br></pre></td></tr></table></figure><h2 id="变量的作用范围"><a href="#变量的作用范围" class="headerlink" title="变量的作用范围"></a>变量的作用范围</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">定义变量时，要遵循作用域最小化原则，尽量将变量定义在尽可能小的作用域，并且，不要重复使用变量名。</span><br></pre></td></tr></table></figure><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Java提供了两种变量类型：基本类型和引用类型</span><br><span class="line">基本类型包括整型，浮点型，布尔型，字符型。</span><br><span class="line">变量可重新赋值，等号是赋值语句，不是数学意义的等号。</span><br><span class="line">常量在初始化后不可重新赋值，使用常量便于理解程序意图。</span><br></pre></td></tr></table></figure><h2 id="整数运算"><a href="#整数运算" class="headerlink" title="整数运算"></a>整数运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">整数的数值表示不但是精确的，而且整数运算永远是精确的，即使是除法也是精确的，因为两个整数相除只能得到结果的整数部分</span><br><span class="line"></span><br><span class="line">int x = <span class="number">12345</span> / <span class="number">67</span>; // <span class="number">184</span></span><br><span class="line">求余运算使用%：</span><br><span class="line"></span><br><span class="line">int y = <span class="number">12345</span> % <span class="number">67</span>; // <span class="number">12345</span>÷<span class="number">67</span>的余数是<span class="number">17</span></span><br><span class="line">特别注意：整数的除法对于除数为<span class="number">0</span>时运行时将报错，但编译不会报错</span><br></pre></td></tr></table></figure><h3 id="溢出"><a href="#溢出" class="headerlink" title="溢出"></a>溢出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要特别注意，整数由于存在范围限制，如果计算结果超出了范围，就会产生溢出，而溢出不会出错，却会得到一个奇怪的结果</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int x = <span class="number">2147483640</span>;</span><br><span class="line">        int y = <span class="number">15</span>;</span><br><span class="line">        int sum = x + y;</span><br><span class="line">        System.out.println(sum); // <span class="number">-2147483641</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="number">0111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1000</span></span><br><span class="line">+ <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">1111</span></span><br><span class="line">-----------------------------------------</span><br><span class="line">  <span class="number">1000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0111</span></span><br><span class="line">由于最高位计算结果为<span class="number">1</span>，因此，加法结果变成了一个负数</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">还有一种简写的运算符，即+=，-=，*=，/=，它们的使用方法如下：</span><br><span class="line"></span><br><span class="line">n += <span class="number">100</span>; // <span class="number">3409</span>, 相当于 n = n + <span class="number">100</span>;</span><br><span class="line">n -= <span class="number">100</span>; // <span class="number">3309</span>, 相当于 n = n - <span class="number">100</span>;</span><br></pre></td></tr></table></figure><h3 id="自增-自减"><a href="#自增-自减" class="headerlink" title="自增/自减"></a>自增/自减</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">++</span><br><span class="line"></span><br><span class="line">--</span><br><span class="line"></span><br><span class="line">*写在变量前面和后面是不同的，前面(先加减在运算)，后面(先运算再加减)*</span><br></pre></td></tr></table></figure><h3 id="移位运算符"><a href="#移位运算符" class="headerlink" title="移位运算符"></a>移位运算符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">*数值的最高位是一个符号位*</span><br><span class="line">1.</span><br><span class="line">&gt;&gt;: 右位移</span><br><span class="line">int n &#x3D; 7;       &#x2F;&#x2F; 00000000 00000000 00000000 00000111 &#x3D; 7</span><br><span class="line">int a &#x3D; n &gt;&gt; 1;  &#x2F;&#x2F; 00000000 00000000 00000000 00000011 &#x3D; 3</span><br><span class="line">int b &#x3D; n &gt;&gt; 2;  &#x2F;&#x2F; 00000000 00000000 00000000 00000001 &#x3D; 1</span><br><span class="line">int c &#x3D; n &gt;&gt; 3;  &#x2F;&#x2F; 00000000 00000000 00000000 00000000 &#x3D; 0</span><br><span class="line">如果对一个负数进行右移，最高位的1不动，结果仍然是一个负数：</span><br><span class="line">int n &#x3D; -536870912;</span><br><span class="line">int a &#x3D; n &gt;&gt; 1;  &#x2F;&#x2F; 11110000 00000000 00000000 00000000 &#x3D; -268435456</span><br><span class="line">int b &#x3D; n &gt;&gt; 2;  &#x2F;&#x2F; 11111000 00000000 00000000 00000000 &#x3D; -134217728</span><br><span class="line">int c &#x3D; n &gt;&gt; 28; &#x2F;&#x2F; 11111111 11111111 11111111 11111110 &#x3D; -2</span><br><span class="line">int d &#x3D; n &gt;&gt; 29; &#x2F;&#x2F; 11111111 11111111 11111111 11111111 &#x3D; -1</span><br><span class="line"></span><br><span class="line">2. </span><br><span class="line">&lt;&lt;: 左位移</span><br><span class="line">int n &#x3D; 7;       &#x2F;&#x2F; 00000000 00000000 00000000 00000111 &#x3D; 7</span><br><span class="line">int a &#x3D; n &lt;&lt; 1;  &#x2F;&#x2F; 00000000 00000000 00000000 00001110 &#x3D; 14</span><br><span class="line">int b &#x3D; n &lt;&lt; 2;  &#x2F;&#x2F; 00000000 00000000 00000000 00011100 &#x3D; 28</span><br><span class="line">int c &#x3D; n &lt;&lt; 28; &#x2F;&#x2F; 01110000 00000000 00000000 00000000 &#x3D; 1879048192</span><br><span class="line">int d &#x3D; n &lt;&lt; 29; &#x2F;&#x2F; 11100000 00000000 00000000 00000000 &#x3D; -536870912</span><br><span class="line"></span><br><span class="line">*上面两种不会改变符号位*</span><br><span class="line"></span><br><span class="line">3.无符号的右移运算</span><br><span class="line">使用&gt;&gt;&gt;，它的特点是不管符号位，右移后高位总是补0，因此，对一个负数进行&gt;&gt;&gt;右移，它会变成正数，原因是最高位的1变成了0</span><br><span class="line">int n &#x3D; -536870912;</span><br><span class="line">int a &#x3D; n &gt;&gt;&gt; 1;  &#x2F;&#x2F; 01110000 00000000 00000000 00000000 &#x3D; 1879048192</span><br><span class="line">int b &#x3D; n &gt;&gt;&gt; 2;  &#x2F;&#x2F; 00111000 00000000 00000000 00000000 &#x3D; 939524096</span><br><span class="line">int c &#x3D; n &gt;&gt;&gt; 29; &#x2F;&#x2F; 00000000 00000000 00000000 00000111 &#x3D; 7</span><br><span class="line">int d &#x3D; n &gt;&gt;&gt; 31; &#x2F;&#x2F; 00000000 00000000 00000000 00000001 &#x3D; 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">对byte和short类型进行移位时，会首先转换为int再进行位移。</span><br><span class="line">仔细观察可发现，左移实际上就是不断地×2，右移实际上就是不断地÷2</span><br></pre></td></tr></table></figure><h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">位运算是按位进行与、或、非和异或的运算</span><br><span class="line"></span><br><span class="line">&amp;(与): 同1才为1</span><br><span class="line"></span><br><span class="line">|(或): 有1则为0</span><br><span class="line"></span><br><span class="line">~(非)： 01互换</span><br><span class="line"></span><br><span class="line">^(异或): 不同才为1</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对两个整数进行位运算，实际上就是按位对齐，然后依次对每一位进行运算。</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int i = <span class="number">167776589</span>; // <span class="number">00001010</span> <span class="number">00000000</span> <span class="number">00010001</span> <span class="number">01001101</span></span><br><span class="line">        int n = <span class="number">167776512</span>; // <span class="number">00001010</span> <span class="number">00000000</span> <span class="number">00010001</span> <span class="number">00000000</span></span><br><span class="line">        System.out.println(i &amp; n); // <span class="number">167776512</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运算优先级"><a href="#运算优先级" class="headerlink" title="运算优先级"></a>运算优先级</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在Java的计算表达式中，运算优先级从高到低依次是：</span><br><span class="line"></span><br><span class="line">()</span><br><span class="line">! ~ ++ --</span><br><span class="line">* / %</span><br><span class="line">+ -</span><br><span class="line">&lt;&lt; &gt;&gt; &gt;&gt;&gt;</span><br><span class="line">&amp;</span><br><span class="line">|</span><br><span class="line">+= -= *= /=</span><br><span class="line"></span><br><span class="line">记不住也没关系，只需要加括号就可以保证运算的优先级正确</span><br></pre></td></tr></table></figure><h3 id="类型自动提升-整与整"><a href="#类型自动提升-整与整" class="headerlink" title="类型自动提升(整与整)"></a>类型自动提升(整与整)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在运算过程中，如果参与运算的两个数类型不一致，那么计算结果为较大类型的整型。例如，short和int计算，结果总是int，原因是short首先自动被转型为int</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        short s = <span class="number">1234</span>;</span><br><span class="line">        int i = <span class="number">123456</span>;</span><br><span class="line">        int x = s + i; // s自动转型为int</span><br><span class="line">        short y = s + i; // 编译错误!</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="强制转换-整与整"><a href="#强制转换-整与整" class="headerlink" title="强制转换(整与整)"></a>强制转换(整与整)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">也可以将结果强制转型，即将大范围的整数转型为小范围的整数。强制转型使用(类型)，例如，将int强制转型为short：</span><br><span class="line"></span><br><span class="line">int i &#x3D; 12345;</span><br><span class="line">short s &#x3D; (short) i; &#x2F;&#x2F; 12345</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">要注意，超出范围的强制转型会得到错误的结果，原因是转型时，int的两个高位字节直接被扔掉，仅保留了低位的两个字节</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int i1 = <span class="number">1234567</span>;</span><br><span class="line">        short s1 = (short) i1; // <span class="number">-10617</span></span><br><span class="line">        System.out.println(s1);</span><br><span class="line">        int i2 = <span class="number">12345678</span>; //short <span class="number">32767</span></span><br><span class="line">        short s2 = (short) i2; // <span class="number">24910</span></span><br><span class="line">        System.out.println(s2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="number">-10617</span></span><br><span class="line"><span class="number">24910</span></span><br><span class="line"></span><br><span class="line">// 因此，强制转型的结果很可能是错的</span><br></pre></td></tr></table></figure><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">整数运算的结果永远是精确的；</span><br><span class="line">运算结果会自动提升；</span><br><span class="line">可以强制转型，但超出范围的强制转型会得到错误的结果；</span><br><span class="line">应该选择合适范围的整型（int或long），没有必要为了节省内存而使用byte和short进行整数运算。</span><br></pre></td></tr></table></figure><h2 id="浮点数计算"><a href="#浮点数计算" class="headerlink" title="浮点数计算"></a>浮点数计算</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">*无法精确表示数值，不能做移位和位运算*</span><br><span class="line">由于浮点数存在运算误差，所以比较两个浮点数是否相等常常会出现错误的结果。正确的比较方法是判断两个浮点数之差的绝对值是否小于一个很小的数</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 比较x和y是否相等，先计算其差的绝对值:</span><br><span class="line">double r = Math.abs(x - y);</span><br><span class="line">// 再判断绝对值是否足够小:</span><br><span class="line"><span class="keyword">if</span> (r &lt; <span class="number">0.00001</span>) &#123;</span><br><span class="line">    // 可以认为相等</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    // 不相等</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="类型提升"><a href="#类型提升" class="headerlink" title="类型提升"></a>类型提升</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果参与运算的两个数其中一个是整型，那么整型可以自动提升到浮点型</span><br></pre></td></tr></table></figure><h3 id="溢出-1"><a href="#溢出-1" class="headerlink" title="溢出"></a>溢出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">*整数在除零时编译时不出错，运行时出错*</span><br><span class="line">*浮点数除零不会报错，会返回特殊值：*</span><br><span class="line"></span><br><span class="line">NaN:<span class="keyword">not</span> a number</span><br><span class="line">Infinity:无穷大</span><br><span class="line">-Infinity:负无穷大</span><br><span class="line"></span><br><span class="line">double d1 = <span class="number">0.0</span> / <span class="number">0</span>; // NaN</span><br><span class="line">double d2 = <span class="number">1.0</span> / <span class="number">0</span>; // Infinity</span><br><span class="line">double d3 = <span class="number">-1.0</span> / <span class="number">0</span>; // -Infinity</span><br><span class="line"></span><br><span class="line">这三种特殊值在实际运算中很少碰到，我们只需要了解即可</span><br></pre></td></tr></table></figure><h3 id="强制转换-整与浮"><a href="#强制转换-整与浮" class="headerlink" title="强制转换(整与浮)"></a>强制转换(整与浮)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以将浮点数强制转型为整数。在转型时，浮点数的小数部分会被丢掉。如果转型后超过了整型能表示的最大范围，将返回整型的最大值。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int n1 = (int) <span class="number">12.3</span>; // <span class="number">12</span></span><br><span class="line">int n2 = (int) <span class="number">12.7</span>; // <span class="number">12</span></span><br><span class="line">int n2 = (int) <span class="number">-12.7</span>; // <span class="number">-12</span></span><br><span class="line">int n3 = (int) (<span class="number">12.7</span> + <span class="number">0.5</span>); // <span class="number">13</span></span><br><span class="line">int n4 = (int) <span class="number">1.2e20</span>; // <span class="number">2147483647</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">如果要进行四舍五入，可以对浮点数加上<span class="number">0.5</span>再强制转型</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        double d = <span class="number">2.6</span>;</span><br><span class="line">        int n = (int) (d + <span class="number">0.5</span>);</span><br><span class="line">        System.out.println(n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">浮点数常常无法精确表示，并且浮点数的运算结果可能有误差；</span><br><span class="line">比较两个浮点数通常比较它们的差的绝对值是否小于一个特定值；</span><br><span class="line">整型和浮点型运算时，整型会自动提升为浮点型；</span><br><span class="line">可以将浮点型强制转为整型，但超出范围后将始终返回整型的最大值。</span><br></pre></td></tr></table></figure><h2 id="布尔运算"><a href="#布尔运算" class="headerlink" title="布尔运算"></a>布尔运算</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">对于布尔类型boolean，永远只有true和false两个值。</span><br><span class="line">布尔运算是一种关系运算，包括以下几类：</span><br><span class="line"></span><br><span class="line">比较运算符：&gt;，&gt;&#x3D;，&lt;，&lt;&#x3D;，&#x3D;&#x3D;，!&#x3D;</span><br><span class="line">与运算 &amp;&amp;</span><br><span class="line">或运算 ||</span><br><span class="line">非运算 !</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">boolean isGreater = <span class="number">5</span> &gt; <span class="number">3</span>; // true</span><br><span class="line">int age = <span class="number">12</span>;</span><br><span class="line">boolean isZero = age == <span class="number">0</span>; // false</span><br><span class="line">boolean isNonZero = !isZero; // true</span><br><span class="line">boolean isAdult = age &gt;= <span class="number">18</span>; // false</span><br><span class="line">boolean isTeenager = age &gt;<span class="number">6</span> &amp;&amp; age &lt;<span class="number">18</span>; // true</span><br></pre></td></tr></table></figure><h3 id="关系运算符的优先级"><a href="#关系运算符的优先级" class="headerlink" title="关系运算符的优先级"></a>关系运算符的优先级</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">!</span><br><span class="line">&gt;，&gt;&#x3D;，&lt;，&lt;&#x3D;</span><br><span class="line">&#x3D;&#x3D;，!&#x3D;</span><br><span class="line">&amp;&amp;</span><br><span class="line">||</span><br></pre></td></tr></table></figure><h3 id="短路运算"><a href="#短路运算" class="headerlink" title="短路运算"></a>短路运算</h3><h4 id="true-amp-amp-任意"><a href="#true-amp-amp-任意" class="headerlink" title="true&amp;&amp;任意"></a>true&amp;&amp;任意</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">布尔运算的一个重要特点是短路运算。如果一个布尔运算的表达式能提前确定结果，则后续的计算不再执行，直接返回结果。</span><br><span class="line"></span><br><span class="line">因为false &amp;&amp; x的结果总是false，无论x是true还是false，因此，与运算在确定第一个值为false后，不再继续计算，而是直接返回false</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        boolean b &#x3D; 5 &lt; 3;</span><br><span class="line">        boolean result &#x3D; b &amp;&amp; (5 &#x2F; 0 &gt; 0);</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">false</span><br><span class="line"></span><br><span class="line">如果没有短路运算，&amp;&amp;后面的表达式会由于除数为0而报错，但实际上该语句并未报错，原因在于与运算是短路运算符，提前计算出了结果false</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HNRoOx" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5CHNRoOx.png" alt="HNRoOx.png"></a></p><h4 id="true-任意"><a href="#true-任意" class="headerlink" title="true || 任意"></a>true || 任意</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">类似的，对于||运算，只要能确定第一个值为true，后续计算也不再进行，而是直接返回true：</span><br><span class="line"></span><br><span class="line">boolean result &#x3D; true || (5 &#x2F; 0 &gt; 0); &#x2F;&#x2F; true</span><br></pre></td></tr></table></figure><h4 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Java还提供一个三元运算符b ? x : y，它根据第一个布尔表达式的结果，分别返回后续两个表达式之一的计算结果</span><br><span class="line"></span><br><span class="line">b为true返回x; b为false返回y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意到三元运算b ? x : y会首先计算b，如果b为true，则只计算x，否则，只计算y。此外，x和y的类型必须相同，因为返回值不是boolean，而是x和y之一。</span><br></pre></td></tr></table></figure><h4 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">与运算和或运算是短路运算；</span><br><span class="line">三元运算b ? x : y后面的类型必须相同，三元运算也是“短路运算”，只计算x或y。</span><br></pre></td></tr></table></figure><h2 id="字符和字符串"><a href="#字符和字符串" class="headerlink" title="字符和字符串"></a>字符和字符串</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Java中，字符和字符串是两个不同的类型</span><br></pre></td></tr></table></figure><h3 id="字符"><a href="#字符" class="headerlink" title="字符"></a>字符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">字符类型char是基本数据类型，它是character的缩写。一个char保存一个Unicode字符</span><br><span class="line"></span><br><span class="line">因为Java在内存中总是使用Unicode表示字符，所以，一个英文字符和一个中文字符都用一个char类型表示，它们都占用两个字节。要显示一个字符的Unicode编码，只需将char类型直接赋值给int类型即可：</span><br><span class="line"></span><br><span class="line">int n1 &#x3D; &#39;A&#39;; &#x2F;&#x2F; 字母“A”的Unicodde编码是65</span><br><span class="line">int n2 &#x3D; &#39;中&#39;; &#x2F;&#x2F; 汉字“中”的Unicode编码是20013</span><br><span class="line">还可以直接用转义字符\u+Unicode编码来表示一个字符：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 注意是十六进制:</span><br><span class="line">char c3 &#x3D; &#39;\u0041&#39;; &#x2F;&#x2F; &#39;A&#39;，因为十六进制0041 &#x3D; 十进制65</span><br><span class="line">char c4 &#x3D; &#39;\u4e2d&#39;; &#x2F;&#x2F; &#39;中&#39;，因为十六进制4e2d &#x3D; 十进制20013</span><br></pre></td></tr></table></figure><h3 id="字符串-1"><a href="#字符串-1" class="headerlink" title="字符串"></a>字符串</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">和char类型不同，字符串类型String是引用类型，我们用双引号<span class="string">"..."</span>表示字符串。一个字符串可以存储<span class="number">0</span>个到任意个字符：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">""</span>; // 空字符串，包含<span class="number">0</span>个字符</span><br><span class="line">String s1 = <span class="string">"A"</span>; // 包含一个字符</span><br><span class="line">String s2 = <span class="string">"ABC"</span>; // 包含<span class="number">3</span>个字符</span><br><span class="line">String s3 = <span class="string">"中文 ABC"</span>; // 包含<span class="number">6</span>个字符，其中有一个空格</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">因为字符串使用双引号<span class="string">"..."</span>表示开始和结束，那如果字符串本身恰好包含一个<span class="string">"字符怎么表示？例如，"</span>abc<span class="string">"xyz"</span>，编译器就无法判断中间的引号究竟是字符串的一部分还是表示字符串结束。这个时候，我们需要借助转义字符\：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"abc\"xyz"</span>; // 包含<span class="number">7</span>个字符: a, b, c, <span class="string">", x, y, z</span></span><br><span class="line"><span class="string">因为\是转义字符，所以，两个\\表示一个\字符：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">String s = "</span>abc\\xyz<span class="string">"; // 包含7个字符: a, b, c, \, x, y, z</span></span><br><span class="line"><span class="string">常见的转义字符包括：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">\" 表示字符"</span></span><br><span class="line">\<span class="string">' 表示字符'</span></span><br><span class="line">\\ 表示字符\</span><br><span class="line">\n 表示换行符</span><br><span class="line">\r 表示回车符</span><br><span class="line">\t 表示Tab</span><br><span class="line">\u<span class="comment">#### 表示一个Unicode编码的字符</span></span><br><span class="line">例如：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"ABC\n\u4e2d\u6587"</span>; // 包含<span class="number">6</span>个字符: A, B, C, 换行符, 中, 文</span><br></pre></td></tr></table></figure><h3 id="字符串连接"><a href="#字符串连接" class="headerlink" title="字符串连接"></a>字符串连接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Java的编译器对字符串做了特殊照顾，可以使用+连接任意字符串和其他数据类型，这样极大地方便了字符串的处理。例如：</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        String s1 = <span class="string">"Hello"</span>;</span><br><span class="line">        String s2 = <span class="string">"world"</span>;</span><br><span class="line">        String s = s1 + <span class="string">" "</span> + s2 + <span class="string">"!"</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">如果用+连接字符串和其他数据类型，会将其他数据类型先自动转型为字符串，再连接：</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int age = <span class="number">25</span>;</span><br><span class="line">        String s = <span class="string">"age is "</span> + age;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // 请将下面一组int值视为字符的Unicode码，把它们拼成一个字符串：</span><br><span class="line">        int a = <span class="number">72</span>;</span><br><span class="line">        int b = <span class="number">105</span>;</span><br><span class="line">        int c = <span class="number">65281</span>;</span><br><span class="line">        // FIXME</span><br><span class="line"> String s = <span class="string">""</span>+<span class="string">'\u0048'</span> +<span class="string">'\u0069'</span>  + <span class="string">'\uff01'</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class array &#123;    </span><br><span class="line">  public static void main(String[] args)&#123;        </span><br><span class="line">    int a = <span class="number">72</span>;        </span><br><span class="line">    int b = <span class="number">105</span>;        </span><br><span class="line">    int c = <span class="number">65281</span>;        </span><br><span class="line">    // FIXME:        </span><br><span class="line">    String s = <span class="string">""</span>+(char)a + (char)b + (char)c;        </span><br><span class="line">    System.out.println(s);    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="多行字符串"><a href="#多行字符串" class="headerlink" title="多行字符串"></a>多行字符串</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">如果我们要表示多行字符串，使用+号连接会非常不方便：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"first line \n"</span></span><br><span class="line">         + <span class="string">"second line \n"</span></span><br><span class="line">         + <span class="string">"end"</span>;</span><br><span class="line">从Java <span class="number">13</span>开始，字符串可以用<span class="string">"""..."""</span>表示多行字符串（Text Blocks）了</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        String s = <span class="string">"""</span></span><br><span class="line"><span class="string">                   SELECT * FROM</span></span><br><span class="line"><span class="string">                     users</span></span><br><span class="line"><span class="string">                   WHERE id &gt; 100</span></span><br><span class="line"><span class="string">                   ORDER BY name DESC</span></span><br><span class="line"><span class="string">                   """</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">上述多行字符串实际上是<span class="number">5</span>行，在最后一个DESC后面还有一个\n。如果我们不想在字符串末尾加一个\n，就需要这么写：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">""" </span></span><br><span class="line"><span class="string">           SELECT * FROM</span></span><br><span class="line"><span class="string">             users</span></span><br><span class="line"><span class="string">           WHERE id &gt; 100</span></span><br><span class="line"><span class="string">           ORDER BY name DESC"""</span>;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">还需要注意到，多行字符串前面共同的空格会被去掉，即：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"""</span></span><br><span class="line"><span class="string">...........SELECT * FROM</span></span><br><span class="line"><span class="string">...........  users</span></span><br><span class="line"><span class="string">...........WHERE id &gt; 100</span></span><br><span class="line"><span class="string">...........ORDER BY name DESC</span></span><br><span class="line"><span class="string">..........."""</span>;</span><br><span class="line">用.标注的空格都会被去掉。</span><br><span class="line"></span><br><span class="line">如果多行字符串的排版不规则，那么，去掉的空格就会变成这样：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"""</span></span><br><span class="line"><span class="string">.........  SELECT * FROM</span></span><br><span class="line"><span class="string">.........    users</span></span><br><span class="line"><span class="string">.........WHERE id &gt; 100</span></span><br><span class="line"><span class="string">.........  ORDER BY name DESC</span></span><br><span class="line"><span class="string">.........  """</span>;</span><br><span class="line">即总是以最短的行首空格为基准。</span><br></pre></td></tr></table></figure><h3 id="不可变特性"><a href="#不可变特性" class="headerlink" title="不可变特性"></a>不可变特性</h3><p><a href="https://imgtu.com/i/HUnuM8" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5CHUnuM8.md.png" alt="HUnuM8.md.png"></a></p><h3 id="空值"><a href="#空值" class="headerlink" title="空值"></a>空值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">引用类型的变量可以指向一个空值null，它表示不存在，即该变量不指向任何对象。例如：</span><br><span class="line"></span><br><span class="line">String s1 = null; // s1是null</span><br><span class="line">String s2; // 没有赋初值值，s2也是null</span><br><span class="line">String s3 = s1; // s3也是null</span><br><span class="line">String s4 = <span class="string">""</span>; // s4指向空字符串，不是null</span><br><span class="line">注意要区分空值null和空字符串<span class="string">""</span>，空字符串是一个有效的字符串对象，它不等于null</span><br></pre></td></tr></table></figure><h3 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Java的字符类型char是基本类型，字符串类型String是引用类型；</span><br><span class="line">基本类型的变量是“持有”某个数值，引用类型的变量是“指向”某个对象；</span><br><span class="line">引用类型的变量可以是空值null；</span><br><span class="line">要区分空值null和空字符串<span class="string">""</span></span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-Java程序基础 Java程序基本结构</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.html</id>
    <published>2023-03-29T14:19:48.000Z</published>
    <updated>2023-03-29T17:52:28.727Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="java程序基本结构"><a href="#java程序基本结构" class="headerlink" title="java程序基本结构"></a>java程序基本结构</h1><h2 id="类名规范"><a href="#类名规范" class="headerlink" title="类名规范"></a>类名规范</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">+ 首字母大写</span><br><span class="line">+ 字母开头，数字，下划线组合</span><br></pre></td></tr></table></figure><h2 id="方法名规范"><a href="#方法名规范" class="headerlink" title="方法名规范"></a>方法名规范</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命名和class一样，但是首字母小写</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里的方法名是main，返回值是void，表示没有任何返回值。</span><br><span class="line"></span><br><span class="line">我们注意到public除了可以修饰class外，也可以修饰方法。而关键字static是另一个修饰符，它表示静态方法，后面我们会讲解方法的类型，目前，我们只需要知道，Java入口程序规定的方法必须是静态方法，方法名必须为main，括号内的参数必须是String数组。</span><br><span class="line"></span><br><span class="line">每一行语句，分号结尾</span><br></pre></td></tr></table></figure><h2 id="注释方法"><a href="#注释方法" class="headerlink" title="注释方法"></a>注释方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">+ &#x2F;&#x2F;</span><br><span class="line">+ &#x2F;*... *&#x2F;</span><br><span class="line">+ &#x2F;**... *&#x2F;  这是一种特殊注释方法，用在类和方法的定义出，用于自动创建文档</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Java程序对格式没有明确的要求，多几个空格或者回车不影响程序的正确性，但是我们要养成良好的编程习惯</span><br><span class="line">*对于eclipse可以用快捷键ctrl+shift+f，快速格式化代码*</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-泛型</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-%E6%B3%9B%E5%9E%8B.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-%E6%B3%9B%E5%9E%8B.html</id>
    <published>2023-03-29T10:03:11.000Z</published>
    <updated>2023-03-29T17:52:04.932Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h1><h2 id="什么是泛型"><a href="#什么是泛型" class="headerlink" title="什么是泛型"></a>什么是泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ArrayList是一种可变长数组，其内部使用的是Object类型</span><br><span class="line"></span><br><span class="line">public class ArrayList &#123;</span><br><span class="line">    private Object[] array;</span><br><span class="line">    private int size;</span><br><span class="line">    public void add(Object e) &#123;...&#125;</span><br><span class="line">    public void remove(int index) &#123;...&#125;</span><br><span class="line">    public Object get(int index) &#123;...&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果用上述ArrayList存储String类型，会有这么几个缺点：</span><br><span class="line">需要强制转型；</span><br><span class="line">不方便，易出错。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ArrayList list &#x3D; new ArrayList();</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">&#x2F;&#x2F; 获取到Object，必须强制转型为String:</span><br><span class="line">String first &#x3D; (String) list.get(0);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">很容易出现ClassCastException，因为容易“误转型”：</span><br><span class="line">list.add(new Integer(123));</span><br><span class="line">&#x2F;&#x2F; ERROR: ClassCastException:</span><br><span class="line">String second &#x3D; (String) list.get(1);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">要解决上述问题，我们可以为String单独编写一种ArrayList：</span><br><span class="line">public class StringArrayList &#123;</span><br><span class="line">    private String[] array;</span><br><span class="line">    private int size;</span><br><span class="line">    public void add(String e) &#123;...&#125;</span><br><span class="line">    public void remove(int index) &#123;...&#125;</span><br><span class="line">    public String get(int index) &#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这样一来，存入的必须是String，取出的也一定是String，不需要强制转型，因为编译器会强制检查放入的类型：</span><br><span class="line">StringArrayList list &#x3D; new StringArrayList();</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">String first &#x3D; list.get(0);</span><br><span class="line">&#x2F;&#x2F; 编译错误: 不允许放入非String类型:</span><br><span class="line">list.add(new Integer(123));</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">问题暂时解决。</span><br><span class="line"></span><br><span class="line">然而，新的问题是，如果要存储Integer，还需要为Integer单独编写一种ArrayList：</span><br><span class="line">实际上，还需要为其他所有class单独编写一种ArrayList：</span><br><span class="line"></span><br><span class="line">LongArrayList</span><br><span class="line">DoubleArrayList</span><br><span class="line">PersonArrayList</span><br><span class="line">...</span><br><span class="line">这是不可能的，JDK的class就有上千个，而且它还不知道其他人编写的class。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">为了解决新的问题，我们必须把ArrayList变成一种模板：ArrayList&lt;T&gt;，代码如下：</span><br><span class="line">public class ArrayList&lt;T&gt; &#123;</span><br><span class="line">    private T[] array;</span><br><span class="line">    private int size;</span><br><span class="line">    public void add(T e) &#123;...&#125;</span><br><span class="line">    public void remove(int index) &#123;...&#125;</span><br><span class="line">    public T get(int index) &#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">T可以是任何class。这样一来，我们就实现了：编写一次模版，可以创建任意类型的ArrayList：</span><br><span class="line">&#x2F;&#x2F; 创建可以存储String的ArrayList:</span><br><span class="line">ArrayList&lt;String&gt; strList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">&#x2F;&#x2F; 创建可以存储Float的ArrayList:</span><br><span class="line">ArrayList&lt;Float&gt; floatList &#x3D; new ArrayList&lt;Float&gt;();</span><br><span class="line">&#x2F;&#x2F; 创建可以存储Person的ArrayList:</span><br><span class="line">ArrayList&lt;Person&gt; personList &#x3D; new ArrayList&lt;Person&gt;();</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">因此，泛型就是定义一种模板，例如ArrayList&lt;T&gt;，然后在代码中为用到的类创建对应的ArrayList&lt;类型&gt;：</span><br><span class="line">ArrayList&lt;String&gt; strList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">由编译器针对类型作检查：</span><br><span class="line"></span><br><span class="line">strList.add(&quot;hello&quot;); &#x2F;&#x2F; OK</span><br><span class="line">String s &#x3D; strList.get(0); &#x2F;&#x2F; OK</span><br><span class="line">strList.add(new Integer(123)); &#x2F;&#x2F; compile error!</span><br><span class="line">Integer n &#x3D; strList.get(0); &#x2F;&#x2F; compile error!</span><br><span class="line"></span><br><span class="line">这样一来，既实现了编写一次，万能匹配，又通过编译器保证了类型安全：这就是泛型。</span><br></pre></td></tr></table></figure><h3 id="向上转型"><a href="#向上转型" class="headerlink" title="向上转型"></a>向上转型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在Java标准库中的ArrayList&lt;T&gt;实现了List&lt;T&gt;接口，它可以向上转型为List&lt;T&gt;：</span><br><span class="line"></span><br><span class="line">public class ArrayList&lt;T&gt; implements List&lt;T&gt; &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">即类型ArrayList&lt;T&gt;可以向上转型为List&lt;T&gt;。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">要特别注意：不能把ArrayList&lt;Integer&gt;向上转型为ArrayList&lt;Number&gt;或List&lt;Number&gt;。</span><br><span class="line"></span><br><span class="line">这是为什么呢？假设ArrayList&lt;Integer&gt;可以向上转型为ArrayList&lt;Number&gt;，观察一下代码：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 创建ArrayList&lt;Integer&gt;类型：</span><br><span class="line">ArrayList&lt;Integer&gt; integerList &#x3D; new ArrayList&lt;Integer&gt;();</span><br><span class="line">&#x2F;&#x2F; 添加一个Integer：</span><br><span class="line">integerList.add(new Integer(123));</span><br><span class="line">&#x2F;&#x2F; “向上转型”为ArrayList&lt;Number&gt;：</span><br><span class="line">ArrayList&lt;Number&gt; numberList &#x3D; integerList;</span><br><span class="line">&#x2F;&#x2F; 添加一个Float，因为Float也是Number：</span><br><span class="line">numberList.add(new Float(12.34));</span><br><span class="line">&#x2F;&#x2F; 从ArrayList&lt;Integer&gt;获取索引为1的元素（即添加的Float）：</span><br><span class="line">Integer n &#x3D; integerList.get(1); &#x2F;&#x2F; ClassCastException!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们把一个ArrayList&lt;Integer&gt;转型为ArrayList&lt;Number&gt;类型后，这个ArrayList&lt;Number&gt;就可以接受Float类型，因为Float是Number的子类。但是，ArrayList&lt;Number&gt;实际上和ArrayList&lt;Integer&gt;是同一个对象，也就是ArrayList&lt;Integer&gt;类型，它不可能接受Float类型， 所以在获取Integer的时候将产生ClassCastException。</span><br><span class="line"></span><br><span class="line">实际上，编译器为了避免这种错误，根本就不允许把ArrayList&lt;Integer&gt;转型为ArrayList&lt;Number&gt;。</span><br><span class="line"></span><br><span class="line"> ArrayList&lt;Integer&gt;和ArrayList&lt;Number&gt;两者完全没有继承关系。</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">泛型就是编写模板代码来适应任意类型；</span><br><span class="line"></span><br><span class="line">泛型的好处是使用时不必对类型进行强制转换，它通过编译器对类型进行检查；</span><br><span class="line"></span><br><span class="line">注意泛型的继承关系：可以把&#96;ArrayList&lt;Integer&gt;&#96;向上转型为&#96;List&lt;Integer&gt;&#96;（&#96;T&#96;不能变！），但不能把&#96;ArrayList&lt;Integer&gt;&#96;向上转型为&#96;ArrayList&lt;Number&gt;&#96;（&#96;T&#96;不能变成父类）。</span><br></pre></td></tr></table></figure><h2 id="使用泛型"><a href="#使用泛型" class="headerlink" title="使用泛型"></a>使用泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">使用ArrayList时，如果不定义泛型类型时，泛型类型实际上就是Object：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 编译器警告:</span><br><span class="line">List list &#x3D; new ArrayList(); &#x2F;&#x2F;这里吧ArrayList向上转型为List</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">list.add(&quot;World&quot;);</span><br><span class="line">String first &#x3D; (String) list.get(0);</span><br><span class="line">String second &#x3D; (String) list.get(1);</span><br><span class="line">此时，只能把&lt;T&gt;当作Object使用，没有发挥泛型的优势。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当我们定义泛型类型&lt;String&gt;后，List&lt;T&gt;的泛型接口变为强类型List&lt;String&gt;：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 无编译器警告:</span><br><span class="line">List&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">list.add(&quot;World&quot;);</span><br><span class="line">&#x2F;&#x2F; 无强制转型:</span><br><span class="line">String first &#x3D; list.get(0);</span><br><span class="line">String second &#x3D; list.get(1);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当我们定义泛型类型&lt;Number&gt;后，List&lt;T&gt;的泛型接口变为强类型List&lt;Number&gt;：</span><br><span class="line"></span><br><span class="line">List&lt;Number&gt; list &#x3D; new ArrayList&lt;Number&gt;();</span><br><span class="line">list.add(new Integer(123));</span><br><span class="line">list.add(new Double(12.34));</span><br><span class="line">Number first &#x3D; list.get(0);</span><br><span class="line">Number second &#x3D; list.get(1);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">编译器如果能自动推断出泛型类型，就可以省略后面的泛型类型。例如，对于下面的代码：</span><br><span class="line"></span><br><span class="line">List&lt;Number&gt; list &#x3D; new ArrayList&lt;Number&gt;();</span><br><span class="line">编译器看到泛型类型List&lt;Number&gt;就可以自动推断出后面的ArrayList&lt;T&gt;的泛型类型必须是ArrayList&lt;Number&gt;，因此，可以把代码简写为：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 可以省略后面的Number，编译器可以自动推断泛型类型：</span><br><span class="line">List&lt;Number&gt; list &#x3D; new ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><h3 id="泛型接口"><a href="#泛型接口" class="headerlink" title="泛型接口"></a>泛型接口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">除了ArrayList&lt;T&gt;使用了泛型，还可以在接口中使用泛型。例如，Arrays.sort(Object[])可以对任意数组进行排序，但待排序的元素必须实现Comparable&lt;T&gt;这个泛型接口：</span><br><span class="line"></span><br><span class="line">public interface Comparable&lt;T&gt; &#123;</span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 返回负数: 当前实例比参数o小</span><br><span class="line">     * 返回0: 当前实例与参数o相等</span><br><span class="line">     * 返回正数: 当前实例比参数o大</span><br><span class="line">     *&#x2F;</span><br><span class="line">    int compareTo(T o);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">可以直接对String数组进行排序：</span><br><span class="line">&#x2F;&#x2F; sort</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">String[] ss &#x3D; new String[] &#123; &quot;Orange&quot;, &quot;Apple&quot;, &quot;Pear&quot; &#125;;</span><br><span class="line">        Arrays.sort(ss);</span><br><span class="line">        System.out.println(Arrays.toString(ss));</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">这是因为String本身已经实现了Comparable&lt;String&gt;接口。如果换成我们自定义的Person类型试试：</span><br><span class="line">&#x2F;&#x2F; sort</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">    Person[] ps &#x3D; new Person[] &#123;</span><br><span class="line">            new Person(&quot;Bob&quot;, 61),</span><br><span class="line">            new Person(&quot;Alice&quot;, 88),</span><br><span class="line">            new Person(&quot;Lily&quot;, 75),</span><br><span class="line">        &#125;;</span><br><span class="line">        Arrays.sort(ps);</span><br><span class="line">        System.out.println(Arrays.toString(ps));</span><br><span class="line"></span><br><span class="line">class Person &#123;</span><br><span class="line">    String name;</span><br><span class="line">    int score;</span><br><span class="line">    Person(String name, int score) &#123;</span><br><span class="line">        this.name &#x3D; name;</span><br><span class="line">        this.score &#x3D; score;</span><br><span class="line">    &#125;</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return this.name + &quot;,&quot; + this.score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">运行程序，我们会得到ClassCastException，即无法将Person转型为Comparable。我们修改代码，让Person实现Comparable&lt;T&gt;接口：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; sort</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Person[] ps &#x3D; new Person[] &#123;</span><br><span class="line">            new Person(&quot;Bob&quot;, 61),</span><br><span class="line">            new Person(&quot;Alice&quot;, 88),</span><br><span class="line">            new Person(&quot;Lily&quot;, 75),</span><br><span class="line">        &#125;;</span><br><span class="line">        Arrays.sort(ps);</span><br><span class="line">        System.out.println(Arrays.toString(ps));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Person implements Comparable&lt;Person&gt; &#123;</span><br><span class="line">    String name;</span><br><span class="line">    int score;</span><br><span class="line">    Person(String name, int score) &#123;</span><br><span class="line">        this.name &#x3D; name;</span><br><span class="line">        this.score &#x3D; score;</span><br><span class="line">    &#125;</span><br><span class="line">    public int compareTo(Person other) &#123;</span><br><span class="line">        return this.name.compareTo(other.name);</span><br><span class="line">    &#125;</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return this.name + &quot;,&quot; + this.score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行上述代码，可以正确实现按name进行排序。</span><br><span class="line"></span><br><span class="line">也可以修改比较逻辑，例如，按score从高到低排序。请自行修改测试。</span><br></pre></td></tr></table></figure><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">使用泛型时，把泛型参数&lt;T&gt;替换为需要的class类型，例如：ArrayList&lt;String&gt;，ArrayList&lt;Number&gt;等；</span><br><span class="line"></span><br><span class="line">可以省略编译器能自动推断出的类型，例如：List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();；</span><br><span class="line"></span><br><span class="line">不指定泛型参数类型时，编译器会给出警告，且只能将&lt;T&gt;视为Object类型；</span><br><span class="line"></span><br><span class="line">可以在接口中定义泛型类型，实现此接口的类必须实现正确的泛型类型。</span><br></pre></td></tr></table></figure><h2 id="编写泛型"><a href="#编写泛型" class="headerlink" title="编写泛型"></a>编写泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">编写泛型类比普通类要复杂。通常来说，泛型类一般用在集合类中，例如ArrayList&lt;T&gt;，我们很少需要编写泛型类。</span><br><span class="line"></span><br><span class="line">如果我们确实需要编写一个泛型类，那么，应该如何编写它？</span><br><span class="line">可以按照以下步骤来编写一个泛型类。</span><br><span class="line">首先，按照某种类型，例如：String，来编写类：</span><br><span class="line"></span><br><span class="line">public class Pair &#123;</span><br><span class="line">    private String first;</span><br><span class="line">    private String last;</span><br><span class="line">    public Pair(String first, String last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">然后，标记所有的特定类型，这里是String：</span><br><span class="line"></span><br><span class="line">public class Pair &#123;</span><br><span class="line">    private String first;</span><br><span class="line">    private String last;</span><br><span class="line">    public Pair(String first, String last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">最后，把特定类型String替换为T，并申明&lt;T&gt;：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">熟练后即可直接从T开始编写。</span><br></pre></td></tr></table></figure><h3 id="静态方法"><a href="#静态方法" class="headerlink" title="静态方法"></a>静态方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">编写泛型类时，要特别注意，泛型类型&lt;T&gt;不能用于静态方法。例如：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 对静态方法使用&lt;T&gt;:</span><br><span class="line">    public static Pair&lt;T&gt; create(T first, T last) &#123;</span><br><span class="line">        return new Pair&lt;T&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">上述代码会导致编译错误，我们无法在静态方法create()的方法参数和返回类型上使用泛型类型T。</span><br><span class="line"></span><br><span class="line">有些同学在网上搜索发现，可以在static修饰符后面加一个&lt;T&gt;，编译就能通过：</span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 可以编译通过:</span><br><span class="line">    public static &lt;T&gt; Pair&lt;T&gt; create(T first, T last) &#123;</span><br><span class="line">        return new Pair&lt;T&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">但实际上，这个&lt;T&gt;和Pair&lt;T&gt;类型的&lt;T&gt;已经没有任何关系了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">对于静态方法，我们可以单独改写为“泛型”方法，只需要使用另一个类型即可。对于上面的create()静态方法，我们应该把它改为另一种泛型类型，例如，&lt;K&gt;：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 静态泛型方法应该使用其他类型区分:</span><br><span class="line">    public static &lt;K&gt; Pair&lt;K&gt; create(K first, K last) &#123;</span><br><span class="line">        return new Pair&lt;K&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这样才能清楚地将静态方法的泛型类型和实例类型的泛型类型区分开。</span><br></pre></td></tr></table></figure><h3 id="多个泛型类型"><a href="#多个泛型类型" class="headerlink" title="多个泛型类型"></a>多个泛型类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">泛型还可以定义多种类型。例如，我们希望Pair不总是存储两个类型一样的对象，就可以使用类型&lt;T, K&gt;：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T, K&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private K last;</span><br><span class="line">    public Pair(T first, K last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public K getLast() &#123; ... &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">使用的时候，需要指出两种类型：</span><br><span class="line">Pair&lt;String, Integer&gt; p &#x3D; new Pair&lt;&gt;(&quot;test&quot;, 123);</span><br><span class="line"></span><br><span class="line">Java标准库的Map&lt;K, V&gt;就是使用两种泛型类型的例子。它对Key使用一种类型，对Value使用另一种类型。</span><br></pre></td></tr></table></figure><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">编写泛型时，需要定义泛型类型&lt;T&gt;；</span><br><span class="line"></span><br><span class="line">静态方法不能引用泛型类型&lt;T&gt;，必须定义其他类型（例如&lt;K&gt;）来实现静态泛型方法；</span><br><span class="line"></span><br><span class="line">泛型可以同时定义多种类型，例如Map&lt;K, V&gt;。</span><br></pre></td></tr></table></figure><h2 id="擦拭法"><a href="#擦拭法" class="headerlink" title="擦拭法"></a>擦拭法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">泛型是一种类似”模板代码“的技术，不同语言的泛型实现方式不一定相同。</span><br><span class="line">Java语言的泛型实现方式是擦拭法（Type Erasure）。</span><br><span class="line">所谓擦拭法是指，虚拟机对泛型其实一无所知，所有的工作都是编译器做的。</span><br><span class="line"></span><br><span class="line">例如，我们编写了一个泛型类Pair&lt;T&gt;，这是编译器看到的代码：</span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">而虚拟机根本不知道泛型。这是虚拟机执行的代码：</span><br><span class="line"></span><br><span class="line">public class Pair &#123;</span><br><span class="line">    private Object first;</span><br><span class="line">    private Object last;</span><br><span class="line">    public Pair(Object first, Object last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public Object getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public Object getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">因此，Java使用擦拭法实现泛型，导致了：</span><br><span class="line"></span><br><span class="line">编译器把类型&lt;T&gt;视为Object；</span><br><span class="line">编译器根据&lt;T&gt;实现安全的强制转型。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">使用泛型的时候，我们编写的代码也是编译器看到的代码：</span><br><span class="line"></span><br><span class="line">Pair&lt;String&gt; p &#x3D; new Pair&lt;&gt;(&quot;Hello&quot;, &quot;world&quot;);</span><br><span class="line">String first &#x3D; p.getFirst();</span><br><span class="line">String last &#x3D; p.getLast();</span><br><span class="line">而虚拟机执行的代码并没有泛型：</span><br><span class="line"></span><br><span class="line">Pair p &#x3D; new Pair(&quot;Hello&quot;, &quot;world&quot;);</span><br><span class="line">String first &#x3D; (String) p.getFirst();</span><br><span class="line">String last &#x3D; (String) p.getLast();</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">所以，Java的泛型是由编译器在编译时实行的，编译器内部永远把所有类型T视为Object处理，但是，在需要转型的时候，编译器会根据T的类型自动为我们实行安全地强制转型。</span><br><span class="line"></span><br><span class="line">了解了Java泛型的实现方式——擦拭法，我们就知道了Java泛型的局限：</span><br><span class="line">局限一：&lt;T&gt;不能是基本类型，例如int，因为实际类型是Object，Object类型无法持有基本类型：</span><br><span class="line"></span><br><span class="line">Pair&lt;int&gt; p &#x3D; new Pair&lt;&gt;(1, 2); &#x2F;&#x2F; compile error!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">局限二：无法取得带泛型的Class。观察以下代码：</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">Pair&lt;String&gt; p1 &#x3D; new Pair&lt;&gt;(&quot;Hello&quot;, &quot;world&quot;);</span><br><span class="line">        Pair&lt;Integer&gt; p2 &#x3D; new Pair&lt;&gt;(123, 456);</span><br><span class="line">        Class c1 &#x3D; p1.getClass();</span><br><span class="line">        Class c2 &#x3D; p2.getClass();</span><br><span class="line">        System.out.println(c1&#x3D;&#x3D;c2); &#x2F;&#x2F; true</span><br><span class="line">        System.out.println(c1&#x3D;&#x3D;Pair.class); &#x2F;&#x2F; true</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">因为T是Object，我们对Pair&lt;String&gt;和Pair&lt;Integer&gt;类型获取Class时，获取到的是同一个Class，也就是Pair类的Class。</span><br><span class="line"></span><br><span class="line">换句话说，所有泛型实例，无论T的类型是什么，getClass()返回同一个Class实例，因为编译后它们全部都是Pair&lt;Object&gt;。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">局限三：无法判断带泛型的类型：</span><br><span class="line"></span><br><span class="line">Pair&lt;Integer&gt; p &#x3D; new Pair&lt;&gt;(123, 456);</span><br><span class="line">&#x2F;&#x2F; Compile error:</span><br><span class="line">if (p instanceof Pair&lt;String&gt;) &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">原因和前面一样，并不存在Pair&lt;String&gt;.class，而是只有唯一的Pair.class。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">局限四：不能实例化T类型：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair() &#123;</span><br><span class="line">        &#x2F;&#x2F; Compile error:</span><br><span class="line">        first &#x3D; new T();</span><br><span class="line">        last &#x3D; new T();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">上述代码无法通过编译，因为构造方法的两行语句：</span><br><span class="line"></span><br><span class="line">first &#x3D; new T();</span><br><span class="line">last &#x3D; new T();</span><br><span class="line"></span><br><span class="line">擦拭后实际上变成了：</span><br><span class="line"></span><br><span class="line">first &#x3D; new Object();</span><br><span class="line">last &#x3D; new Object();</span><br><span class="line"></span><br><span class="line">这样一来，创建new Pair&lt;String&gt;()和创建new Pair&lt;Integer&gt;()就全部成了Object，显然编译器要阻止这种类型不对的代码。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">要实例化T类型，我们必须借助额外的Class&lt;T&gt;参数：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(Class&lt;T&gt; clazz) &#123;</span><br><span class="line">        first &#x3D; clazz.newInstance();</span><br><span class="line">        last &#x3D; clazz.newInstance();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">上述代码借助Class&lt;T&gt;参数并通过反射来实例化T类型，使用的时候，也必须传入Class&lt;T&gt;。例如：</span><br><span class="line"></span><br><span class="line">Pair&lt;String&gt; pair &#x3D; new Pair&lt;&gt;(String.class);</span><br><span class="line">因为传入了Class&lt;String&gt;的实例，所以我们借助String.class就可以实例化String类型。</span><br></pre></td></tr></table></figure><h3 id="不恰当的覆写方法"><a href="#不恰当的覆写方法" class="headerlink" title="不恰当的覆写方法"></a>不恰当的覆写方法</h3><h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 SparkSql-5</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-SparkSql-5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-SparkSql-5.html</id>
    <published>2023-03-28T07:23:48.000Z</published>
    <updated>2023-04-08T13:44:54.570Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-SparkSql-极速上手SparkSql-5"><a href="#第十一周-SparkSql-极速上手SparkSql-5" class="headerlink" title="第十一周 SparkSql-极速上手SparkSql-5"></a>第十一周 SparkSql-极速上手SparkSql-5</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">之前已学完，spark core，离线数据计算</span><br></pre></td></tr></table></figure><h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL和我们之前讲Hive的时候说的hive on spark是不一样的。</span><br><span class="line">hive on spark是表示把底层的mapreduce引擎替换为spark引擎。</span><br><span class="line">而Spark SQL是Spark自己实现的一套SQL处理引擎。</span><br><span class="line"></span><br><span class="line">Spark SQL是Spark中的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象，就是DataFrame。</span><br><span class="line"></span><br><span class="line">DataFrame&#x3D;RDD+Schema 。</span><br><span class="line">它其实和关系型数据库中的表非常类似，RDD可以认为是表中的数据，Schema是表结构信息。</span><br><span class="line">DataFrame可以通过很多来源进行构建，包括：结构化的数据文件，Hive中的表，外部的关系型数据库，以及RDD</span><br><span class="line">Spark1.3出现的DataFrame ，Spark1.6出现了DataSet，在Spark2.0中两者统一，DataFrame等于DataSet[Row]</span><br></pre></td></tr></table></figure><h2 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">要使用Spark SQL，首先需要创建一个SpakSession对象</span><br><span class="line">SparkSession中包含了SparkContext和SqlContext</span><br><span class="line">所以说想通过SparkSession来操作RDD的话需要先通过它来获取SparkContext</span><br><span class="line"></span><br><span class="line">这个SqlContext是使用sparkSQL操作hive的时候会用到的。</span><br></pre></td></tr></table></figure><h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">使用SparkSession，可以从RDD、HIve表或者其它数据源创建DataFrame</span><br><span class="line">那下面我们来使用JSON文件来创建一个DataFrame</span><br><span class="line">想要使用spark-sql需要先添加spark-sql的依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在项目中添加sql这个包名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281536605.png" alt="image-20230328153653400"></p><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281542225.png" alt="image-20230328154207572"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.sql</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：使用json文件创建DataFrame</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object SqlDemoScala&#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">         val conf &#x3D; new SparkConf()</span><br><span class="line">         .setMaster(&quot;local&quot;)</span><br><span class="line">         &#x2F;&#x2F;创建SparkSession对象，里面包含SparkContext和SqlContext</span><br><span class="line">         val sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">         .appName(&quot;SqlDemoScala&quot;)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         &#x2F;&#x2F;读取json文件，获取DataFrame</span><br><span class="line">         val stuDf &#x3D; sparkSession.read.json(&quot;D:\\student.json&quot;)</span><br><span class="line">         &#x2F;&#x2F;查看DataFrame中的数据</span><br><span class="line">         stuDf.show()</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281546823.png" alt="image-20230328154309744"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用json文件创建DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SqlDemoJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"SqlDemoJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//读取json文件，获取Dataset&lt;Row&gt;</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.read().json(<span class="string">"D:\\student.json"</span>); <span class="comment">// 返回的dataset其实和dataframe一样的，新版本合并了</span></span><br><span class="line">         stuDf.show();</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281546440.png" alt="image-20230328154617041"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于DataFrame等于DataSet[Row]，它们两个可以互相转换，所以创建哪个都是一样的</span><br><span class="line">咱们前面的scala代码默认创建的是DataFrame，java代码默认创建的是DataSet</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">尝试对他们进行转换</span><br><span class="line">在Scala代码中将DataFrame转换为DataSet[Row]，对后面的操作没有影响</span><br><span class="line">&#x2F;&#x2F;将DataFrame转换为DataSet[Row]</span><br><span class="line">val stuDf &#x3D; sparkSession.read.json(&quot;D:\\student.json&quot;).as(&quot;stu&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在Java代码中将DataSet[Row]转换为DataFrame</span><br><span class="line">&#x2F;&#x2F;将Dataset&lt;Row&gt;转换为DataFrame</span><br><span class="line">Dataset&lt;Row&gt; stuDf &#x3D; sparkSession.read().json(&quot;D:\\student.json&quot;).toDF();</span><br></pre></td></tr></table></figure><h3 id="DataFrame常见算子操作"><a href="#DataFrame常见算子操作" class="headerlink" title="DataFrame常见算子操作"></a>DataFrame常见算子操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下Spark sql中针对DataFrame常见的算子操作</span><br><span class="line">先看一下官方文档</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281552610.png" alt="image-20230328155250497"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">printSchema()</span><br><span class="line">show()</span><br><span class="line">select()</span><br><span class="line">filter()、where()</span><br><span class="line">groupBy()</span><br><span class="line">count()</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.sql</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：DataFrame常见操作</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object DataFrameOpScala &#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">         val conf &#x3D; new SparkConf()</span><br><span class="line">         .setMaster(&quot;local&quot;)</span><br><span class="line">         &#x2F;&#x2F;创建SparkSession对象，里面包含SparkContext和SqlContext</span><br><span class="line">         val sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">         .appName(&quot;DataFrameOpScala&quot;)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">        val stuDf &#x3D; sparkSession.read.json(&quot;D:\\student.json&quot;)</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;打印schema信息</span><br><span class="line">         stuDf.printSchema()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;默认显示所有数据，可以通过参数控制显示多少条</span><br><span class="line">         stuDf.show(2)</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;查询数据中的指定字段信息</span><br><span class="line">         stuDf.select(&quot;name&quot;,&quot;age&quot;).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;在使用select的时候可以对数据做一些操作，需要添加隐式转换函数，否则语法报错</span><br><span class="line">         import sparkSession.implicits._</span><br><span class="line">         stuDf.select($&quot;name&quot;,$&quot;age&quot; + 1).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;对数据进行过滤，需要添加隐式转换函数，否则语法报错</span><br><span class="line">         stuDf.filter($&quot;age&quot;&gt;18).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;where底层调用的就是filter</span><br><span class="line">        stuDf.where($&quot;age&quot;&gt;18).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;对数据进行分组求和</span><br><span class="line">         stuDf.groupBy(&quot;age&quot;).count().show()</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281556810.png" alt="image-20230328155651098"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281557659.png" alt="image-20230328155732191"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281558631.png" alt="image-20230328155839255"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281601278.png" alt="image-20230328160101149"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281602257.png" alt="image-20230328160159987"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281602257.png" alt></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281603534.png" alt="image-20230328160345238"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.java.sql;</span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import static org.apache.spark.sql.functions.col;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：DataFrame常见操作</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class DataFrameOpJava &#123;</span><br><span class="line">     public static void main(String[] args) &#123;</span><br><span class="line">         SparkConf conf &#x3D; new SparkConf();</span><br><span class="line">         conf.setMaster(&quot;local&quot;);</span><br><span class="line">         &#x2F;&#x2F;创建SparkSession对象，里面包含SparkContext和SqlContext</span><br><span class="line">         SparkSession sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">         .appName(&quot;DataFrameOpJava&quot;)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         Dataset&lt;Row&gt; stuDf &#x3D; sparkSession.read().json(&quot;D:\\student.json&quot;);</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;打印schema信息</span><br><span class="line">         stuDf.printSchema();</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;默认显示所有数据，可以通过参数控制显示多少条</span><br><span class="line">         stuDf.show(2);</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;查询数据中的指定字段信息</span><br><span class="line">         stuDf.select(&quot;name&quot;,&quot;age&quot;).show();</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;在select的时候可以对数据做一些操作,需要引入import static org.apache.spa</span><br><span class="line">         stuDf.select(col(&quot;name&quot;),col(&quot;age&quot;).plus(1)).show();</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;对数据进行过滤</span><br><span class="line">         stuDf.filter(col(&quot;age&quot;).gt(18)).show();</span><br><span class="line">         stuDf.where(col(&quot;age&quot;).gt(18)).show();</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;对数据进行分组求和</span><br><span class="line">         stuDf.groupBy(&quot;age&quot;).count().show();</span><br><span class="line">         sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这些就是针对DataFrame的一些常见的操作。</span><br><span class="line">但是现在这种方式其实用起来还是不方便，只是提供了一些类似于可以操作表的算子，很对一些简单的查询还是可以的，但是针对一些复杂的操作，使用算子写起来就很麻烦了，所以我们希望能够直接支持用sql的方式执行，Spark SQL也是支持的</span><br></pre></td></tr></table></figure><h3 id="DataFrame的sql操作"><a href="#DataFrame的sql操作" class="headerlink" title="DataFrame的sql操作"></a>DataFrame的sql操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">想要实现直接支持sql语句查询DataFrame中的数据</span><br><span class="line">需要两步操作</span><br><span class="line">1. 先将DataFrame注册为一个临时表</span><br><span class="line">2. 使用sparkSession中的sql函数执行sql语句</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用sql操作DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameSqlScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"DataFrameSqlScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="keyword">val</span> stuDf = sparkSession.read.json(<span class="string">"D:\\student.json"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//将DataFrame注册为一个临时表</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">         <span class="comment">//使用sql查询临时表中的数据</span></span><br><span class="line">         sparkSession.sql(<span class="string">"select age,count(*) as num from student group by age"</span>)</span><br><span class="line">         .show()</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303282324292.png" alt="image-20230328232442914"></p><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用sql操作DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataFrameSqlJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"DataFrameSqlJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.read().json(<span class="string">"D:\\student.json"</span>);</span><br><span class="line">         <span class="comment">//将Dataset&lt;Row&gt;注册为一个临时表</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>);</span><br><span class="line">         <span class="comment">//使用sql查询临时表中的数据</span></span><br><span class="line">         sparkSession.sql(<span class="string">"select age,count(*) as num from student group by ag</span></span><br><span class="line"><span class="string">         .show();</span></span><br><span class="line"><span class="string">         sparkSession.stop();</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="RDD转换为DataFrame-包含DataFrame转RDD"><a href="#RDD转换为DataFrame-包含DataFrame转RDD" class="headerlink" title="RDD转换为DataFrame(包含DataFrame转RDD)"></a>RDD转换为DataFrame(包含DataFrame转RDD)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">为什么要将RDD转换为DataFrame?</span><br><span class="line"></span><br><span class="line">在实际工作中我们可能会先把hdfs上的一些日志数据加载进来，然后进行一些处理，最终变成结构化的数据，希望对这些数据做一些统计分析，当然了我们可以使用spark中提供的transformation算子来实现，只不过会有一些麻烦，毕竟是需要写代码的，如果能够使用sql实现，其实是更加方便的。</span><br><span class="line"></span><br><span class="line">所以可以针对我们前面创建的RDD，将它转换为DataFrame，这样就可以使用dataFrame中的一些算子或者直接写sql来操作数据了。</span><br><span class="line">Spark SQL支持这两种方式将RDD转换为DataFrame</span><br><span class="line">1. 反射方式</span><br><span class="line">2. 编程方式</span><br></pre></td></tr></table></figure><h4 id="反射方式"><a href="#反射方式" class="headerlink" title="反射方式"></a>反射方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下反射方式：</span><br><span class="line">这种方式是使用反射来推断RDD中的元数据。</span><br><span class="line">基于反射的方式，代码比较简洁，也就是说当你在写代码的时候，已经知道了RDD中的元数据(已经知道RDD里面的数据长什么样子)，这样的话使用反射这种方式是一种非常不错的选择。</span><br><span class="line">Scala具有隐式转换的特性，所以spark sql的scala接口是支持自动将包含了case class的RDD转换为DataFrame的</span><br><span class="line"></span><br><span class="line">下面来举一个例子</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><h5 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用反射方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RddToDataFrameByReflectScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByReflectScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="keyword">val</span> sc = sparkSession.sparkContext</span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>((<span class="string">"jack"</span>,<span class="number">18</span>),(<span class="string">"tom"</span>,<span class="number">20</span>),(<span class="string">"jessic"</span>,<span class="number">30</span>)))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//基于反射直接将包含Student对象的dataRDD转换为DataFrame</span></span><br><span class="line">         <span class="comment">//需要导入隐式转换</span></span><br><span class="line">         <span class="keyword">import</span> sparkSession.implicits._ <span class="comment">//不导入，toDF()不能用</span></span><br><span class="line">         <span class="keyword">val</span> stuDf = dataRDD.map(tup=&gt;<span class="type">Student</span>(tup._1,tup._2)).toDF()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//下面就可以通过DataFrame的方式操作dataRDD中的数据了</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         <span class="keyword">val</span> resDf = sparkSession.sql(<span class="string">"select name,age from student where age &gt; 18"</span>)</span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD，或者直接在这里show()也行</span></span><br><span class="line">         <span class="keyword">val</span> resRDD = resDf.rdd</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//从row中取数据 ，封装成student，打印到控制台</span></span><br><span class="line">     resRDD.map(row=&gt;<span class="type">Student</span>(row(<span class="number">0</span>).toString,row(<span class="number">1</span>).toString.toInt))</span><br><span class="line">         .collect()</span><br><span class="line">         .foreach(println(_))</span><br><span class="line">         <span class="comment">//使用row的getAs()方法，获取指定列名的值</span></span><br><span class="line">         resRDD.map(row=&gt;<span class="type">Student</span>(row.getAs[<span class="type">String</span>](<span class="string">"name"</span>),row.getAs[<span class="type">Int</span>](<span class="string">"age"</span>)))</span><br><span class="line">         .collect()</span><br><span class="line">         .foreach(println(_))</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//定义一个Student</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">name: <span class="type">String</span>,age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303282345692.png" alt="image-20230328234459220"></p><h5 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.RDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用反射方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RddToDataFrameByReflectJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByReflectJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="comment">//从sparkSession中获取的是scala中的sparkContext，所以需要转换成java中的sparkContext</span></span><br><span class="line">         JavaSparkContext sc = JavaSparkContext.fromSparkContext(sparkSession.sparkContext());</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t1 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jack"</span>, <span class="number">18</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t2 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"tom"</span>, <span class="number">20</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t3 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jessic"</span>, <span class="number">30</span>);</span><br><span class="line">         JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; dataRDD = sc.parallelize(Arrays.asList(t1,t2,t3));</span><br><span class="line">         JavaRDD&lt;Student&gt; stuRDD = dataRDD.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;String, Integer&gt;, Student&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Student <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new <span class="title">Student</span><span class="params">(tup._1, tup._2)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//注意：Student这个类必须声明为public(一个文件里只能有一个public class，所以只能在另一个文件里创建)，并且必须实现序列化</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.createDataFrame(stuRDD, Student<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">         </span><br><span class="line">                                                                      stuDf.createOrReplaceTempView(<span class="string">"student"</span>);</span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         Dataset&lt;Row&gt; resDf = sparkSession.sql(<span class="string">"select name,age from student where age&gt;18"</span>);</span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD，注意：这里需要转为JavaRDD</span></span><br><span class="line">         JavaRDD&lt;Row&gt; resRDD = resDf.javaRDD();</span><br><span class="line">         <span class="comment">//从row中取数据，封装成student，打印到控制台</span></span><br><span class="line">         List&lt;Student&gt; resList = resRDD.map(<span class="keyword">new</span> Function&lt;Row, Student&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Student <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">//return new Student(row.getString(0), row.getInt(1));</span></span><br><span class="line">         <span class="comment">//通过getAs获取数据</span></span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Student(row.getAs(<span class="string">"name"</span>).toString(), Integer.parseInt(row.getAs(<span class="string">"age"</span>).toString());</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).collect();</span><br><span class="line">         <span class="keyword">for</span>(Student stu : resList)&#123;</span><br><span class="line">         System.out.println(stu);</span><br><span class="line">        sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">student</span> <span class="keyword">implements</span> <span class="title">Serializable</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(String name,<span class="keyword">int</span> age)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name=name;</span><br><span class="line">        <span class="keyword">this</span>.age=age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span></span>&#123;</span><br><span class="line"><span class="keyword">this</span>.name=name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getAge</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">this</span>.age=age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Student&#123;"</span>+</span><br><span class="line">            <span class="string">"name='"</span>+name+<span class="string">'\''</span>+</span><br><span class="line">            <span class="string">",age="</span>+age+</span><br><span class="line">            <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java类中快速为字段生成get,set方法，右键-&gt;generate-&gt;...</span><br><span class="line">    快速生成构造方法 右键-&gt;generate-&gt;constructor</span><br><span class="line">    快速生成toString()方法 右键-&gt;generate-&gt;toString</span><br></pre></td></tr></table></figure><h4 id="编程方式"><a href="#编程方式" class="headerlink" title="编程方式"></a>编程方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来是编程的方式</span><br><span class="line">这种方式是通过编程接口来创建DataFrame，你可以在程序运行时动态构建一份元数据，就是Schema，然后将其应用到已经存在的RDD上。这种方式的代码比较冗长，但是如果在编写程序时，还不知道RDD的元数据，只有在程序运行时，才能动态得知其元数据，那么只能通过这种动态构建元数据的方式。</span><br><span class="line"></span><br><span class="line">也就是说当case calss中的字段无法预先定义的时候，就只能用编程方式动态指定元数据了</span><br></pre></td></tr></table></figure><h5 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">Stru</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用编程方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RddToDataFrameByProgramScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByProgramScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="keyword">val</span> sc = sparkSession.sparkContext</span><br><span class="line">         <span class="comment">// 假设这里不知道RDD的数据结构</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>((<span class="string">"jack"</span>,<span class="number">18</span>),(<span class="string">"tom"</span>,<span class="number">20</span>),(<span class="string">"jessic"</span>,<span class="number">30</span>)))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//组装rowRDD</span></span><br><span class="line">         <span class="keyword">val</span> rowRDD = dataRDD.map(tup=&gt;<span class="type">Row</span>(tup._1,tup._2))</span><br><span class="line">         <span class="comment">// dataframe=rdd+schema</span></span><br><span class="line">         <span class="comment">//指定元数据信息【这个元数据信息就可以动态从外部获取了，比较灵活】</span></span><br><span class="line">         <span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"name"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"age"</span>,<span class="type">IntegerType</span>,<span class="literal">true</span>)</span><br><span class="line">         ))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//组装DataFrame</span></span><br><span class="line">         <span class="keyword">val</span> stuDf = sparkSession.createDataFrame(rowRDD,schema)</span><br><span class="line">         <span class="comment">//下面就可以通过DataFrame的方式操作dataRDD中的数据了</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         <span class="keyword">val</span> resDf = sparkSession.sql(<span class="string">"select name,age from student where age &gt; 18"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD</span></span><br><span class="line">         <span class="keyword">val</span> resRDD = resDf.rdd </span><br><span class="line">         resRDD.map(row=&gt;(row(<span class="number">0</span>).toString,row(<span class="number">1</span>).toString.toInt))</span><br><span class="line">         .collect()</span><br><span class="line">         .foreach(println(_))</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IDEA编程时使用一个未知的东西，alt+回车,会提示需要导入的东西</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303290043493.png" alt="image-20230329004355828"></p><h5 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.RowFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用编程方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RddToDataFrameByProgramJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByProgramJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="comment">//从sparkSession中获取的是scala中的sparkContext，所以需要转换成java中的sp</span></span><br><span class="line">         JavaSparkContext sc = JavaSparkContext.fromSparkContext(sparkSession.</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t1 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jack"</span>, <span class="number">18</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t2 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"tom"</span>, <span class="number">20</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t3 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jessic"</span>, <span class="number">30</span>);</span><br><span class="line">         JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; dataRDD = sc.parallelize(Arrays.asList(t1,t2,t3));</span><br><span class="line">         <span class="comment">//组装rowRDD</span></span><br><span class="line">         JavaRDD&lt;Row&gt; rowRDD = dataRDD.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;String, Integer&gt;,Row&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> RowFactory.create(tup._1, tup._2);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//指定元数据信息</span></span><br><span class="line">        ArrayList&lt;StructField&gt; structFieldList = <span class="keyword">new</span> ArrayList&lt;StructField&gt;();</span><br><span class="line">         structFieldList.add(DataTypes.createStructField(<span class="string">"name"</span>, DataTypes.StringType,<span class="keyword">true</span>));</span><br><span class="line">         structFieldList.add(DataTypes.createStructField(<span class="string">"age"</span>, DataTypes.IntegerType,True));</span><br><span class="line">         StructType schema = DataTypes.createStructType(structFieldList);</span><br><span class="line">         <span class="comment">//构建DataFrame</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.createDataFrame(rowRDD, schema);</span><br><span class="line"></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>);</span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         Dataset&lt;Row&gt; resDf = sparkSession.sql(<span class="string">"select name,age from student where age&gt;18"</span>);</span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD，注意：这里需要转为JavaRDD</span></span><br><span class="line">         JavaRDD&lt;Row&gt; resRDD = resDf.javaRDD();</span><br><span class="line">         List&lt;Tuple2&lt;String, Integer&gt;&gt; resList = resRDD.map(<span class="keyword">new</span> Function&lt;Row, Tuple2&lt;String,Intege&gt;&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(row.getString(<span class="number">0</span>), row.getInt(<span class="number">1</span>));</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).collect();</span><br><span class="line">         </span><br><span class="line">                                                                      <span class="keyword">for</span>(Tuple2&lt;String,Integer&gt; tup : resList)&#123;</span><br><span class="line">         System.out.println(tup);</span><br><span class="line">         &#125;</span><br><span class="line">         sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="load和save操作"><a href="#load和save操作" class="headerlink" title="load和save操作"></a>load和save操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对于Spark SQL的DataFrame来说，无论是从什么数据源创建出来的DataFrame，都有一些共同的load和save操作。(不使用这两个方法，使用TextFile(),再转成DataFrame操作，再转RDD,saveAsText()也可以)</span><br><span class="line"></span><br><span class="line">load操作主要用于加载数据，创建出DataFrame；</span><br><span class="line">save操作，主要用于将DataFrame中的数据保存到文件中。</span><br><span class="line"></span><br><span class="line">我们前面操作json格式的数据的时候好像没有使用load方法，而是直接使用的json方法，这是什么特殊用法吗？</span><br><span class="line">查看json方法的源码会发现，它底层调用的是format和load方法</span><br><span class="line">def json(paths: String*): DataFrame &#x3D; format(&quot;json&quot;).load(paths : _*)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">我们如果使用原始的format和load方法加载数据，</span><br><span class="line">此时如果不指定format，则默认读取的数据源格式是parquet，也可以手动指定数据源格式。Spark SQL内置了一些常见的数据源类型，比如json, parquet, jdbc, orc, csv, text</span><br><span class="line"></span><br><span class="line">通过这个功能，就可以在不同类型的数据源之间进行转换了。</span><br></pre></td></tr></table></figure><h4 id="scala-5"><a href="#scala-5" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：load和save的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LoadAndSaveOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="comment">//读取数据(这种方式与前面讲的创建dataframe是一样的，sparkSession.read.json("xxx"))</span></span><br><span class="line">         <span class="keyword">val</span> stuDf = sparkSession.read</span><br><span class="line">         .format(<span class="string">"json"</span>)</span><br><span class="line">         .load(<span class="string">"D:\\student.json"</span>)</span><br><span class="line">         <span class="comment">//保存数据</span></span><br><span class="line">         stuDf.select(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">         .write</span><br><span class="line">         .format(<span class="string">"csv"</span>)</span><br><span class="line">         .save(<span class="string">"hdfs://bigdata01:9000/out-save001"</span>)</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291340241.png" alt="image-20230329134033719"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291340871.png" alt="image-20230329134053438"></p><h4 id="java-5"><a href="#java-5" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：load和save的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoadAndSaveOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//读取数据</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.read()</span><br><span class="line">         .format(<span class="string">"json"</span>)</span><br><span class="line">         .load(<span class="string">"D:\\student.json"</span>);</span><br><span class="line">         <span class="comment">//保存数据</span></span><br><span class="line">         stuDf.select(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">         .write()</span><br><span class="line">         .format(<span class="string">"csv"</span>)</span><br><span class="line">         .save(<span class="string">"hdfs://bigdata01:9000/out-save002"</span>);</span><br><span class="line">         sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="SaveMode"><a href="#SaveMode" class="headerlink" title="SaveMode"></a>SaveMode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL对于save操作，提供了不同的save mode。</span><br><span class="line">主要用来处理，当目标位置已经有数据时应该如何处理。save操作不会执行锁操作，并且也不是原子的，因此是有一定风险出现脏数据的。</span><br><span class="line"></span><br><span class="line">SaveMode 解释</span><br><span class="line">SaveMode.ErrorIfExists (默认) 如果目标位置已经存在数据，那么抛出一个异常</span><br><span class="line">SaveMode.Append 如果目标位置已经存在数据，那么将数据追加进去</span><br><span class="line">SaveMode.Overwrite 如果目标位置已经存在数据，那么就将已经存在的数据删除，用新数据进行覆盖</span><br><span class="line">SaveMode.Ignore 如果目标位置已经存在数据，那么就忽略，不做任何操作</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在LoadAndSaveOpScala中增加SaveMode的设置，重新执行，验证结果</span><br><span class="line">将SaveMode设置为Append，如果目标已存在，则追加</span><br><span class="line">stuDf.select(&quot;name&quot;,&quot;age&quot;)</span><br><span class="line"> .write</span><br><span class="line"> .format(&quot;csv&quot;)</span><br><span class="line"> .mode(SaveMode.Append)&#x2F;&#x2F;追加</span><br><span class="line"> .save(&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;out-save001&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291355158.png" alt="image-20230329135505416"></p><h4 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291356152.png" alt="image-20230329135637838"></p><h3 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Spark中提供了很多内置的函数</span><br><span class="line">种类 函数</span><br><span class="line">聚合函数 avg, count, countDistinct, first, last, max, mean, min, sum, </span><br><span class="line"></span><br><span class="line">集合函数 array_contains, explode, size</span><br><span class="line"></span><br><span class="line">日期&#x2F;时间函数 datediff, date_add, date_sub, add_months, last_day, next_day, </span><br><span class="line"></span><br><span class="line">数学函数 abs, ceil, floor, round</span><br><span class="line"></span><br><span class="line">混合函数 if, isnull, md5, not, rand, when</span><br><span class="line"></span><br><span class="line">字符串函数 concat, get_json_object, length, reverse, split, upper</span><br><span class="line"></span><br><span class="line">窗口函数 denseRank, rank, rowNumber</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实这里面的函数和hive中的函数是类似的</span><br><span class="line">注意：SparkSQL中的SQL函数文档不全，其实在使用这些函数的时候，大家完全可以去查看hive中sql的文档，使用的时候都是一样的。</span><br></pre></td></tr></table></figure><h2 id="实战：TopN主播统计"><a href="#实战：TopN主播统计" class="headerlink" title="实战：TopN主播统计"></a>实战：TopN主播统计</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在前面讲Spark core的时候我们讲过一个案例，TopN主播统计，计算每个大区当天金币收入TopN的主播，之前我们使用spark中的transformation算子去计算，实现起来还是比较麻烦的，代码量相对来说比较多，下面我们就使用咱们刚学习的Spark sql去实现一下，你会发现，使用sql之后确实简单多了。</span><br><span class="line"></span><br><span class="line">回顾以下我们的两份原始数据，数据都是json格式的</span><br><span class="line">video_info.log 主播的开播记录，其中包含主播的id：uid、直播间id：vid 、大区：area、视频开播时长：length、增加粉丝数量：follow等信息</span><br><span class="line">gift_record.log 用户送礼记录，其中包含送礼人id：uid，直播间id：vid，礼物id：good_id，金币数量：gold 等信息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最终需要的结果是这样的</span><br><span class="line">US 8407173251015:180,8407173251012:70,8407173251001:60</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">分析一下具体步骤</span><br><span class="line">1. 直接使用SparkSession中的load方式加载json的数据</span><br><span class="line">2. 对这两份数据注册临时表</span><br><span class="line">3. 执行sql计算TopN主播</span><br><span class="line">4. 使用foreach将结果打印到控制台</span><br></pre></td></tr></table></figure><h3 id="scala-6"><a href="#scala-6" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：计算TopN主播</span></span><br><span class="line"><span class="comment"> * 1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line"><span class="comment"> * 2：对这两份数据注册临时表</span></span><br><span class="line"><span class="comment"> * 3：执行sql计算TopN主播</span></span><br><span class="line"><span class="comment"> * 4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopNAnchorScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line">         <span class="keyword">val</span> videoInfoDf = sparkSession.read.json(<span class="string">"D:\\video_info.log"</span>)</span><br><span class="line">         <span class="keyword">val</span> giftRecordDf = sparkSession.read.json(<span class="string">"D:\\gift_record.log"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//2：对这两份数据注册临时表</span></span><br><span class="line">         videoInfoDf.createOrReplaceTempView(<span class="string">"video_info"</span>)</span><br><span class="line">         giftRecordDf.createOrReplaceTempView(<span class="string">"gift_record"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//3：执行sql计算TopN主播</span></span><br><span class="line">         <span class="keyword">val</span> sql =<span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t4.area, "</span>+</span><br><span class="line">         <span class="string">"concat_ws(',',collect_list(t4.topn)) as topn_list "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t3.area,concat(t3.uid,':',cast(t3.gold_sum_all as int)) as topn "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t2.uid,t2.area,t2.gold_sum_all,row_number() over (partition by area orde</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>t1.uid,max(t1.area) as area,sum(t1.gold_sum) as gold_sum_all <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vi.uid,vi.vid,vi.area,gr.gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>video_info as vi <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>join <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>(select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vid,sum(gold) as gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>gift_record <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as gr <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>on vi.vid = gr.vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t1 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t1.uid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t2 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as t3 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>where t3.num &lt;=<span class="number">3</span> <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t4 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t4.area <span class="string">"</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         val resDf = sparkSession.sql(sql)</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         //4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="string">         resDf.rdd.foreach(row=&gt;println(row.getAs[String]("</span><span class="string">area")+"</span>\<span class="string">t"+row.getAs[String]))</span></span><br><span class="line"><span class="string">         sparkSession.stop()</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">t4.area,</span><br><span class="line"><span class="keyword">concat_ws</span>(<span class="string">','</span>,collect_list(t4.topn)) <span class="keyword">as</span> topn_list</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">t3.area,<span class="keyword">concat</span>(t3.uid,<span class="string">':'</span>,<span class="keyword">cast</span>(t3.gold_sum_all <span class="keyword">as</span> <span class="built_in">int</span>)) <span class="keyword">as</span> topn // <span class="keyword">cast</span>() <span class="keyword">as</span> <span class="built_in">int</span> 是因为结果中有xxx<span class="number">.0</span></span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">t2.uid,t2.area,t2.gold_sum_all,row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> area <span class="keyword">order</span> <span class="keyword">by</span> gold_sum_all <span class="keyword">desc</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">t1.uid,<span class="keyword">max</span>(t1.area) <span class="keyword">as</span> area,<span class="keyword">sum</span>(t1.gold_sum) <span class="keyword">as</span> gold_sum_all</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">vi.uid,vi.vid,vi.area,gr.gold_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">video_info <span class="keyword">as</span> vi</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">vid,<span class="keyword">sum</span>(gold) <span class="keyword">as</span> gold_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">gift_record</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vid</span><br><span class="line">)<span class="keyword">as</span> gr</span><br><span class="line"><span class="keyword">on</span> vi.vid = gr.vid</span><br><span class="line">) <span class="keyword">as</span> t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t1.uid</span><br><span class="line">) <span class="keyword">as</span> t2</span><br><span class="line">)<span class="keyword">as</span> t3</span><br><span class="line"><span class="keyword">where</span> t3.num &lt;=<span class="number">3</span></span><br><span class="line">) <span class="keyword">as</span> t4</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t4.area</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3再select的结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301508636.png" alt="image-20230330150555300"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301509104.png" alt="image-20230330150618987"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t4的prinln(_)是这样时</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301510849.png" alt="image-20230330151021942"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最终结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301511207.png" alt="image-20230330151135975"></p><h3 id="java-6"><a href="#java-6" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：计算TopN主播</span></span><br><span class="line"><span class="comment"> * 1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line"><span class="comment"> * 2：对这两份数据注册临时表</span></span><br><span class="line"><span class="comment"> * 3：执行sql计算TopN主播</span></span><br><span class="line"><span class="comment"> * 4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNAnchorJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"TopNAnchorJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line">         Dataset&lt;Row&gt; videoInfoDf = sparkSession.read().json(<span class="string">"D:\\video_info.log"</span>);</span><br><span class="line">         Dataset&lt;Row&gt; giftRecordDf = sparkSession.read().json(<span class="string">"D:\\gift_record.log"</span>);</span><br><span class="line">         <span class="comment">//2：对这两份数据注册临时表</span></span><br><span class="line">         videoInfoDf.createOrReplaceTempView(<span class="string">"video_info"</span>);</span><br><span class="line">         giftRecordDf.createOrReplaceTempView(<span class="string">"gift_record"</span>);</span><br><span class="line">         <span class="comment">//3：执行sql计算TopN主播</span></span><br><span class="line">         String sql = <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t4.area, "</span>+</span><br><span class="line">         <span class="string">"concat_ws(',',collect_list(t4.topn)) as topn_list "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t3.area,concat(t3.uid,':',cast(t3.gold_sum_all as int)) as t</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>t2.uid,t2.area,t2.gold_sum_all,row_number() over (partition </span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t1.uid,max(t1.area) as area,sum(t1.gold_sum) as gold_sum_all </span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vi.uid,vi.vid,vi.area,gr.gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>video_info as vi <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>join <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>(select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vid,sum(gold) as gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>gift_record <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as gr <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>on vi.vid = gr.vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t1 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t1.uid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t2 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as t3 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>where t3.num &lt;=<span class="number">3</span> <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t4 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t4.area <span class="string">";</span></span><br><span class="line"><span class="string">         Dataset&lt;Row&gt; resDf = sparkSession.sql(sql);</span></span><br><span class="line"><span class="string">         //4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="string">         resDf.javaRDD().foreach(new VoidFunction&lt;Row&gt;() &#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public void call(Row row) throws Exception &#123;</span></span><br><span class="line"><span class="string">         System.out.println(row.getString(0)+"</span>\t<span class="string">"+row.getString(1));</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         sparkSession.stop();</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">代码执行结果如下：</span><br><span class="line">CN 8407173251008:120,8407173251003:60,8407173251014:50</span><br><span class="line">ID 8407173251005:160,8407173251010:140,8407173251002:70</span><br><span class="line">US 8407173251015:180,8407173251012:70,8407173251001:60</span><br></pre></td></tr></table></figure><h3 id="集群执行"><a href="#集群执行" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">新建一个object：TopNAnchorClusterScala，修改代码，将任务的输出数据保存到hdfs上面</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：计算TopN主播</span></span><br><span class="line"><span class="comment"> * 1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line"><span class="comment"> * 2：对这两份数据注册临时表</span></span><br><span class="line"><span class="comment"> * 3：执行sql计算TopN主播</span></span><br><span class="line"><span class="comment"> * 4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopNAnchorClusterScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="comment">//1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line">         <span class="keyword">val</span> videoInfoDf = sparkSession.read.json(<span class="string">"hdfs://bigdata01:9000/video_inf</span></span><br><span class="line"><span class="string">         val giftRecordDf = sparkSession.read.json("</span>hdfs:<span class="comment">//bigdata01:9000/gift_rec</span></span><br><span class="line">         <span class="comment">//2：对这两份数据注册临时表</span></span><br><span class="line">         videoInfoDf.createOrReplaceTempView(<span class="string">"video_info"</span>)</span><br><span class="line">         giftRecordDf.createOrReplaceTempView(<span class="string">"gift_record"</span>)</span><br><span class="line">         <span class="comment">//3：执行sql计算TopN主播</span></span><br><span class="line">         <span class="keyword">val</span> sql =<span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t4.area, "</span>+</span><br><span class="line">         <span class="string">"concat_ws(',',collect_list(t4.topn)) as topn_list "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t3.area,concat(t3.uid,':',cast(t3.gold_sum_all as int)) as topn "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t2.uid,t2.area,t2.gold_sum_all,row_number() over (partition by area orde</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>t1.uid,max(t1.area) as area,sum(t1.gold_sum) as gold_sum_all <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vi.uid,vi.vid,vi.area,gr.gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>video_info as vi <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>join <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>(select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vid,sum(gold) as gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>gift_record <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as gr <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>on vi.vid = gr.vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t1 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t1.uid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t2 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as t3 <span class="string">"+</span></span><br><span class="line"><span class="string">        "</span>where t3.num &lt;=<span class="number">3</span> <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t4 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t4.area <span class="string">"</span></span><br><span class="line"><span class="string">         val resDf = sparkSession.sql(sql)</span></span><br><span class="line"><span class="string">         //4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="string">         resDf.rdd</span></span><br><span class="line"><span class="string">         .map(row=&gt;row.getAs[String]("</span><span class="string">area")+"</span>\<span class="string">t"+row.getAs[String]("</span>topn_<span class="string">list")</span></span><br><span class="line"><span class="string">         .saveAsTextFile("</span>hdfs:<span class="comment">//bigdata01:9000/out-topn")</span></span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">修改pom中依赖的配置，全部设置为provided</span><br><span class="line"></span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">         &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">         &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">         &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">         &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">         &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">         &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">         &lt;version&gt;1.2.68&lt;&#x2F;version&gt;</span><br><span class="line">         &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">         &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">         &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">         &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">         &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对spark-core和spark-sql在打包的时候是不需要的，针对fastjson有的spark job是需要的，不过建议</span><br><span class="line">在这设置为provided，打包的时候不要打进去，在具体使用的时候可以在spark-submit脚本中通过–jar</span><br><span class="line">来动态指定这个jar包，最好把这个jar包上传到hdfs上面统一管理和维护。</span><br><span class="line">编译打包，上传到bigdta04上的 &#x2F;data&#x2F;soft&#x2F;sparkjars 目录</span><br><span class="line">创建spark-submit脚本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 sparkjars]# vi topnJob.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.sql.TopNAnchorClusterScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot; \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">提交任务</span><br><span class="line">[root@bigdata04 sparkjars]# sh -x topnJob.sh</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291557223.png" alt="image-20230329155709778"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E7%AE%97%E5%AD%90%E4%BC%98%E5%8C%96-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E7%AE%97%E5%AD%90%E4%BC%98%E5%8C%96-4.html</id>
    <published>2023-03-27T17:50:58.000Z</published>
    <updated>2023-04-08T13:45:13.433Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-算子优化-4"><a href="#第十一周-Spark性能优化的道与术-算子优化-4" class="headerlink" title="第十一周 Spark性能优化的道与术-算子优化-4"></a>第十一周 Spark性能优化的道与术-算子优化-4</h1><h2 id="map-vs-mapPartitions"><a href="#map-vs-mapPartitions" class="headerlink" title="map vs mapPartitions"></a>map vs mapPartitions</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">map操作：对RDD 中的每个元素进行操作，一次处理一条数据</span><br><span class="line"></span><br><span class="line">mapPartitions操作(transformation操作)：对RDD中每个partition进行操作，一次处理一个分区的数据</span><br><span class="line"></span><br><span class="line">所以：</span><br><span class="line">map操作：执行1次map算子只处理1个元素，如果partition中元素较多，假设当前已经处理了1000个元素，在内存不足的情况下，Spark可以通过GC等方法回收内存（比如将已处理掉的</span><br><span class="line">1000个元素从内存中回收）。因此，map操作通常不会导致OOM异常；</span><br><span class="line"></span><br><span class="line">mapPartitions操作：执行1次map算子需要接收该partition 中的所有元素，因此一旦元素很多而内存不足，就容易导致OOM的异常，也不是说一定就会产生OOM异常，只是和map算子对比的话，</span><br><span class="line">相对来说容易产生OOM异常(OOM，java.lang.OutOfMemoryError 错误，也就是java内存溢出错误。)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">不过一般情况下，mapPartitions的性能更高；初始化操作、数据库链接等操作适合使用mapPartitions操作</span><br><span class="line"></span><br><span class="line">这是因为：</span><br><span class="line">假设需要将RDD中的每个元素写入数据库中，这时候就应该把创建数据库链接的操作放置在mapPartitions中，创建数据库链接这个操作本身就是个比较耗时的，如果该操作放在map中执行，将会频繁执行，比较耗时且影响数据库的稳定性。</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：mapPartitons的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapPartitionsOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"MapPartitionsOpScala"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="comment">//设置分区数量为2</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>),<span class="number">2</span>)</span><br><span class="line">         <span class="comment">//map算子一次处理一条数据</span></span><br><span class="line">         <span class="comment">/*val sum = dataRDD.map(item=&gt;&#123;</span></span><br><span class="line"><span class="comment">         println("==============")</span></span><br><span class="line"><span class="comment">         item * 2</span></span><br><span class="line"><span class="comment">         &#125;).reduce( _ + _)*/</span></span><br><span class="line">        <span class="comment">//mapPartitions算子一次处理一个分区的数据</span></span><br><span class="line">         <span class="keyword">val</span> sum = dataRDD.mapPartitions(it=&gt;&#123;</span><br><span class="line">         <span class="comment">//建议针对初始化链接之类的操作，使用mapPartitions，放在mapPartitions内部</span></span><br><span class="line">         <span class="comment">//例如：创建数据库链接，使用mapPartitions可以减少链接创建的次数，提高性能</span></span><br><span class="line">         <span class="comment">//注意：创建数据库链接的代码建议放在次数，不要放在Driver端或者it.foreach内部</span></span><br><span class="line">         <span class="comment">//数据库链接放在Driver端会导致链接无法序列化，无法传递到对应的task中执行，所以</span></span><br><span class="line">         <span class="comment">//数据库链接放在it.foreach()内部还是会创建多个链接，和使用map算子的效果是一样</span></span><br><span class="line">         println(<span class="string">"=================="</span>)</span><br><span class="line">         <span class="keyword">val</span> result = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">         <span class="comment">//这个foreach是调用的scala里面的函数</span></span><br><span class="line">         it.foreach(item=&gt;&#123;</span><br><span class="line">         result.+=(item * <span class="number">2</span>)</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">         result.toIterator</span><br><span class="line">         &#125;).reduce(_ + _)</span><br><span class="line">         println(<span class="string">"sum:"</span>+sum)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：mapPartitons的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapPartitionsOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"MapPartitionsOpJava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>));</span><br><span class="line">         Integer sum = dataRDD.mapPartitions(<span class="keyword">new</span> FlatMapFunction&lt;Iterator&lt;Inte</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterator&lt;Integer&gt; <span class="title">call</span><span class="params">(Iterator&lt;Integer&gt; it)</span> <span class="keyword">throws</span> Except</span></span><br><span class="line"><span class="function">         <span class="comment">//数据库链接的代码需要放在这个位置</span></span></span><br><span class="line"><span class="function">         ArrayList&lt;Integer&gt; list </span>= <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">         list.add(it.next() * <span class="number">2</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">        &#125;</span><br><span class="line">         &#125;).reduce(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> i1 + i2;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         System.out.println(<span class="string">"sum:"</span>+sum);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="foreach-vs-foreachPartition"><a href="#foreach-vs-foreachPartition" class="headerlink" title="foreach vs foreachPartition"></a>foreach vs foreachPartition</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">foreach：一次处理一条数据</span><br><span class="line">foreachPartition：一次处理一个分区的数据</span><br><span class="line">foreachPartition的特性和mapPartitions的特性是一样的，唯一的区别就是mapPartitions是transformation操作（不会立即执行），foreachPartition是action操作（会立即执行）</span><br></pre></td></tr></table></figure><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：foreachPartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ForeachPartitionOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"ForeachPartitionOpScala"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="comment">//设置分区数量为2</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>),<span class="number">2</span>)</span><br><span class="line">         <span class="comment">//foreachPartition：一次处理一个分区的数据，作用和mapPartitions类似</span></span><br><span class="line">         <span class="comment">//唯一的区是mapPartitions是transformation算子，foreachPartition是action算子</span></span><br><span class="line">         dataRDD.foreachPartition(it=&gt;&#123;</span><br><span class="line">         <span class="comment">//在此处获取数据库链接</span></span><br><span class="line">         println(<span class="string">"==============="</span>)</span><br><span class="line">         it.foreach(item=&gt;&#123; <span class="comment">// 和对RDD处理的foreach不一样</span></span><br><span class="line">             <span class="comment">//在这里使用数据库链接</span></span><br><span class="line">             println(item)</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">         &#125;)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281441692.png" alt="image-20230328144113035"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281440127.png" alt="image-20230328144052865"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：foreachPartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ForeachPartitionOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"ForeachPartitionOpJava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span></span><br><span class="line">         dataRDD.foreachPartition(<span class="keyword">new</span> VoidFunction&lt;Iterator&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Iterator&lt;Integer&gt; it)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         System.out.println(<span class="string">"============="</span>);</span><br><span class="line">         <span class="comment">//在此处获取数据库链接</span></span><br><span class="line">         <span class="keyword">while</span> (it.hasNext())&#123;</span><br><span class="line">         <span class="comment">//在这里使用数据库链接</span></span><br><span class="line">         System.out.println(it.next());</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="repartition的使用"><a href="#repartition的使用" class="headerlink" title="repartition的使用"></a>repartition的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">对RDD进行重分区，repartition主要有两个应用场景：</span><br><span class="line">1. 可以调整RDD的并行度</span><br><span class="line">针对个别RDD，如果感觉分区数量不合适，想要调整，可以通过repartition进行调整，分区调整了之后，对应的并行度也就可以调整了</span><br><span class="line">2. 可以解决RDD中数据倾斜的问题</span><br><span class="line">如果RDD中不同分区之间的数据出现了数据倾斜，可以通过repartition实现数据重新分发，可以均匀分发到不同分区中</span><br></pre></td></tr></table></figure><h3 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：repartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RepartitionOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"RepartitionOpScala"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="comment">//设置分区数量为2</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>),<span class="number">2</span>)</span><br><span class="line">         <span class="comment">//重新设置RDD的分区数量为3，这个操作会产生shuffle</span></span><br><span class="line">         <span class="comment">//也可以解决RDD中数据倾斜的问题</span></span><br><span class="line">         dataRDD.repartition(<span class="number">3</span>)</span><br><span class="line">         .foreachPartition(it=&gt;&#123;</span><br><span class="line">         println(<span class="string">"========="</span>) <span class="comment">// 运行程序会输出3次</span></span><br><span class="line">         it.foreach(println(_))</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//通过repartition可以控制输出数据产生的文件个数</span></span><br><span class="line">         dataRDD.saveAsTextFile(<span class="string">"hdfs://bigdata01:9000/rep-001"</span>) <span class="comment">// 生成3个文件</span></span><br><span class="line">                dataRDD.repartition(<span class="number">1</span>).saveAsTextFile(<span class="string">"hdfs://bigdata01:9000/rep-002"</span>) <span class="comment">// 生成1个文件 </span></span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：repartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RepartitionOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"RepartitionOpJava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>),<span class="number">2</span>);</span><br><span class="line">         dataRDD.repartition(<span class="number">3</span>)</span><br><span class="line">         .foreachPartition(<span class="keyword">new</span> VoidFunction&lt;Iterator&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Iterator&lt;Integer&gt; it)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         System.out.println(<span class="string">"=============="</span>);</span><br><span class="line">         <span class="keyword">while</span> (it.hasNext())&#123;</span><br><span class="line">         System.out.println(it.next());</span><br><span class="line">         &#125;</span><br><span class="line">             &#125;);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="reduceByKey和groupByKey的区别"><a href="#reduceByKey和groupByKey的区别" class="headerlink" title="reduceByKey和groupByKey的区别"></a>reduceByKey和groupByKey的区别</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">在实现分组聚合功能时这两个算子有什么区别？</span><br><span class="line">看这两行代码</span><br><span class="line">val counts &#x3D; wordCountRDD.reduceByKey(_ + _)</span><br><span class="line">val counts &#x3D; wordCountRDD.groupByKey().map(wc &#x3D;&gt; (wc._1, wc._2.sum))</span><br><span class="line"></span><br><span class="line">这两行代码的最终效果是一样的，都是对wordCountRDD中每个单词出现的次数进行聚合统计</span><br><span class="line">那这两种方式在原理层面有什么区别吗？</span><br><span class="line">首先这两个算子在执行的时候都会产生shuffle</span><br><span class="line">但是：</span><br><span class="line">1：当采用reduceByKey时，数据在进行shuffle之前会先进行局部聚合</span><br><span class="line">2：当使用groupByKey时，数据在shuffle之间不会进行局部聚合，会原样进行shuffle</span><br><span class="line"></span><br><span class="line">这样的话reduceByKey就减少了shuffle的数据传送，所以效率会高一些。</span><br><span class="line">如果能用reduceByKey，就用reduceByKey，因为它会在map端，先进行本地combine，可以大大减少要传输到reduce端的数据量，减少网络传输的开销</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面来看这个图，加深一下理解</span><br><span class="line"></span><br><span class="line">从图中可以看出来reduceByKey在shuffle之前会先对数据进行局部聚合，而groupByKey不会，所以在实现分组聚合的需求中，reduceByKey性能略胜一筹。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281513504.png" alt="image-20230328151345043"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-3.html</id>
    <published>2023-03-27T11:31:29.000Z</published>
    <updated>2023-04-08T13:45:18.422Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-企业级最佳实践-3"><a href="#第十一周-Spark性能优化的道与术-企业级最佳实践-3" class="headerlink" title="第十一周 Spark性能优化的道与术-企业级最佳实践-3"></a>第十一周 Spark性能优化的道与术-企业级最佳实践-3</h1><h2 id="性能优化分析"><a href="#性能优化分析" class="headerlink" title="性能优化分析"></a>性能优化分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一个计算任务的执行主要依赖于CPU、内存、带宽</span><br><span class="line">Spark是一个基于内存的计算引擎，所以对它来说，影响最大的可能就是内存，一般我们的任务遇到了性能瓶颈大概率都是内存的问题，当然了CPU和带宽也可能会影响程序的性能，这个情况也不是没有的，只是比较少。</span><br><span class="line">Spark性能优化，其实主要就是在于对内存的使用进行调优。通常情况下，如果你的Spark程序计算的数据量比较小，并且你的内存足够使用，那么只要网络不至于卡死，一般是不会有大的性能问题的。但是Spark程序的性能问题往往出现在针对大数据量进行计算（比如上亿条数的数据，或者上T规模的数据），这个时候如果内存分配不合理就会比较慢，所以，Spark性能优化，主要是对内存进行优化。</span><br></pre></td></tr></table></figure><h2 id="内存都去哪了"><a href="#内存都去哪了" class="headerlink" title="内存都去哪了"></a>内存都去哪了</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 每个Java对象，都有一个对象头，会占用16个字节，主要是包括了一些对象的元信息，比如指向它的类的指针。如果一个对象本身很小，比如就包括了一个int类型的field，那么它的对象头实际上比对象自身还要大。</span><br><span class="line">2. Java的String对象的对象头，会比它内部的原始数据，要多出40个字节。因为它内部使用char数组来保存内部的字符序列，并且还要保存数组长度之类的信息。</span><br><span class="line">3. Java中的集合类型，比如HashMap和LinkedList，内部使用的是链表数据结构，所以对链表中的每一个数据，都使用了Entry对象来包装。Entry对象不光有对象头，还有指向下一个Entry的指针，通常占用8个字节。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">所以把原始文件中的数据转化为内存中的对象之后，占用的内存会比原始文件中的数据要大</span><br><span class="line"></span><br><span class="line">那我如何预估程序会消耗多少内存呢？</span><br><span class="line">通过cache方法，可以看到RDD中的数据cache到内存中之后占用多少内存，这样就能看出了</span><br><span class="line">代码如下：这个测试代码就只写一个scala版本的了</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：测试内存占用情况</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestMemoryScala</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">     conf.setAppName(<span class="string">"TestMemoryScala"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">     <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_10000000.dat"</span>).cache()</span><br><span class="line">     <span class="keyword">val</span> count = dataRDD.count()</span><br><span class="line">         println(count)</span><br><span class="line">     <span class="comment">//while循环是为了保证程序不结束，方便在本地查看4040页面(在本地运行时的spark web页面)中的storage信息</span></span><br><span class="line">     <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">     ;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271945127.png" alt="image-20230327194520543"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271944394.png" alt="image-20230327194456852"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">执行代码，访问localhost的4040端口界面</span><br><span class="line">这个界面其实就是spark的任务界面，在本地运行任务的话可以直接访问4040界面查看</span><br><span class="line">点击stages可以看到任务的原始输入数据是多大</span><br><span class="line"></span><br><span class="line">点击storage可以看到将数据加载到内存，生成RDD之后的大小</span><br><span class="line"></span><br><span class="line">这样我们就能知道这一份数据在RDD中会占用多少内存了，这样在使用的时候，如果想要把数据全部都加载进内存，就需要给这个任务分配这么多内存了，当然了你分配少一些也可以，只不过这样计算效率会变低，因为RDD中的部分数据内存放不下就会放到磁盘了。</span><br></pre></td></tr></table></figure><h2 id="性能优化方案"><a href="#性能优化方案" class="headerlink" title="性能优化方案"></a>性能优化方案</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面我们通过这几个方式来实现对Spark程序的性能优化</span><br><span class="line">    高性能序列化类库</span><br><span class="line">    持久化或者checkpoint</span><br><span class="line">    JVM垃圾回收调优</span><br><span class="line">    提高并行度</span><br><span class="line">    数据本地化</span><br><span class="line">    算子优化</span><br></pre></td></tr></table></figure><h3 id="高性能序列化类库"><a href="#高性能序列化类库" class="headerlink" title="高性能序列化类库"></a>高性能序列化类库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在任何分布式系统中，序列化都是扮演着一个重要的角色的。</span><br><span class="line">如果使用的序列化技术，在执行序列化操作的时候很慢，或者是序列化后的数据还是很大，那么会让分布式应用程序的性能下降很多。所以，进行Spark性能优化的第一步，就是进行序列化的性能优化。</span><br><span class="line"></span><br><span class="line">Spark默认会在一些地方对数据进行序列化，如果我们的算子函数使用到了外部的数据（比如Java中的自定义类型），那么也需要让其可序列化，否则程序在执行的时候是会报错的，提示没有实现序列化，这个一定要注意。</span><br><span class="line"></span><br><span class="line">原因是这样的：</span><br><span class="line">因为Spark的初始化工作是在Driver进程中进行的，但是实际执行是在Worker节点的Executor进程中进行的；当Executor端需要用到Driver端封装的对象时，就需要把Driver端的对象通过序列化传输到Executor端，这个对象就需要实现序列化。</span><br><span class="line">否则会报错，提示对象没有实现序列化</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，其实遇到这种没有实现序列化的对象，解决方法有两种</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 如果此对象可以支持序列化，则将其实现Serializable接口，让它支持序列化</span><br><span class="line">2. 如果此对象不支持序列化，针对一些数据库连接之类的对象，这种对象是不支持序列化的，所以可以把这个代码放到算子内部，这样就不会通过driver端传过去了，它会直接在executor中执行。</span><br><span class="line">Spark对于序列化的便捷性和性能进行了一个取舍和权衡。默认情况下，Spark倾向于序列化的便捷性，使用了Java自身提供的序列化机制——基于ObjectInputStream和 ObjectOutputStream的序列化机制，因为这种方式是Java原生提供的，使用起来比较方便，但是Java序列化机制的性能并不高。序列化的速度相对较慢，而且序列化以后的数据，相对来说还是比较大，比较占空间。所以，如果你的Spark应用程序对内存很敏感，那默认的Java序列化机制并不是最好的选择。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark实际上提供了两种序列化机制：</span><br><span class="line">Java序列化机制和Kryo序列化机制</span><br><span class="line">Spark只是默认使用了java这种序列化机制</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Java序列化机制：默认情况下，Spark使用Java自身的ObjectInputStream和ObjectOutputStream机制进行对象的序列化。只要你的类实现了Serializable接口，那么都是可以序列化的。Java序列化机制的速度比较慢，而且序列化后的数据占用的内存空间比较大，这是它的缺点</span><br><span class="line"></span><br><span class="line">Kryo序列化机制：Spark也支持使用Kryo序列化。Kryo序列化机制比Java序列化机制更快，而且序列化后的数据占用的空间更小，通常比Java序列化的数据占用的空间要小10倍左右。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Kryo序列化机制之所以不是默认序列化机制的原因：</span><br><span class="line"></span><br><span class="line">第一点：因为有些类型虽然实现了Seriralizable接口，但是它也不一定能够被Kryo进行序列化；</span><br><span class="line">第二点：如果你要得到最佳的性能，Kryo还要求你在Spark应用程序中，对所有你需要序列化的类型都进行手工注册，这样就比较麻烦了</span><br><span class="line"></span><br><span class="line">如果要使用Kryo序列化机制</span><br><span class="line">首先要用SparkConf设置spark.serializer的值为 org.apache.spark.serializer.KryoSerializer，就是将Spark的序列化器设置为KryoSerializer。这样，Spark在进行序列化时，就会使用Kryo进行序列化</span><br><span class="line">了。使用Kryo时针对需要序列化的类，需要预先进行注册，这样才能获得最佳性能——如果不注册的话，Kryo也能正常工作，只是Kryo必须时刻保存类型的全类名，反而占用不少内存。</span><br><span class="line">Spark默认对Scala中常用的类型在Kryo中做了注册，但是，如果在自己的算子中，使用了外部的自定义类型的对象，那么还是需要对其进行注册。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注册自定义的数据类型格式：</span><br><span class="line">conf.registerKryoClasses(...)</span><br><span class="line"></span><br><span class="line">注意：如果要序列化的自定义的类型，字段特别多，此时就需要对Kryo本身进行优化，因为Kryo内部的缓存可能不够存放那么大的class对象</span><br><span class="line"></span><br><span class="line">需要调用SparkConf.set()方法，设置spark.kryoserializer.buffer.mb参数的值，将其调大，默认值为2，单位是MB ，也就是说最大能缓存2M的对象，然后进行序列化。可以在必要时将其调大。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">什么场景下适合使用Kryo序列化？</span><br><span class="line"></span><br><span class="line">一般是针对一些自定义的对象，例如我们自己定义了一个对象，这个对象里面包含了几十M，或者上百M的数据，然后在算子函数内部，使用到了这个外部的大对象</span><br><span class="line">如果默认情况下，让Spark用java序列化机制来序列化这种外部的大对象，那么就会导致序列化速度比较慢，并且序列化以后的数据还是比较大。</span><br><span class="line"></span><br><span class="line">所以，在这种情况下，比较适合使用Kryo序列化类库，来对外部的大对象进行序列化，提高序列化速度，减少序列化后的内存空间占用。</span><br><span class="line">用代码实现一个案例：</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><h4 id="使用kryo实现序列化"><a href="#使用kryo实现序列化" class="headerlink" title="使用kryo实现序列化"></a>使用kryo实现序列化</h4><h5 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.<span class="type">Kryo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.<span class="type">KryoRegistrator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Kryo序列化的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KryoSerScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">     conf.setAppName(<span class="string">"KryoSerScala"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     <span class="comment">//指定使用kryo序列化机制，注意：如果使用了registerKryoClasses，其实这一行设置可以省略的</span></span><br><span class="line">     .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">     .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Person</span>]))<span class="comment">//注册自定义的数据类型(Array里也可以传多个)，注册了性能高一些</span></span><br><span class="line">     <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">     <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="string">"hello you"</span>,<span class="string">"hello me"</span>))</span><br><span class="line">     <span class="keyword">val</span> wordsRDD = dataRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">     <span class="keyword">val</span> personRDD = wordsRDD.map(word=&gt;<span class="type">Person</span>(word,<span class="number">18</span>)).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>)</span><br><span class="line">     personRDD.foreach(println(_))</span><br><span class="line">     <span class="comment">//while循环是为了保证程序不结束，方便在本地查看4040页面中的storage信息</span></span><br><span class="line">     <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">     ;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">                                                                 <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>,age: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span></span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行任务，然后访问localhost的4040界面</span><br><span class="line">在界面中可以看到cache的数据大小是 31 字节。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272100115.png" alt="image-20230327210029655"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那我们把kryo序列化设置去掉，使用默认的java序列化看一下效果</span><br><span class="line">修改代码，注释掉这两行代码即可</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">&#x2F;&#x2F;.registerKryoClasses(Array(classOf[Person]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行任务，再访问4040界面</span><br><span class="line">发现此时占用的内存空间是 138 字节，比使用kryo的方式内存空间多占用了将近5倍。</span><br><span class="line">所以从这可以看出来，使用 kryo 序列化方式对内存的占用会降低很多。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272101635.png" alt="image-20230327210118413"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：如果我们只是将spark的序列化机制改为了kryo序列化，但是没有对使用到的自定义类型手工进行注册，那么此时内存的占用会介于前面两种情况之间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改代码，只注释掉registerKryoClasses这一行代码</span><br><span class="line"></span><br><span class="line">.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">&#x2F;&#x2F;.registerKryoClasses(Array(classOf[Person]))&#x2F;&#x2F;注册自定义的数据类型</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行任务，再访问4040界面</span><br><span class="line">发现此时的内存占用为123字节，介于前面的31字节和138字节之间。</span><br><span class="line">所以从这可以看出来，在使用kryo序列化的时候，针对自定义的类型最好是手工注册一下，否则就算开启了kryo序列化，性能的提升也是有限的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272103931.png" alt="image-20230327210309583"></p><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.Kryo;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.KryoRegistrator;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.StorageLevel;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Kryo序列化的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KryoSerjava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//创建SparkContext：</span></span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"KryoSerjava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         .set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSer</span></span><br><span class="line"><span class="string">         .set("</span>spark.kryo.classesToRegister<span class="string">", "</span>com.imooc.java.Person<span class="string">");</span></span><br><span class="line"><span class="string">         JavaSparkContext sc = new JavaSparkContext(conf);</span></span><br><span class="line"><span class="string">         JavaRDD&lt;String&gt; dataRDD = sc.parallelize(Arrays.asList("</span>hello you<span class="string">", "</span>hello me<span class="string">"));</span></span><br><span class="line"><span class="string">         JavaRDD&lt;String&gt; wordsRDD = dataRDD.flatMap(new FlatMapFunction&lt;String,String&gt;()&#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span></span><br><span class="line"><span class="string">         return Arrays.asList(line.split("</span> <span class="string">")).iterator();</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         JavaRDD&lt;Person&gt; personRDD = wordsRDD.map(new Function&lt;String, Person&gt;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public Person call(String word) throws Exception &#123;</span></span><br><span class="line"><span class="string">         return new Person(word, 18);</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;).persist(StorageLevel.MEMORY_ONLY_SER());</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         personRDD.foreach(new VoidFunction&lt;Person&gt;() &#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public void call(Person person) throws Exception &#123;</span></span><br><span class="line"><span class="string">         System.out.println(person);</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         while (true)&#123;</span></span><br><span class="line"><span class="string">         ;</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">class Person implements Serializable&#123;</span></span><br><span class="line"><span class="string">    private String name;</span></span><br><span class="line"><span class="string">    private int age;</span></span><br><span class="line"><span class="string">    Person(String name,int age)&#123; // 这里讲可以通过什么自动生成，没听清</span></span><br><span class="line"><span class="string">        this.name = name;</span></span><br><span class="line"><span class="string">        this.age = age;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    @Override</span></span><br><span class="line"><span class="string">    public String toString() &#123; // alt+shift+s,右键generate-&gt;toString</span></span><br><span class="line"><span class="string">        return "</span>Person&#123;<span class="string">" +</span></span><br><span class="line"><span class="string">            "</span>name=<span class="string">'" + name + '</span>\<span class="string">''</span> +</span><br><span class="line">            <span class="string">", age="</span> + age +</span><br><span class="line">            <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="持久化或者checkpoint"><a href="#持久化或者checkpoint" class="headerlink" title="持久化或者checkpoint"></a>持久化或者checkpoint</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对程序中多次被transformation或者action操作的RDD进行持久化操作，避免对一个RDD反复进行计算，再进一步优化，使用Kryo序列化的持久化级别，减少内存占用</span><br><span class="line">为了保证RDD持久化数据在可能丢失的情况下还能实现高可靠，则需要对RDD执行Checkpoint操作</span><br><span class="line">这两个操作我们前面讲过了，在这就不再演示了</span><br></pre></td></tr></table></figure><h3 id="JVM垃圾回收调优"><a href="#JVM垃圾回收调优" class="headerlink" title="JVM垃圾回收调优"></a>JVM垃圾回收调优</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">由于Spark是基于内存的计算引擎，RDD缓存的数据，以及算子执行期间创建的对象都是放在内存中的，所以针对Spark任务如果内存设置不合理会导致大部分时间都消耗在垃圾回收上</span><br><span class="line"></span><br><span class="line">对于垃圾回收来说，最重要的就是调节RDD缓存占用的内存空间，和算子执行时创建的对象占用的内存空间的比例。</span><br><span class="line"></span><br><span class="line">默认情况下，Spark使用每个executor 60%的内存空间来缓存RDD，那么只有40%的内存空间来存放算子执行期间创建的对象</span><br><span class="line">在这种情况下，可能由于内存空间的不足，并且算子对应的task任务在运行时创建的对象过大，那么一旦发现40%的内存空间不够用了，就会触发Java虚拟机的垃圾回收操作。</span><br><span class="line"></span><br><span class="line">因此在极端情况下，垃圾回收操作可能会被频繁触发。</span><br><span class="line">在这种情况下，如果发现垃圾回收频繁发生。那么就需要对这个比例进行调优了， spark.storage.memoryFraction参数的值默认是0.6。</span><br><span class="line">使用SparkConf().set(&quot;spark.storage.memoryFraction&quot;, &quot;0.5&quot;) 可以进行修改，就是将RDD缓存占用内存空间的比例降低为 50% ，从而提供更多的内存空间来保存task运行时创建的对象。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">因此，对于RDD持久化而言，完全可以使用Kryo序列化，加上降低其executor内存占比的方式，来减少其内存消耗。给task提供更多的内存，从而避免task在执行时频繁触发垃圾回收。</span><br><span class="line">我们可以对task的垃圾回收进行监测，在spark的任务执行界面，可以查看每个task执行消耗的时间，以及task gc消耗的时间。</span><br><span class="line"></span><br><span class="line">重新向集群中提交checkpoint的代码，查看spark任务的task指标信息</span><br><span class="line">确保Hadoop集群、yarn的historyserver进程以及spark的historyserver进程是正常运行的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">删除checkpoint任务的输出目录</span><br><span class="line">[root@bigdata04 sparkjars]# hdfs dfs -rm -r &#x2F;out-chk001</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# sh -x checkPointJob.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击生成的第一个job，再点击进去查看这个job的stage，进入第一个stage，查看task的执行情况，看这里面的GC time的数值会不会比较大，最直观的就是如果gc time这里标红了，则说明gc时间过长。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272230037.png" alt="image-20230327223033714"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上面这个是分任务查看，其实还可以查看全局的，看Executor进程中整个任务执行总时间和gc的消耗时间。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272232315.png" alt="image-20230327223148318"></p><h4 id="java-GC"><a href="#java-GC" class="headerlink" title="java GC"></a>java GC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">既然说到了Java中的GC，那我们就需要说道说道了。</span><br><span class="line">Java堆空间被划分成了两块空间：一个是年轻代，一个是老年代。</span><br><span class="line">年轻代放的是短时间存活的对象</span><br><span class="line">老年代放的是长时间存活的对象。</span><br><span class="line">年轻代又被划分了三块空间， Eden、Survivor1、Survivor2</span><br><span class="line"></span><br><span class="line">来看一下这个内存划分比例图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272234159.png" alt="image-20230327223408841"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">年轻代占堆内存的1&#x2F;3，老年代占堆内存的2&#x2F;3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">其中年轻代又被划分了三块， Eden，Survivor1，Survivor2 的比例为 8:1:1</span><br><span class="line">Eden区域和Survivor1区域用于存放对象，Survivor2区域备用。</span><br><span class="line"></span><br><span class="line">我们创建的对象，首先会放入Eden区域，如果Eden区域满了，那么就会触发一次Minor GC，进行年轻代的垃圾回收(其实就是回收Eden区域内没有人使用的对象)，然后将存活的对象存入Survivor1区域，再创建对象的时候继续放入Eden区域。</span><br><span class="line"></span><br><span class="line">第二次Eden区域满了，那么Eden和Survivor1区域中存活的对象，会一块被移动到Survivor2区域中。然后Survivor1和Survivor2的角色调换，Survivor1变成了备用。</span><br><span class="line"></span><br><span class="line">当第三次Eden区域再满了的时候，Eden和Survivor2区域中存活的对象，会一块被移动到Survivor1区域中，按照这个规律进行循环</span><br><span class="line"></span><br><span class="line">如果一个对象，在年轻代中，撑过了多次垃圾回收(默认是15次)，都没有被回收掉，那么会被认为是长时间存活的，此时就会被移入老年代。此外，如果在将Eden和Survivor1中的存活对象，尝试放入Survivor2中时，发现Survivor2放满了，那么会直接放入老年代。此时就出现了，短时间存活的对象，也会进入老年代的问题。</span><br><span class="line"></span><br><span class="line">如果老年代的空间满了，那么就会触发Full GC，进行老年代的垃圾回收操作，如果执行Full GC也释放不了内存空间，就会报内存溢出的错误了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，Full GC是一个重量级的垃圾回收，Full GC执行的时候，程序是处于暂停状态的，这样会非常影响性能。</span><br></pre></td></tr></table></figure><h4 id="spark-GC调优方案"><a href="#spark-GC调优方案" class="headerlink" title="spark GC调优方案"></a>spark GC调优方案</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark中，垃圾回收调优的目标就是，只有真正长时间存活的对象，才能进入老年代，短时间存活的对象，只能呆在年轻代。不能因为某个Survivor区域空间不够，在Minor GC时，就进入了老年代，从而造成短时间存活的对象，长期呆在老年代中占据了空间，这样Full GC时要回收大量的短时间存活的对象，导致Full GC速度缓慢。</span><br><span class="line"></span><br><span class="line">如果发现，在task执行期间，大量full gc发生了，那么说明，年轻代的Eden区域，给的空间不够大。此时可以执行一些操作来优化垃圾回收行为</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1：最直接的就是提高Executor的内存</span><br><span class="line">在spark-submit中通过参数指定executor的内存</span><br><span class="line">--executor-memory 1G </span><br><span class="line"></span><br><span class="line">2：调整Eden与s1和s2的比值【一般情况下不建议调整这块的比值】</span><br><span class="line">-XX:NewRatio&#x3D;4：设置年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代).设置为4,则年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1&#x2F;5</span><br><span class="line">-XX:SurvivorRatio&#x3D;4：设置年轻代中Eden区与Survivor区的大小比值.设置为4,则两个Survivor区与一个Eden区的比值为2:4,一个Survivor区占整个年轻代的1&#x2F;6</span><br><span class="line">具体使用的时候在 spark-submit 脚本中通过 --conf 参数设置即可</span><br><span class="line">--conf &quot;spark.executor.extraJavaOptions&#x3D; -XX:SurvivorRatio&#x3D;4 -XX:NewRatio&#x3D;4&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">其实最直接的就是增加Executor的内存，如果这个内存上不去，其它的修改都是徒劳。</span><br><span class="line">举个例子就是说，一个20岁的成年人和一个3岁的小孩</span><br><span class="line">3岁的小孩掌握再多的格斗技巧都没有用，在绝对的实力面前一切都是花架子。</span><br><span class="line">所以说我们一般很少需要去调整Eden、s1、s2的比值，一般都是直接增加Executor的内存比较靠谱。</span><br></pre></td></tr></table></figure><h3 id="提高并行度"><a href="#提高并行度" class="headerlink" title="提高并行度"></a>提高并行度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实际上Spark集群的资源并不一定会被充分利用到，所以要尽量设置合理的并行度，来充分地利用集群的资源，这样才能提高Spark程序的性能。</span><br><span class="line"></span><br><span class="line">Spark会自动设置以文件作为输入源的RDD的并行度，依据其大小，比如HDFS，就会给每一个block创建一个partition，也依据这个设置并行度。对于reduceByKey等会发生shuffle操作的算子，会使用并行度最大的父RDD的并行度</span><br><span class="line"></span><br><span class="line">可以手动使用textFile()、parallelize()等方法的第二个参数来设置并行度(只针对这一个RDD)；也可以使用spark.default.parallelism参数(全局)，来设置统一的并行度。Spark官方的推荐是，给集群中的每个cpu core设置2~3个task。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">下面来举个例子</span><br><span class="line">我在spark-submit 脚本中给任务设置了5个executor，每个executor，设置了2个cpu core</span><br><span class="line">spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \   &#x2F;&#x2F; 为job设置5个executor</span><br><span class="line">--executor-cores 2 \   &#x2F;&#x2F; 分配两个CPU</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时，如果我在代码中设置了默认并行度为5</span><br><span class="line">conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br><span class="line"></span><br><span class="line">这个参数设置完了以后，也就意味着所有RDD的partition都被设置成了5个，针对RDD的每一个partition，spark会启动一个task来进行计算，所以对于所有的算子操作，都只会创建5个task来处理对应的RDD中的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">但是注意了，我们前面在spark-submit脚本中设置了5个executor，每个executor 2个cpu core，所以这个时候spark其实会向yarn集群申请10个cpu core，但是我们在代码中设置了默认并行度为5，只会产生5个task，一个task使用一个cpu core，那也就意味着有5个cpu core是空闲的，这样申请的资源就浪费了一半。</span><br><span class="line"></span><br><span class="line">其实最好的情况，就是每个cpu core都不闲着，一直在运行，这样可以达到资源的最大使用率，其实让一个cpu core运行一个task都是有点浪费的，官方也建议让每个cpu core运行2~3个task，这样可以充分压榨CPU的性能</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">为什么这样说呢？</span><br><span class="line"></span><br><span class="line">是这样的，因为每个task执行的顺序和执行结束的时间很大概率是不一样的，如果正好有10个cpu，运行10个taks，那么某个task可能很快就执行完了，那么这个CPU就空闲下来了，这样资源就浪费了。</span><br><span class="line">所以说官方推荐，给每个cpu分配2~3个task是比较合理的，可以充分利用CPU资源，发挥它最大的价值。</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">下面我们来实际写个案例看一下效果</span><br><span class="line">Scala代码如下：</span><br><span class="line"></span><br><span class="line">package com.imooc.scala</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：设置并行度</span><br><span class="line"> * 1：可以在textFile或者parallelize等方法的第二个参数中设置并行度</span><br><span class="line"> * 2：或者通过spark.default.parallelism参数统一设置并行度</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object MoreParallelismScala &#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">     val conf &#x3D; new SparkConf()</span><br><span class="line">     conf.setAppName(&quot;MoreParallelismScala&quot;)</span><br><span class="line">     &#x2F;&#x2F;.setMaster(&quot;local&quot;)</span><br><span class="line">    &#x2F;&#x2F;设置全局并行度</span><br><span class="line">     conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br><span class="line">     val sc &#x3D; new SparkContext(conf)</span><br><span class="line">     val dataRDD &#x3D; sc.parallelize(Array(&quot;hello&quot;,&quot;you&quot;,&quot;hello&quot;,&quot;me&quot;,&quot;hehe&quot;,&quot;hello&quot;,&quot;you&quot;,&quot;hello&quot;,&quot;me&quot;,&quot;hehe&quot;))</span><br><span class="line">     dataRDD.map((_,1))</span><br><span class="line">     .reduceByKey(_ + _)</span><br><span class="line">     .foreach(println(_))</span><br><span class="line">     sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：设置并行度</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MoreParallelismJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">     conf.setAppName(<span class="string">"MoreParallelismJava"</span>);</span><br><span class="line">     <span class="comment">//设置全局并行度</span></span><br><span class="line">     conf.set(<span class="string">"spark.default.parallelism"</span>,<span class="string">"5"</span>);</span><br><span class="line">     JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">     JavaRDD&lt;String&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="string">"hello"</span>,<span class="string">"you"</span>,<span class="string">"hello"</span>,<span class="string">"me"</span>,<span class="string">"hehe"</span>,<span class="string">"hello"</span>,<span class="string">"you"</span>,<span class="string">"hello"</span>,<span class="string">"me"</span>,<span class="string">"hehe"</span>));</span><br><span class="line">     dataRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">     return new Tuple2&lt;String, Integer&gt;<span class="params">(word,<span class="number">1</span>)</span></span>;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;).reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     <span class="keyword">return</span> i1 + i2;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;).foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     System.out.println(tup);</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;);</span><br><span class="line">     sc.stop();                                                      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">对代码编译打包</span><br><span class="line">spark-submit脚本内容如下：</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# vi moreParallelismJob.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.MoreParallelismScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280039960.png" alt="image-20230328003937453"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">任务提交到集群运行之后，查看spark的任务界面(job)</span><br><span class="line">先看executors，这里显示了4个executor和1个driver进程，为什么不是5个executor进程呢？</span><br><span class="line"></span><br><span class="line">是因为我们现在使用的是yarn-cluster模式，driver进程运行在集群内部，所以它占了一个executor，如果使用的是yarn-client模式，就会产生5个executor和1个单独的driver进程。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280042154.png" alt="image-20230328004230697"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后去看satges界面，两个Stage都是5个task并行执行，这5个task会使用5个cpu，但是我们给这个任务申请了10个cpu，所以就有5个是空闲的了(这里没考虑driver的占用)。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280057687.png" alt="image-20230328005706629"></p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280054889.png" alt="image-20230328005414569"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当在sparkContext生成对象后，再设置默认并行度会出现问题</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280055569.png" alt="image-20230328005531042"></p><h4 id="提高性能"><a href="#提高性能" class="headerlink" title="提高性能"></a>提高性能</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果想要最大限度利用CPU的性能，至少将spark.default.parallelism的值设置为10，这样可以实现一个cpu运行一个task，其实官方推荐是设置为20或者30。</span><br><span class="line">其实这个参数也可以在spark-submit脚本中动态设置，通过--conf参数设置，这样就比较灵活了。</span><br><span class="line"></span><br><span class="line">注意：此时需要将代码中设置spark.default.parallelism的配置注释掉</span><br><span class="line">&#x2F;&#x2F;conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">为了看起来更清晰，在这我们使用 yarn-client 模式，这样driver就不会占用我们的分配的executor了</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# vi moreParallelismJob2.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.MoreParallelismScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot; \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于修改了代码，所以需要重新编译，打包，执行</span><br><span class="line">执行结束后再来查看spark的任务界面，可以看到此时有10个task并行执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280117859.png" alt="image-20230328011738873"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280118186.png" alt="image-20230328011801559"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280116405.png" alt="image-20230328011600636"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是并行度相关的设置</span><br><span class="line">接下来我们来看一个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280126634.png" alt="image-20230328012605704"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个图中描述的就是刚才我们演示的两种情况下Executor和Task之间的关系</span><br></pre></td></tr></table></figure><h4 id="spark-submit常用参数"><a href="#spark-submit常用参数" class="headerlink" title="spark-submit常用参数"></a>spark-submit常用参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">最后我们来分析总结一下spark-submit脚本中经常配置的一些参数</span><br><span class="line"></span><br><span class="line">--name mySparkJobName：指定任务名称(代码里也可以设置)</span><br><span class="line">--class com.imooc.scala.xxxxx ：指定入口类</span><br><span class="line">--master yarn ：指定集群地址(standalone)，on yarn模式指定yarn</span><br><span class="line">--deploy-mode cluster ：client代表yarn-client，cluster代表yarn-cluster</span><br><span class="line">--executor-memory 1G ：executor进程的内存大小，实际工作中设置2~4G即可</span><br><span class="line">--num-executors 2 ：分配多少个executor进程</span><br><span class="line">--executor-cores 2 : 一个executor进程分配多少个cpu core</span><br><span class="line"></span><br><span class="line">--driver-cores 1 ：(如果使用yarn-cluster模式，这里不用设置也行，直接按executor的配置来)driver进程分配多少cpu core，默认为1即可</span><br><span class="line">--driver-memory 1G：(如果使用yarn-cluster模式，这里不用设置也行，直接按executor的配置来)driver进程的内存，如果需要使用类似于collect之类的action算子向Driver端拉取数据，则这里可以设置大一些</span><br><span class="line">--jars fastjson.jar,abc.jar 在这里可以设置job依赖的第三方jar包(pom里spark-core没提供的)【不建议把第三方依赖打入程序的jar包中，一方面会导致jar变大；另一方面，同一个项目组同事用的第三方依赖版本问题；还有这里可以使用本地路径，或者hdfs路径(建议使用hdfs路径，因为使用本地路径依赖，还是会读取到hdfs上)】</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot;：可以动态指定一些spark任务的参数，指定多个参</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">最后注意一点：针对 --num-executors 和 --executor-cores 的设置</span><br><span class="line">大家看这两种方式设置有什么区别：</span><br><span class="line">第一种方式：</span><br><span class="line">--num-executors 2</span><br><span class="line">--executor-cores 1</span><br><span class="line">第二种方式：</span><br><span class="line">--num-executors 1</span><br><span class="line">--executor-cores 2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这两种设置最终都会向集群申请2个cpu core，可以并行运行两个task，但是这两种设置方式有什么区别呢？</span><br><span class="line"></span><br><span class="line">第一种方法：多executor模式</span><br><span class="line">由于每个executor只分配了一个cpu core，我们将无法利用在同一个JVM中运行多个任务的优点。我们假设这两个executor是在两个节点中启动的，那么针对广播变量这种操作，将在两个节点的中都复制1份，最终会复制两份</span><br><span class="line"></span><br><span class="line">第二种方法：多core模式</span><br><span class="line">此时一个executor中会有2个cpu core，这样可以利用同一个JVM中运行多个任务的优点，并且针对广播变量的这种操作，只会在这个executor对应的节点中复制1份即可。</span><br><span class="line"></span><br><span class="line">那是不是我可以给一个executor分配很多的cpu core，也不是的，因为一个executor的内存大小是固定的，如果在里面运行过多的task可能会导致内存不够用，所以这块一般在工作中我们会给一个executor分配 2~4G 内存，对应的分配 2~4个cpu core。</span><br></pre></td></tr></table></figure><h3 id="数据本地化"><a href="#数据本地化" class="headerlink" title="数据本地化"></a>数据本地化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">数据本地化对于Spark Job性能有着巨大的影响。如果数据以及要计算它的代码是在一起的，那么性能当然会非常高。但是，如果数据和计算它的代码是分开的，那么其中之一必须到另外一方的机器上。通常来说，移动代码到其它节点，会比移动数据到代码所在的节点，速度要得多，因为代码比较小。Spark也正是基于这个数据本地化的原则来构建task调度算法的。</span><br><span class="line"></span><br><span class="line">数据本地化，指的是，数据离计算它的代码有多近。基于数据距离代码的距离，有几种数据本地化级别：</span><br><span class="line"></span><br><span class="line">数据本地化级别 解释</span><br><span class="line">PROCESS_LOCAL 进程本地化，性能最好：数据和计算它的代码在同一个JVM进程中</span><br><span class="line">NODE_LOCAL 节点本地化：数据和计算它的代码在一个节点上，但是不在一个JVM进程</span><br><span class="line">NO_PREF 数据从哪里过来，性能都是一样的，比如从数据库中获取数据，对于task而言在哪个机器上都是一样的</span><br><span class="line">RACK_LOCAL 数据和计算它的代码在一个机架上，数据需要通过网络在节点之间进行传输</span><br><span class="line">ANY 数据可能在任意地方，比如其它网络环境内，或者其它机架上，性能最差</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281058668.png" alt="image-20230328105831703"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Spark倾向使用最好的本地化级别调度task，但这是不现实的</span><br><span class="line">如果目前我们要处理的数据所在的executor上目前没有空闲的CPU，那么Spark就会放低本地化级别。这时有两个选择：</span><br><span class="line">第一，等待，直到executor上的cpu释放出来，那么就分配task过去；</span><br><span class="line">第二，立即在任意一个其它executor上启动一个task。</span><br><span class="line"></span><br><span class="line">Spark默认会等待指定时间，期望task要处理的数据所在的节点上的executor空闲出一个cpu，从而将task分配过去，只要超过了时间，那么Spark就会将task分配到其它任意一个空闲的executor上</span><br><span class="line"></span><br><span class="line">可以设置参数， spark.locality 系列参数，来调节Spark等待task可以进行数据本地化的时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait（3000毫秒）：默认等待3秒(通用的所有级别)</span><br><span class="line">spark.locality.wait.process：等待指定的时间看能否达到数据和计算它的代码在同一个JVM</span><br><span class="line">spark.locality.wait.node：等待指定的时间看能否达到数据和计算它的代码在一个节点上执行</span><br><span class="line">spark.locality.wait.rack：等待指定的时间看能否达到数据和计算它的代码在一个机架上</span><br><span class="line"></span><br><span class="line">看这个图里面的task，此时的数据本地化级别是最优的 PROCESS_LOCAL</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281108354.png" alt="image-20230328110840007"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-checkpoint-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-checkpoint-2.html</id>
    <published>2023-03-27T06:42:14.000Z</published>
    <updated>2023-04-08T13:43:16.383Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-2"><a href="#第十一周-Spark性能优化的道与术-2" class="headerlink" title="第十一周 Spark性能优化的道与术-2"></a>第十一周 Spark性能优化的道与术-2</h1><h2 id="checkpoint概述"><a href="#checkpoint概述" class="headerlink" title="checkpoint概述"></a>checkpoint概述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">checkpoint，是Spark提供的一个比较高级的功能。有时候，我们的Spark任务，比较复杂，从初始化RDD开始，到最后整个任务完成，有比较多的步骤，比如超过10个transformation算子。而且，整个任务运行的时间也特别长，比如通常要运行1~2个小时。在这种情况下，就比较适合使用checkpoint功能了。</span><br><span class="line"></span><br><span class="line">因为对于特别复杂的Spark任务，有很高的风险会出现某个要反复使用的RDD因为节点的故障导致丢失，虽然之前持久化过，但是还是导致数据丢失了。那么也就是说，出现失败的时候，没有容错机制，所以当后面的transformation算子，又要使用到该RDD时，就会发现数据丢失了，此时如果没有进行容错处理的话，那么就需要再重新计算一次数据了。</span><br><span class="line">所以针对这种Spark Job，如果我们担心某些关键的，在后面会反复使用的RDD，因为节点故障导致数据丢失，那么可以针对该RDD启动checkpoint机制，实现容错和高可用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那如何使用checkPoint呢？</span><br><span class="line">首先要调用SparkContext的setCheckpointDir()方法，设置一个容错的文件系统的目录，比如HDFS；然后，对RDD调用checkpoint()方法。</span><br><span class="line">最后，在RDD所在的job运行结束之后，会启动一个单独的job，将checkpoint设置过的RDD的数据写入之前设置的文件系统中。</span><br><span class="line"></span><br><span class="line">这是checkpoint使用的基本步骤，很简单，那我们下面先从理论层面分析一下当我们设置好checkpoint之后，Spark底层都做了哪些事情</span><br></pre></td></tr></table></figure><h2 id="RDD之checkpoint流程"><a href="#RDD之checkpoint流程" class="headerlink" title="RDD之checkpoint流程"></a>RDD之checkpoint流程</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271450477.png" alt="image-20230327144700422"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1：SparkContext设置checkpoint目录，用于存放checkpoint的数据；</span><br><span class="line">对RDD调用checkpoint方法，然后它就会被RDDCheckpointData对象进行管理，此时这个RDD的checkpoint状态会被设置为Initialized</span><br><span class="line">2：待RDD所在的job运行结束，会调用job中最后一个RDD的doCheckpoint方法，该方法沿着RDD的血缘关系向上查找被checkpoint()方法标记过的RDD， 并将其checkpoint状态从Initialized设置为CheckpointingInProgress</span><br><span class="line">3：启动一个单独的job，来将血缘关系中标记为CheckpointInProgress的RDD执行checkpoint操作，也就是将其数据写入checkpoint目录</span><br><span class="line">4：将RDD数据写入checkpoint目录之后，会将RDD状态改变Checkpointed；</span><br><span class="line">并且还会改变RDD的血缘关系，即会清除掉RDD所有依赖的RDD；最后还会设置其父RDD为新创建的CheckpointRDD</span><br></pre></td></tr></table></figure><h2 id="checkpoint与持久化的区别"><a href="#checkpoint与持久化的区别" class="headerlink" title="checkpoint与持久化的区别"></a>checkpoint与持久化的区别</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">那这里所说的checkpoint和我们之前讲的RDD持久化有什么区别吗？</span><br><span class="line">lineage是否发生改变linage（血缘关系）说的就是RDD之间的依赖关系</span><br><span class="line"></span><br><span class="line">持久化，只是将数据保存在内存中或者本地磁盘文件中，RDD的lineage(血缘关系)是不变的；</span><br><span class="line"></span><br><span class="line">Checkpoint执行之后，RDD就没有依赖的RDD了，也就是它的lineage改变了</span><br><span class="line">丢失数据的可能性持久化的数据丢失的可能性较大，如果采用persist把数据存在内存中的话，虽然速度最快但是也是最不可靠的，就算放在磁盘上也不是完全可靠的，因为磁盘也会损坏。Checkpoint的数据通常是保存在高可用文件系统中(HDFS),丢失的可能性很低</span><br><span class="line"></span><br><span class="line">建议：对需要checkpoint的RDD，先执行persist(StorageLevel.DISK_ONLY)</span><br><span class="line">为什么呢？</span><br><span class="line"></span><br><span class="line">因为默认情况下，如果某个RDD没有持久化，但是设置了checkpoint，那么这个时候，本来Spark任务已经执行结束了，但是由于中间的RDD没有持久化，在进行checkpoint的时候想要将这个RDD的数据写入外部存储系统的话，就需要重新计算这个RDD的数据，再将其checkpoint到外部存储系统中。</span><br><span class="line">如果对需要checkpoint的rdd进行了基于磁盘的持久化，那么后面进行checkpoint操作时，就会直接从磁盘上读取rdd的数据了，就不需要重新再计算一次了，这样效率就高了。</span><br><span class="line"></span><br><span class="line">那在这能不能使用基于内存的持久化呢？当然是可以的，不过没那个必要。</span><br></pre></td></tr></table></figure><h2 id="checkPoint的使用"><a href="#checkPoint的使用" class="headerlink" title="checkPoint的使用"></a>checkPoint的使用</h2><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们来演示一下：将一个RDD的数据持久化到HDFS上面</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：checkpoint的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CheckPointOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"CheckPointOpScala"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="keyword">if</span>(args.length==<span class="number">0</span>)&#123;</span><br><span class="line">         <span class="type">System</span>.exit(<span class="number">100</span>)</span><br><span class="line">     &#125;</span><br><span class="line">         <span class="keyword">val</span> outputPath = args(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//1：设置checkpint目录</span></span><br><span class="line">         sc.setCheckpointDir(<span class="string">"hdfs://bigdata01:9000/chk001"</span>)</span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_10000000.dat"</span>)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//2：对rdd执行checkpoint操作</span></span><br><span class="line">         dataRDD.checkpoint()</span><br><span class="line">         dataRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">         .map((_,<span class="number">1</span>))</span><br><span class="line">         .reduceByKey(_ + _)</span><br><span class="line">         .saveAsTextFile(outputPath)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.StorageLevel;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：checkpoint的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckPointOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"CheckPointOpJava"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         <span class="keyword">if</span>(args.length==<span class="number">0</span>)&#123;</span><br><span class="line">         System.exit(<span class="number">100</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         String outputPath = args[<span class="number">0</span>];</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//1：设置checkpint目录</span></span><br><span class="line">         sc.setCheckpointDir(<span class="string">"hdfs://bigdata01:9000/chk002"</span>);</span><br><span class="line">         JavaRDD&lt;String&gt; dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_100000.dat"</span>);</span><br><span class="line">         <span class="comment">//2: 对rdd执行checkpoint操作</span></span><br><span class="line">         dataRDD.checkpoint();</span><br><span class="line">         dataRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">          <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new Tuple2&lt;String, Integer&gt;<span class="params">(word,<span class="number">1</span>)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> i1 + i2;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).saveAsTextFile(outputPath);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="提交到集群执行"><a href="#提交到集群执行" class="headerlink" title="提交到集群执行"></a>提交到集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下面我们把这个任务打包提交到集群上运行一下，看一下效果。</span><br><span class="line">代码master部分注释掉</span><br><span class="line"></span><br><span class="line">先确保hadoop集群是正常运行的，以及hadoop中的historyserver进程和spark的historyserver进程也是正常运行的。</span><br><span class="line">测试数据之前已经上传到了hdfs上面，如果没有则需要上传</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271517969.png" alt="image-20230327151632675"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将pom.xml中的spark-core的依赖设置为provided，然后编译打包</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line"> &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">将打包的jar包上传到bigdata04的&#x2F;data&#x2F;soft&#x2F;sparkjars目录，创建一个新的spark-submit脚本</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271519124.png" alt="image-20230327151903625"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行成功之后可以到 setCheckpointDir 指定的目录中查看一下，可以看到目录中会生成对应的文件保存rdd中的数据，只不过生成的文件不是普通文本文件，直接查看文件中的内容显示为乱码。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271523654.png" alt="image-20230327152317696"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271523426.png" alt="image-20230327152338046"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271525661.png" alt="image-20230327152509680"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来进到YARN的8088界面查看</span><br><span class="line">点击Tracking UI进入spark的ui界面看第一个界面jobs</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271526703.png" alt="image-20230327152656935"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在这可以看出来产生了2个job，</span><br><span class="line">第一个job是我们正常的任务执行，执行了39s，一共产生了28个task任务</span><br><span class="line">第二个job是checkpoint启动的job，执行了35s，一共产生了14个task任务</span><br><span class="line"></span><br><span class="line">看第二个界面Stages，这里面的3个Stage是前面2个job产生的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">具体想知道哪些Stage属于哪个job任务的话，可以在任务界面，点击Description中的链接就可以看到job对应的Stage</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271528015.png" alt="image-20230327152845211"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个job其实就是我们实现的单词计数的功能，这个任务产生了两个stage，这两个stage具体是如何划分的呢？</span><br><span class="line">咱们前面讲过，stage的划分是由宽依赖决定的，在这个任务中reduceByKey这个过程会产生宽依赖，所以会产生2个Stage</span><br><span class="line">这里面显示的有这两个stage的一些基本信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271531710.png" alt="image-20230327153130946"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stage id：stage的编号，从0开始</span><br><span class="line">Duration：stage执行消耗的时间</span><br><span class="line">Tasks：Successed&#x2F;Total：task执行成功数量&#x2F;task总量</span><br><span class="line">Input：输入数据量</span><br><span class="line">ouput：输出数据量</span><br><span class="line">shuffle read&#x2F;shuffle read：shuffle过程传输数据量</span><br><span class="line">点击这个界面中的DAG Visualization可以看到当前这个任务stage的划分情况，可以看到每个Stage包含哪些算子</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271534605.png" alt="image-20230327153403130"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">进到Stage内部看一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271541155.png" alt="image-20230327154126374"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271542549.png" alt="image-20230327154248904"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面可以看到每个task的具体执行情况，执行状态，执行消耗的时间，GC消耗的时间，处理的数据量和数据条数、通过shuffle输出的数据量和数据条数</span><br><span class="line">其实从这里也可以看出来文件的每一个block块会产生一个task</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271546781.png" alt="image-20230327154559437"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是这个Stage执行的基本信息了。</span><br><span class="line">加下来看一下第二个Job，这个job是checkpoint启动的任务，查看它的stage的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271546799.png" alt="image-20230327154637511"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个job只会产生一个stage，因为我们只针对textFile的结果设置了checkpoint</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271547533.png" alt="image-20230327154722955"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个stage执行消耗了35s，说明这份数据是重新通过textFile读取过来的。</span><br><span class="line">针对Storage这块，显示的其实就是持久化的数据，如果对RDD做了持久化，那么在任务执行过程中能看到，任务执行结束就看不到了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271548520.png" alt="image-20230327154816099"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来验证一下在开启持久化的情况下执行checkpoint操作时的区别</span><br><span class="line">在代码中针对RDD开启持久化</span><br><span class="line">1：对比此时产生的两个job总的消耗的时间，以及job中的Stage消耗的时间</span><br><span class="line">其实你会发现开启持久化之后，checkpoint的那个job消耗的时间就变少了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271553264.png" alt="image-20230327155305626"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2：查看DAG Visualization，你会发现stage里面也会有有一些不一样的地方</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271556557.png" alt="image-20230327155658220"></p><h2 id="checkpoint源码分析"><a href="#checkpoint源码分析" class="headerlink" title="checkpoint源码分析"></a>checkpoint源码分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们通过理论层面分析了checkpoint的原理，以及演示了checkpoint的使用</span><br><span class="line">下面我们通过源码层面来对我们前面分析的理论进行验证</span><br><span class="line">先下载spark源码，下载流程和下载spark安装包的流程一样</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271953796.png" alt="image-20230327195356368"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">把下载的安装包解压到idea项目目录中</span><br><span class="line"></span><br><span class="line">打开spark-2.4.3源码目录，进入core目录，这个是spark的核心代码，我们要查看的checkpoint的源码就在这个项目中</span><br><span class="line">在idea中打开core这个子项目</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271954921.png" alt="image-20230327195430272"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面我们就来分析一下RDD的checkpoint功能：</span><br><span class="line">checkpoint功能可以分为两块</span><br><span class="line">1：checkpoint的写操作</span><br><span class="line">将指定RDD的数据通过checkpoint存储到指定外部存储中</span><br><span class="line">2：checkpoint的读操作</span><br><span class="line">任务中RDD数据在使用过程中丢失了，正好这个RDD之前做过checkpoint，所以这时就需要通过checkpoint来恢复数据</span><br></pre></td></tr></table></figure><h3 id="checkpoint的写操作"><a href="#checkpoint的写操作" class="headerlink" title="checkpoint的写操作"></a>checkpoint的写操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.1 ：当我们在自己开发的spark任务中先调用 sc.setCheckpointDir时，底层其实就会调用</span><br><span class="line">SparkContext中的setCheckpointDir方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def setCheckpointDir(directory: String) &#123;</span><br><span class="line">     &#x2F;&#x2F; If we are running on a cluster, log a warning if the directory is local.</span><br><span class="line">     &#x2F;&#x2F; Otherwise, the driver may attempt to reconstruct the checkpointed RDD fr</span><br><span class="line">     &#x2F;&#x2F; its own local file system, which is incorrect because the checkpoint fil</span><br><span class="line">     &#x2F;&#x2F; are actually on the executor machines.</span><br><span class="line">     if (!isLocal &amp;&amp; Utils.nonLocalPaths(directory).isEmpty) &#123;</span><br><span class="line">         logWarning(&quot;Spark is not running in local mode, therefore the checkpoint </span><br><span class="line">         s&quot;must not be on the local filesystem. Directory &#39;$directory&#39; &quot; +</span><br><span class="line">         &quot;appears to be on the local filesystem.&quot;)</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;根据我们传过来的目录，后面再拼上一个子目录，子目录使用一个UUID随机字符串</span><br><span class="line">     &#x2F;&#x2F;使用HDFS的javaAPI 在HDFS上创建目录</span><br><span class="line">     checkpointDir &#x3D; Option(directory).map &#123; dir &#x3D;&gt;</span><br><span class="line">     val path &#x3D; new Path(dir, UUID.randomUUID().toString)</span><br><span class="line">     val fs &#x3D; path.getFileSystem(hadoopConfiguration)</span><br><span class="line">     fs.mkdirs(path)</span><br><span class="line">     fs.getFileStatus(path).getPath.toString</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1.2：接着我们会调用RDD.checkpoint方法，此时会执行RDD这个class中的 checkpoint方法</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;这里相当于是checkpoint的一个标记，并没有真正执行checkpoint</span><br><span class="line">def checkpoint(): Unit &#x3D; RDDCheckpointData.synchronized &#123;</span><br><span class="line">     &#x2F;&#x2F; NOTE: we use a global lock here due to complexities downstream with ensu</span><br><span class="line">     &#x2F;&#x2F; children RDD partitions point to the correct parent partitions. In the f</span><br><span class="line">     &#x2F;&#x2F; we should revisit this consideration.</span><br><span class="line">     &#x2F;&#x2F;如果SparkContext没有设置checkpointDir，则抛出异常</span><br><span class="line">     if (context.checkpointDir.isEmpty) &#123;</span><br><span class="line">     throw new SparkException(&quot;Checkpoint directory has not been set in the Sp</span><br><span class="line">     &#125; else if (checkpointData.isEmpty) &#123;</span><br><span class="line">     &#x2F;&#x2F;如果设置了，则创建RDDCheckpointData的子类，这个子类主要负责管理RDD的checkpoi</span><br><span class="line">     &#x2F;&#x2F;并且会初始化checkpoint状态为Initialized</span><br><span class="line">     checkpointData &#x3D; Some(new ReliableRDDCheckpointData(this))</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这个checkpoint方法执行完成之后，这个流程就结束了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.3：剩下的就是在这个设置了checkpint的RDD所在的job执行结束之后，Spark会调用job中最后一个RDD的doCheckpoint方法</span><br><span class="line">这个逻辑是在SparkContext这个class的runJob方法中，当执行到Spark中的action算子时，这个runJob方法会被触发，开始执行任务。</span><br><span class="line">这个runJob的最后一行会调用rdd中的 doCheckpoint 方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;在有action动作时，会触发sparkcontext对runJob的调用</span><br><span class="line">def runJob[T, U: ClassTag](</span><br><span class="line">     rdd: RDD[T],</span><br><span class="line">     func: (TaskContext, Iterator[T]) &#x3D;&gt; U,</span><br><span class="line">     partitions: Seq[Int],</span><br><span class="line">     resultHandler: (Int, U) &#x3D;&gt; Unit): Unit &#x3D; &#123;</span><br><span class="line">     if (stopped.get()) &#123;</span><br><span class="line">     throw new IllegalStageException(&quot;SparkContext has been shutdown&quot;)</span><br><span class="line">     &#125;</span><br><span class="line">     val callSite &#x3D; getCallSite</span><br><span class="line">     val cleanedFunc &#x3D; clean(func)</span><br><span class="line">     logInfo(&quot;Starting job: &quot; + callSite.shortForm)</span><br><span class="line">     if (conf.getBoolean(&quot;spark.logLineage&quot;, false)) &#123;</span><br><span class="line">     logInfo(&quot;RDD&#39;s recursive dependencies:\n&quot; + rdd.toDebugString)</span><br><span class="line">     &#125;</span><br><span class="line">     dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, </span><br><span class="line">     progressBar.foreach(_.finishAll())</span><br><span class="line">     &#x2F;&#x2F;在这里会执行doCheckpoint()</span><br><span class="line">     rdd.doCheckpoint()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.4：接着会进入到RDD中的 doCheckpoint 方法</span><br><span class="line">这里面最终会调用 RDDCheckpointData 的 checkpoint 方法</span><br><span class="line">checkpointData.get.checkpoint()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def doCheckpoint(): Unit &#x3D; &#123;</span><br><span class="line">     RDDOperationScope.withScope(sc, &quot;checkpoint&quot;, allowNesting &#x3D; false, ignoreP</span><br><span class="line">     &#x2F;&#x2F;该rdd是否已经调用doCheckpoint，如果还没有，则开始处理</span><br><span class="line">     if (!doCheckpointCalled) &#123;</span><br><span class="line">     doCheckpointCalled &#x3D; true</span><br><span class="line">     &#x2F;&#x2F;若已经被checkpoint()标记过，则checkpointData.isDefined为true</span><br><span class="line">     if (checkpointData.isDefined) &#123;</span><br><span class="line">     &#x2F;&#x2F;查看是否需要把该rdd的所有依赖全部checkpoint</span><br><span class="line">     &#x2F;&#x2F;checkpointAllMarkedAncestors取自配置&quot;spark.checkpoint.checkpointAllM</span><br><span class="line">     &#x2F;&#x2F;默认不配时值为false</span><br><span class="line">     if (checkpointAllMarkedAncestors) &#123;</span><br><span class="line">     &#x2F;&#x2F; TODO We can collect all the RDDs that needs to be checkpointed, </span><br><span class="line">     &#x2F;&#x2F; them in parallel.</span><br><span class="line">     &#x2F;&#x2F; Checkpoint parents first because our lineage will be truncated af</span><br><span class="line">     &#x2F;&#x2F; checkpoint ourselves</span><br><span class="line">     &#x2F;&#x2F; 血缘上的每一个父rdd递归调用该方法</span><br><span class="line">     dependencies.foreach(_.rdd.doCheckpoint())</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;调用RDDCheckpointData的checkpoint方法</span><br><span class="line">     checkpointData.get.checkpoint()</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">     &#x2F;&#x2F;沿着rdd的血缘关系向上查找被checkpoint()标记过的RDD</span><br><span class="line">     dependencies.foreach(_.rdd.doCheckpoint())</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1.5：接下来进入到 RDDCheckpointData 的 checkpoint 方法中</span><br><span class="line">这里面会调用子类 ReliableCheckpointRDD 中的 doCheckpoint()方法</span><br><span class="line"></span><br><span class="line">final def checkpoint(): Unit &#x3D; &#123;</span><br><span class="line">     &#x2F;&#x2F; Guard against multiple threads checkpointing the same RDD by</span><br><span class="line">     &#x2F;&#x2F; atomically flipping the Stage of this RDDCheckpointData</span><br><span class="line">     &#x2F;&#x2F;&#x2F;&#x2F;将checkpoint的状态从Initialized置为CheckpointingInProgress</span><br><span class="line">     RDDCheckpointData.synchronized &#123;</span><br><span class="line">     if (cpStage &#x3D;&#x3D; Initialized) &#123;</span><br><span class="line">     cpStage &#x3D; CheckpointingInProgress</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">     return</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;调用子类的doCheckpoint，默认会使用ReliableCheckpointRDD子类，创建一个新的Chec</span><br><span class="line">     val newRDD &#x3D; doCheckpoint()</span><br><span class="line">     &#x2F;&#x2F; Update our Stage and truncate the RDD lineage</span><br><span class="line">     &#x2F;&#x2F;将checkpoint状态置为Checkpointed状态，并且改变rdd之前的依赖，设置父rdd为新创建</span><br><span class="line">     RDDCheckpointData.synchronized &#123;</span><br><span class="line">     cpRDD &#x3D; Some(newRDD)</span><br><span class="line">     cpStage &#x3D; Checkpointed</span><br><span class="line">     rdd.markCheckpointed()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.6：接着来进入 ReliableCheckpointRDD 中的 doCheckpoint() 方法</span><br><span class="line">这里面会调用 ReliableCheckpointRDD 中的 writeRDDToCheckpointDirectory 方法将rdd的数据写入HDFS</span><br><span class="line">中的 checkpoint 目录，并且返回创建的 CheckpointRDD</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">protected override def doCheckpoint(): CheckpointRDD[T] &#x3D; &#123;</span><br><span class="line">     &#x2F;&#x2F;将rdd的数据写入HDFS中的checkpoint目录，并且创建的CheckpointRDD</span><br><span class="line">     val newRDD &#x3D; ReliableCheckpointRDD.writeRDDToCheckpointDirectory(rdd, cpDir</span><br><span class="line">     &#x2F;&#x2F; Optionally clean our checkpoint files if the reference is out of scope</span><br><span class="line">     if (rdd.conf.getBoolean(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, </span><br><span class="line">     rdd.context.cleaner.foreach &#123; cleaner &#x3D;&gt;</span><br><span class="line">     cleaner.registerRDDCheckpointDataForCleanup(newRDD, rdd.id)</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line">logInfo(s&quot;Done checkpointing RDD $&#123;rdd.id&#125; to $cpDir, new parent is RDD $&#123;n</span><br><span class="line"> newRDD</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.7：接下来进入 ReliableCheckpointRDD 的 writeRDDToCheckpointDirectory 方法</span><br><span class="line">这里面最终会启动一个job，将checkpoint的数据写入到指定的HDFS目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;将rdd的数据写入HDFS中checkpoint目录，并且创建CheckpointRDD</span><br><span class="line">def writeRDDToCheckpointDirectory[T: ClassTag](</span><br><span class="line"> originalRDD: RDD[T],</span><br><span class="line"> checkpointDir: String,</span><br><span class="line"> blockSize: Int &#x3D; -1): ReliableCheckpointRDD[T] &#x3D; &#123;</span><br><span class="line"> val checkpointStartTimeNs &#x3D; System.nanoTime()</span><br><span class="line"> val sc &#x3D; originalRDD.sparkContext</span><br><span class="line"> &#x2F;&#x2F;Create the output path for the checkpoint</span><br><span class="line"> &#x2F;&#x2F;创建checkpoint输出目录</span><br><span class="line"> val checkpointDirPath &#x3D; new Path(checkpointDir)</span><br><span class="line"> &#x2F;&#x2F;获取HDFS文件系统API接口</span><br><span class="line"> val fs &#x3D; checkpointDirPath.getFileSystem(sc.hadoopConfiguration)</span><br><span class="line"> &#x2F;&#x2F;创建目录</span><br><span class="line"> if (!fs.mkdirs(checkpointDirPath)) &#123;</span><br><span class="line"> throw new SparkException(s&quot;Failed to create checkpoint path $checkpointDi</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Save to file, and reload it as an RDD</span><br><span class="line"> &#x2F;&#x2F;将Hadoop配置文件信息广播到所有节点</span><br><span class="line"> val broadcastedConf &#x3D; sc.broadcast(</span><br><span class="line"> new SerializableConfiguration(sc.hadoopConfiguration))</span><br><span class="line"> &#x2F;&#x2F; TODO: This is expensive because it computes the RDD again unnecessarily </span><br><span class="line"> &#x2F;&#x2F;这里强调了checkpoint是一个昂贵的操作，主要是说它昂贵在需要沿着血缘关系重新计算该</span><br><span class="line"> &#x2F;&#x2F;重新启动一个job,将rdd的分区数据写入HDFS</span><br><span class="line"> sc.runJob(originalRDD,</span><br><span class="line"> writePartitionToCheckpointFile[T](checkpointDirPath.toString, broadcasted</span><br><span class="line"> &#x2F;&#x2F;如果rdd的partitioner不为空，则将partitioner写入checkpoint目录</span><br><span class="line"> if (originalRDD.partitioner.nonEmpty) &#123;</span><br><span class="line"> writePartitionerToCheckpointDir(sc, originalRDD.partitioner.get, checkpoi</span><br><span class="line"> &#125;</span><br><span class="line"> val checkpointDurationMs &#x3D;</span><br><span class="line"> TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - checkpointStartTimeNs)</span><br><span class="line"> logInfo(s&quot;Checkpointing took $checkpointDurationMs ms.&quot;)</span><br><span class="line"> &#x2F;&#x2F;创建一个CheckpointRDD,该RDD的分区数目和原始的rdd的分区数是一样的</span><br><span class="line"> val newRDD &#x3D; new ReliableCheckpointRDD[T](</span><br><span class="line"> sc, checkpointDirPath.toString, originalRDD.partitioner)</span><br><span class="line"> if (newRDD.partitions.length !&#x3D; originalRDD.partitions.length) &#123;</span><br><span class="line"> throw new SparkException(</span><br><span class="line"> &quot;Checkpoint RDD has a different number of partitions from original RDD. </span><br><span class="line"> s&quot;RDD [ID: $&#123;originalRDD.id&#125;, num of partitions: $&#123;originalRDD.partit</span><br><span class="line"> s&quot;Checkpoint RDD [ID: $&#123;newRDD.id&#125;, num of partitions: &quot; +</span><br><span class="line"> s&quot;$&#123;newRDD.partitions.length&#125;].&quot;)</span><br><span class="line"> &#125;</span><br><span class="line"> newRDD</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行到这，其实调用过checkpoint方法的RDD就被保存到HDFS上了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：在这里通过checkpoint操作将RDD中的数据写入到HDFS中的时候，会调用RDD中的</span><br><span class="line">iterator方法，遍历RDD中所有分区的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们来分析一下这块的代码</span><br><span class="line">此时我们没有对RDD进行持久化，所以走else中的代码</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">进入 computeOrReadCheckpoint(split, context) 中</span><br><span class="line">此时这个RDD是将要进行checkpoint，还没有完成checkpoint，所以走 else ，会执行 compute 方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskCont</span><br><span class="line">&#123;</span><br><span class="line"> &#x2F;&#x2F;当前rdd是否已经checkpoint并且物化，如果已经checkpoint并且物化</span><br><span class="line"> &#x2F;&#x2F;则调用firstParent的iterator方法获取</span><br><span class="line"> if (isCheckpointedAndMaterialized) &#123;</span><br><span class="line"> &#x2F;&#x2F;注意：针对checpoint操作，它的血缘关系已经被切断了，那么它的firstParent就是前面</span><br><span class="line"> firstParent[T].iterator(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;如果没有，则表示持久化数据丢失，或者根本就没有持久化，</span><br><span class="line"> &#x2F;&#x2F;需要调用rdd的compute方法开始重新计算，返回一个Iterator对象</span><br><span class="line"> compute(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在这会执行RDD的子类 HadoopRDD 中的 compute 方法</span><br><span class="line">在这里会通过 RecordReader 获取RDD中指定分区的数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">override def compute(theSplit: Partition, context: TaskContext): Interruptibl</span><br><span class="line"> val iter &#x3D; new NextIterator[(K, V)] &#123;</span><br><span class="line"> private val split &#x3D; theSplit.asInstanceOf[HadoopPartition]</span><br><span class="line"> logInfo(&quot;Input split: &quot; + split.inputSplit)</span><br><span class="line"> private val jobConf &#x3D; getJobConf()</span><br><span class="line"> private val inputMetrics &#x3D; context.taskMetrics().inputMetrics</span><br><span class="line"> private val existingBytesRead &#x3D; inputMetrics.bytesRead</span><br><span class="line"> &#x2F;&#x2F; Sets InputFileBlockHolder for the file block&#39;s information</span><br><span class="line"> split.inputSplit.value match &#123;</span><br><span class="line"> case fs: FileSplit &#x3D;&gt;</span><br><span class="line"> InputFileBlockHolder.set(fs.getPath.toString, fs.getStart, fs.getLengt</span><br><span class="line"> case _ &#x3D;&gt;</span><br><span class="line"> InputFileBlockHolder.unset()</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Find a function that will return the FileSystem bytes read by this thr</span><br><span class="line"> &#x2F;&#x2F; creating RecordReader, because RecordReader&#39;s constructor might read s</span><br><span class="line"> private val getBytesReadCallback: Option[() &#x3D;&gt; Long] &#x3D; split.inputSplit.v</span><br><span class="line"> case _: FileSplit | _: CombineFileSplit &#x3D;&gt;</span><br><span class="line"> Some(SparkHadoopUtil.get.getFSBytesReadOnThreadCallback())</span><br><span class="line"> case _ &#x3D;&gt; None</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; We get our input bytes from thread-local Hadoop FileSystem statistics.</span><br><span class="line"> &#x2F;&#x2F; If we do a coalesce, however, we are likely to compute multiple partit</span><br><span class="line"> &#x2F;&#x2F; task and in the same thread, in which case we need to avoid override v</span><br><span class="line"> &#x2F;&#x2F; previous partitions (SPARK-13071).</span><br><span class="line"> private def updateBytesRead(): Unit &#x3D; &#123;</span><br><span class="line"> getBytesReadCallback.foreach &#123; getBytesRead &#x3D;&gt;</span><br><span class="line"> inputMetrics.setBytesRead(existingBytesRead + getBytesRead())</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> private var reader: RecordReader[K, V] &#x3D; null</span><br><span class="line"> private val inputFormat &#x3D; getInputFormat(jobConf)</span><br><span class="line"> HadoopRDD.addLocalConfiguration(</span><br><span class="line"> new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;, Locale.US).format(createTime),</span><br><span class="line"> context.stageId, theSplit.index, context.attemptNumber, jobConf)</span><br><span class="line"> reader &#x3D;</span><br><span class="line"> try &#123;</span><br><span class="line"> inputFormat.getRecordReader(split.inputSplit.value, jobConf, Reporter</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: FileNotFoundException if ignoreMissingFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped missing file: $&#123;split.inputSplit&#125;&quot;, e)</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> null</span><br><span class="line"> &#x2F;&#x2F; Throw FileNotFoundException even if &#96;ignoreCorruptFiles&#96; is true</span><br><span class="line"> case e: FileNotFoundException if !ignoreMissingFiles &#x3D;&gt; throw e</span><br><span class="line"> case e: IOException if ignoreCorruptFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped the rest content in the corrupted file: $&#123;split</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> null</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Register an on-task-completion callback to close the input stream.</span><br><span class="line"> context.addTaskCompletionListener[Unit] &#123; context &#x3D;&gt;</span><br><span class="line"> &#x2F;&#x2F; Update the bytes read before closing is to make sure lingering bytes</span><br><span class="line"> &#x2F;&#x2F; this thread get correctly added.</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> closeIfNeeded()</span><br><span class="line"> &#125;</span><br><span class="line"> private val key: K &#x3D; if (reader &#x3D;&#x3D; null) null.asInstanceOf[K] else reader</span><br><span class="line"> private val value: V &#x3D; if (reader &#x3D;&#x3D; null) null.asInstanceOf[V] else read</span><br><span class="line"> override def getNext(): (K, V) &#x3D; &#123;</span><br><span class="line"> try &#123;</span><br><span class="line"> finished &#x3D; !reader.next(key, value)</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: FileNotFoundException if ignoreMissingFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped missing file: $&#123;split.inputSplit&#125;&quot;, e)</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> &#x2F;&#x2F; Throw FileNotFoundException even if &#96;ignoreCorruptFiles&#96; is true</span><br><span class="line"> case e: FileNotFoundException if !ignoreMissingFiles &#x3D;&gt; throw e</span><br><span class="line"> case e: IOException if ignoreCorruptFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped the rest content in the corrupted file: $&#123;split</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> &#125;</span><br><span class="line"> if (!finished) &#123;</span><br><span class="line"> inputMetrics.incRecordsRead(1)</span><br><span class="line"> &#125;</span><br><span class="line"> if (inputMetrics.recordsRead % SparkHadoopUtil.UPDATE_INPUT_METRICS_INT</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> &#125;</span><br><span class="line"> (key, value)</span><br><span class="line"> &#125;</span><br><span class="line"> override def close(): Unit &#x3D; &#123;</span><br><span class="line"> if (reader !&#x3D; null) &#123;</span><br><span class="line"> InputFileBlockHolder.unset()</span><br><span class="line"> try &#123;</span><br><span class="line"> reader.close()</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: Exception &#x3D;&gt;</span><br><span class="line"> if (!ShutdownHookManager.inShutdown()) &#123;</span><br><span class="line"> logWarning(&quot;Exception in RecordReader.close()&quot;, e)</span><br><span class="line"> &#125;</span><br><span class="line"> &#125; finally &#123;</span><br><span class="line"> reader &#x3D; null</span><br><span class="line"> &#125;</span><br><span class="line"> if (getBytesReadCallback.isDefined) &#123;</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> &#125; else if (split.inputSplit.value.isInstanceOf[FileSplit] ||</span><br><span class="line"> split.inputSplit.value.isInstanceOf[CombineFileSplit]) &#123;</span><br><span class="line"> &#x2F;&#x2F; If we can&#39;t get the bytes read from the FS stats, fall back to t</span><br><span class="line"> &#x2F;&#x2F; which may be inaccurate.</span><br><span class="line"> try &#123;</span><br><span class="line"> inputMetrics.incBytesRead(split.inputSplit.value.getLength)</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: java.io.IOException &#x3D;&gt;</span><br><span class="line"> logWarning(&quot;Unable to get input size to set InputMetrics for ta</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> new InterruptibleIterator[(K, V)](context, iter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这样经过几次迭代之后就可以获取到RDD中所有分区的数据了，因为这个compute是一次获取一个分区</span><br><span class="line">的数据。获取到之后checkpoint就可以把这个RDD的数据存储到HDFS上了。</span><br><span class="line">这就是checkpoint的写操作</span><br></pre></td></tr></table></figure><h3 id="checkpoint的读操作"><a href="#checkpoint的读操作" class="headerlink" title="checkpoint的读操作"></a>checkpoint的读操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来分析一下checkpoint读数据这个操作</span><br><span class="line">当RDD中的数据丢失了以后，需要通过checkpoint读取存储在hdfs上的数据，</span><br><span class="line">2.1：这个时候还是会执行RDD中的iterator方法</span><br><span class="line">由于我们没有做持久化，只做了checkpoint，所以还是会走 else</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">进入 computeOrReadCheckpoint 方法</span><br><span class="line">此时rdd已经 checkpoint 并且物化，所以 if 分支满足</span><br><span class="line">执行 firstParent[T].iterator(split, context) 这行代码</span><br><span class="line"></span><br><span class="line">这行代码的意思是会找到当前这个RDD的父RDD，其实这个RDD执行过checkpoint之后，血缘关系已经</span><br><span class="line">被切断了，它的父RDD就是我们前面创建的那个 ReliableCheckpointRDD</span><br><span class="line">这个 ReliableCheckpointRDD 中没有覆盖 iterator 方法，所以在调用 iterator 的时候还是执行RDD这个</span><br><span class="line">父类中的 iterator ，重新进来之后再判断，这个 ReliableCheckpointRDD 再执行if判断的时候就不满足</span><br><span class="line">了，因为它的 checkpoint 属性不满足，所以会走 else ，执行 compute</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskCont</span><br><span class="line">&#123;</span><br><span class="line"> &#x2F;&#x2F;当前rdd是否已经checkpoint并且物化，如果已经checkpoint并且物化</span><br><span class="line"> &#x2F;&#x2F;则调用firstParent的iterator方法获取</span><br><span class="line"> if (isCheckpointedAndMaterialized) &#123;</span><br><span class="line"> &#x2F;&#x2F;注意：针对checpoint操作，它的血缘关系已经被切断了，那么它的firstParent就是前面</span><br><span class="line"> firstParent[T].iterator(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;如果没有，则表示持久化数据丢失，或者根本就没有持久化，</span><br><span class="line"> &#x2F;&#x2F;需要调用rdd的compute方法开始重新计算，返回一个Iterator对象</span><br><span class="line"> compute(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时会执行 ReliableCheckpointRDD 这个子类中的 compute 方法</span><br><span class="line">这里面就会找到之前checkpoint的文件，从HDFS上恢复RDD中的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">override def compute(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;获取checkpoint文件</span><br><span class="line"> val file &#x3D; new Path(checkpointPath, ReliableCheckpointRDD.checkpointFileNam</span><br><span class="line"> &#x2F;&#x2F;从HDFS上的checkpoint文件中读取checkpoint的数据</span><br><span class="line"> ReliableCheckpointRDD.readCheckpointFile(file, broadcastedConf, context)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这是从checkpoint中读取数据的流程</span><br><span class="line">咱们前面说过，建议对需要做checkpoint的数据先进行持久化，如果我们设置了持久化，针对</span><br><span class="line">checkpoint的写操作，在执行iterator方法的时候会是什么现象呢？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时在最后将RDD中的数据通过checkpoint存储到HDFS上的时候，会调用RDD的iterator方法，不过此</span><br><span class="line">时 storageLevel 就不为 null 了，因为我们对这个RDD做了基于磁盘的持久化，所以会走 if 分支，执行</span><br><span class="line">getOrCompute</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">进入 getOrCompute 方法</span><br><span class="line">由于这个RDD的数据已经做了持久化，所以在这就可以从 blockmanager 中读取数据了，就不需要重新从</span><br><span class="line">源头计算或者拉取数据了，所以会提高 checkpoint 的效率</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def getOrCompute(partition: Partition, context: TaskContext): </span><br><span class="line"> val blockId &#x3D; RDDBlockId(id, partition.index)</span><br><span class="line"> var readCachedBlock &#x3D; true</span><br><span class="line"> &#x2F;&#x2F; This method is called on executors, so we need call SparkEnv.get instead </span><br><span class="line"> SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementCla</span><br><span class="line"> readCachedBlock &#x3D; false</span><br><span class="line"> computeOrReadCheckpoint(partition, context)</span><br><span class="line"> &#125;) match &#123;</span><br><span class="line"> case Left(blockResult) &#x3D;&gt;</span><br><span class="line"> if (readCachedBlock) &#123;</span><br><span class="line"> val existingMetrics &#x3D; context.taskMetrics().inputMetrics</span><br><span class="line"> existingMetrics.incBytesRead(blockResult.bytes)</span><br><span class="line"> new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[It</span><br><span class="line"> override def next(): T &#x3D; &#123;</span><br><span class="line"> existingMetrics.incRecordsRead(1)</span><br><span class="line"> delegate.next()</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iter</span><br><span class="line"> &#125;</span><br><span class="line"> case Right(iter) &#x3D;&gt;</span><br><span class="line"> new InterruptibleIterator(context, iter.asInstanceOf[Iterator[T]])</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
</feed>
