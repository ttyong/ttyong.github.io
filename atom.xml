<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2022-02-06T04:29:38.779Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发工程师-第三周</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%89%E5%91%A8%20Hadoop%E4%B9%8BHDFS%E7%9A%84%E4%BD%BF%E7%94%A8.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%89%E5%91%A8%20Hadoop%E4%B9%8BHDFS%E7%9A%84%E4%BD%BF%E7%94%A8.html</id>
    <published>2022-02-05T02:57:04.000Z</published>
    <updated>2022-02-06T04:29:38.779Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="大数据开发工程师-第三周"><a href="#大数据开发工程师-第三周" class="headerlink" title="大数据开发工程师-第三周"></a>大数据开发工程师-第三周</h1><h2 id="第1章-HDFS介绍"><a href="#第1章-HDFS介绍" class="headerlink" title="第1章 HDFS介绍"></a>第1章 HDFS介绍</h2><p><code>假设让我们来设计一个分布式的文件系统，我们该如何设计呢？</code></p><p><a href="https://imgtu.com/i/Hm1DTe" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/05/Hm1DTe.png" alt="Hm1DTe.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这种设计架构会存在一个问题，假设同时过来很多人都需要租房子，那么一个二房东是忙不过来的，就会造成阻塞。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/Hm3i11" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/05/Hm3i11.png" alt="Hm3i11.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">现在这种设计是，我们去找一个中介公司，这里的主节点就可以理解为一个中介公司</span><br><span class="line">这里的从节点就可以理解为是房源，中介公司会在每块房源都安排一个工作人员，当我们找房子的时候，先联系中介公司，中介公司会告诉我们哪里有房子，并且把对应工作人员的信息告诉我们，我们就可以直接去找对应的工作人员去租房子。这样对于中介公司而言，就没什么压力了。</span><br><span class="line">中介公司只负责管理房源和工作人员信息，具体干活的是工作人员。</span><br><span class="line">这样就算同时过来很多人，中介公司也是可以扛得住的，因为具体看房租房的流程是我们直接和工作人员联系的，不会造成阻塞。</span><br></pre></td></tr></table></figure><h3 id="HDFS设计思想"><a href="#HDFS设计思想" class="headerlink" title="HDFS设计思想"></a>HDFS设计思想</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  用户请求查看数据时候会请求主节点，主节点上面会维护所有数据的存储信息，</span><br><span class="line">主节点会把对应数据所在的节点信息返回给用户，</span><br><span class="line">  然后用户根据数据所在的节点信息去对应的节点去读取数据，这样压力就不会全部在主节点上面。</span><br></pre></td></tr></table></figure><h3 id="HDFS-Hadoop-Distributed-File-System"><a href="#HDFS-Hadoop-Distributed-File-System" class="headerlink" title="HDFS(Hadoop Distributed File System)"></a>HDFS(Hadoop Distributed File System)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Hadoop的 分布式文件系统</span><br><span class="line">  它是一种允许文件通过网络在多台主机上分享的文件系统，可以让多台机器上的多个用户分享文件和存储空间</span><br><span class="line">其实分布式文件管理系统有很多，HDFS只是其中一种实现而已</span><br><span class="line">  HDFS是一种适合大文件存储的分布式文件系统，不适合小文件存储，什么叫小文件，例如，几KB，几M的文件都可以认为是小文件</span><br></pre></td></tr></table></figure><h3 id="HDFS的Shell介绍"><a href="#HDFS的Shell介绍" class="headerlink" title="HDFS的Shell介绍"></a>HDFS的Shell介绍</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  针对HDFS，我们可以在shell命令行下进行操作，就类似于我们操作linux中的文件系统一样，但是具体命令的操作格式是有一些区别的</span><br><span class="line">格式如下：</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/Hm8WdS" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/05/Hm8WdS.png" alt="Hm8WdS.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  使用hadoop bin目录的hdfs命令，后面指定dfs，表示是操作分布式文件系统的，这些属于固定格式。</span><br><span class="line">  如果在PATH中配置了hadoop的bin目录，那么这里可以直接使用hdfs就可以了</span><br><span class="line">  这里的xxx是一个占位符，具体我们想对hdfs做什么操作，就可以在这里指定对应的命令了</span><br><span class="line">大多数hdfs 的命令和对应的Linux命令类似</span><br><span class="line"></span><br><span class="line">  HDFS的schema是hdfs，authority是集群中namenode所在节点的ip和对应的端口号，把ip换成主机名也是一样的，path是我们要操作的文件路径信息</span><br><span class="line">  其实后面这一长串内容就是core-site.xml配置文件中fs.defaultFS属性的值，这个代表的是HDFS的地址。</span><br></pre></td></tr></table></figure><h2 id="第2章-HDFS基础操作"><a href="#第2章-HDFS基础操作" class="headerlink" title="第2章 HDFS基础操作"></a>第2章 HDFS基础操作</h2><h3 id="HDFS的常见Shell操作"><a href="#HDFS的常见Shell操作" class="headerlink" title="HDFS的常见Shell操作"></a>HDFS的常见Shell操作</h3><h4 id="管理命令"><a href="#管理命令" class="headerlink" title="管理命令"></a>管理命令</h4><a href="/hadoop%E5%B8%B8%E7%94%A8HDFS%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4.html" title="第三章 shell命令操作HDFS-常用HDFS管理命令">第三章 shell命令操作HDFS-常用HDFS管理命令</a><h4 id="操作命令"><a href="#操作命令" class="headerlink" title="操作命令"></a>操作命令</h4><a href="/hadoop%E5%B8%B8%E7%94%A8HDFS%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4.html" title="第三章 shell命令操作HDFS-常用HDFS操作命令">第三章 shell命令操作HDFS-常用HDFS操作命令</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1.-ls [-R]：查询指定路径信息</span><br><span class="line"></span><br><span class="line">首先看第一个ls命令</span><br><span class="line">查看hdfs根目录下的内容，什么都不显示，因为默认情况下hdfs中什么都没有</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;</span><br><span class="line">其实后面hdfs的url这一串内容在使用时默认是可以省略的，因为hdfs在执行的时候会根据HDOOP_HOME自动识别配置文件中的fs.defaultFS属性</span><br><span class="line">所以这样简写也是可以的</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F;</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]#</span><br><span class="line"></span><br><span class="line">想要递归显示所有目录的信息，可以在ls后面添加-R参数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2.-put：从本地上传文件</span><br><span class="line">直接上传到hdfs的根目录即可</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -put README.txt  &#x2F;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3.-cat：查看HDFS文件内容</span><br><span class="line"></span><br><span class="line">4.-get：下载文件到本地</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -get &#x2F;README.txt .</span><br><span class="line">注意：这样执行报错了，提示文件已存在，我这条命令的意思是要把HDFS中的README.txt下载当前目录中，但是当前目录中已经有这个文件了，要么换到其它目录，要么给文件重命名</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -get &#x2F;README.txt README.txt.bak</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5.-mkdir [-p]：创建文件夹</span><br><span class="line">如果要递归创建多级目录，还需要再指定-p参数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">6.-rm [-r]：删除文件&#x2F;文件夹</span><br><span class="line">删除目录，注意，删除目录需要指定-r参数</span><br><span class="line">如果是多级目录，可以递归删除吗？可以</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -rm -r &#x2F;abc</span><br><span class="line">Deleted &#x2F;abc</span><br></pre></td></tr></table></figure><h3 id="HDFS案例实操"><a href="#HDFS案例实操" class="headerlink" title="HDFS案例实操"></a>HDFS案例实操</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">需求：统计HDFS中文件的个数和每个文件的大小</span><br><span class="line">1：统计根目录下文件的个数</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F; |grep &#x2F;| wc -l    </span><br><span class="line">注：wc -l 是打印换行符</span><br><span class="line">2：统计根目录下每个文件的大小，最终把文件名称和大小打印出来</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F; |grep &#x2F; |  awk &#39;&#123;print $8,$5&#125;&#39;</span><br><span class="line">&#x2F;LICENSE.txt 150569</span><br><span class="line">&#x2F;NOTICE.txt 22125</span><br><span class="line">&#x2F;README.txt 1361</span><br><span class="line"></span><br><span class="line">[root@bigdata01 my_shell]# hdfs dfs -ls &#x2F; | grep ^- | awk &#39;&#123;print $8,$5&#125;&#39;</span><br><span class="line">&#x2F;for1.sh 48</span><br><span class="line">&#x2F;hello.txt 59</span><br><span class="line">&#x2F;if1.sh 108</span><br></pre></td></tr></table></figure><h2 id="第3章-java操作HDFS"><a href="#第3章-java操作HDFS" class="headerlink" title="第3章 java操作HDFS"></a>第3章 java操作HDFS</h2><h3 id="Java代码操作HDFS"><a href="#Java代码操作HDFS" class="headerlink" title="Java代码操作HDFS"></a>Java代码操作HDFS</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了在shell命令行下操作hdfs，shell中操作hdfs是比较常见的操作，但是在工作中也会遇到一些需求是需要通过代码操作hdfs的，下面我们就来看一下如何使用java代码操作hdfs</span><br><span class="line">在具体操作之前需要先明确一下开发环境，代码编辑器使用idea，当然了eclipse也可以</span><br><span class="line">在创建项目的时候我们会创建maven项目，使用maven来管理依赖，是比较方便的。</span><br></pre></td></tr></table></figure><h3 id="HDFS体系结构"><a href="#HDFS体系结构" class="headerlink" title="HDFS体系结构"></a>HDFS体系结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  前面我们掌握了HDFS的基本使用，下面我们来详细分析一下HDFS深层次的内容</span><br><span class="line">HDFS支持主从结构，主节点称为 NameNode ，是因为主节点上运行的NameNode进程，NameNode支持多个，目前我们的集群中只配置了一个</span><br><span class="line">  从节点称为 DataNode ，是因为从节点上面运行的有DataNode进程，DataNode支持多个，目前我们的集群中有两个</span><br><span class="line">  HDFS中还包含一个 SecondaryNameNode 进程，这个进程从字面意思上看像是第二个NameNode的意思，其实不是，一会我们会详细分析。</span><br></pre></td></tr></table></figure><h4 id="NameNode介绍"><a href="#NameNode介绍" class="headerlink" title="NameNode介绍"></a>NameNode介绍</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  首先是NameNode，NameNode是整个文件系统的管理节点</span><br><span class="line">它主要维护着整个文件系统的文件目录树，文件&#x2F;目录的信息 和 每个文件对应的数据块列表，并且还负责接收用户的操作请求</span><br><span class="line"></span><br><span class="line">  目录树：表示目录之间的层级关系，就是我们在hdfs上执行ls命令可以看到的那个目录结构信息。</span><br><span class="line">  文件&#x2F;目录的信息：表示文件&#x2F;目录的的一些基本信息，所有者 属组 修改时间 文件大小等信息</span><br><span class="line"> 每个文件对应的数据块列表：如果一个文件太大，那么在集群中存储的时候会对文件进行切割，这个时候就类似于会给文件分成一块一块的，存储到不同机器上面。所以HDFS还要记录一下一个文件到底被分了多少块，每一块都在什么地方存储着</span><br></pre></td></tr></table></figure><h4 id="SecondaryNameNode介绍"><a href="#SecondaryNameNode介绍" class="headerlink" title="SecondaryNameNode介绍"></a>SecondaryNameNode介绍</h4><h4 id="DataNode介绍"><a href="#DataNode介绍" class="headerlink" title="DataNode介绍"></a>DataNode介绍</h4><h4 id="NameNode总结"><a href="#NameNode总结" class="headerlink" title="NameNode总结"></a>NameNode总结</h4><h3 id="HDFS的回收站"><a href="#HDFS的回收站" class="headerlink" title="HDFS的回收站"></a>HDFS的回收站</h3><h3 id="HDFS的安全模式"><a href="#HDFS的安全模式" class="headerlink" title="HDFS的安全模式"></a>HDFS的安全模式</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第一周 第5章 Linux总结与走进大数据</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%80%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Linux%E6%80%BB%E7%BB%93%E4%B8%8E%E8%B5%B0%E8%BF%9B%E5%A4%A7%E6%95%B0%E6%8D%AE.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%80%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Linux%E6%80%BB%E7%BB%93%E4%B8%8E%E8%B5%B0%E8%BF%9B%E5%A4%A7%E6%95%B0%E6%8D%AE.html</id>
    <published>2022-01-30T05:46:58.000Z</published>
    <updated>2022-01-30T06:26:42.370Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第一周-第5章-走进大数据"><a href="#第一周-第5章-走进大数据" class="headerlink" title="第一周 第5章 走进大数据"></a>第一周 第5章 走进大数据</h1><h2 id="什么是大数据？"><a href="#什么是大数据？" class="headerlink" title="什么是大数据？"></a>什么是大数据？</h2><h3 id="百度地图实时路况"><a href="#百度地图实时路况" class="headerlink" title="百度地图实时路况"></a>百度地图实时路况</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">百度地图软件实时上传每个人的位置信息，根据这些大量数据进行路况分析</span><br><span class="line">精准路况信息要求：大量数据，相当快的计算速度</span><br></pre></td></tr></table></figure><h3 id="今日头条为你推荐"><a href="#今日头条为你推荐" class="headerlink" title="今日头条为你推荐"></a>今日头条为你推荐</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户画像-&gt;用户划分-&gt;根据同类型用户喜好相互推荐</span><br></pre></td></tr></table></figure><h3 id="买披萨的故事"><a href="#买披萨的故事" class="headerlink" title="买披萨的故事"></a>买披萨的故事</h3><h2 id="大数据的产生背景"><a href="#大数据的产生背景" class="headerlink" title="大数据的产生背景"></a>大数据的产生背景</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">信息技术的进步</span><br><span class="line">云计算技术的兴起：可以将分散的数据集中在数据中心，使处理和分析海量数据成为可能；云计算技术为海量数据存储和访问提供了必要的空间和途径</span><br><span class="line">数据资源化的趋势</span><br></pre></td></tr></table></figure><h2 id="大数据的4v特征"><a href="#大数据的4v特征" class="headerlink" title="大数据的4v特征"></a>大数据的4v特征</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">volume(量大)：存储量大，计算  量大   #包括：采集，存储和计算的数据量大</span><br><span class="line">variety(多样):来源多，格式多   #数据的种类和来源多样化 结构化，半结构化，非结构化数据</span><br><span class="line">velocity(快速):数据增长速度快，处理速度要求快</span><br><span class="line">value:价值密度低，和数据总量的大小成反比</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H9XGwD" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/30/H9XGwD.png" alt="H9XGwD.png"></a></p><h2 id="大数据的行业应用"><a href="#大数据的行业应用" class="headerlink" title="大数据的行业应用"></a>大数据的行业应用</h2><p><a href="https://imgtu.com/i/H9XgYj" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/30/H9XgYj.png" alt="H9XgYj.png"></a></p><h3 id="天猫双十一实时动态大屏"><a href="#天猫双十一实时动态大屏" class="headerlink" title="天猫双十一实时动态大屏"></a>天猫双十一实时动态大屏</h3><h3 id="公共出行与运营车辆调度"><a href="#公共出行与运营车辆调度" class="headerlink" title="公共出行与运营车辆调度"></a>公共出行与运营车辆调度</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
    
  </entry>
  
  <entry>
    <title>学习 从零开始学大数据和大数据开发工程师 linux上创建的文件</title>
    <link href="http://tianyong.fun/%E5%AD%A6%E4%B9%A0-%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-linux%E4%B8%8A%E5%88%9B%E5%BB%BA%E7%9A%84%E6%96%87%E4%BB%B6.html"/>
    <id>http://tianyong.fun/%E5%AD%A6%E4%B9%A0-%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-linux%E4%B8%8A%E5%88%9B%E5%BB%BA%E7%9A%84%E6%96%87%E4%BB%B6.html</id>
    <published>2022-01-29T10:19:59.000Z</published>
    <updated>2022-01-29T10:23:03.906Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="linux上创建的文件或文件夹"><a href="#linux上创建的文件或文件夹" class="headerlink" title="linux上创建的文件或文件夹"></a>linux上创建的文件或文件夹</h1><h2 id="大数据开发工程师"><a href="#大数据开发工程师" class="headerlink" title="大数据开发工程师"></a>大数据开发工程师</h2><h3 id="home-ttyong-my-shell"><a href="#home-ttyong-my-shell" class="headerlink" title="/home/ttyong/my_shell"></a>/home/ttyong/my_shell</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">该文件夹包含自己创建的shell脚本</span><br></pre></td></tr></table></figure><h3 id="data-soft"><a href="#data-soft" class="headerlink" title="/data/soft"></a>/data/soft</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">安装的软件</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="从零开始学大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
    
  </entry>
  
  <entry>
    <title>linux相关文件信息</title>
    <link href="http://tianyong.fun/linux%E7%9B%B8%E5%85%B3%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF.html"/>
    <id>http://tianyong.fun/linux%E7%9B%B8%E5%85%B3%E6%96%87%E4%BB%B6%E4%BF%A1%E6%81%AF.html</id>
    <published>2022-01-29T10:01:32.000Z</published>
    <updated>2022-02-05T04:30:35.736Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="linux相关文件信息"><a href="#linux相关文件信息" class="headerlink" title="linux相关文件信息"></a>linux相关文件信息</h1><h2 id="etc-crontab"><a href="#etc-crontab" class="headerlink" title="/etc/crontab"></a>/etc/crontab</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crontab定时器的配置文件</span><br></pre></td></tr></table></figure><h2 id="var-log-cron"><a href="#var-log-cron" class="headerlink" title="/var/log/cron"></a>/var/log/cron</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于使用crontab执行的脚本且脚本无输出信息时(因为有输出信息，可以使用重定向标准信息或标准错误信息到某文件)，可以通过这个文件查看crontab是否正常执行</span><br></pre></td></tr></table></figure><h2 id="etc-profile"><a href="#etc-profile" class="headerlink" title="/etc/profile"></a>/etc/profile</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">配置信息及环境变量</span><br><span class="line"></span><br><span class="line">刷新 source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><h2 id="etc-sudoers"><a href="#etc-sudoers" class="headerlink" title="/etc/sudoers"></a>/etc/sudoers</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo权限相关，给非root用户赋权配置过这个文件</span><br></pre></td></tr></table></figure><h2 id="etc-sysconfig-network-scripts-ifcfg-ens33"><a href="#etc-sysconfig-network-scripts-ifcfg-ens33" class="headerlink" title="/etc/sysconfig/network-scripts/ifcfg-ens33"></a>/etc/sysconfig/network-scripts/ifcfg-ens33</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">网络配置相关文件</span><br></pre></td></tr></table></figure><h2 id="etc-hostname"><a href="#etc-hostname" class="headerlink" title="/etc/hostname"></a>/etc/hostname</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">配置永久主机名需要这个文件</span><br></pre></td></tr></table></figure><h2 id="etc-hosts"><a href="#etc-hosts" class="headerlink" title="/etc/hosts"></a>/etc/hosts</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip和主机名的映射</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第一周 第5章 Linux总结与走进大数据</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%BA%8C%E5%91%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%B5%B7%E6%BA%90%E4%B9%8B%E5%88%9D%E8%AF%86Hadoop.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%BA%8C%E5%91%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%B5%B7%E6%BA%90%E4%B9%8B%E5%88%9D%E8%AF%86Hadoop.html</id>
    <published>2022-01-29T09:05:28.000Z</published>
    <updated>2022-02-06T04:28:21.649Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第二周-大数据起源之初识Hadoop"><a href="#第二周-大数据起源之初识Hadoop" class="headerlink" title="第二周 大数据起源之初识Hadoop"></a>第二周 大数据起源之初识Hadoop</h1><h2 id="第一章初识Hadoop"><a href="#第一章初识Hadoop" class="headerlink" title="第一章初识Hadoop"></a>第一章初识Hadoop</h2><h3 id="什么是Hadoop"><a href="#什么是Hadoop" class="headerlink" title="什么是Hadoop"></a>什么是Hadoop</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hadoop是一个适合海量数据的分布式存储和分布式计算的框架。</span><br></pre></td></tr></table></figure><h3 id="Hadoop发行版介绍"><a href="#Hadoop发行版介绍" class="headerlink" title="Hadoop发行版介绍"></a>Hadoop发行版介绍</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在这里我们挑几个重点的分析一下：</span><br><span class="line">  首先是官方原生版本：Apache Hadoop</span><br><span class="line">  那接着往下面看 Cloudera Hadoop(CDH)</span><br><span class="line">  还有一个比较常用的是HortonWorks(HDP)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在这里我们会学习原生的Hadoop，只要掌握了原生Hadoop使用，后期想要操作其它发行版的Hadoop也是很简单的，其它发行版都是会兼容原生Hadoop的，这一点大家不同担心。 原生Hadoop的缺点是没有技术支持，遇到问题需要自己解决，或者通过官网的社区提问，但是回复一般比较慢，也不保证能解决问题， 还有一点就是原生Hadoop搭建集群的时候比较麻烦，需要修改很多配置文件，如果集群机器过多的话，针对运维人员的压力是比较大的，这块等后面我们自己在搭建集群的时候大家就可以感受到了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最终的建议：建议在实际工作中搭建大数据平台时选择 CDH或者HDP，方便运维管理，要不然，管理上千台机器的原生Hadoop集群，运维同学是会哭的。</span><br></pre></td></tr></table></figure><h3 id="Hadoop版本演变历史"><a href="#Hadoop版本演变历史" class="headerlink" title="Hadoop版本演变历史"></a>Hadoop版本演变历史</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop1.x：HDFS+MapReduce</span><br><span class="line">hadoop2.x：HDFS+YARN+MapReduce</span><br><span class="line">hadoop3.x：HDFS+YARN+MapReduce</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H9TvdA" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/30/H9TvdA.png" alt="H9TvdA.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在Hadoop1.x中，分布式计算和资源管理都是MapReduce负责的，从Hadoop2.x开始把资源管理单独拆分出来了，拆分出来的好处就是，YARN变成了一个公共的资源管理平台，在它上面不仅仅可以跑MapReduce程序，还可以跑很多其他的程序，只要你的程序满足YARN的规则即可</span><br><span class="line"></span><br><span class="line">Hadoop的这一步棋走的是最好的，这样自己摇身一变就变成了一个公共的平台，由于它起步早，占有的市场份额也多，后期其它新兴起的计算框架一般都会支持在YARN上面运行，这样Hadoop就保证了自己的地位。</span><br><span class="line">咱们后面要学的Spark、Flink等计算框架都是支持在YARN上面执行的，并且在实际工作中也都是在YARN上面执行。</span><br></pre></td></tr></table></figure><h3 id="Hadoop3-x的细节优化"><a href="#Hadoop3-x的细节优化" class="headerlink" title="Hadoop3.x的细节优化"></a>Hadoop3.x的细节优化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">在这里我挑几个常见点说一下：</span><br><span class="line"></span><br><span class="line">1：最低Java版本要求从Java7变为Java8</span><br><span class="line"></span><br><span class="line">2：在Hadoop 3中，HDFS支持纠删码，纠删码是一种比副本存储更节省存储空间的数据持久化存储方法，使用这种方法，相同容错的情况下可以比之前节省一半的存储空间</span><br><span class="line">详细介绍在这里： https:&#x2F;&#x2F;hadoop.apache.org&#x2F;docs&#x2F;r3.0.0&#x2F;hadoop-project-dist&#x2F;hadoop-hdfs&#x2F;HDFSErasureCoding.html</span><br><span class="line"></span><br><span class="line">3： Hadoop 2中的HDFS最多支持两个NameNode，一主一备，而Hadoop 3中的HDFS支持多个NameNode，一主多备</span><br><span class="line">详细介绍在这里： https:&#x2F;&#x2F;hadoop.apache.org&#x2F;docs&#x2F;r3.0.0&#x2F;hadoop-project-dist&#x2F;hadoop-hdfs&#x2F;HDFSErasureCoding.html</span><br><span class="line"></span><br><span class="line">4：MapReduce任务级本地优化，MapReduce添加了映射输出收集器的本地化实现的支持。对于密集型的洗牌操作（shuffle-intensive）jobs，可以带来30%的性能提升，</span><br><span class="line">详细介绍在这里： https:&#x2F;&#x2F;issues.apache.org&#x2F;jira&#x2F;browse&#x2F;MAPREDUCE-2841</span><br><span class="line"></span><br><span class="line">5：修改了多重服务的默认端口，Hadoop2中一些服务的端口和Hadoop3中是不一样的</span><br><span class="line">总结： Hadoop 3和2之间的主要区别在于新版本提供了更好的优化和可用性</span><br></pre></td></tr></table></figure><h3 id="Hadoop三大核心组件介绍"><a href="#Hadoop三大核心组件介绍" class="headerlink" title="Hadoop三大核心组件介绍"></a>Hadoop三大核心组件介绍</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HDFS负责海量数据的分布式存储</span><br><span class="line">MapReduce是一个计算模型，负责海量数据的分布式计算</span><br><span class="line">YARN主要负责集群资源的管理和调度</span><br></pre></td></tr></table></figure><h2 id="第二章Hadoop的两种安装方式"><a href="#第二章Hadoop的两种安装方式" class="headerlink" title="第二章Hadoop的两种安装方式"></a>第二章Hadoop的两种安装方式</h2><h3 id="伪分布集群安装"><a href="#伪分布集群安装" class="headerlink" title="伪分布集群安装"></a>伪分布集群安装</h3><p><a href="https://imgtu.com/i/HCnzRI" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/30/HCnzRI.png" alt="HCnzRI.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">伪分布集群部署:仅需要一台虚拟机</span><br><span class="line"> 这张图代表是一台Linux机器，也可以称为是一个节点，上面安装的有JDK环境</span><br><span class="line">最上面的是Hadoop集群会启动的进程，其中NameNode、SecondaryNameNode、DataNode是HDFS服务的进程，ResourceManager、NodeManager是YARN服务的进程，MapRedcue在这里没有进程，因为它是一个计算框架，等Hadoop集群安装好了以后MapReduce程序可以在上面执行。</span><br></pre></td></tr></table></figure><h4 id="配置基础环境"><a href="#配置基础环境" class="headerlink" title="配置基础环境"></a>配置基础环境</h4><h5 id="设置静态ip"><a href="#设置静态ip" class="headerlink" title="设置静态ip"></a>设置静态ip</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33</span><br></pre></td></tr></table></figure><h5 id="设置临时和永久hostname"><a href="#设置临时和永久hostname" class="headerlink" title="设置临时和永久hostname"></a>设置临时和永久hostname</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hostname xxx</span><br><span class="line">&#x2F;etc&#x2F;hostname</span><br></pre></td></tr></table></figure><h5 id="关闭firewalld"><a href="#关闭firewalld" class="headerlink" title="关闭firewalld"></a>关闭firewalld</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld 临时关闭</span><br><span class="line">systemctl disable firewalld  永久关闭</span><br></pre></td></tr></table></figure><h5 id="ssh免密码登录"><a href="#ssh免密码登录" class="headerlink" title="ssh免密码登录"></a>ssh免密码登录</h5><p><code>我们下面要讲的hadoop集群就会使用到ssh，我们在启动集群的时候只需要在一台机器上启动就行，然后hadoop会通过ssh连到其它机器，把其它机器上面对应的程序也启动起来。但是现在有一个问题，就是我们使用ssh连接其它机器的时候会发现需要输入密码，所以现在需要实现ssh免密码登录。</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  那有同学可能有疑问了，你这里说的多台机器需要配置免密码登录，但是我们现在是伪分布集群啊，只有一台机器</span><br><span class="line">注意了，不管是几台机器的集群，启动集群中程序的步骤都是一样的，都是通过ssh远程连接去操作，就算是一台机器，它也会使用ssh自己连自己，我们现在使用ssh自己连自己也是需要密码的。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HCMKVU" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/30/HCMKVU.png" alt="HCMKVU.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  下面详细讲一下ssh免密码登录 ssh这种安全&#x2F;加密的shell，使用的是非对称加密，加密有两种，对称加密和非对称加密。非对称加密的解密过程是不可逆的，所以这种加密方式比较安全。</span><br><span class="line">  非对称加密会产生秘钥，秘钥分为公钥和私钥，在这里公钥是对外公开的，私钥是自己持有的。</span><br><span class="line">  那么ssh通信的这个过程是，第一台机器会把自己的公钥给到第二台机器，</span><br><span class="line">当第一台机器要给第二台机器通信的时候，第一台机器会给第二台机器发送一个随机的字符串，第二台机器会使用公钥对这个字符串加密，同时第一台机器会使用自己的私钥也对这个字符串进行加密，然后也传给第二台机器这个时候，第二台机器就有了两份加密的内容，一份是自己使用公钥加密的，一份是第一台机器使用私钥加密传过来的，公钥和私钥是通过一定的算法计算出来的，这个时候，第二台机器就会对比这两份加密之后的内容是否匹配。如果匹配，第二台机器就会认为第一台机器是可信的，就允许登录。如果不相等 就认为是非法的机器。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">下面就开始正式配置一下ssh免密码登录，由于我们这里要配置自己免密码登录自己，所以第一台机器和第二台机器都是同一台</span><br><span class="line"></span><br><span class="line">首先在bigdata01上执行 ssh-keygen -t rsa</span><br><span class="line">rsa表示的是一种加密算法</span><br><span class="line">注意：执行这个命令以后，需要连续按 4 次回车键回到 linux 命令行才表示这个操作执行 结束，在按回车的时候不需要输入任何内容。</span><br><span class="line"></span><br><span class="line">执行以后会在~&#x2F;.ssh目录下生产对应的公钥和秘钥文件</span><br><span class="line">[root@bigdata01 ~]# ll ~&#x2F;.ssh&#x2F;</span><br><span class="line">total 12</span><br><span class="line">-rw-------. 1 root root 1679 Apr  7 16:39 id_rsa</span><br><span class="line">-rw-r--r--. 1 root root  396 Apr  7 16:39 id_rsa.pub</span><br><span class="line">-rw-r--r--. 1 root root  203 Apr  7 16:21 known_hosts</span><br><span class="line"></span><br><span class="line">下一步是把公钥拷贝到需要免密码登录的机器上面</span><br><span class="line">[root@bigdata01 ~]# cat ~&#x2F;.ssh&#x2F;id_rsa.pub &gt;&gt; ~&#x2F;.ssh&#x2F;authorized_keys</span><br><span class="line"></span><br><span class="line">然后就可以通过ssh 免密码登录到bigdata01机器了</span><br><span class="line">[root@bigdata01 ~]# ssh bigdata01</span><br><span class="line">Last login: Tue Apr  7 15:05:55 2020 from 192.168.182.1</span><br><span class="line">[root@bigdata01 ~]#</span><br></pre></td></tr></table></figure><h5 id="JDK安装"><a href="#JDK安装" class="headerlink" title="JDK安装"></a>JDK安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h5 id="hadoop安装"><a href="#hadoop安装" class="headerlink" title="hadoop安装"></a>hadoop安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1.将文件解压到创建的&#x2F;data&#x2F;soft&#x2F;下</span><br><span class="line">  hadoop目录下面有两个重要的目录，一个是bin目录，一个是sbin目录：</span><br><span class="line">bin目录，这里面有hdfs，yarn等脚本，这些脚本后期主要是为了操作hadoop集群中的hdfs和yarn组件的</span><br><span class="line">sbin目录，这里面有很多start stop开头的脚本，这些脚本是负责启动 或者停止集群中的组件的。</span><br><span class="line">2.配置环境变量</span><br><span class="line">   因为我们会用到bin目录和sbin目录下面的一些脚本，为了方便使用，我们需要配置一下环境变量。</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">.......</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br><span class="line">export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin:$HADOOP_HOME&#x2F;bin:$PATH</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">3：修改Hadoop相关配置文件</span><br><span class="line">主要修改下面这几个文件：</span><br><span class="line">hadoop-env.sh</span><br><span class="line">core-site.xml</span><br><span class="line">hdfs-site.xml</span><br><span class="line">mapred-site.xml</span><br><span class="line">yarn-site.xml </span><br><span class="line">workers</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">首先修改 hadoop-env.sh 文件，增加环境变量信息，添加到hadoop-env.sh 文件末尾即可。</span><br><span class="line">JAVA_HOME：指定java的安装位置</span><br><span class="line">HADOOP_LOG_DIR：hadoop的日志的存放目录</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi hadoop-env.sh</span><br><span class="line">.......</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HADOOP_LOG_DIR&#x3D;&#x2F;data&#x2F;hadoop_repo&#x2F;logs&#x2F;hadoop</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">修改 core-site.xml 文件</span><br><span class="line">注意 fs.defaultFS 属性中的主机名需要和你配置的主机名保持一致</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:&#x2F;&#x2F;bigdata01:9000&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;data&#x2F;hadoop_repo&lt;&#x2F;value&gt;</span><br><span class="line">   &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">修改hdfs-site.xml文件，把hdfs中文件副本的数量设置为1，因为现在伪分布集群只有一个节点</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">修改mapred-site.xml，设置mapreduce使用的资源调度框架</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi mapred-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">修改yarn-site.xml，设置yarn上支持运行的服务和环境变量白名单</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;&#x2F;name&gt;</span><br><span class="line">   &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改workers，设置集群中从节点的主机名信息，在这里就一台集群，所以就填写bigdata01即可</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi workers</span><br><span class="line">bigdata01</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">配置文件到这就修改好了，但是还不能直接启动，因为Hadoop中的HDFS是一个分布式的文件系统，文件系统在使用之前是需要先格式化的，就类似我们买一块新的磁盘，在安装系统之前需要先格式化才可以使用。</span><br><span class="line"></span><br><span class="line">4：格式化HDFS</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# cd &#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# bin&#x2F;hdfs namenode -format</span><br><span class="line"></span><br><span class="line">注意：格式化操作只能执行一次，如果格式化的时候失败了，可以修改配置文件后再执行格式化，如果格式化成功了就不能再重复执行了，否则集群就会出现问题。</span><br><span class="line">如果确实需要重复执行，那么需要把&#x2F;data&#x2F;hadoop_repo目录中的内容全部删除，再执行格式化</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5：启动伪分布集群</span><br><span class="line"></span><br><span class="line">使用sbin目录下的start-all.sh脚本</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# sbin&#x2F;start-all.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">执行的时候发现有很多ERROR信息，提示缺少HDFS和YARN的一些用户信息。</span><br><span class="line"></span><br><span class="line">解决方案如下：</span><br><span class="line">修改sbin目录下的start-dfs.sh，stop-dfs.sh这两个脚本文件，在文件前面增加如下内容</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# cd sbin&#x2F;</span><br><span class="line">[root@bigdata01 sbin]# vi start-dfs.sh</span><br><span class="line">HDFS_DATANODE_USER&#x3D;root</span><br><span class="line">HDFS_DATANODE_SECURE_USER&#x3D;hdfs</span><br><span class="line">HDFS_NAMENODE_USER&#x3D;root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER&#x3D;root</span><br><span class="line"></span><br><span class="line">[root@bigdata01 sbin]# vi stop-dfs.sh</span><br><span class="line">HDFS_DATANODE_USER&#x3D;root</span><br><span class="line">HDFS_DATANODE_SECURE_USER&#x3D;hdfs</span><br><span class="line">HDFS_NAMENODE_USER&#x3D;root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER&#x3D;root</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">修改sbin目录下的start-yarn.sh，stop-yarn.sh这两个脚本文件，在文件前面增加如下内容</span><br><span class="line"></span><br><span class="line">[root@bigdata01 sbin]# vi start-yarn.sh</span><br><span class="line">YARN_RESOURCEMANAGER_USER&#x3D;root</span><br><span class="line">HADOOP_SECURE_DN_USER&#x3D;yarn</span><br><span class="line">YARN_NODEMANAGER_USER&#x3D;root</span><br><span class="line"></span><br><span class="line">[root@bigdata01 sbin]# vi stop-yarn.sh</span><br><span class="line">YARN_RESOURCEMANAGER_USER&#x3D;root</span><br><span class="line">HADOOP_SECURE_DN_USER&#x3D;yarn</span><br><span class="line">YARN_NODEMANAGER_USER&#x3D;root</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">再启动集群</span><br><span class="line">6：验证集群进程信息</span><br><span class="line"></span><br><span class="line">执行jps命令可以查看集群的进程信息，去掉Jps这个进程之外还需要有5个进程才说明集群是正常启动的</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# jps</span><br><span class="line">3267 NameNode</span><br><span class="line">3859 ResourceManager</span><br><span class="line">3397 DataNode</span><br><span class="line">3623 SecondaryNameNode</span><br><span class="line">3996 NodeManager</span><br><span class="line">4319 Jps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  还可以通过webui界面来验证集群服务是否正常</span><br><span class="line">HDFS webui界面：http:&#x2F;&#x2F;192.168.182.100:9870</span><br><span class="line">YARN webui界面：http:&#x2F;&#x2F;192.168.182.100:8088</span><br><span class="line">  如果想通过主机名访问，则需要修改windows机器中的hosts文件</span><br><span class="line">文件所在位置为：C:\Windows\System32\drivers\etc\HOSTS</span><br><span class="line"></span><br><span class="line">在文件中增加下面内容，这个其实就是Linux虚拟机的ip和主机名，在这里做一个映射之后，就可以在Windows机器中通过主机名访问这个Linux虚拟机了。</span><br><span class="line"></span><br><span class="line">192.168.182.100 bigdata01</span><br><span class="line">注意：如果遇到这个文件无法修改，一般是由于权限问题，在打开的时候可以选择使用管理员模式打开。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">7：停止集群</span><br><span class="line"></span><br><span class="line">如果修改了集群的配置文件或者是其它原因要停止集群，可以使用下面命令</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# sbin&#x2F;stop-all.sh</span><br></pre></td></tr></table></figure><h3 id="分布式集群安装"><a href="#分布式集群安装" class="headerlink" title="分布式集群安装"></a>分布式集群安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">伪分布集群搞定了以后我们来看一下真正的分布式集群是什么样的</span><br><span class="line"></span><br><span class="line">看一下这张图，图里面表示是三个节点，左边这一个是主节点，右边的两个是从节点，hadoop集群是支持主从架构的。</span><br><span class="line"></span><br><span class="line">不同节点上面启动的进程默认是不一样的</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HmeG0s" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/05/HmeG0s.png" alt="HmeG0s.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">下面我们就根据图中的规划实现一个一主两从的hadoop集群</span><br><span class="line">环境准备：三个节点</span><br><span class="line">bigdata01 192.168.182.100</span><br><span class="line">bigdata02 192.168.182.101</span><br><span class="line">bigdata03 192.168.182.102</span><br><span class="line"></span><br><span class="line">注意：每个节点的基础环境都要先配置好，先把ip、hostname、firewalld、ssh免密码登录、JDK这些基础环境配置好</span><br><span class="line"></span><br><span class="line">目前的节点数量是不够的，按照第一周学习的内容，通过克隆的方式创建多个节点，具体克隆的步骤在这就不再赘述了。</span><br><span class="line">先把bigdata01中之前按照的hadoop删掉，删除解压的目录，修改环境变量即可。</span><br><span class="line"></span><br><span class="line">注意：我们需要把bigdata01节点中&#x2F;data目录下的hadoop_repo目录和&#x2F;data&#x2F;soft下的hadoop-3.2.0目录删掉，恢复此节点的环境，这里面记录的有之前伪分布集群的一些信息。</span><br><span class="line"></span><br><span class="line">注意：针对这三台机器的ip、hostname、firewalld、ssh免密码登录、JDK这些基础环境的配置步骤在这里就不再记录了，具体步骤参考2.1中的步骤。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这些基础环境配置好以后还没完，还有一些配置需要完善。</span><br><span class="line"></span><br><span class="line">配置&#x2F;etc&#x2F;hosts</span><br><span class="line">因为需要在主节点远程连接两个从节点，所以需要让主节点能够识别从节点的主机名，使用主机名远程访问，默认情况下只能使用ip远程访问，想要使用主机名远程访问的话需要在节点的&#x2F;etc&#x2F;hosts文件中配置对应机器的ip和主机名信息。</span><br><span class="line"></span><br><span class="line">所以在这里我们就需要在bigdata01的&#x2F;etc&#x2F;hosts文件中配置下面信息，最好把当前节点信息也配置到里面，这样这个文件中的内容就通用了，可以直接拷贝到另外两个从节点</span><br><span class="line"></span><br><span class="line">[root@bigdata01 ~]# vi &#x2F;etc&#x2F;hosts</span><br><span class="line">192.168.182.100 bigdata01</span><br><span class="line">192.168.182.101 bigdata02</span><br><span class="line">192.168.182.102 bigdata03</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">集群节点之间时间同步</span><br><span class="line">集群只要涉及到多个节点的就需要对这些节点做时间同步，如果节点之间时间不同步相差太多，会应该集群的稳定性，甚至导致集群出问题。</span><br><span class="line"></span><br><span class="line">首先在bigdata01节点上操作</span><br><span class="line"></span><br><span class="line">使用ntpdate -u ntp.sjtu.edu.cn实现时间同步，但是执行的时候提示找不到ntpdata命令</span><br><span class="line">[root@bigdata01 ~]# ntpdate -u ntp.sjtu.edu.cn</span><br><span class="line">-bash: ntpdate: command not found</span><br><span class="line">默认是没有ntpdate命令的，需要使用yum在线安装，执行命令 yum install -y ntpdate</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">然后手动执行ntpdate -u ntp.sjtu.edu.cn 确认是否可以正常执行</span><br><span class="line"></span><br><span class="line">[root@bigdata01 ~]# ntpdate -u ntp.sjtu.edu.cn</span><br><span class="line"> 7 Apr 21:21:01 ntpdate[5447]: step time server 185.255.55.20 offset 6.252298 sec</span><br><span class="line"> </span><br><span class="line">建议把这个同步时间的操作添加到linux的crontab定时器中，每分钟执行一次</span><br><span class="line">[root@bigdata01 ~]# vi &#x2F;etc&#x2F;crontab</span><br><span class="line">* * * * * root &#x2F;usr&#x2F;sbin&#x2F;ntpdate -u ntp.sjtu.edu.cn</span><br><span class="line">然后在bigdata02和bigdata03节点上配置时间同步</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">SSH免密码登录完善</span><br><span class="line">注意：针对免密码登录，目前只实现了自己免密码登录自己，最终需要实现主机点可以免密码登录到所有节点，所以还需要完善免密码登录操作。</span><br><span class="line"></span><br><span class="line">首先在bigdata01机器上执行下面命令，将公钥信息拷贝到两个从节点</span><br><span class="line"></span><br><span class="line">[root@bigdata01 ~]# scp ~&#x2F;.ssh&#x2F;authorized_keys bigdata02:~&#x2F;</span><br><span class="line"></span><br><span class="line">[root@bigdata01 ~]# scp ~&#x2F;.ssh&#x2F;authorized_keys bigdata03:~&#x2F;</span><br><span class="line"></span><br><span class="line">然后在bigdata02和bigdata03上执行</span><br><span class="line">bigdata02：</span><br><span class="line">[root@bigdata02 ~]# cat ~&#x2F;authorized_keys  &gt;&gt; ~&#x2F;.ssh&#x2F;authorized_keys</span><br><span class="line"></span><br><span class="line">bigdata03:</span><br><span class="line">[root@bigdata03 ~]# cat ~&#x2F;authorized_keys  &gt;&gt; ~&#x2F;.ssh&#x2F;authorized_keys</span><br><span class="line">验证一下效果，在bigdata01节点上使用ssh远程连接两个从节点，如果不需要输入密码就表示是成功的，此时主机点可以免密码登录到所有节点。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  有没有必要实现从节点之间互相免密码登录呢？</span><br><span class="line">这个就没有必要了，因为在启动集群的时候只有主节点需要远程连接其它节点。</span><br><span class="line">  OK，那到这为止，集群中三个节点的基础环境就都配置完毕了，接下来就需要在这三个节点中安装Hadoop了。</span><br><span class="line">  首先在bigdata01节点上安装。</span><br><span class="line"></span><br><span class="line">1：把hadoop-3.2.0.tar.gz安装包上传到linux机器的&#x2F;data&#x2F;soft目录下</span><br><span class="line">2：解压hadoop安装包</span><br><span class="line">3：修改hadoop相关配置文件</span><br><span class="line">进入配置文件所在目录</span><br><span class="line">[root@bigdata01 soft]# cd hadoop-3.2.0&#x2F;etc&#x2F;hadoop&#x2F;</span><br><span class="line"></span><br><span class="line">首先修改hadoop-env.sh文件，在文件末尾增加环境变量信息</span><br><span class="line">[root@bigdata01 hadoop]# vi hadoop-env.sh </span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HADOOP_LOG_DIR&#x3D;&#x2F;data&#x2F;hadoop_repo&#x2F;logs&#x2F;hadoop</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">修改core-site.xml文件，注意fs.defaultFS属性中的主机名需要和主节点的主机名保持一致</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:&#x2F;&#x2F;bigdata01:9000&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;data&#x2F;hadoop_repo&lt;&#x2F;value&gt;</span><br><span class="line">   &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">修改hdfs-site.xml文件，把hdfs中文件副本的数量设置为2，最多为2，因为现在集群中有两个从节点，还有secondaryNamenode进程所在的节点信息</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi hdfs-site.xml </span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;bigdata01:50090&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">修改mapred-site.xml，设置mapreduce使用的资源调度框架</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi mapred-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">修改yarn-site.xml，设置yarn上支持运行的服务和环境变量白名单</span><br><span class="line"></span><br><span class="line">注意，针对分布式集群在这个配置文件中还需要设置resourcemanager的hostname，否则nodemanager找不到resourcemanager节点。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;bigdata01&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">修改workers文件，增加所有从节点的主机名，一个一行</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# vi workers</span><br><span class="line">bigdata02</span><br><span class="line">bigdata03</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">修改启动脚本</span><br><span class="line"></span><br><span class="line">修改start-dfs.sh，stop-dfs.sh这两个脚本文件，在文件前面(一定要注意位置,license后面)增加如下内容</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop]# cd &#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0&#x2F;sbin</span><br><span class="line">[root@bigdata01 sbin]# vi start-dfs.sh</span><br><span class="line">HDFS_DATANODE_USER&#x3D;root</span><br><span class="line">HDFS_DATANODE_SECURE_USER&#x3D;hdfs</span><br><span class="line">HDFS_NAMENODE_USER&#x3D;root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER&#x3D;root</span><br><span class="line"></span><br><span class="line">[root@bigdata01 sbin]# vi stop-dfs.sh</span><br><span class="line">HDFS_DATANODE_USER&#x3D;root</span><br><span class="line">HDFS_DATANODE_SECURE_USER&#x3D;hdfs</span><br><span class="line">HDFS_NAMENODE_USER&#x3D;root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER&#x3D;root</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">修改start-yarn.sh，stop-yarn.sh这两个脚本文件，在文件前面(一定要注意位置,license后面)增加如下内容</span><br><span class="line"></span><br><span class="line">[root@bigdata01 sbin]# vi start-yarn.sh</span><br><span class="line">YARN_RESOURCEMANAGER_USER&#x3D;root</span><br><span class="line">HADOOP_SECURE_DN_USER&#x3D;yarn</span><br><span class="line">YARN_NODEMANAGER_USER&#x3D;root</span><br><span class="line"></span><br><span class="line">[root@bigdata01 sbin]# vi stop-yarn.sh</span><br><span class="line">YARN_RESOURCEMANAGER_USER&#x3D;root</span><br><span class="line">HADOOP_SECURE_DN_USER&#x3D;yarn</span><br><span class="line">YARN_NODEMANAGER_USER&#x3D;root</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">4：把bigdata01节点上将修改好配置的安装包拷贝到其他两个从节点</span><br><span class="line">[root@bigdata01 sbin]# cd &#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">[root@bigdata01 soft]# scp -rq hadoop-3.2.0 bigdata02:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">[root@bigdata01 soft]# scp -rq hadoop-3.2.0 bigdata03:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line"></span><br><span class="line">5：在bigdata01节点上格式化HDFS(再次格式化时，一定要注意将配置的数据目录删除)</span><br><span class="line">[root@bigdata01 soft]# cd &#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# bin&#x2F;hdfs namenode -format</span><br><span class="line">如果在后面的日志信息中能看到这一行，则说明namenode格式化成功。</span><br><span class="line">common.Storage: Storage directory &#x2F;data&#x2F;hadoop_repo&#x2F;dfs&#x2F;name has been successfully formatted.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">6：启动集群，在bigdata01节点上执行下面命令</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# sbin&#x2F;start-all.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">7：验证集群</span><br><span class="line">分别在3台机器上执行jps命令，进程信息如下所示：</span><br><span class="line">在bigdata01节点执行</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# jps</span><br><span class="line">6128 NameNode</span><br><span class="line">6621 ResourceManager</span><br><span class="line">6382 SecondaryNameNode</span><br><span class="line"></span><br><span class="line">在bigdata02节点执行</span><br><span class="line">[root@bigdata02 ~]# jps</span><br><span class="line">2385 NodeManager</span><br><span class="line">2276 DataNode</span><br><span class="line"></span><br><span class="line">在bigdata03节点执行</span><br><span class="line">[root@bigdata03 ~]# jps</span><br><span class="line">2326 NodeManager</span><br><span class="line">2217 DataNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">8：停止集群</span><br><span class="line">在bigdata01节点上执行停止命令</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# sbin&#x2F;stop-all.sh</span><br></pre></td></tr></table></figure><h3 id="Hadoop的客户端节点"><a href="#Hadoop的客户端节点" class="headerlink" title="Hadoop的客户端节点"></a>Hadoop的客户端节点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  在实际工作中不建议直接连接集群中的节点来操作集群，直接把集群中的节点暴露给普通开发人员是不安全的</span><br><span class="line">  建议在业务机器上安装Hadoop，只需要保证业务机器上的Hadoop的配置和集群中的配置保持一致即可，这样就可以在业务机器上操作Hadoop集群了，此机器就称为是Hadoop的客户端节点</span><br><span class="line">Hadoop的客户端节点可能会有多个，理论上是我们想要在哪台机器上操作hadoop集群就可以把这台机器配置为hadoop集群的客户端节点。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HmuO9x" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/05/HmuO9x.png" alt="HmuO9x.png"></a></p><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="external nofollow noopener noreferrer">hadoop官方文档</a></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第一周 第4章 Linux试炼之配置与shell实战</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%80%E5%91%A8%20%E7%AC%AC4%E7%AB%A0%20Linux%E8%AF%95%E7%82%BC%E4%B9%8B%E9%85%8D%E7%BD%AE%E4%B8%8Eshell%E5%AE%9E%E6%88%98.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%80%E5%91%A8%20%E7%AC%AC4%E7%AB%A0%20Linux%E8%AF%95%E7%82%BC%E4%B9%8B%E9%85%8D%E7%BD%AE%E4%B8%8Eshell%E5%AE%9E%E6%88%98.html</id>
    <published>2022-01-28T03:27:25.000Z</published>
    <updated>2022-01-30T03:40:35.426Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第一周-第4章-Linux试炼之配置与shell实战"><a href="#第一周-第4章-Linux试炼之配置与shell实战" class="headerlink" title="第一周 第4章 Linux试炼之配置与shell实战"></a>第一周 第4章 Linux试炼之配置与shell实战</h1><h2 id="linux高级配置"><a href="#linux高级配置" class="headerlink" title="linux高级配置"></a>linux高级配置</h2><h3 id="给linux设置静态ip地址"><a href="#给linux设置静态ip地址" class="headerlink" title="给linux设置静态ip地址"></a>给linux设置静态ip地址</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">一个简单的方法：</span><br><span class="line"> 1.在虚拟机和主机互通的条件下</span><br><span class="line"> 2.如果之前设置的vmnet8是自动获取，可以根据cmd里的vmnet8来设置ip,网关，掩码；如果之前已经是设置的固定信息那就不管</span><br><span class="line"> 3.修改虚拟机文件&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33</span><br><span class="line">   dhcp修改为static</span><br><span class="line">   onboot的值修改为yes</span><br><span class="line">   尾外加：</span><br><span class="line">   IPADDR&#x3D;&quot;根据虚拟机ipaddr 或 ipconfig获得的值来设置&quot;</span><br><span class="line">   GATEWAY&#x3D;&quot;根据vmnet8的值来设置&quot;</span><br><span class="line">   ENS1&#x3D;&quot;与上一个相同&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改好以后还有最重要的一步，重启网卡。如果结果显示的是OK，就说明是没有问题的。</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# service network restart</span><br><span class="line">Restarting network (via systemctl):                        [  OK  ]</span><br></pre></td></tr></table></figure><h3 id="Linux起名字-hostname"><a href="#Linux起名字-hostname" class="headerlink" title="Linux起名字(hostname)"></a>Linux起名字(hostname)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  那针对linux机器也是一样的，ip不好记，所以针对每一台linux机器还有一个主机名，hostname，我们可以给hostname设置一个通俗易懂、方便记忆的名字。</span><br><span class="line">针对hostname的设置分为两种:</span><br><span class="line">  一种是临时设置，立刻生效，但是机器重启之后就失效了。</span><br><span class="line">  还有一种是永久设置，但需要重启之后才生效。</span><br><span class="line">所以在实际工作中这两个要结合起来使用，临时+永久设置就可以实现立刻生效、永久有效。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hostname &#x2F;&#x2F;获取主机名</span><br><span class="line"></span><br><span class="line">hostname bigdata01  &#x2F;&#x2F;设置临时名</span><br><span class="line"></span><br><span class="line">设置&#x2F;etc&#x2F;hostname文件   &#x2F;&#x2F; 永久修改</span><br></pre></td></tr></table></figure><h3 id="Linux的金钟罩铁布衫-防火墙"><a href="#Linux的金钟罩铁布衫-防火墙" class="headerlink" title="Linux的金钟罩铁布衫(防火墙)"></a>Linux的金钟罩铁布衫(防火墙)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  我们在学习阶段，建议关闭防火墙，因为在后面我们会使用到多台机器，如果不关闭防火墙，会遇到机器之间无法通信的场景，比较麻烦，在实际工作中这块工作是由运维负责管理的，我们不需要关注这块。</span><br><span class="line">注意：在实际工作中一般是不需要关闭防火墙的，大家可千万别到时候，上去就把防火墙给关闭了，那样的话针对线上的服务器是有很大安全风险的，我们现在学习阶段是使用的自己本地搭建的虚拟机，不会出现任何安全风险，你们现在遇到的风险都是来源于你们自己。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对防火墙的关闭也分为两种方式，临时关闭和永久关闭，</span><br><span class="line">  临时关闭的特性是立刻生效，重启失效</span><br><span class="line">  永久关闭的特性是重启生效，永久有效</span><br><span class="line">那在这里使用的时候还是要结合这两种方式</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">centos7:</span><br><span class="line">首先实现临时关闭</span><br><span class="line">  systemctl stop firewalld</span><br><span class="line">执行临时关闭以后可以通过status确认当前防火墙的状态</span><br><span class="line">  systemctl status firewalld</span><br><span class="line">  </span><br><span class="line">然后再实现永久关闭，防止重启后生效</span><br><span class="line">  systemctl disable firewalld</span><br><span class="line">关闭以后我们可以通过这个list-unit-files来确认一下是否从开机启动项中关闭了</span><br><span class="line">  systemctl list-unit-files | grep firewalld</span><br><span class="line">firewalld.service                             disabled</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">centos6:</span><br><span class="line">首先实现临时关闭</span><br><span class="line">  service iptables stop</span><br><span class="line">然后再实现永久关闭，防止重启后生效</span><br><span class="line">  chkconfig iptables off</span><br></pre></td></tr></table></figure><h2 id="linux之shell编程"><a href="#linux之shell编程" class="headerlink" title="linux之shell编程"></a>linux之shell编程</h2><h3 id="什么是shell"><a href="#什么是shell" class="headerlink" title="什么是shell"></a>什么是shell</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> shell编程其实就是把之前在shell中执行的单个命令按照一定的逻辑和规则，组装到一个文件中，后面执行的时候就可以直接执行这个文件了，这个文件我们称之为shell脚本。</span><br><span class="line">所以shell编程，最终其实就是要开发一个shell脚本。</span><br></pre></td></tr></table></figure><h3 id="我的第一个shell脚本"><a href="#我的第一个shell脚本" class="headerlink" title="我的第一个shell脚本"></a>我的第一个shell脚本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  shell脚本的后缀倒没有那么严格的要求，只是建议大家以.sh结尾，这算是一个约定，大家都遵守这个约定，后期只要看到.sh结尾的文件就知道这个是shell脚本了。</span><br><span class="line">  shell脚本的第一行内容是： #!&#x2F;bin&#x2F;bash</span><br><span class="line">  </span><br><span class="line">  这句话相当于是一个导包语句，将shell的执行环境引入进去了。</span><br><span class="line">注意了，第一行的#号可不是注释，其它行的#号才是注释</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; vi hello.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># first command</span><br><span class="line">echo hello world!</span><br></pre></td></tr></table></figure><h3 id="执行我的shell脚本"><a href="#执行我的shell脚本" class="headerlink" title="执行我的shell脚本"></a>执行我的shell脚本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt;&gt;&gt; bash hello.sh </span><br><span class="line">hello world!</span><br><span class="line">还有一种写法是sh hello.sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">其实这里不管是bash 还是sh 都是一样的</span><br><span class="line">bash对应的是&#x2F;bin目录下面的bash文件</span><br><span class="line"></span><br><span class="line">sh是一个链接文件，指向的也是&#x2F;bin目录下面的bash文件</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其实bash和sh在之前对应的是两种类型的shell，不过后来统一了，我们在这也就不区分了，所以在shell脚本中的第一行引入&#x2F;bin&#x2F;bash，或者&#x2F;bin&#x2F;sh都是一样的，这块大家知道就行了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">注意了，大家在看其它资料的时候，资料中一般都会说需要先给脚本添加执行权限，然后才能执行，为什么我们在这里没有给脚本增加执行权限就能执行呢？</span><br><span class="line"></span><br><span class="line">在这里可以看到这个脚本确实只有读写权限</span><br><span class="line">[root@bigdata01 shell]# ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 root root 45 Apr  2 16:11 hello.sh</span><br><span class="line"></span><br><span class="line">  主要原因是这样的，我们现在执行的时候前面指定bash或者sh，表示把hello.sh这个脚本中的内容作为参数直接传给了bash或者sh命令来执行，所以这个脚本有没有执行权限都无所谓了。</span><br><span class="line">  那下面我们就来给这个脚本添加执行权限</span><br><span class="line">chmod u+x hello.sh</span><br><span class="line"></span><br><span class="line">添加完执行权限之后，再执行的时候就可以使用简化形式了</span><br><span class="line"></span><br><span class="line">.&#x2F;hello.sh</span><br><span class="line">这里的.表示是当前目录，表示在当前目录下执行这个脚本</span><br><span class="line"></span><br><span class="line">这里指定全路径也可以执行</span><br><span class="line">[root@bigdata01 shell]# &#x2F;root&#x2F;shell&#x2F;hello.sh </span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# hello.sh</span><br><span class="line">-bash: hello.sh: command not found</span><br><span class="line">这样直接执行却提示命令没找到？有没有感到疑惑？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  主要原因是这样的，因为在这里我们直接指定的文件名称，前面没有带任何路径信息，那么按照linux的查找规则，它会到PATH这个环境变量中指定的路径里面查找，这个时候PATH环境变量中都有哪些路径呢，我们来看一下</span><br><span class="line">[root@bigdata01 shell]# echo $PATH</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;root&#x2F;bin</span><br><span class="line">  所以说到这些路径里面去找肯定是找不到的，那怎么办呢？如果大家在windows中配置过JAVA的PATH环境变量的话就比较容易理解了，在这里我们只需要在PATH中加一个.即可，.表示当前目录，这样在执行的时候会自动到当前目录查找。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">打开&#x2F;etc&#x2F;profile文件，在最后一行添加export PATH&#x3D;.:$PATH,保存文件即可</span><br><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line">........</span><br><span class="line">.......</span><br><span class="line">.......</span><br><span class="line">export PATH&#x3D;.:$PATH</span><br><span class="line"></span><br><span class="line">然后执行source &#x2F;etc&#x2F;profile 重新加载环境变量配置文件，这样才会立刻生效</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">最后再讲一个小命令，shell脚本的单步执行，可以方便脚本调试</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# bash -x hello.sh </span><br><span class="line">+ echo hello &#39;world!&#39;</span><br><span class="line">hello world!</span><br><span class="line"></span><br><span class="line">+号开头的内容表示是脚本中将要执行的命令，下面的内容是执行的结果，这样如果脚本中的命令比较多的话，看起来是比较清晰的。</span><br></pre></td></tr></table></figure><h3 id="shell中的变量"><a href="#shell中的变量" class="headerlink" title="shell中的变量"></a>shell中的变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">学习任何编程语言都需要先学习变量，shell也不例外，但是要注意，shell中的变量不需要声明，初始化也不需要指定类型，shell是一门弱类型的语言，JAVA则是强类型的语言，需要提前声明变量，并且指定变量类型。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  shell中变量的命名要求：</span><br><span class="line">只能使用数字、字母和下划线，且不能以数字开头</span><br><span class="line">变量赋值是通过&quot;&#x3D;&quot;进行赋值，在变量、等号和值之间不能出现空格！</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建一些变量，执行之后提示-bash: name: command not found的都表示是错误的，执行成功的话是没有任何输出的，没有反馈就是最好的结果</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">打印变量的值，通过echo命令</span><br><span class="line">[root@bigdata01 shell]# echo $name</span><br><span class="line">zs</span><br><span class="line">[root@bigdata01 shell]# echo $&#123;name&#125;</span><br><span class="line">zs</span><br><span class="line">这两种形式都可以，一个是完整写法，一个是简化写法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">有什么区别吗？</span><br><span class="line">  如果我们想在变量的结果后面直接无缝拼接其它字符串，那就只能使用带花括号的形式</span><br><span class="line">[root@bigdata01 shell]# echo $namehehe</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# echo $&#123;name&#125;hehe</span><br><span class="line">zshehe</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果带空格的话就无所谓了</span><br><span class="line">[root@bigdata01 shell]# echo $name hehe </span><br><span class="line">zs hehe</span><br></pre></td></tr></table></figure><h4 id="变量的分类"><a href="#变量的分类" class="headerlink" title="变量的分类"></a>变量的分类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shell中的变量可以分为四种：</span><br><span class="line"></span><br><span class="line">本地变量</span><br><span class="line">环境变量</span><br><span class="line">位置变量</span><br><span class="line">特殊变量</span><br></pre></td></tr></table></figure><h5 id="本地变量"><a href="#本地变量" class="headerlink" title="本地变量"></a>本地变量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">本地变量的格式是VAR_NAME&#x3D;VALUE</span><br><span class="line"></span><br><span class="line">这种变量一般用于在shell脚本中定义一些临时变量，只对当前shell进程有效，关闭shell进程之后就消失了，对当前shell进程的子进程和其它shell进程无效，</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">当前shell的子进程进入方法：</span><br><span class="line">直接bash</span><br><span class="line"></span><br><span class="line">退出exit</span><br><span class="line"></span><br><span class="line">可以用pstree查看，但用不来</span><br></pre></td></tr></table></figure><h5 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">它的格式为：export VAR_NAME&#x3D;VALUE</span><br><span class="line"></span><br><span class="line">  它的格式是在本地变量格式的基础上添加一个export参数</span><br><span class="line">  环境变量的这种格式主要用于设置临时环境变量，当你关闭当前shell进程之后环境变量就消失了，还有就是对子shell进程有效，对其它shell进程无效</span><br><span class="line">注意了，环境变量的生效范围和本地变量是不一样的，环境变量对子shell进程是有效的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  注意了，在实际工作中我们设置环境变量一般都是需要让它永久生效，这种临时的并不适用，如何设置为永久的呢？</span><br><span class="line">  其实就是把这个临时的设置添加到指定配置文件中，以后每次开启shell进程的时候，都会去加载那个指定的配置文件中的命令，这样就可以实现永久生效了</span><br><span class="line"></span><br><span class="line">在这里我们一般添加到&#x2F;etc&#x2F;profile文件中，这样可以保证对所有用户都生效</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; vim &#x2F;etc&#x2F;profile</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">export az&#x3D;4</span><br><span class="line"></span><br><span class="line">设置完后，当前shell窗口需要执行 source &#x2F;etc&#x2F;profile</span><br><span class="line">其它新打开的shell不需要</span><br></pre></td></tr></table></figure><h5 id="位置变量"><a href="#位置变量" class="headerlink" title="位置变量"></a>位置变量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在进行shell编程的时候，有时候我们想给shell脚本动态的传递一些参数，这个时候就需要用到位置变量，类似于$0 $1 $2这样的，$后面的数字理论上没有什么限制，</span><br><span class="line"></span><br><span class="line">它的格式是：location.sh abc xyz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">创建一个脚本文件，location.sh 在里面打印一下这些位置变量看看到底是什么内容</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# vi location.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">echo $0</span><br><span class="line">echo $1</span><br><span class="line">echo $2</span><br><span class="line">echo $3</span><br><span class="line"></span><br><span class="line">执行脚本sh location.sh abc xyz</span><br><span class="line">[root@bigdata01 shell]# sh location.sh abc xyz</span><br><span class="line">location.sh</span><br><span class="line">abc</span><br><span class="line">xyz</span><br><span class="line"></span><br><span class="line">结果发现 $0的值是这个脚本的名称</span><br><span class="line">$1 是脚本后面的第一个参数</span><br><span class="line">$2是脚本后面的第二个参数</span><br><span class="line">$3为空，是因为脚本后面就只有两个参数</span><br><span class="line">理论上来说，脚本后面有多少个参数，在脚本中就可以通过$和角标获取对应参数的值。</span><br><span class="line">多个参数中间使用空格分隔。</span><br></pre></td></tr></table></figure><h5 id="特殊变量"><a href="#特殊变量" class="headerlink" title="特殊变量"></a>特殊变量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后来看一下shell中的特殊变量，针对特殊变量我们主要学习下面列出来的两个</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">首先是$?</span><br><span class="line">它表示是上一条命令的返回状态码，状态码在0~255之间</span><br><span class="line">如果命令执行成功，这个返回状态码是0，如果失败，则是在1~255之间，不同的状态码代表着不同的错误信息，也就是说，正确的道路只有一条，失败的道路有很多。</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; echo $?</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">状态码    描述</span><br><span class="line">0    命令成功结束</span><br><span class="line">1    通用未知错误　　</span><br><span class="line">2    误用Shell命令</span><br><span class="line">126    命令不可执行</span><br><span class="line">127    没找到命令</span><br><span class="line">128    无效退出参数</span><br><span class="line">128+x    Linux信号x的严重错误</span><br><span class="line">130    命令通过Ctrl+C控制码越界</span><br><span class="line">255    退出码越界</span><br><span class="line"></span><br><span class="line">这个状态码在工作中的应用场景是这样的，我们有时候会根据上一条命令的执行结果来执行后面不同的业务逻辑</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">第二个特殊变量是$#，它表示的是shell脚本所有参数的个数</span><br><span class="line"></span><br><span class="line">  我们来演示一下，先创建paramnum.sh</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# vi paramnum.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">echo $#</span><br><span class="line">然后执行</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# sh paramnum.sh a b c</span><br><span class="line">3</span><br><span class="line">[root@bigdata01 shell]# sh paramnum.sh a b c d</span><br><span class="line">4</span><br><span class="line">  这个特殊变量的应用场景是这样的，假设我们的脚本在运行的时候需要从外面动态获取三个参数，那么在执行脚本之前就需要先判断一下脚本后面有没有指定三个参数，如果就指定了1个参数，那这个脚本就没有必要执行了，直接停止就可以了，参数个数都不够，执行是没有意义的。</span><br></pre></td></tr></table></figure><h5 id="变量和引号的特殊使用"><a href="#变量和引号的特殊使用" class="headerlink" title="变量和引号的特殊使用"></a>变量和引号的特殊使用</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  前面我们学习了shell中的变量，那针对变量和引号在工作中有一些特殊的使用场景</span><br><span class="line">单引号  首先是单引号不解析变量</span><br><span class="line">&gt;&gt;&gt; name&#x3D;jack</span><br><span class="line">&gt;&gt;&gt; echo &#39;$name&#39;</span><br><span class="line">&#39;$name&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">双引号解析变量</span><br><span class="line">&gt;&gt;&gt; name&#x3D;jack</span><br><span class="line">&gt;&gt;&gt; echo &#39;&#39;$name&#39;&#39;</span><br><span class="line">jack</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">反引号   执行并引用命令的执行结果</span><br><span class="line">&gt;&gt;&gt; name&#x3D;pwd</span><br><span class="line">&gt;&gt;&gt; echo &#96;$name&#96;</span><br><span class="line">&#x2F;root&#x2F;shell</span><br><span class="line"></span><br><span class="line">反引号还有另一种写法，$() 他们的效果一致</span><br><span class="line">[root@bigdata01 shell]# echo $($name)</span><br><span class="line">&#x2F;root&#x2F;shell</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">  最后还有一个大招 大家注意一下</span><br><span class="line">有时候我们想在变量的值外面套一层引号，该怎么写呢？</span><br><span class="line">echo &quot;$name&quot;是不行的，最终的值是不带引号的</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# echo &quot;$name&quot;</span><br><span class="line">pwd</span><br><span class="line">那我在外面套一层单引号呢？这样虽然值里面带双引号了，但是这个变量却没有解析</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# echo &#39;&quot;$name&quot;&#39;  </span><br><span class="line">&quot;$name&quot;</span><br><span class="line">还能怎么办呢？</span><br><span class="line">看一下这个骚操作，先套一个单引号，再套一个双引号，这样就可以了。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# echo &quot;&#39;$name&#39;&quot;</span><br><span class="line">&#39;pwd&#39;</span><br><span class="line">什么时候需要在结果里面带引号呢？在后面课程中我们在脚本中动态拼接sql的时候会用到。</span><br></pre></td></tr></table></figure><h2 id="shell中的循环和判断"><a href="#shell中的循环和判断" class="headerlink" title="shell中的循环和判断"></a>shell中的循环和判断</h2><h3 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">第一种格式：和java中的for循环格式有点类似，但是也不一样</span><br><span class="line">for((i&#x3D;0;i&lt;10;i++))</span><br><span class="line">do</span><br><span class="line">循环体...</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# vi for1.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">for((i&#x3D;0;i&lt;10;i++))</span><br><span class="line">do</span><br><span class="line">echo $i</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">注意了，这里的do也可以和for写在一行，只是需要加一个分号;</span><br><span class="line">[root@bigdata01 shell]# vi for1.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">for((i&#x3D;0;i&lt;10;i++));do</span><br><span class="line">echo $i</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# sh for1.sh </span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">这一种格式适合用在迭代多次，步长一致的情况</span><br><span class="line"></span><br><span class="line">接下来看第二种格式，这种格式针对没有规律的列表，或者是有限的几种情况进行迭代是比较方便的</span><br><span class="line">for i in 1 2 3</span><br><span class="line">do</span><br><span class="line">循环体...</span><br><span class="line">done</span><br><span class="line">演示一下，</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# vi for2.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">for i in 1 2 3</span><br><span class="line">do</span><br><span class="line">echo $i</span><br><span class="line">done</span><br><span class="line">执行，看结果</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# sh for2.sh </span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td></tr></table></figure><h3 id="while循环"><a href="#while循环" class="headerlink" title="while循环"></a>while循环</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  while循环主要适用于循环次数未知，或不便于使用for直接生成较大列表时</span><br><span class="line">  while循环的格式为：</span><br><span class="line">while 测试条件</span><br><span class="line">do</span><br><span class="line">循环体...</span><br><span class="line">done</span><br><span class="line">  注意这里面的测试条件，测试条件为&quot;真&quot;，则进入循环，测试条件为&quot;假&quot;，则退出循环</span><br><span class="line">那这个测试条件该如何定义呢？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  test EXPR 或者 [ EXPR ] ，第二种形式里面中括号和表达式之间的空格不能少</span><br><span class="line">这个EXPR表达式里面写的就是具体的比较逻辑，shell中的比较有一些不同之处，针对整型数据和字符串数据是不一样的，来看一下</span><br><span class="line"></span><br><span class="line">  整型测试：-gt(大于)、-lt(小于)、-ge(大于等于)、-le(小于等于)、-eq(等于)、-ne(不等于)</span><br><span class="line"></span><br><span class="line">  针对整型数据，需要使用-gt、-lt这样的写法，而不是大于号或小于号，这个需要注意一下</span><br><span class="line">  还有就是字符串数据，如果判断两个字符串相等，使用&#x3D;号，这里的&#x3D;号不是赋值的意思，不等于就使用!&#x3D;就可以了</span><br><span class="line">字符串测试：&#x3D;(等于)、!&#x3D;(不等于)</span><br><span class="line"></span><br><span class="line">下面来演示一下，创建 while1.sh，注意，这里面需要使用sleep实现休眠操作，否则程序会一直连续的打印内容</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 shell]# vi while1.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">while test 2 -gt 1</span><br><span class="line">do</span><br><span class="line">echo yes</span><br><span class="line">sleep 1</span><br><span class="line">done</span><br><span class="line">执行脚本，按ctrl+c可强制退出程序</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# sh while1.sh </span><br><span class="line">yes</span><br><span class="line">yes</span><br><span class="line">yes</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">中括号这种，看起来比较清晰</span><br><span class="line">只是这种一定要注意，中括号和里面的表达式之间一定要有空格，否则就报错</span><br><span class="line">[root@bigdata01 shell]# cp while1.sh  while2.sh  </span><br><span class="line">[root@bigdata01 shell]# vi while2.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">while [ 2 -gt 1 ]</span><br><span class="line">do</span><br><span class="line">echo yes</span><br><span class="line">sleep 1</span><br><span class="line">done</span><br><span class="line">[root@bigdata01 shell]# sh while2.sh </span><br><span class="line">yes</span><br><span class="line">yes</span><br><span class="line">yes</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="if判断"><a href="#if判断" class="headerlink" title="if判断"></a>if判断</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if判断分为三种形式</span><br><span class="line"></span><br><span class="line">单分支</span><br><span class="line">双分支</span><br><span class="line">多分支</span><br></pre></td></tr></table></figure><h6 id="单分支"><a href="#单分支" class="headerlink" title="单分支"></a>单分支</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">单分支</span><br><span class="line">先看一下单分支，它的格式是这样的</span><br><span class="line">if 测试条件</span><br><span class="line">then</span><br><span class="line">    选择分支</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 shell]# vi if1.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">flag&#x3D;$1</span><br><span class="line">if [ $flag -eq 1 ]</span><br><span class="line">then</span><br><span class="line">echo one</span><br><span class="line">fi</span><br><span class="line">执行脚本</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# sh if1.sh 1</span><br><span class="line">one</span><br><span class="line">[root@bigdata01 shell]# sh if1.sh</span><br><span class="line">if1.sh: line 3: [: -eq: unary operator expected</span><br><span class="line">在这里发现，如果脚本后面没有传参数的话，执行程序会抱错</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">改进</span><br><span class="line">[root@bigdata01 shell]# vi if1.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">if [ $# -lt 1 ] #判断脚本后面参数的个数</span><br><span class="line">then</span><br><span class="line">echo &quot;not found param&quot;</span><br><span class="line">exit 100    # 这个状态码其实就是我们之前使用$?获取到的状态码，如果这个程</span><br><span class="line">#序不传任何参数，就会执行exit 100，结束程序，并且返回状态码100</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">flag&#x3D;$1</span><br><span class="line">if [ $flag -eq 1 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;one&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# sh if1.sh </span><br><span class="line">not found param</span><br><span class="line">[root@bigdata01 shell]# echo $?</span><br><span class="line">100</span><br></pre></td></tr></table></figure><h6 id="双分支"><a href="#双分支" class="headerlink" title="双分支"></a>双分支</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">格式如下：</span><br><span class="line">if 测试条件</span><br><span class="line">then</span><br><span class="line">    选择分支1</span><br><span class="line">else</span><br><span class="line">    选择分支2</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h6 id="多分支"><a href="#多分支" class="headerlink" title="多分支"></a>多分支</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">格式如下：</span><br><span class="line"></span><br><span class="line">if 测试条件1</span><br><span class="line">then</span><br><span class="line">    选择分支1</span><br><span class="line">elif 测试条件2</span><br><span class="line">then</span><br><span class="line">    选择分支2</span><br><span class="line">  ...</span><br><span class="line">else</span><br><span class="line">    选择分支n</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="shell扩展"><a href="#shell扩展" class="headerlink" title="shell扩展"></a>shell扩展</h2><h3 id="shell后台运行"><a href="#shell后台运行" class="headerlink" title="shell后台运行"></a>shell后台运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  在实际工作中会遇到这种情况，针对带有while无限循环的shell脚本，我们希望它能够一直运行，不影响我在这个窗口执行其它操作</span><br><span class="line"></span><br><span class="line">  但是现在它会一直占用这个shell窗口，我们称这个脚本现在是在前台执行，不想让它一直占用shell窗口的话，需要把它放到后台执行，如何放到后台呢？很简单，在脚本后面添加一个&amp;即可</span><br><span class="line">  </span><br><span class="line">[root@bigdata01 shell]# sh while2.sh &amp;</span><br><span class="line">[1]2228</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">但是当我们把这个窗口关闭以后会发现之前放到后台运行的shell脚本也停止了，我们是希望这个脚本能够一直在后台运行的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  如何保证关闭shell窗口而不影响放到后台的shell脚本执行呢？</span><br><span class="line">也很简单，在命令前面加上nohup 即可</span><br><span class="line">  原理就是，默认情况下，当我们关闭shell窗口时，shell窗口会向之前通过它启动的所有shell脚本发送停止信号，当我们加上nohup之后，就会阻断这个停止信号的发送，所以已经放到后台的shell脚本就不会停止了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 shell]# nohup sh while2.sh &amp;</span><br><span class="line">[1]2326</span><br><span class="line">nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line">注意：使用nohup之后，脚本输出的信息默认都会存储到当前目录下的一个nohup.out日志文件中，后期想要排查脚本的执行情况的话就可以看这个日志文件。</span><br><span class="line"></span><br><span class="line">此时如果想要停止这个shell脚本的话就只能使用kill了</span><br></pre></td></tr></table></figure><h3 id="标准输出、标准错误输出、和重定向的用法"><a href="#标准输出、标准错误输出、和重定向的用法" class="headerlink" title="标准输出、标准错误输出、和重定向的用法"></a>标准输出、标准错误输出、和重定向的用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">标准输出：表示是命令或者程序输出的正常信息</span><br><span class="line">标准错误输出：表示是命令或者程序输出的错误信息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">标准输出可以使用文件描述符1来表示，标准错误输出可以使用文件描述符2来表示</span><br><span class="line">针对标准输出和标准错误输出，可以使用重定向操作将这些输出信息保存到文件中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">演示一下重定向 &gt;，这里的1表示把标准输出重定向到文件中</span><br><span class="line">[root@bigdata01 shell]# ll 1&gt; a.txt</span><br><span class="line">[root@bigdata01 shell]# more a.txt </span><br><span class="line">total 52</span><br><span class="line">-rw-r--r--. 1 root root    0 Apr  3 21:39 a.txt</span><br><span class="line">-rw-r--r--. 1 root root   48 Apr  3 17:32 for1.sh</span><br><span class="line">-rw-r--r--. 1 root root   43 Apr  3 17:40 for2.sh</span><br><span class="line">-rwxr--r--. 1 root root   45 Apr  2 16:11 hello.sh</span><br><span class="line">-rw-r--r--. 1 root root  121 Apr  3 18:30 if1.sh</span><br><span class="line">-rw-r--r--. 1 root root  147 Apr  3 18:30 if2.sh</span><br><span class="line">-rw-r--r--. 1 root root  227 Apr  3 18:34 if3.sh</span><br><span class="line">-rw-r--r--. 1 root root   44 Apr  3 16:23 location.sh</span><br><span class="line">-rw-------. 1 root root 4692 Apr  3 21:11 nohup.out</span><br><span class="line">-rw-r--r--. 1 root root   20 Apr  3 16:48 paramnum.sh</span><br><span class="line">-rw-r--r--. 1 root root   56 Apr  3 17:59 while1.sh</span><br><span class="line">-rw-r--r--. 1 root root   55 Apr  3 18:01 while2.sh</span><br><span class="line">-rw-r--r--. 1 root root   61 Apr  3 18:03 while3.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">重复执行此命令会发现文件内的内容没有变化，这是因为 &gt; 会覆盖掉之前的内容</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">如果想要追加的话需要使用 &gt;&gt;</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# ll 1&gt;&gt; a.txt</span><br><span class="line">[root@bigdata01 shell]# more a.txt  </span><br><span class="line">total 52</span><br><span class="line">-rw-r--r--. 1 root root    0 Apr  3 21:39 a.txt</span><br><span class="line">-rw-r--r--. 1 root root   48 Apr  3 17:32 for1.sh</span><br><span class="line">-rw-r--r--. 1 root root   43 Apr  3 17:40 for2.sh</span><br><span class="line">-rwxr--r--. 1 root root   45 Apr  2 16:11 hello.sh</span><br><span class="line">-rw-r--r--. 1 root root  121 Apr  3 18:30 if1.sh</span><br><span class="line">-rw-r--r--. 1 root root  147 Apr  3 18:30 if2.sh</span><br><span class="line">-rw-r--r--. 1 root root  227 Apr  3 18:34 if3.sh</span><br><span class="line">-rw-r--r--. 1 root root   44 Apr  3 16:23 location.sh</span><br><span class="line">-rw-------. 1 root root 4692 Apr  3 21:11 nohup.out</span><br><span class="line">-rw-r--r--. 1 root root   20 Apr  3 16:48 paramnum.sh</span><br><span class="line">-rw-r--r--. 1 root root   56 Apr  3 17:59 while1.sh</span><br><span class="line">-rw-r--r--. 1 root root   55 Apr  3 18:01 while2.sh</span><br><span class="line">-rw-r--r--. 1 root root   61 Apr  3 18:03 while3.sh</span><br><span class="line">total 56</span><br><span class="line">-rw-r--r--. 1 root root  671 Apr  3 21:39 a.txt</span><br><span class="line">-rw-r--r--. 1 root root   48 Apr  3 17:32 for1.sh</span><br><span class="line">-rw-r--r--. 1 root root   43 Apr  3 17:40 for2.sh</span><br><span class="line">-rwxr--r--. 1 root root   45 Apr  2 16:11 hello.sh</span><br><span class="line">-rw-r--r--. 1 root root  121 Apr  3 18:30 if1.sh</span><br><span class="line">-rw-r--r--. 1 root root  147 Apr  3 18:30 if2.sh</span><br><span class="line">-rw-r--r--. 1 root root  227 Apr  3 18:34 if3.sh</span><br><span class="line">-rw-r--r--. 1 root root   44 Apr  3 16:23 location.sh</span><br><span class="line">-rw-------. 1 root root 4692 Apr  3 21:11 nohup.out</span><br><span class="line">-rw-r--r--. 1 root root   20 Apr  3 16:48 paramnum.sh</span><br><span class="line">-rw-r--r--. 1 root root   56 Apr  3 17:59 while1.sh</span><br><span class="line">-rw-r--r--. 1 root root   55 Apr  3 18:01 while2.sh</span><br><span class="line">-rw-r--r--. 1 root root   61 Apr  3 18:03 while3.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意，这里的1可以省略，因为默认情况下不写也是1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">标准错误输出的用法和这个一样，标准错误输出需要使用2，使用1是无法把这个错误输出信息重定向到文件中的</span><br><span class="line"></span><br><span class="line">下面这个写法是错误的。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# lk 1&gt; b.txt</span><br><span class="line">-bash: lk: command not found</span><br><span class="line">正确的写法是这样的。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# lk 2&gt; b.txt</span><br><span class="line">[root@bigdata01 shell]# more b.txt </span><br><span class="line">-bash: lk: command not found</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#注意 1和2与&gt;之间不能有空格</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">最后来看一个综合案例</span><br><span class="line">nohup hello.sh &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">我们来解释一下</span><br><span class="line">nohup和&amp;：可以让程序一直在后台运行</span><br><span class="line">&#x2F;dev&#x2F;null：是linux中的黑洞，任何数据扔进去都找不到了</span><br><span class="line">&gt;&#x2F;dev&#x2F;null：把标准输出重定向到黑洞中，表示脚本的输出信息不需要存储</span><br><span class="line">2&gt;&amp;1 ：表示是把标准错误输出重定向到标准输出中</span><br><span class="line">最终这条命令的意思就是把脚本放在后台一直运行，并且把脚本的所有输出都扔到黑洞里面</span><br></pre></td></tr></table></figure><h2 id="Linux中的定时器crontab"><a href="#Linux中的定时器crontab" class="headerlink" title="Linux中的定时器crontab"></a>Linux中的定时器crontab</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">crontab的格式是这样的：* * * * * user-name command</span><br><span class="line"></span><br><span class="line">*：分钟(0-59)</span><br><span class="line">*：小时(0-23)</span><br><span class="line">*：一个月中的第几天(1-31)</span><br><span class="line">*：月份(1-12)</span><br><span class="line">*：星期几(0-7) (星期天为0)</span><br><span class="line">user-name：用户名，用哪个用户执行</span><br><span class="line">command：具体需要指定的命令</span><br><span class="line"></span><br><span class="line">这条配置需要添加到crontab服务对应的文件中，在配置之前，需要先确认crontab的服务是否正常</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">查看crontab服务状态：systemctl status crond</span><br><span class="line">如果服务没有启动可以使用systemctl start crond 来启动</span><br><span class="line">如果想要停止 可以使用systemctl stop crond</span><br><span class="line"></span><br><span class="line">确认这个服务是ok的之后，我们就可以操作这个服务对应的配置文件了，&#x2F;etc&#x2F;crontab</span><br><span class="line">可以先打开看一下这个配置文件</span><br><span class="line">[root@bigdata01 shell]# vi &#x2F;etc&#x2F;crontab          </span><br><span class="line"></span><br><span class="line">SHELL&#x3D;&#x2F;bin&#x2F;bash</span><br><span class="line">PATH&#x3D;&#x2F;sbin:&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin</span><br><span class="line">MAILTO&#x3D;root</span><br><span class="line"></span><br><span class="line"># For details see man 4 crontabs</span><br><span class="line"></span><br><span class="line"># Example of job definition:</span><br><span class="line"># .---------------- minute (0 - 59)</span><br><span class="line"># |  .------------- hour (0 - 23)</span><br><span class="line"># |  |  .---------- day of month (1 - 31)</span><br><span class="line"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</span><br><span class="line"># |  |  |  |  .---- day of week (0 - 6) (Sunday&#x3D;0 or 7) OR sun,mon,tue,wed,thu,fri,sat</span><br><span class="line"># |  |  |  |  |</span><br><span class="line"># *  *  *  *  * user-name  command to be executed</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">下面我们就来配置一个。</span><br><span class="line">假设我们有一个需求，每隔1分钟打印一次当前时间，时间格式为年月日 时分秒</span><br><span class="line">这个需求需要写到脚本中，然后在crontab中直接调用脚本即可。</span><br><span class="line">其实我们只需要在脚本中实现打印当前时间的操作即可，每隔1分钟执行一次这个操作让crontab实现即可</span><br><span class="line"></span><br><span class="line">创建脚本文件 vi showTime.sh</span><br><span class="line">[root@bigdata01 shell]# vi showTime.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">showTime&#x3D;&#96;date &quot;+%Y-%m-%d %H:%M:%S&quot;&#96;  #有空格所以要加引号</span><br><span class="line">echo $showTime</span><br></pre></td></tr></table></figure> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">然后在&#x2F;etc&#x2F;crontab文件中配置</span><br><span class="line">每1分钟执行一次，其实是最简单的写法，前面都是*号就行，表示都匹配</span><br><span class="line">最终的效果就是这样的</span><br><span class="line"></span><br><span class="line">* * * * * root sh &#x2F;root&#x2F;shell&#x2F;showTime.sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：这里建议指定脚本的全路径，这样不容易出问题，还有就是执行命令在这里写好了以后建议拿出来单独执行一下，确认能不能正常执行，这样可以避免出现一些低级别的问题</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">现在这种情况脚本执行之后的结果我们是没有保存的，如果让crontab定时去调度执行，我们压根就看不到执行的结果信息，所以需要把脚本执行的结果重定向到一个文件中，</span><br><span class="line">需要使用追加重定向</span><br><span class="line"></span><br><span class="line">* * * * * root sh &#x2F;root&#x2F;shell&#x2F;showTime.sh &gt;&gt; &#x2F;root&#x2F;shell&#x2F;showTime.log</span><br><span class="line"></span><br><span class="line">保存配置文件即可，等待执行。</span><br><span class="line">我们来看查看一下结果文件，确认一下是否正常执行，可以使用tail -f 监控一会</span><br><span class="line">[root@bigdata01 shell]# tail -f &#x2F;root&#x2F;shell&#x2F;showTime.log </span><br><span class="line">2026-04-06 21:14:01</span><br><span class="line">2026-04-06 21:15:01</span><br><span class="line">2026-04-06 21:16:01</span><br><span class="line">......</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">如果我们执行的脚本确实不会产生任何输出信息，那么我们如何确认脚本是否被成功调度了呢？</span><br><span class="line">这个时候可以通过查看crontab的日志来确认</span><br><span class="line">crontab的日志在&#x2F;var&#x2F;log&#x2F;cron文件中，使用tail -f命令实时监控</span><br><span class="line"></span><br><span class="line">[root@bigdata01 shell]# tail -f &#x2F;var&#x2F;log&#x2F;cron</span><br><span class="line">.........</span><br><span class="line">Apr  6 21:14:01 bigdata01 CROND[1577]: (root) CMD (sh &#x2F;root&#x2F;shell&#x2F;showTime.sh &gt;&gt; &#x2F;root&#x2F;shell&#x2F;showTime.log)</span><br><span class="line">Apr  6 21:15:01 bigdata01 CROND[1584]: (root) CMD (sh &#x2F;root&#x2F;shell&#x2F;showTime.sh &gt;&gt; &#x2F;root&#x2F;shell&#x2F;showTime.log)</span><br><span class="line">Apr  6 21:16:01 bigdata01 CROND[1591]: (root) CMD (sh &#x2F;root&#x2F;shell&#x2F;showTime.sh &gt;&gt; &#x2F;root&#x2F;shell&#x2F;showTime.log)</span><br><span class="line">Apr  6 21:17:01 bigdata01 CROND[1597]: (root) CMD (sh &#x2F;root&#x2F;shell&#x2F;showTime.sh &gt;&gt; &#x2F;root&#x2F;shell&#x2F;showTime.log)</span><br><span class="line">Apr  6 21:18:01 bigdata01 CROND[1603]: (root) CMD (sh &#x2F;root&#x2F;shell&#x2F;showTime.sh &gt;&gt; &#x2F;root&#x2F;shell&#x2F;showTime.log)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">如果这个任务暂时不想调度了，想临时停止一段时间，可以修改配置文件，在这一行配置前面加上#号就可以了，这样这一行配置就被注释了，后期想使用的时候把#号去掉就可以了。</span><br><span class="line"></span><br><span class="line">下面大家思考一个问题，如果设置任务每7分钟执行一次，那么任务分别会在什么时间点执行？</span><br><span class="line">  任务会在我们配置好之后7分钟执行吗？ 不会的，</span><br><span class="line">  注意了，crontab中任务是这样执行的，我们这里设置的7分钟执行一次，那么就会在每个小时的第0、7、14、21、28.....分钟执行，而不是根据你配置好的时候往后推，这个一定要注意了</span><br><span class="line"></span><br><span class="line">我们来验证一下，修改配置文件</span><br><span class="line"></span><br><span class="line">*&#x2F;7 * * * * root sh &#x2F;root&#x2F;shell&#x2F;showTime.sh &gt;&gt; &#x2F;root&#x2F;shell&#x2F;showTime.log</span><br><span class="line"></span><br><span class="line">还有就是这里的间隔时间是7分钟，7分钟无法被60整除，那执行到这个小时的最后一次以后会怎么办呢？它最后会在第56分钟执行一次，再往后的话继续往后面顺延7分钟吗？不是的，下一次执行就是下一个小时的0分开始执行了，所以针对这种除不尽的到下一小时就开始重新计算了，不累计。</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/weixin_30670103/article/details/116923246" target="_blank" rel="external nofollow noopener noreferrer">crontab运维必会面试题</a></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
    
  </entry>
  
  <entry>
    <title>从零开始学大数据-07讲为什么说MapReduce既是编程模型又是计算框架</title>
    <link href="http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-07%E8%AE%B2%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4MapReduce%E6%97%A2%E6%98%AF%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%8F%88%E6%98%AF%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6.html"/>
    <id>http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-07%E8%AE%B2%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4MapReduce%E6%97%A2%E6%98%AF%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%8F%88%E6%98%AF%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6.html</id>
    <published>2022-01-28T03:23:46.000Z</published>
    <updated>2022-01-28T03:25:06.692Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="07讲为什么说MapReduce既是编程模型又是计算框架"><a href="#07讲为什么说MapReduce既是编程模型又是计算框架" class="headerlink" title="07讲为什么说MapReduce既是编程模型又是计算框架"></a>07讲为什么说MapReduce既是编程模型又是计算框架</h1><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="从零开始学大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第一周 第3章 Linux极速上手</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%80%E5%91%A8%20%E7%AC%AC3%E7%AB%A0%20Linux%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%80%E5%91%A8%20%E7%AC%AC3%E7%AB%A0%20Linux%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B.html</id>
    <published>2022-01-15T04:26:25.000Z</published>
    <updated>2022-01-15T04:32:57.574Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><a href="/linux%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%E5%92%8C%E5%BC%82%E5%B8%B8.html" title="linux相关命令和异常">linux相关命令和异常</a><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第一周 第2章 Linux虚拟机安装配置</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%80%E5%91%A8%20%E7%AC%AC2%E7%AB%A0-Linux%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%80%E5%91%A8%20%E7%AC%AC2%E7%AB%A0-Linux%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE.html</id>
    <published>2022-01-14T10:57:23.000Z</published>
    <updated>2022-02-05T09:12:30.944Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第一周-第2章-Linux虚拟机安装配置"><a href="#第一周-第2章-Linux虚拟机安装配置" class="headerlink" title="第一周 第2章 Linux虚拟机安装配置"></a>第一周 第2章 Linux虚拟机安装配置</h1><p>首先确认“虚拟化”是否启动？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">任务栏---&gt;右键---&gt;任务管理器---&gt;性能---&gt;cpu</span><br></pre></td></tr></table></figure><h2 id="vmWare安装"><a href="#vmWare安装" class="headerlink" title="vmWare安装"></a>vmWare安装</h2><p>略</p><h2 id="vmWare安装linux虚拟机"><a href="#vmWare安装linux虚拟机" class="headerlink" title="vmWare安装linux虚拟机"></a>vmWare安装linux虚拟机</h2><p>略</p><h2 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h2><p>略</p><h2 id="xshell或secueCRT连接虚拟机"><a href="#xshell或secueCRT连接虚拟机" class="headerlink" title="xshell或secueCRT连接虚拟机"></a>xshell或secueCRT连接虚拟机</h2><h3 id="secueCRT连接"><a href="#secueCRT连接" class="headerlink" title="secueCRT连接"></a>secueCRT连接</h3><p>略</p><h3 id="xshell"><a href="#xshell" class="headerlink" title="xshell"></a>xshell</h3><p>略</p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
    
  </entry>
  
  <entry>
    <title>从零开始学大数据-06讲新技术层出不穷，HDFS依然是存储的王者</title>
    <link href="http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-06%E8%AE%B2%E6%96%B0%E6%8A%80%E6%9C%AF%E5%B1%82%E5%87%BA%E4%B8%8D%E7%A9%B7%EF%BC%8CHDFS%E4%BE%9D%E7%84%B6%E6%98%AF%E5%AD%98%E5%82%A8%E7%9A%84%E7%8E%8B%E8%80%85.html"/>
    <id>http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-06%E8%AE%B2%E6%96%B0%E6%8A%80%E6%9C%AF%E5%B1%82%E5%87%BA%E4%B8%8D%E7%A9%B7%EF%BC%8CHDFS%E4%BE%9D%E7%84%B6%E6%98%AF%E5%AD%98%E5%82%A8%E7%9A%84%E7%8E%8B%E8%80%85.html</id>
    <published>2022-01-14T07:45:44.000Z</published>
    <updated>2022-01-14T10:25:20.363Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="06讲新技术层出不穷，HDFS依然是存储的王者"><a href="#06讲新技术层出不穷，HDFS依然是存储的王者" class="headerlink" title="06讲新技术层出不穷，HDFS依然是存储的王者"></a>06讲新技术层出不穷，HDFS依然是存储的王者</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我们知道，Google大数据“三驾马车”的第一驾是GFS（Google 文件系统），而Hadoop的第一个产品是HDFS，可以说分布式文件存储是分布式计算的基础，也可见分布式文件存储的重要性。如果我们将大数据计算比作烹饪，那么数据就是食材，而Hadoop分布式文件系统HDFS就是烧菜的那口大锅。</span><br><span class="line">这些年来，各种计算框架、各种算法、各种应用场景不断推陈出新，让人眼花缭乱，但是大数据存储的王者依然是HDFS。</span><br></pre></td></tr></table></figure><h2 id="为什么HDFS的地位如此稳固呢？"><a href="#为什么HDFS的地位如此稳固呢？" class="headerlink" title="为什么HDFS的地位如此稳固呢？"></a>为什么HDFS的地位如此稳固呢？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在整个大数据体系里面，最宝贵、最难以代替的资产就是数据，大数据所有的一切都要围绕数据展开。HDFS作为最早的大数据存储系统，存储着宝贵的数据资产，各种新的算法、框架要想得到人们的广泛使用，必须支持HDFS才能获取已经存储在里面的数据。所以大数据技术越发展，新技术越多，HDFS得到的支持越多，我们越离不开HDFS。HDFS也许不是最好的大数据存储技术，但依然最重要的大数据存储技术。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那我们就从HDFS的原理说起，今天我们来聊聊HDFS是如何实现大数据高速、可靠的存储和访问的。</span><br><span class="line">Hadoop分布式文件系统HDFS的设计目标是管理数以千计的服务器、数以万计的磁盘，将这么大规模的服务器计算资源当作一个单一的存储系统进行管理，对应用程序提供数以PB计的存储容量，让应用程序像使用普通文件系统一样存储大规模的文件数据。</span><br><span class="line">如何设计这样一个分布式文件系统？其实思路很简单。</span><br><span class="line">我们先复习一下专栏上一期，我讲了RAID磁盘阵列存储，RAID将数据分片后在多块磁盘上并发进行读写访问，从而提高了存储容量、加快了访问速度，并通过数据的冗余校验提高了数据的可靠性，即使某块磁盘损坏也不会丢失数据。将RAID的设计理念扩大到整个分布式服务器集群，就产生了分布式文件系统，Hadoop分布式文件系统的核心原理就是如此。</span><br><span class="line">和RAID在多个磁盘上进行文件存储及并行读写的思路一样，HDFS是在一个大规模分布式服务器集群上，对数据分片后进行并行读写及冗余存储。因为HDFS可以部署在一个比较大的服务器集群上，集群中所有服务器的磁盘都可供HDFS使用，所以整个HDFS的存储空间可以达到PB级容量。</span><br></pre></td></tr></table></figure><h2 id="HDFS的架构图"><a href="#HDFS的架构图" class="headerlink" title="HDFS的架构图"></a>HDFS的架构图</h2><p><a href="https://imgtu.com/i/73hClR" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/14/73hClR.png" alt="73hClR.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">上图是HDFS的架构图，从图中你可以看到HDFS的关键组件有两个，一个是DataNode，一个是NameNode。</span><br><span class="line">DataNode负责文件数据的存储和读写操作，HDFS将文件数据分割成若干数据块（Block），每个DataNode存储一部分数据块，这样文件就分布存储在整个HDFS服务器集群中。应用程序客户端（Client）可以并行对这些数据块进行访问，从而使得HDFS可以在服务器集群规模上实现数据并行访问，极大地提高了访问速度。</span><br><span class="line">在实践中，HDFS集群的DataNode服务器会有很多台，一般在几百台到几千台这样的规模，每台服务器配有数块磁盘，整个集群的存储容量大概在几PB到数百PB。</span><br><span class="line">NameNode负责整个分布式文件系统的元数据（MetaData）管理，也就是文件路径名、数据块的ID以及存储位置等信息，相当于操作系统中文件分配表（FAT）的角色。HDFS为了保证数据的高可用，会将一个数据块复制为多份（缺省情况为3份），并将多份相同的数据块存储在不同的服务器上，甚至不同的机架上。这样当有磁盘损坏，或者某个DataNode服务器宕机，甚至某个交换机宕机，导致其存储的数据块不能访问的时候，客户端会查找其备份的数据块进行访问。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面这张图是数据块多份复制存储的示意，图中对于文件&#x2F;users&#x2F;sameerp&#x2F;data&#x2F;part-0，其复制备份数设置为2，存储的BlockID分别为1、3。Block1的两个备份存储在DataNode0和DataNode2两个服务器上，Block3的两个备份存储DataNode4和DataNode6两个服务器上，上述任何一台服务器宕机后，每个数据块都至少还有一个备份存在，不会影响对文件&#x2F;users&#x2F;sameerp&#x2F;data&#x2F;part-0的访问。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/73IvuR" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/14/73IvuR.png" alt="73IvuR.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">和RAID一样，数据分成若干数据块后存储到不同服务器上，可以实现数据大容量存储，并且不同分片的数据可以并行进行读&#x2F;写操作，进而实现数据的高速访问。你可以看到，HDFS的大容量存储和高速访问相对比较容易实现</span><br></pre></td></tr></table></figure><h2 id="但是HDFS是如何保证存储的高可用性呢？"><a href="#但是HDFS是如何保证存储的高可用性呢？" class="headerlink" title="但是HDFS是如何保证存储的高可用性呢？"></a>但是HDFS是如何保证存储的高可用性呢？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  我们尝试从不同层面来讨论一下HDFS的高可用设计。</span><br><span class="line">  1.数据存储故障容错</span><br><span class="line">磁盘介质在存储过程中受环境或者老化影响，其存储的数据可能会出现错乱。HDFS的应对措施是，对于存储在DataNode上的数据块，计算并存储校验和（CheckSum）。在读取数据的时候，重新计算读取出来的数据的校验和，如果校验不正确就抛出异常，应用程序捕获异常后就到其他DataNode上读取备份数据。</span><br><span class="line">  2.磁盘故障容错</span><br><span class="line">如果DataNode监测到本机的某块磁盘损坏，就将该块磁盘上存储的所有BlockID报告给NameNode，NameNode检查这些数据块还在哪些DataNode上有备份，通知相应的DataNode服务器将对应的数据块复制到其他服务器上，以保证数据块的备份数满足要求。</span><br><span class="line">  3.DataNode故障容错</span><br><span class="line">DataNode会通过心跳和NameNode保持通信，如果DataNode超时未发送心跳，NameNode就会认为这个DataNode已经宕机失效，立即查找这个DataNode上存储的数据块有哪些，以及这些数据块还存储在哪些服务器上，随后通知这些服务器再复制一份数据块到其他服务器上，保证HDFS存储的数据块备份数符合用户设置的数目，即使再出现服务器宕机，也不会丢失数据。</span><br><span class="line">  4.NameNode故障容错</span><br><span class="line">NameNode是整个HDFS的核心，记录着HDFS文件分配表信息，所有的文件路径和数据块存储信息都保存在NameNode，如果NameNode故障，整个HDFS系统集群都无法使用；如果NameNode上记录的数据丢失，整个集群所有DataNode存储的数据也就没用了。所以，NameNode高可用容错能力非常重要。NameNode采用主从热备的方式提供高可用服务，请看下图。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/73THl4" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/14/73THl4.png" alt="73THl4.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">集群部署两台NameNode服务器，一台作为主服务器提供服务，一台作为从服务器进行热备，两台服务器通过ZooKeeper选举，主要是通过争夺znode锁资源，决定谁是主服务器。而DataNode则会向两个NameNode同时发送心跳数据，但是只有主NameNode才能向DataNode返回控制信息。</span><br><span class="line">正常运行期间，主从NameNode之间通过一个共享存储系统shared edits来同步文件系统的元数据信息。当主NameNode服务器宕机，从NameNode会通过ZooKeeper升级成为主服务器，并保证HDFS集群的元数据信息，也就是文件分配表信息完整一致。</span><br><span class="line">对于一个软件系统而言，性能差一点，用户也许可以接受；使用体验差，也许也能忍受。但是如果可用性差，经常出故障导致不可用，那就比较麻烦了；如果出现重要数据丢失，那开发工程师绝对是摊上大事了。</span><br><span class="line">而分布式系统可能出故障地方又非常多，内存、CPU、主板、磁盘会损坏，服务器会宕机，网络会中断，机房会停电，所有这些都可能会引起软件系统的不可用，甚至数据永久丢失。所以在设计分布式系统的时候，软件工程师一定要绷紧可用性这根弦，思考在各种可能的故障情况下，如何保证整个软件系统依然是可用的。</span><br><span class="line">根据我的经验，一般说来，常用的保证系统可用性的策略有冗余备份、失效转移和降级限流。虽然这3种策略你可能早已耳熟能详，但还是有一些容易被忽略的地方。</span><br><span class="line">比如冗余备份，任何程序、任何数据，都至少要有一个备份，也就是说程序至少要部署到两台服务器，数据至少要备份到另一台服务器上。此外，稍有规模的互联网企业都会建设多个数据中心，数据中心之间互相进行备份，用户请求可能会被分发到任何一个数据中心，即所谓的异地多活，在遭遇地域性的重大故障和自然灾害的时候，依然保证应用的高可用。</span><br><span class="line">当要访问的程序或者数据无法访问时，需要将访问请求转移到备份的程序或者数据所在的服务器上，这也就是失效转移。失效转移你应该注意的是失效的鉴定，像NameNode这样主从服务器管理同一份数据的场景，如果从服务器错误地以为主服务器宕机而接管集群管理，会出现主从服务器一起对DataNode发送指令，进而导致集群混乱，也就是所谓的“脑裂”。这也是这类场景选举主服务器时，引入ZooKeeper的原因。ZooKeeper的工作原理，我将会在后面专门分析。</span><br><span class="line">当大量的用户请求或者数据处理请求到达的时候，由于计算资源有限，可能无法处理如此大量的请求，进而导致资源耗尽，系统崩溃。这种情况下，可以拒绝部分请求，即进行限流；也可以关闭部分功能，降低资源消耗，即进行降级。限流是互联网应用的常备功能，因为超出负载能力的访问流量在何时会突然到来，你根本无法预料，所以必须提前做好准备，当遇到突发高峰流量时，就可以立即启动限流。而降级通常是为可预知的场景准备的，比如电商的“双十一”促销，为了保障促销活动期间应用的核心功能能够正常运行，比如下单功能，可以对系统进行降级处理，关闭部分非重要功能，比如商品评价功能。</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><h3 id="我们小结一下，看看HDFS是如何通过大规模分布式服务器集群实现数据的大容量、高速、可靠存储、访问的。"><a href="#我们小结一下，看看HDFS是如何通过大规模分布式服务器集群实现数据的大容量、高速、可靠存储、访问的。" class="headerlink" title="我们小结一下，看看HDFS是如何通过大规模分布式服务器集群实现数据的大容量、高速、可靠存储、访问的。"></a>我们小结一下，看看HDFS是如何通过大规模分布式服务器集群实现数据的大容量、高速、可靠存储、访问的。</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.文件数据以数据块的方式进行切分，数据块可以存储在集群任意DataNode服务器上，所以HDFS存储的文件可以非常大，一个文件理论上可以占据整个HDFS服务器集群上的所有磁盘，实现了大容量存储。</span><br><span class="line">2.HDFS一般的访问模式是通过MapReduce程序在计算时读取，MapReduce对输入数据进行分片读取，通常一个分片就是一个数据块，每个数据块分配一个计算进程，这样就可以同时启动很多进程对一个HDFS文件的多个数据块进行并发访问，从而实现数据的高速访问。关于MapReduce的具体处理过程，我们会在专栏后面详细讨论。</span><br><span class="line">3.DataNode存储的数据块会进行复制，使每个数据块在集群里有多个备份，保证了数据的可靠性，并通过一系列的故障容错手段实现HDFS系统中主要组件的高可用，进而保证数据和整个系统的高可用。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="从零开始学大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>从零开始学大数据-05讲从RAID看垂直伸缩到水平伸缩的演化</title>
    <link href="http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-05%E8%AE%B2%E4%BB%8ERAID%E7%9C%8B%E5%9E%82%E7%9B%B4%E4%BC%B8%E7%BC%A9%E5%88%B0%E6%B0%B4%E5%B9%B3%E4%BC%B8%E7%BC%A9%E7%9A%84%E6%BC%94%E5%8C%96.html"/>
    <id>http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-05%E8%AE%B2%E4%BB%8ERAID%E7%9C%8B%E5%9E%82%E7%9B%B4%E4%BC%B8%E7%BC%A9%E5%88%B0%E6%B0%B4%E5%B9%B3%E4%BC%B8%E7%BC%A9%E7%9A%84%E6%BC%94%E5%8C%96.html</id>
    <published>2022-01-14T03:47:22.000Z</published>
    <updated>2022-01-14T05:33:41.826Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="05讲从RAID看垂直伸缩到水平伸缩的演化"><a href="#05讲从RAID看垂直伸缩到水平伸缩的演化" class="headerlink" title="05讲从RAID看垂直伸缩到水平伸缩的演化"></a>05讲从RAID看垂直伸缩到水平伸缩的演化</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">大数据技术主要是要解决大规模数据的计算处理问题，但是我们要想对数据进行计算，首先要解决的其实是大规模数据的存储问题。</span><br></pre></td></tr></table></figure><h2 id="如果一个文件的大小超过了一张磁盘的大小，你该如何存储？"><a href="#如果一个文件的大小超过了一张磁盘的大小，你该如何存储？" class="headerlink" title="如果一个文件的大小超过了一张磁盘的大小，你该如何存储？"></a>如果一个文件的大小超过了一张磁盘的大小，你该如何存储？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我的答案是，单机时代，主要的解决方案是RAID；分布式时代，主要解决方案是分布式文件系统。</span><br></pre></td></tr></table></figure><h2 id="其实不论是在单机时代还是分布式时代，大规模数据存储都需要解决几个核心问题，这些问题都是什么呢？总结一下，主要有以下三个方面。"><a href="#其实不论是在单机时代还是分布式时代，大规模数据存储都需要解决几个核心问题，这些问题都是什么呢？总结一下，主要有以下三个方面。" class="headerlink" title="其实不论是在单机时代还是分布式时代，大规模数据存储都需要解决几个核心问题，这些问题都是什么呢？总结一下，主要有以下三个方面。"></a>其实不论是在单机时代还是分布式时代，大规模数据存储都需要解决几个核心问题，这些问题都是什么呢？总结一下，主要有以下三个方面。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.数据存储容量的问题。既然大数据要解决的是数以PB计的数据计算问题，而一般的服务器磁盘容量通常1～2TB，那么如何存储这么大规模的数据呢？</span><br><span class="line">2.数据读写速度的问题。一般磁盘的连续读写速度为几十MB，以这样的速度，几十PB的数据恐怕要读写到天荒地老。</span><br><span class="line">3.数据可靠性的问题。磁盘大约是计算机设备中最易损坏的硬件了，通常情况一块磁盘使用寿命大概是一年，如果磁盘损坏了，数据怎么办？</span><br><span class="line"></span><br><span class="line">   在大数据技术出现之前，我们就需要面对这些关于存储的问题，对应的解决方案就是RAID技术。今天我们就先从RAID开始，一起看看大规模数据存储方式的演化过程。</span><br><span class="line">   RAID（独立磁盘冗余阵列）技术是将多块普通磁盘组成一个阵列，共同对外提供服务。主要是为了改善磁盘的存储容量、读写速度，增强磁盘的可用性和容错能力。在RAID之前，要使用大容量、高可用、高速访问的存储系统需要专门的存储设备，这类设备价格要比RAID的几块普通磁盘贵几十倍。RAID刚出来的时候给我们的感觉像是一种黑科技，但其原理却不复杂。</span><br><span class="line">   目前服务器级别的计算机都支持插入多块磁盘（8块或者更多），通过使用RAID技术，实现数据在多块磁盘上的并发读写和数据备份。常用RAID技术有图中下面这几种，光看图片你可能觉得它们都差不多，下面我给你讲讲它们之间的区别。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/71IVVf" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/14/71IVVf.png" alt="71IVVf.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">首先，我们先假设服务器有N块磁盘，RAID 0是数据在从内存缓冲区写入磁盘时，根据磁盘数量将数据分成N份，这些数据同时并发写入N块磁盘，使得数据整体写入速度是一块磁盘的N倍；读取的时候也一样，因此RAID 0具有极快的数据读写速度。但是RAID 0不做数据备份，N块磁盘中只要有一块损坏，数据完整性就被破坏，其他磁盘的数据也都无法使用了。</span><br><span class="line">RAID 1是数据在写入磁盘时，将一份数据同时写入两块磁盘，这样任何一块磁盘损坏都不会导致数据丢失，插入一块新磁盘就可以通过复制数据的方式自动修复，具有极高的可靠性。</span><br><span class="line">结合RAID 0和RAID 1两种方案构成了RAID 10，它是将所有磁盘N平均分成两份，数据同时在两份磁盘写入，相当于RAID 1；但是平分成两份，在每一份磁盘（也就是N&#x2F;2块磁盘）里面，利用RAID 0技术并发读写，这样既提高可靠性又改善性能。不过RAID 10的磁盘利用率较低，有一半的磁盘用来写备份数据。</span><br><span class="line">一般情况下，一台服务器上很少出现同时损坏两块磁盘的情况，在只损坏一块磁盘的情况下，如果能利用其他磁盘的数据恢复损坏磁盘的数据，这样在保证可靠性和性能的同时，磁盘利用率也得到大幅提升。</span><br><span class="line">顺着这个思路，RAID 3可以在数据写入磁盘的时候，将数据分成N-1份，并发写入N-1块磁盘，并在第N块磁盘记录校验数据，这样任何一块磁盘损坏（包括校验数据磁盘），都可以利用其他N-1块磁盘的数据修复。</span><br><span class="line">但是在数据修改较多的场景中，任何磁盘数据的修改，都会导致第N块磁盘重写校验数据。频繁写入的后果是第N块磁盘比其他磁盘更容易损坏，需要频繁更换，所以RAID 3很少在实践中使用，因此在上面图中也就没有单独列出。</span><br><span class="line">相比RAID 3，RAID 5是使用更多的方案。RAID 5和RAID 3很相似，但是校验数据不是写入第N块磁盘，而是螺旋式地写入所有磁盘中。这样校验数据的修改也被平均到所有磁盘上，避免RAID 3频繁写坏一块磁盘的情况。</span><br><span class="line">如果数据需要很高的可靠性，在出现同时损坏两块磁盘的情况下（或者运维管理水平比较落后，坏了一块磁盘但是迟迟没有更换，导致又坏了一块磁盘），仍然需要修复数据，这时候可以使用RAID 6。</span><br><span class="line">RAID 6和RAID 5类似，但是数据只写入N-2块磁盘，并螺旋式地在两块磁盘中写入校验信息（使用不同算法生成）。</span><br><span class="line">从下面表格中你可以看到在相同磁盘数目（N）的情况下，各种RAID技术的比较。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/71Lx9f" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/14/71Lx9f.png" alt="71Lx9f.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   RAID技术有硬件实现，比如专用的RAID卡或者主板直接支持；也可以通过软件实现，在操作系统层面将多块磁盘组成RAID，从逻辑上视作一个访问目录。RAID技术在传统关系数据库及文件系统中应用比较广泛，是改善计算机存储特性的重要手段。</span><br><span class="line">现在我来总结一下，看看RAID是如何解决我一开始提出的，关于存储的三个关键问题。</span><br><span class="line">  1.数据存储容量的问题。RAID使用了N块磁盘构成一个存储阵列，如果使用RAID 5，数据就可以存储在N-1块磁盘上，这样将存储空间扩大了N-1倍。</span><br><span class="line">  2.数据读写速度的问题。RAID根据可以使用的磁盘数量，将待写入的数据分成多片，并发同时向多块磁盘进行写入，显然写入的速度可以得到明显提高；同理，读取速度也可以得到明显提高。不过，需要注意的是，由于传统机械磁盘的访问延迟主要来自于寻址时间，数据真正进行读写的时间可能只占据整个数据访问时间的一小部分，所以数据分片后对N块磁盘进行并发读写操作并不能将访问速度提高N倍。</span><br><span class="line">  3.数据可靠性的问题。使用RAID 10、RAID 5或者RAID 6方案的时候，由于数据有冗余存储，或者存储校验信息，所以当某块磁盘损坏的时候，可以通过其他磁盘上的数据和校验数据将丢失磁盘上的数据还原。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我们对更强计算能力和更大规模数据存储的追求几乎是没有止境的，这似乎是源于人类的天性。神话里人类试图建立一座通天塔到神居住的地方，就是这种追求的体现。</span><br><span class="line">我在上一期提到过，在计算机领域，实现更强的计算能力和更大规模的数据存储有两种思路，一种是升级计算机，一种是用分布式系统。前一种也被称作“垂直伸缩”（scaling up），通过升级CPU、内存、磁盘等将一台计算机变得更强大；后一种是“水平伸缩”（scaling out），添加更多的计算机到系统中，从而实现更强大的计算能力。</span><br></pre></td></tr></table></figure><h2 id="垂直伸缩VS水平伸缩"><a href="#垂直伸缩VS水平伸缩" class="headerlink" title="垂直伸缩VS水平伸缩"></a>垂直伸缩VS水平伸缩</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在计算机发展的早期，我们获得更强大计算能力的手段主要依靠垂直伸缩。一方面拜摩尔定律所赐，每18个月计算机的处理能力提升一倍；另一方面由于不断研究新的计算机体系结构，小型机、中型机、大型机、超级计算机，不断刷新我们的认知。</span><br><span class="line">但是到了互联网时代，这种垂直伸缩的路子走不通了，一方面是成本问题，互联网公司面对巨大的不确定性市场，无法为一个潜在的需要巨大计算资源的产品一下投入很多钱去购买大型计算机；另一方面，对于Google这样的公司和产品而言，即使是世界上最强大的超级计算机也无法满足其对计算资源的需求。</span><br><span class="line">所以互联网公司走向了一条新的道路：水平伸缩，在一个系统中不断添加计算机，以满足不断增长的用户和数据对计算资源的需求。这就是最近十几年引导技术潮流的分布式与大数据技术。</span><br><span class="line">RAID可以看作是一种垂直伸缩，一台计算机集成更多的磁盘实现数据更大规模、更安全可靠的存储以及更快的访问速度。而HDFS则是水平伸缩，通过添加更多的服务器实现数据更大、更快、更安全存储与访问。</span><br><span class="line">RAID技术只是在单台服务器的多块磁盘上组成阵列，大数据需要更大规模的存储空间和更快的访问速度。将RAID思想原理应用到分布式服务器集群上，就形成了Hadoop分布式文件系统HDFS的架构思想。</span><br><span class="line">垂直伸缩总有尽头，水平伸缩理论上是没有止境的，在实践中，数万台服务器的HDFS集群已经出现，我会在下一期谈谈HDFS的架构。</span><br></pre></td></tr></table></figure><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">传统机械磁盘进行数据连续写入的时候，比如磁盘以日志格式连续写入操作，其写入速度远远大于磁盘随机写入的速度，比如关系数据库连续更新若干条数据记录，你知道这是为什么吗？</span><br><span class="line">  1.磁盘的读写过程，最消耗时间的地方就是在磁盘中磁道寻址的过程，而一旦寻址完成，写入数据的速度很快。</span><br><span class="line">顺序写入只要一次寻址操作，而随机写入要多次寻址操作。所以顺序写入速度明显高于随机写入。</span><br><span class="line">  2.连续写入：写入只寻址一次 存储位置与逻辑位置相邻 不用多次寻址</span><br><span class="line">    随机写入：每写一次 便寻址一次 增加了磁盘的寻址时间</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="从零开始学大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>从零开始学大数据-04讲移动计算比移动数据更划算</title>
    <link href="http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-04%E8%AE%B2%E7%A7%BB%E5%8A%A8%E8%AE%A1%E7%AE%97%E6%AF%94%E7%A7%BB%E5%8A%A8%E6%95%B0%E6%8D%AE%E6%9B%B4%E5%88%92%E7%AE%97.html"/>
    <id>http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-04%E8%AE%B2%E7%A7%BB%E5%8A%A8%E8%AE%A1%E7%AE%97%E6%AF%94%E7%A7%BB%E5%8A%A8%E6%95%B0%E6%8D%AE%E6%9B%B4%E5%88%92%E7%AE%97.html</id>
    <published>2022-01-13T13:39:25.000Z</published>
    <updated>2022-01-13T14:39:09.128Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="04讲移动计算比移动数据更划算"><a href="#04讲移动计算比移动数据更划算" class="headerlink" title="04讲移动计算比移动数据更划算"></a>04讲移动计算比移动数据更划算</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">大数据技术和传统的软件开发技术在架构思路上有很大不同，大数据技术更为关注数据，所以相关的架构设计也围绕数据展开，如何存储、计算、传输大规模的数据是要考虑的核心要素。</span><br><span class="line">传统的软件计算处理模型，都是“输入 -&gt; 计算 -&gt; 输出”模型。也就是说，一个程序给它传入一些数据也好，它自己从某个地方读取一些数据也好，总是先有一些输入数据，然后对这些数据进行计算处理，最后得到输出结果。</span><br><span class="line">但是在互联网大数据时代，需要计算处理的数据量急速膨胀。一来是因为互联网用户数远远超过传统企业的用户，相应产生了更大量的数据；二来很多以往被忽视的数据重新被发掘利用，比如用户在一个页面的停留时长、鼠标在屏幕移动的轨迹都会被记录下来进行分析。在稍微大一点的互联网企业，需要计算处理的数据量常常以PB计（10^15 Byte）。</span><br><span class="line">正因为如此，传统的计算处理模型不能适用于大数据时代的计算要求。你能想象一个程序读取PB级的数据进行计算是怎样一个场景吗？一个程序所能调度的网络带宽（通常数百MB）、内存容量（通常几十GB ）、磁盘大小（通常数TB）、CPU运算速度是不可能满足这种计算要求的。</span><br></pre></td></tr></table></figure><h2 id="那么如何解决PB级数据进行计算的问题呢？"><a href="#那么如何解决PB级数据进行计算的问题呢？" class="headerlink" title="那么如何解决PB级数据进行计算的问题呢？"></a>那么如何解决PB级数据进行计算的问题呢？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个问题的解决思路其实跟大型网站的分布式架构思路是一样的，采用分布式集群的解决方案，用数千台甚至上万台计算机构建一个大数据计算处理集群，利用更多的网络带宽、内存空间、磁盘容量、CPU核心数去进行计算处理。关于分布式架构，你可以参考我写的《大型网站技术架构：核心原理与案例分析》这本书，但是大数据计算处理的场景跟网站的实时请求处理场景又有很大不同。</span><br><span class="line">网站实时处理通常针对单个用户的请求操作，虽然大型网站面临大量的高并发请求，比如天猫的“双十一”活动。但是每个用户之间的请求是独立的，只要网站的分布式系统能将不同用户的不同业务请求分配到不同的服务器上，只要这些分布式的服务器之间耦合关系(事物之间存在的相互作用、相互影响的关系)足够小，就可以通过添加更多的服务器去处理更多的用户请求及由此产生的用户数据。这也正是网站系统架构的核心原理。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">大数据计算处理通常针对的是网站的存量数据，也就是刚才我提到的全部用户在一段时间内请求产生的数据，这些数据之间是有大量关联的，比如购买同一个商品用户之间的关系，这是使用协同过滤进行商品推荐；比如同一件商品的历史销量走势，这是对历史数据进行统计分析。网站大数据系统要做的就是将这些统计规律和关联关系计算出来，并由此进一步改善网站的用户体验和运营决策。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">为了解决这种计算场景的问题，技术专家们设计了一套相应的技术架构方案。最早的时候由Google实现并通过论文的方式发表出来，随后根据这些论文，开源社区开发出对应的开源产品，并得到业界的普遍支持和应用。</span><br><span class="line">这套方案的核心思路是，既然数据是庞大的，而程序要比数据小得多，将数据输入给程序是不划算的，那么就反其道而行之，将程序分发到数据所在的地方进行计算，也就是所谓的移动计算比移动数据更划算。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">有一句古老的谚语，说的是“当一匹马拉不动车的时候，用两匹马拉”。听起来是如此简单的道理，但是在计算机这个最年轻的科技领域，在很长一段时间里却并没有这样做。当一台计算机的处理能力不能满足计算要求的时候，我们并没有想办法用两台计算机去处理，而是换更强大的计算机。商业级的服务器不够用，就升级小型机；小型机不够用，就升级中型机；还不够，升级大型机，升级超级计算机。</span><br><span class="line">在互联网时代之前，这种不断升级计算机硬件的办法还是行得通的，凭借摩尔定律，计算机硬件的处理能力每18个月增强一倍，越来越强大的计算机被制造出来。传统企业虽然对计算机的处理需求越来越高，但是工程师和科学家总能制造出满足需求的计算机。</span><br><span class="line">但是这种思路并不适合互联网的技术要求。Google、Facebook、阿里巴巴这些网站每天需要处理数十亿次的用户请求、产生上百PB的数据，不可能有一台计算机能够支撑起这么大的计算需求。</span><br><span class="line">于是互联网公司不得不换一种思路解决问题，当一台计算机的计算能力不能满足需求的时候，就增加一台计算机，还不够的话，就再增加一台。就这样，由一台计算机起家的小网站，逐渐成长为百万台服务器的巨无霸。Google、Facebook、阿里巴巴这些公司的成长过程都是如此。</span><br><span class="line">但是买一台新计算机和一台老计算机放在一起，就能自己开始工作了吗？两台计算机要想合作构成一个系统，必须要在技术上重新架构。这就是现在互联网企业广泛使用的负载均衡、分布式缓存、分布式数据库、分布式服务等种种分布式系统。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当这些分布式技术满足互联网的日常业务需求时，对离线数据和存量数据的处理就被提了出来，当时这些分布式技术并不能满足要求，于是大数据技术就出现了。</span><br></pre></td></tr></table></figure><h2 id="现在我们来看，移动计算程序到数据所在位置进行计算是如何实现的呢？"><a href="#现在我们来看，移动计算程序到数据所在位置进行计算是如何实现的呢？" class="headerlink" title="现在我们来看，移动计算程序到数据所在位置进行计算是如何实现的呢？"></a>现在我们来看，移动计算程序到数据所在位置进行计算是如何实现的呢？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> 1.将待处理的大规模数据存储在服务器集群的所有服务器上，主要使用HDFS分布式文件存储系统，将文件分成很多块（Block），以块为单位存储在集群的服务器上。</span><br><span class="line"> 2.大数据引擎根据集群里不同服务器的计算能力，在每台服务器上启动若干分布式任务执行进程，这些进程会等待给它们分配执行任务。</span><br><span class="line"> 3.使用大数据计算框架支持的编程模型进行编程，比如Hadoop的MapReduce编程模型，或者Spark的RDD编程模型。应用程序编写好以后，将其打包，MapReduce和Spark都是在JVM环境中运行，所以打包出来的是一个Java的JAR包。</span><br><span class="line"> 4.用Hadoop或者Spark的启动命令执行这个应用程序的JAR包，首先执行引擎会解析程序要处理的数据输入路径，根据输入数据量的大小，将数据分成若干片（Split），每一个数据片都分配给一个任务执行进程去处理。</span><br><span class="line"> 5.任务执行进程收到分配的任务后，检查自己是否有任务对应的程序包，如果没有就去下载程序包，下载以后通过反射的方式加载程序。走到这里，最重要的一步，也就是移动计算就完成了。</span><br><span class="line"> 6.加载程序后，任务执行进程根据分配的数据片的文件地址和数据在文件内的偏移量读取数据，并把数据输入给应用程序相应的方法去执行，从而实现在分布式服务器集群中移动计算程序，对大规模数据进行并行处理的计算目标。</span><br><span class="line"></span><br><span class="line">这只是大数据计算实现过程的简单描述，具体过程我们会在讲到HDFS、MapReduce和Spark的时候详细讨论。</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">移动程序到数据所在的地方去执行，这种技术方案其实我们并不陌生。从事Java开发的同学可能有过用反射的方式热加载代码执行的经验，如果这个代码是从网络其他地方传输过来的，那就是在移动计算。杀毒软件从服务器更新病毒库，然后在Windows内查杀病毒，也是一种移动计算（病毒库）比移动数据（Windows可能感染病毒的程序）更划算的例子。</span><br><span class="line">大数据技术将移动计算这一编程技巧上升到编程模型的高度，并开发了相应的编程框架，使得开发人员只需要关注大数据的算法实现，而不必关注如何将这个算法在分布式的环境中执行，这极大地简化了大数据的开发难度，并统一了大数据的开发方式，从而使大数据从原来的高高在上，变成了今天的人人参与。</span><br></pre></td></tr></table></figure><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">互联网应用系统架构中有一种重要架构原则(分布式架构的原则)是尽量使用无状态的服务，不同服务实例之间不共享状态，也就是不持有数据，用户请求交给任何一个服务实例计算，处理的结果都是一样的，为什么要这样设计？这种架构有什么好处？</span><br><span class="line">1.这个应该是分布式架构的设计者在考虑架构的可扩展行（伸缩性）的时候设计出来的这样一个针对于服务的一个要求或者是标准（也就是原则）</span><br><span class="line">2.无状态服务的主要好处是服务间无需同步状态或者数据，便于扩缩容。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="从零开始学大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>从零开始学大数据-03讲大数据应用领域：数据驱动一切</title>
    <link href="http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-03%E8%AE%B2%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E4%B8%80%E5%88%87.html"/>
    <id>http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-03%E8%AE%B2%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E4%B8%80%E5%88%87.html</id>
    <published>2022-01-13T10:12:08.000Z</published>
    <updated>2022-01-13T13:39:23.058Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="03讲大数据应用领域：数据驱动一切"><a href="#03讲大数据应用领域：数据驱动一切" class="headerlink" title="03讲大数据应用领域：数据驱动一切"></a>03讲大数据应用领域：数据驱动一切</h1><h2 id="大数据在医疗健康领域的应用"><a href="#大数据在医疗健康领域的应用" class="headerlink" title="大数据在医疗健康领域的应用"></a>大数据在医疗健康领域的应用</h2><h3 id="医学影像智能识别"><a href="#医学影像智能识别" class="headerlink" title="医学影像智能识别"></a>医学影像智能识别</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">图像识别是机器学习获得的重大突破之一，使用大量的图片数据进行深度机器学习训练，机器可以识别出特定的图像元素，比如猫或者人脸，当然也可以识别出病理特征。</span><br><span class="line">可以说医学影像智能识别在某些方面已经比一般医生拥有更高的读图和识别能力，但是鉴于医疗的严肃性，现在还很少有临床方面的实践。</span><br><span class="line">虽然在临床实践方面应用有限，但是医疗影像AI还是在一些领域取得一定的进展。医学影像智能识别，一方面可以帮助医生进行辅助诊疗，另一方面对于皮肤病等有外部表现的病症，病人可以自己拍照然后使用AI智能识别做一个初步诊断。</span><br></pre></td></tr></table></figure><h3 id="病历大数据智能诊疗"><a href="#病历大数据智能诊疗" class="headerlink" title="病历大数据智能诊疗"></a>病历大数据智能诊疗</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">病历，特别是专家写的病历，本身就是一笔巨大的知识财富，利用大数据技术将这些知识进行处理、分析、统计、 挖掘，可以构成一个病历知识库，可以分享给更多人，即构成一个智能辅助诊疗系统。</span><br></pre></td></tr></table></figure><h2 id="大数据在教育领域的应用"><a href="#大数据在教育领域的应用" class="headerlink" title="大数据在教育领域的应用"></a>大数据在教育领域的应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">教育倡导“因人施教”，但是在传统教育过程中要做到因人施教，需要老师本身能力很强才能把握好。但是大数据在线教育利用大数据技术进行分析统计，完全可以做到根据学生能力和学习节奏，及时调整学习大纲和学习进度，提供个性化和自适应的学习体验。除此之外，人工智能在教育的其他方面也取得很好的进展。</span><br></pre></td></tr></table></figure><h3 id="AI外语老师"><a href="#AI外语老师" class="headerlink" title="AI外语老师"></a>AI外语老师</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">得益于语音识别和语音合成技术的成熟（语音识别与合成技术同样是利用大数据技术进行机器学习与训练），一些在线教育网站尝试用人工智能外语老师进行外语教学。这里面的原理其实并不复杂，聊天机器人技术已经普遍应用，只要将学习的知识点设计进聊天的过程中，就可以实现一个简单的AI外语老师了。</span><br></pre></td></tr></table></figure><h3 id="智能解题"><a href="#智能解题" class="headerlink" title="智能解题"></a>智能解题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">比较简单的智能解题系统其实是利用搜索引擎技术，在收集大量的试题以及答案的基础上，进行试题匹配，将匹配成功的答案返回。这个过程看起来就像智能做题一样，表面看给个题目就能解出答案，而实际上只是找到答案。</span><br><span class="line">进阶一点的智能解题系统，通过图像识别与自然语言处理（这两项技术依然使用大数据技术实现），进行相似性匹配。更改试题的部分数字、文字表述，但是不影响实质性解答思路，依然可以解答。</span><br><span class="line">高阶的智能解题系统，利用神经网络机器学习技术，将试题的自然语言描述转化成形式语言，然后分析知识点和解题策略，进行自动推导，从而完成实质性的解题。</span><br></pre></td></tr></table></figure><h2 id="大数据在社交媒体领域的应用"><a href="#大数据在社交媒体领域的应用" class="headerlink" title="大数据在社交媒体领域的应用"></a>大数据在社交媒体领域的应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大数据有一个重要的、和我们大多数人密切相关，但是又不太引人注目的一个应用领域是舆情监控与分析。我们日常在各种互联网应用和社交媒体上发表各种言论，这些言论事实上反映了最准确的民情舆论。一个个体的言论基本没有意义，但是大量的、全国乃至全球的言论数据表现出的统计特性，就有了非常重要的意义。</span><br><span class="line">编写数据爬虫，实时爬取各个社交新媒体上的各种用户内容和媒体信息，然后通过自然语言处理，就可以进行情感分析、热点事件追踪等。舆情实时监控可用于商业领域，引导智能广告投放；可用于金融领域，辅助执行自动化股票、期权、数字货币交易；可用于社会管理，及时发现可能引发社会问题的舆论倾向。</span><br><span class="line">在美国总统大选期间，候选人就曾雇佣大数据公司利用社交媒体的数据进行分析，发现选票可能摇摆的地区，有针对性前去进行竞选演讲。并利用大数据分析选民关注的话题，包装自己的竞选主张。</span><br></pre></td></tr></table></figure><h2 id="大数据在金融领域的应用"><a href="#大数据在金融领域的应用" class="headerlink" title="大数据在金融领域的应用"></a>大数据在金融领域的应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">大数据在金融领域应用比较成熟的是大数据风控。在金融借贷中，如何识别出高风险用户，要求其提供更多抵押、支付更高利息、调整更低的额度，甚至拒绝贷款，从而降低金融机构的风险？事实上，金融行业已经沉淀了大量的历史数据，利用这些数据进行计算，可以得到用户特征和风险指数的曲线（即风控模型）。当新用户申请贷款的时候，将该用户特征带入曲线进行计算，就可以得到该用户的风险指数，进而自动给出该用户的贷款策略。</span><br><span class="line">利用股票、外汇等历史交易记录，分析交易规律，结合当前的新闻热点、舆论倾向、财经数据构建交易模型，进行自动化交易，这就是金融领域的量化交易。这些数据量特别巨大，交易涉及金额也同样巨大，所以金融机构在大数据领域常常不惜血本，大手笔投入。</span><br></pre></td></tr></table></figure><h2 id="大数据在新零售领域的应用"><a href="#大数据在新零售领域的应用" class="headerlink" title="大数据在新零售领域的应用"></a>大数据在新零售领域的应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">区别于传统零售，新零售使用大数据进行全链路管理。从生产、物流、购物体验，使用大数据进行分析和预判，实现精准生产、零库存、全新的购物体验。</span><br><span class="line">亚马逊Go无人店使用大量的摄像头，实时捕捉用户行为，判断用户取出还是放回商品、取了何种商品等。这实际上是大数据流计算与机器学习的结合，最终实现的购物效果是，无需排队买单，进去就拿东西，拿好了就走，超级科幻有没有。</span><br><span class="line">虽然无人店现在看起来噱头的意味更多一点，但是利用大数据技术提升购物体验、节省商家人力成本一定是正确的方向。</span><br></pre></td></tr></table></figure><h2 id="大数据在交通领域的应用"><a href="#大数据在交通领域的应用" class="headerlink" title="大数据在交通领域的应用"></a>大数据在交通领域的应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">交通也是一个对大数据实时采集与处理应用比较广的领域。现在几乎所有的城市路段、交通要点都有不止一个监控摄像头在实时监控，一线城市大约有百万计的摄像头在不停地采集数据。这些数据一方面可以用于公共安全，比如近年来一些警匪片里会有一些场景：犯罪嫌疑人驾车出逃，警方只要定位了车辆，不管它到哪里，系统都可以自动调出相应的摄像头，实时看到现场画面。应该说这项技术已经成熟，大数据流计算可以对百万计的流数据实时处理计算，电影里的场景计算其实并不复杂。</span><br><span class="line">此外，各种导航软件也在不停采集数据，通过分析用户当前位置和移动速度，判断道路拥堵状态，并实时修改推荐的导航路径。</span><br><span class="line">还有就是无人驾驶技术，无人驾驶就是在人的驾驶过程中实时采集车辆周边数据和驾驶控制信息，然后通过机器学习，获得周边信息与驾驶方式的对应关系（自动驾驶模型）。然后将这个模型应用到无人驾驶汽车上，传感器获得车辆周边数据后，就可以通过自动驾驶模型计算出车辆控制信息（转向、刹车等）。计算自动驾驶模型需要大量的数据，所以我们看到，这些无人驾驶创业公司都在不断攀比自己的训练数据有几十万公里、几百万公里，因为训练数据的量意味着模型的完善程度。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">利用大数据和机器学习，发掘数据中的规律，进而对当前的事情做出预测和判断，使机器表现出智能的特性，正变得越来越普及。</span><br><span class="line">大数据主要来自企业自身所产生，还有一些数据来自互联网，通过网络爬虫可以获取；再有就是公共数据，比如气象数据等。所有这些数据汇聚在一起，计算其内在的关系，可以发现很多肉眼和思维无法得到的知识。然后进一步计算其内在的模型，可以使系统获得智能的特性。当系统具备智能的特性，可以使机器对当前的事情做出预测和判断，正如我今天和你聊的，大数据技术应用正变得越来越普及。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="从零开始学大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>从零开始学大数据-02 大数据应用发展史: 从搜索引擎到人工智能</title>
    <link href="http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-02-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8%E5%8F%91%E5%B1%95%E5%8F%B2-%E4%BB%8E%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%88%B0%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.html"/>
    <id>http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-02-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8%E5%8F%91%E5%B1%95%E5%8F%B2-%E4%BB%8E%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%88%B0%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD.html</id>
    <published>2022-01-12T14:51:09.000Z</published>
    <updated>2022-01-12T15:57:44.184Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="02-大数据应用发展史-从搜索引擎到人工智能"><a href="#02-大数据应用发展史-从搜索引擎到人工智能" class="headerlink" title="02 大数据应用发展史: 从搜索引擎到人工智能"></a>02 大数据应用发展史: 从搜索引擎到人工智能</h1><h2 id="大数据应用的搜索引擎时代"><a href="#大数据应用的搜索引擎时代" class="headerlink" title="大数据应用的搜索引擎时代"></a>大数据应用的搜索引擎时代</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Google公认的大数据鼻祖，存储着全世界大量的网页，大约需要数万块磁盘----&gt;GFS，将数千台服务器上的数万块磁盘统一管理起来，然后当作一个文件系统，统一存储所有这些网页文件。----&gt;构建搜索引擎，需要对这数万块磁盘上的文件中的单词进行词频统计，然后根据PageRank算法计算网页排名。这中间Google需要对磁盘上的文件进行计算处理----&gt;MapReduce大数据计算框架应运而生</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Google之前，世界闻名的搜索引擎是yahoo。Google凭借自己的大数据技术和pagerank算法，使搜索引擎体验得到质的飞跃，yahoo没落。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Doug cutting率先根据论文做了Hadoop。yahoo挖Doug Cutting----&gt;Doug Cutting跳槽到专职做Hadoop的商业化公司Cloudera.</span><br></pre></td></tr></table></figure><h2 id="大数据应用的数据仓库时代"><a href="#大数据应用的数据仓库时代" class="headerlink" title="大数据应用的数据仓库时代"></a>大数据应用的数据仓库时代</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当Facebook推出Hive的时候，嗅觉敏感的科技公司都不淡定了，他们开始意识到，大数据的时代真正开启了。</span><br><span class="line">曾经进行数据分析和统计，仅仅局限于数据库，在数据库的计算环境中对数据库中的数据表进行统计分析。受数据量和计算能力的限制，只能对最重要的数据进行统计和分析(这里所谓最重要的数据，通常指的都是给老板看的数据和财务相关的数据)。</span><br><span class="line">而HIve可以在Hadoop上进行SQL操作，实现数据统计和分析。可以用更低廉的价格获得比以往多更多的数据存储和计算能力。我们可以把运行日志、应用采集数据、数据库数据放到一起进行计算分析，获得以前无法得到的数据结果，企业的数据仓库也随之呈指数级膨胀。</span><br><span class="line">不仅是老板，公司中每个普通员工比如产品经理、运营人员、工程师，只要有数据访问权限，都可以提出分析需求，从大数据仓库中获得自己想要了解的数据分析结果。</span><br><span class="line">在数据仓库时代，只要有数据，几乎就一定要进行统计分析，如果数据规模比较大，我们就会想到要用Hadoop大数据技术，这也是Hadoop在这个时期发展特别快的一个原因。技术的发展同时又促进了技术应用，这也为接下来大数据应用走进数据挖掘时代埋下伏笔。</span><br></pre></td></tr></table></figure><h2 id="大数据应用的数据挖掘时代"><a href="#大数据应用的数据挖掘时代" class="headerlink" title="大数据应用的数据挖掘时代"></a>大数据应用的数据挖掘时代</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">大数据一旦进入更多的企业，我们就会对大数据提出更多期望，除了数据统计，我们还希望发掘出更多数据的价值，大数据随之进入数据挖掘时代</span><br><span class="line">真实的案例，很早以前商家就通过数据发现，买尿不湿的人通常也会买啤酒，于是精明的商家就把这两样商品放在一起，以促进销售。啤酒和尿不湿的关系，你可以有各种解读，但是如果不是通过数据挖掘，可能打破脑袋也想不出它们之间会有关系。在商业环境中，如何解读这种关系并不重要，重要的是它们之间只要存在关联，就可以进行关联分析，最终目的是让用户尽可能看到想购买的商品。</span><br><span class="line">除了商品和商品有关系，还可以利用人和人之间的关系推荐商品。如果两个人购买的商品有很多都是类似甚至相同的，不管这两个人天南海北相隔多远，他们一定有某种关系，比如可能有差不多的教育背景、经济收入、兴趣爱好。根据这种关系，可以进行关联推荐，让他们看到自己感兴趣的商品。</span><br><span class="line">更进一步，大数据还可以将每个人身上的不同特性挖掘出来，打上各种各样的标签：90后、生活在一线城市、月收入1～2万、宅……这些标签组成了用户画像，并且只要这样的标签足够多，就可以完整描绘出一个人，甚至比你最亲近的人对你的描述还要完整、准确。</span><br><span class="line">除了商品销售，数据挖掘还可以用于人际关系挖掘。你听过“六度分隔理论”吗，它认为世界上两个互不认识的人，只需要很少的中间人就能把他们联系起来。这个理论在美国的实验结果是，通过六步就能联系上两个不认识的美国人。也是基于这个理论，Facebook研究了十几亿用户的数据，试图找到关联两个陌生人之间的数字，答案是惊人的3.57。你可以看到，各种各样的社交软件记录着我们的好友关系，通过关系图谱挖掘，几乎可以把世界上所有的人际关系网都描绘出来。</span><br></pre></td></tr></table></figure><h2 id="大数据应用的机器学习时代"><a href="#大数据应用的机器学习时代" class="headerlink" title="大数据应用的机器学习时代"></a>大数据应用的机器学习时代</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们很早就发现，数据中蕴藏着规律，这个规律是所有数据都遵循的，过去发生的事情遵循这个规律，将来要发生的事情也遵循这个规律。一旦找到了这个规律，对于正在发生的事情，就可以按照这个规律进行预测。</span><br><span class="line">在过去，我们受数据采集、存储、计算能力的限制，只能通过抽样的方式获取小部分数据，无法得到完整的、全局的、细节的规律。而现在有了大数据，可以把全部的历史数据都收集起来，统计其规律，进而预测正在发生的事情。-----这就是机器学习。</span><br><span class="line">把历史上人类围棋对弈的棋谱数据都存储起来，针对每一种盘面记录如何落子可以得到更高的赢面。得到这个统计规律以后，就可以利用这个规律用机器和人下棋，每一步都计算落在何处将得到更大的赢面，于是我们就得到了一个会下棋的机器人，这就是前两年轰动一时的AlphaGo，以压倒性优势下赢了人类的顶尖棋手。</span><br><span class="line">把人聊天的对话数据都收集起来，记录每一次对话的上下文，如果上一句是问今天过得怎么样，那么下一句该如何应对，通过机器学习可以统计出来。将来有人再问今天过得怎么样，就可以自动回复下一句话，于是我们就得到一个会聊天的机器人。Siri、天猫精灵、小爱同学，这样的语音聊天机器人在机器学习时代已经满大街都是了。</span><br><span class="line">将人类活动产生的数据，通过机器学习得到统计规律，进而可以模拟人的行为，使机器表现出人类特有的智能，这就是人工智能AI。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">大数据从搜索引擎到机器学习，发展思路其实是一脉相承的，就是想发现数据中的规律并为我们所用。所以很多人把数据称作金矿，大数据应用就是从这座蕴含知识宝藏的金矿中发掘中有商业价值的真金白银出来。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据中蕴藏着价值已经是众所周知的事情了，那么如何从这些庞大的数据中发掘出我们想要的知识价值，这正是大数据技术目前正在解决的事情，包括大数据存储与计算，也包括大数据分析、挖掘、机器学习等应用。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们不曾生活在美国西部淘金的繁荣时代，错过了那个光荣与梦想、自由与激情的个人英雄主义时代。但是现在，一个更具划时代意义的大数据淘金时代已经到来，而你我正身处其中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">统计人的驾驶行为进行机器学习，就是无人驾驶；统计股票的历史交易数据进行机器学习，就得到量化交易系统。统计大家p图的参数进行智能美颜。统计过去在网上的商品浏览进行机器学习，就有了亚马逊的智能推荐物品；蚂蚁金服上统计过去的消费能力和信贷进行机器学习，来预测可在借呗上借多少钱，这是一个大数据和机器智能的时代，我们身处其中，需要的是适应并不断学习前行才不会在一段接一段的浪潮中被退去。推荐系统、广告系统、估价系统、风控系统都是现在广泛使用了机器学习的。微软识花</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="从零开始学大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>从零开始学大数据-01 大数据的前世今生</title>
    <link href="http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-01-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F.html"/>
    <id>http://tianyong.fun/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE-01-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F.html</id>
    <published>2022-01-12T07:23:40.000Z</published>
    <updated>2022-01-12T15:03:43.043Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="大数据的前世今生"><a href="#大数据的前世今生" class="headerlink" title="大数据的前世今生"></a>大数据的前世今生</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">大多数公司还专注于提升单机性能，寻找更贵更好的服务器，google的思路是部署一个大规模服务器集群，通过分布式的方式将海量数据存储在这个集群上，然后利用集群上所有服务器进行数据计算。这样不需要昂贵的服务器，却可以更好实现目的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lucene:全文检索引擎工具包</span><br><span class="line">nutch:开源搜索引擎</span><br><span class="line">Doug Cutting:lucene的创始人，nutch的开发者，hadoop之父</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2004前后google三篇论文(三驾马车: 分布式文件系统GFS、大数据分布式计算框架MapReduce和NoSQL数据库系统BigTable)——————&gt;doug cutting实现了类似GFS和mapreduce的功能————————&gt;2006年 doug cutting将nutch的大数据相关功能分离，形成独立的项目专门维护大数据技术也就是hadoop,主要包括hadoop分布式文件系统hdfs,大数据计算引擎mapreduce。------&gt;yahoo,baidu,alibaba使用Hadoop进行大数据存储和计算-----&gt;2008年Hadoop正式成为Apache的顶级项目。同年专门运营Hadoop的商业公司Cloudera成立。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hadoop纯用java编写的软件</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yahoo开发pig脚本语言(由于使用mapreduce进行大数据编程太麻烦)，类似于sql语法，开发者可以使用pig脚本描述要对大数据集上进行的操作，pig进行编译后会生成mapreduce程序，然后在hadoop上运行。缺点:虽然比直接mapredue编程容易，但需学习新的脚本语法。-----&gt;facebook发布Hive,支持sql语法来进行大数据计算，hive会将sql语句转换成mapreduce程序。这样熟悉数据库的数据分析师和工程师便可以无门槛的进行大数据分析和处理了。极大的降低了hadoop的使用难度。----&gt;随后众多hadoop的周边产品开始出现，大数据生态体系开始形成(包括sqoop:专门将关系数据库中的数据导入导出到hadoop平台; Flume:聚合和传输; Oozie:MapReduce工作流调度引擎)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在hadoop早期，mapreduce既是执行引擎，又是资源调度框架，服务器集群的资源调度由MapReduce自己完成。但这样不利于资源复用，也使得mapreduce非常的臃肿。yarn应运而生，将mapreduce资源调度和执行引擎分离开来。2012年，yarn成为一个独立项目开始运营，随后被各大大数据产品支持，成为大数据平台上最主流的资源调度系统。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于MapReduce进行机器学习计算时性能非常差因为机器学习算法通常需要很多次迭代计算，而mapreduce每执行一次map和reduce计算都需要重新启动一次作业，带来了大量的无谓消耗。还有一点就是mapreduce使用磁盘作为存储介质，而2012年，内存已经突破容量和成本限制，成为数据运行过程中的主要存储介质。spark一经推出，立即受到业界的追捧，并逐步替代MapReduce在企业应用中的地位。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">批处理计算：像mapreduce，spark这类计算框架处理的业务场景，因为它们通常针对以&#39;天&#39;为单位产生的数据进行一次计算，得到所需的结果，这中间计算需要花费的时间大概是几十分钟或更长的时间。</span><br><span class="line">大数据离线计算：计算的数据是非在线得到的实时数据，而是历史数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在典型的大数据业务场景下，数据业务最通常的做法是，采用批处理的技术处理历史全量数据，采用流式计算处理实时新增数据。flink计算引擎，同时支持流式计算和批处理计算。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NoSql系统处理的也是大规模海量数据的存储和访问，所以也被归为大数据技术。曾在2011年左右非常火爆，涌现了Hbase,Cassandra等许多优秀的产品。其中Hbase是从Hadoop中分离出来的，基于HDFS的NoSql系统。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大数据处理的主要应用场景包括数据分析，数据挖掘，机器学习。</span><br><span class="line">大数据分析：主要用hive,spark sql等sql引擎来完成</span><br><span class="line">数据挖掘和机器学习：有专门的机器学习框架TensorFlow,Mahout以及MLlib等，内置了主要的机器学习和数据挖掘算法。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/7uqqHK" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/01/12/7uqqHK.jpg" alt="7uqqHK.jpg"></a></p><hr><blockquote><p>在历史前进逻辑中前进，在时代发展的潮流中发展。</p><p>在风口中飞翔。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="从零开始学大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>python-基础操作</title>
    <link href="http://tianyong.fun/python-%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C.html"/>
    <id>http://tianyong.fun/python-%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C.html</id>
    <published>2021-12-31T02:42:47.000Z</published>
    <updated>2021-12-31T02:42:47.947Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>pandas-基础操作</title>
    <link href="http://tianyong.fun/pandas-%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C.html"/>
    <id>http://tianyong.fun/pandas-%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C.html</id>
    <published>2021-05-27T06:29:21.000Z</published>
    <updated>2021-05-27T08:06:53.393Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="DataFrame基础操作"><a href="#DataFrame基础操作" class="headerlink" title="DataFrame基础操作"></a>DataFrame基础操作</h1><h2 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h2><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame([<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],index=[<span class="number">5</span>,<span class="number">8</span>,<span class="number">1</span>,<span class="number">4</span>],columns=[<span class="string">'a'</span>])</span><br></pre></td></tr></table></figure><h3 id="多维列表"><a href="#多维列表" class="headerlink" title="多维列表"></a>多维列表</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1=pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="literal">None</span>,<span class="literal">None</span>,<span class="number">2</span>],[<span class="literal">None</span>,<span class="literal">None</span>,<span class="literal">None</span>],[<span class="number">8</span>,<span class="number">8</span>,<span class="literal">None</span>]])</span><br></pre></td></tr></table></figure><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df2=pd.DataFrame(&#123;<span class="string">'b'</span>:[<span class="number">4</span>,<span class="number">7</span>,<span class="number">-3</span>,<span class="number">2</span>],<span class="string">'a'</span>:[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]&#125;)</span><br></pre></td></tr></table></figure><h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df4=pd.DataFrame(np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>),index=[<span class="string">'a'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>],columns=[<span class="string">'oh'</span>,<span class="string">'te'</span>,<span class="string">'ca'</span>])</span><br></pre></td></tr></table></figure><h2 id="排序DataFrame"><a href="#排序DataFrame" class="headerlink" title="排序DataFrame"></a>排序DataFrame</h2><h3 id="sort-index"><a href="#sort-index" class="headerlink" title="sort_index()"></a>sort_index()</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1=pd.DataFrame([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],index=[<span class="number">10</span>,<span class="number">52</span>,<span class="number">24</span>,<span class="number">158</span>,<span class="number">112</span>],columns=[<span class="string">'s'</span>])</span><br><span class="line">df1.sort_index()</span><br></pre></td></tr></table></figure><h3 id="sort-values"><a href="#sort-values" class="headerlink" title="sort_values()"></a>sort_values()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2&#x3D;pd.DataFrame(&#123;&#39;b&#39;:[4,7,-3,2],&#39;a&#39;:[0,1,0,1]&#125;)</span><br><span class="line">df2.sort_values(by&#x3D;&#39;b&#39;)</span><br></pre></td></tr></table></figure><h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><h3 id="drop"><a href="#drop" class="headerlink" title="drop()"></a>drop()</h3><h4 id="删除行"><a href="#删除行" class="headerlink" title="删除行"></a>删除行</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df4=pd.DataFrame(np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>),index=[<span class="string">'a'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>],columns=[<span class="string">'oh'</span>,<span class="string">'te'</span>,<span class="string">'ca'</span>])</span><br><span class="line">df4.drop(<span class="string">'a'</span>)</span><br></pre></td></tr></table></figure><h4 id="删除列"><a href="#删除列" class="headerlink" title="删除列"></a>删除列</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df4=pd.DataFrame(np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>),index=[<span class="string">'a'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>],columns=[<span class="string">'oh'</span>,<span class="string">'te'</span>,<span class="string">'ca'</span>])</span><br><span class="line">df4.drop([<span class="string">'oh'</span>],axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="删除None值"><a href="#删除None值" class="headerlink" title="删除None值"></a>删除None值</h3><h4 id="dropna"><a href="#dropna" class="headerlink" title="dropna()"></a>dropna()</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1=pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="literal">None</span>,<span class="literal">None</span>,<span class="number">2</span>],[<span class="literal">None</span>,<span class="literal">None</span>,<span class="literal">None</span>],[<span class="number">8</span>,<span class="number">8</span>,<span class="literal">None</span>]])</span><br><span class="line"> df1.dropna()</span><br></pre></td></tr></table></figure><h4 id="dropna-how-’all’"><a href="#dropna-how-’all’" class="headerlink" title="dropna(how=’all’)"></a>dropna(how=’all’)</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1=pd.DataFrame([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="literal">None</span>,<span class="literal">None</span>,<span class="number">2</span>],[<span class="literal">None</span>,<span class="literal">None</span>,<span class="literal">None</span>],[<span class="number">8</span>,<span class="number">8</span>,<span class="literal">None</span>]])</span><br><span class="line">df1.dropna(how=<span class="string">'all'</span>)</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>python廖雪峰-基础</title>
    <link href="http://tianyong.fun/python%E5%BB%96%E9%9B%AA%E5%B3%B0-%E5%9F%BA%E7%A1%80.html"/>
    <id>http://tianyong.fun/python%E5%BB%96%E9%9B%AA%E5%B3%B0-%E5%9F%BA%E7%A1%80.html</id>
    <published>2021-04-15T06:40:29.000Z</published>
    <updated>2021-04-15T07:21:46.385Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h2 id="python基础"><a href="#python基础" class="headerlink" title="python基础"></a>python基础</h2><h3 id="数据类型和变量"><a href="#数据类型和变量" class="headerlink" title="数据类型和变量"></a>数据类型和变量</h3><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><h5 id="整型"><a href="#整型" class="headerlink" title="整型"></a>整型</h5><p><em>Python可以处理任意大小的整数，当然包括负整数</em></p><p><em>计算机由于使用二进制，所以，有时候用十六进制表示整数比较方便，十六进制用<code>0x</code>前缀和0-9，a-f表示，例如：<code>0xff00</code>，<code>0xa5b4c3d2</code>，等等</em></p><p><em>对于很大的数，例如<code>10000000000</code>，很难数清楚0的个数。Python允许在数字中间以<code>_</code>分隔，因此，写成<code>10_000_000_000</code>和<code>10000000000</code>是完全一样的。十六进制数也可以写成<code>0xa1b2_c3d4</code></em></p><h5 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a>浮点型</h5><p><em>Python的浮点数也没有大小限制，但是超出一定范围就直接表示为<code>inf</code>（无限大）</em></p><p><em>之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，比如，<code>1.23x10**9</code>和<code>12.3x10**8</code>是完全相等的</em></p><p><em>但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23x109就是<code>1.23e9</code>，或者<code>12.3e8</code>，0.000012可以写成<code>1.2e-5</code>，等等</em></p><p><em>整数和浮点数在计算机内部存储的方式是不同的，<strong>整数运算永远是精确的</strong>（除法难道也是精确的？是的！），而<strong>浮点数运算</strong>则可能会有<strong>四舍五入的</strong>误差</em></p><h5 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h5><p><em>字符串是以单引号<code>&#39;</code>或双引号<code>&quot;</code>括起来的任意文本</em></p><p><em>如果<code>&#39;</code>本身也是一个字符，那就可以用<code>&quot;&quot;</code>括起来，比如<code>&quot;I&#39;m OK&quot;</code>包含的字符是<code>I</code>，<code>&#39;</code>，<code>m</code>，空格，<code>O</code>，<code>K</code>这6个字符</em></p><p><em>如果字符串内部既包含<code>&#39;</code>又包含<code>&quot;</code>怎么办？可以用转义字符<code>\</code>来标识</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'I\'m \"OK\"!'</span> <span class="comment"># I'm "OK"!</span></span><br></pre></td></tr></table></figure><p><em>如果字符串里面有很多字符都需要转义，就需要加很多<code>\</code>，为了简化，Python还允许用<code>r&#39;&#39;</code>表示<code>&#39;&#39;</code>内部的字符串默认不转义</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">'\\\t\\'</span>)</span><br><span class="line">\       \</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">r'\\\t\\'</span>)</span><br><span class="line">\\\t\\</span><br></pre></td></tr></table></figure><p><em>如果字符串内部有很多换行，用<code>\n</code>写在一行里不好阅读，为了简化，Python允许用<code>&#39;&#39;&#39;...&#39;&#39;&#39;</code>的格式表示多行内容</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">'''line1</span></span><br><span class="line"><span class="string"><span class="meta">... </span>line2</span></span><br><span class="line"><span class="string"><span class="meta">... </span>line3'''</span>)</span><br><span class="line">line1</span><br><span class="line">line2</span><br><span class="line">line3</span><br></pre></td></tr></table></figure><h5 id="布尔值"><a href="#布尔值" class="headerlink" title="布尔值"></a>布尔值</h5><h5 id="空值"><a href="#空值" class="headerlink" title="空值"></a>空值</h5><p><em>None</em></p><h4 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h4><p><em>可以将一个类型的变量赋值为另一种类型</em></p><p><em>这种变量本身类型不固定的语言称之为<strong>动态语言</strong>，与之对应的是<strong>静态语言</strong>。静态语言在定义变量时必须指定变量类型，如果赋值的时候类型不匹配，就会报错。</em></p><h4 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h4><p><em>常量就是不能变的变量</em></p><p><em>在Python中，通常用全部大写的变量名表示常量</em></p><h3 id="字符串和编码"><a href="#字符串和编码" class="headerlink" title="字符串和编码"></a>字符串和编码</h3><h4 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h4><p><em>计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理</em></p><p><em>最早只有127个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为<code>ASCII</code>编码</em></p><p><em>但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了<code>GB2312</code>编码，用来把中文编进去</em></p><p><em>各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。</em></p><p><strong>Unicode</strong>把所有语言都统一到一套编码里，这样就不会再有乱码问题了。</p><p><strong>ASCII编码和Unicode编码的区别</strong>：<em>ASCII编码是1个字节，而Unicode编码通常是2个字节</em></p><p><em>如果把ASCII编码的<code>A</code>用Unicode编码，只需要在前面补0就可以，因此，<code>A</code>的Unicode编码是<code>00000000 01000001</code></em></p><p><em>新<strong>的问题又出现了</strong>：如果统一成Unicode编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算</em></p><p><em>本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的<code>UTF-8</code>编码，<strong>UTF-8编码</strong>把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间</em></p><p><em>在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码</em></p><p><em>用记事本编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里，编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件</em></p><p><em>浏览网页的时候，服务器会把动态生成的Unicode内容转换为UTF-8再传输到浏览器</em></p><h4 id="Python的字符串"><a href="#Python的字符串" class="headerlink" title="Python的字符串"></a>Python的字符串</h4><p><em>在最新的Python 3版本中，字符串是以Unicode编码的，也就是说，Python的字符串支持多语言</em></p><p><strong>对于单个字符的编码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(ord(<span class="string">'a'</span>)) <span class="comment"># 97</span></span><br><span class="line">print(chr(<span class="number">66</span>))  <span class="comment"># 'B'</span></span><br></pre></td></tr></table></figure><p><strong>知道字符的整数编码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">'\u4e2d\u6587'</span></span><br><span class="line"><span class="comment"># 中文</span></span><br></pre></td></tr></table></figure><h5 id="str-replace-‘x’-‘b’"><a href="#str-replace-‘x’-‘b’" class="headerlink" title="str.replace(‘x’, ‘b’)"></a>str.replace(‘x’, ‘b’)</h5><h4 id="x-encode-and-b’xx’-decode"><a href="#x-encode-and-b’xx’-decode" class="headerlink" title="x.encode() and b’xx’.decode()"></a>x.encode() and b’xx’.decode()</h4><h5 id="encode"><a href="#encode" class="headerlink" title="encode"></a>encode</h5><p><em>一个字符对应若干个字节。如果要在网络上传输，或者保存到磁盘上，就需要把<code>str</code>变为以字节为单位的<code>bytes</code></em></p><p><em>Python对<code>bytes</code>类型的数据用带<code>b</code>前缀的单引号或双引号表示</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="string">b'ABC'</span></span><br></pre></td></tr></table></figure><p><strong>以Unicode表示的<code>str</code>通过<code>encode()</code>方法可以编码为指定的<code>bytes</code></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'ABC'</span>.encode(<span class="string">'ascii'</span>) <span class="comment"># b'ABC'</span></span><br><span class="line"><span class="string">'中文'</span>.encode(<span class="string">'utf-8'</span>) <span class="comment"># b'\xe4\xb8\xad\xe6\x96\x87'</span></span><br><span class="line"><span class="comment"># 含有中文的str无法用ASCII编码，因为中文编码的范围超过了ASCII编码的范围，Python会报错</span></span><br></pre></td></tr></table></figure><h5 id="decode"><a href="#decode" class="headerlink" title="decode"></a>decode</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'ABC'</span>.decode(<span class="string">'ascii'</span>)</span><br><span class="line"><span class="string">'ABC'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b'\xe4\xb8\xad\xe6\x96\x87'</span>.decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="string">'中文'</span></span><br></pre></td></tr></table></figure><h4 id="字符串格式化"><a href="#字符串格式化" class="headerlink" title="字符串格式化"></a>字符串格式化</h4><p><em><code>%</code>运算符就是用来格式化字符串的。在字符串内部，<code>%s</code>表示用字符串替换，<code>%d</code>表示用整数替换，有几个<code>%?</code>占位符，后面就跟几个变量或者值，顺序要对应好。如果只有一个<code>%?</code>，括号可以省略</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'abc %s'</span>%<span class="string">'dhkhs'</span></span><br><span class="line"><span class="string">'hdhi %s %d'</span>%(<span class="string">'kjkd'</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%d</span><br><span class="line">%s  <span class="comment"># 不清楚就直接用它，%s永远起作用，它会把任何数据类型转换为字符串</span></span><br><span class="line">%f</span><br><span class="line">%x  十六进制整数</span><br></pre></td></tr></table></figure><p><strong>字符串里面的<code>%</code></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">'a%%b'</span> <span class="comment"># a%b</span></span><br></pre></td></tr></table></figure><h5 id="format"><a href="#format" class="headerlink" title="format"></a>format</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'Hello, &#123;0&#125;, 成绩提升了 &#123;1:.1f&#125;%'</span>.format(<span class="string">'小明'</span>, <span class="number">17.125</span>)</span><br><span class="line"><span class="string">'Hello, 小明, 成绩提升了 17.1%'</span></span><br></pre></td></tr></table></figure><h5 id="f-string"><a href="#f-string" class="headerlink" title="f-string"></a>f-string</h5><p><em>它和普通字符串不同之处在于，字符串如果包含<code>{xxx}</code>，就会以对应的变量替换</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = <span class="number">2.5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = <span class="number">3.14</span> * r ** <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">f'The area of a circle with radius <span class="subst">&#123;r&#125;</span> is <span class="subst">&#123;s:<span class="number">.2</span>f&#125;</span>'</span>)</span><br><span class="line">The area of a circle <span class="keyword">with</span> radius <span class="number">2.5</span> <span class="keyword">is</span> <span class="number">19.62</span></span><br></pre></td></tr></table></figure><h3 id="tuple和list"><a href="#tuple和list" class="headerlink" title="tuple和list"></a>tuple和list</h3><h4 id="list"><a href="#list" class="headerlink" title="list"></a>list</h4><p><em>list是一种有序的集合</em></p><h5 id="append"><a href="#append" class="headerlink" title="append"></a>append</h5><h5 id="insert"><a href="#insert" class="headerlink" title="insert"></a>insert</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l = [<span class="string">'a'</span>,<span class="string">'b'</span>]</span><br><span class="line">l.insert(<span class="number">1</span>, <span class="string">'x'</span>)</span><br></pre></td></tr></table></figure><h5 id="pop"><a href="#pop" class="headerlink" title="pop"></a>pop</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l.pop() <span class="comment"># 删除末尾</span></span><br><span class="line">l.pop(i) <span class="comment"># 删除指定位置</span></span><br></pre></td></tr></table></figure><h5 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h5><h4 id="tuple"><a href="#tuple" class="headerlink" title="tuple"></a>tuple</h4><p><em>有序列表叫元组，tuple一旦初始化就不能修改</em></p><p><em>但如果元组里的某元素是列表时，列表里的元素可以改变</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = ()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">()</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = (<span class="number">1</span>) <span class="comment"># 定义一个元素的元组不能这样定义，解释器默认为是数学中的小空号，可以在后面加‘，’</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a>条件判断</h3><p><em>if-elif…..else只要某条件满足，后续的不再判断</em></p><p><em>if-if-if-else</em>:<em>每步都会判断，除了else</em></p><h3 id="dict和set"><a href="#dict和set" class="headerlink" title="dict和set"></a>dict和set</h3><h4 id="dict"><a href="#dict" class="headerlink" title="dict"></a>dict</h4><p><em>dict内部存放的顺序和key放入的顺序是没有关系的</em></p><p><em>dict的key必须是<strong>不可变对象</strong>.在Python中，字符串、整数等都是不可变的</em></p><h5 id="赋值"><a href="#赋值" class="headerlink" title="赋值"></a>赋值</h5><h5 id="获取值"><a href="#获取值" class="headerlink" title="获取值"></a>获取值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="string">'b'</span>:<span class="number">2</span>&#125;</span><br><span class="line">d[<span class="string">'c'</span>] <span class="comment"># 会报错，避免报错用下面的方法</span></span><br><span class="line"><span class="string">'c'</span> <span class="keyword">in</span> d</span><br><span class="line">d.get(<span class="string">'c'</span>) <span class="comment"># 返回None</span></span><br><span class="line">d.get(<span class="string">'c'</span>, <span class="number">1</span>) <span class="comment"># 1</span></span><br></pre></td></tr></table></figure><h5 id="删除键"><a href="#删除键" class="headerlink" title="删除键"></a>删除键</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d.pop(<span class="string">'a'</span>) <span class="comment"># 值也会随之删除</span></span><br></pre></td></tr></table></figure><h5 id="dict和list比较"><a href="#dict和list比较" class="headerlink" title="dict和list比较"></a>dict和list比较</h5><p>1.查找和插入的速度极快，不会随着key的增加而变慢；</p><p>2.需要占用大量的内存，内存浪费多</p><h4 id="set"><a href="#set" class="headerlink" title="set"></a>set</h4><p><em>在set中，没有重复的key，无序</em></p><p><em>要创建一个set，需要提供一个list作为输入集合</em>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s</span><br><span class="line">&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><h5 id="add"><a href="#add" class="headerlink" title="add"></a>add</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = set([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">s.add(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h5 id="remove"><a href="#remove" class="headerlink" title="remove"></a>remove</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s.remove(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="python" scheme="http://tianyong.fun/categories/python/"/>
    
    
  </entry>
  
  <entry>
    <title>python廖雪峰-简介</title>
    <link href="http://tianyong.fun/python%E5%BB%96%E9%9B%AA%E5%B3%B0-%E7%AE%80%E4%BB%8B.html"/>
    <id>http://tianyong.fun/python%E5%BB%96%E9%9B%AA%E5%B3%B0-%E7%AE%80%E4%BB%8B.html</id>
    <published>2021-04-15T01:54:31.000Z</published>
    <updated>2021-04-15T06:41:01.372Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="python廖雪峰"><a href="#python廖雪峰" class="headerlink" title="python廖雪峰"></a>python廖雪峰</h1><h2 id="python简介"><a href="#python简介" class="headerlink" title="python简介"></a>python简介</h2><p><em>Python是著名的“龟叔”Guido van Rossum在1989年圣诞节期间</em></p><p><em>荷兰人</em></p><h2 id="Python解释器"><a href="#Python解释器" class="headerlink" title="Python解释器"></a>Python解释器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Cpython: 从Python官方网站下载并安装好Python 3.x后，我们就直接获得了一个官方版本的解释器：CPython ## 由C语言编写</span><br><span class="line">Ipython: 基于CPython之上的一个交互式解释器，也就是说，只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的</span><br><span class="line">PyPy: 是另一个Python解释器，它的目标是执行速度</span><br><span class="line">Jython: Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行</span><br><span class="line">IronPython: IronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器</span><br></pre></td></tr></table></figure><h2 id="第一个python程序"><a href="#第一个python程序" class="headerlink" title="第一个python程序"></a>第一个python程序</h2><h3 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h3><h4 id="print"><a href="#print" class="headerlink" title="print()"></a>print()</h4><p><em>可以接受多个字符串，用逗号隔开(逗号以空格呈现)，并一行输出</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'The quick brown fox'</span>, <span class="string">'jumps over'</span>, <span class="string">'the lazy dog'</span>)</span><br><span class="line"><span class="comment"># The quick brown fox jumps over the lazy dog</span></span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="python" scheme="http://tianyong.fun/categories/python/"/>
    
    
  </entry>
  
  <entry>
    <title>jsDeliver+github打造属于自己的图床</title>
    <link href="http://tianyong.fun/jsDeliver-github%E6%89%93%E9%80%A0%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9B%BE%E5%BA%8A.html"/>
    <id>http://tianyong.fun/jsDeliver-github%E6%89%93%E9%80%A0%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9B%BE%E5%BA%8A.html</id>
    <published>2021-01-21T06:40:55.000Z</published>
    <updated>2021-01-21T06:55:17.940Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="jsDeliver-github打造属于自己的图床"><a href="#jsDeliver-github打造属于自己的图床" class="headerlink" title="jsDeliver+github打造属于自己的图床"></a>jsDeliver+github打造属于自己的图床</h1><h2 id="什么是图床"><a href="#什么是图床" class="headerlink" title="什么是图床"></a>什么是图床</h2><p><em>“图床一般是指储存图片的服务器，有国内和国外之分。国外的图床由于有空间距离等因素决定访问速度很慢影响图片显示速度。国内也分为单线空间、多线空间和cdn加速三种。”<br>注意：<code>github 支持的就是cdn加速</code></em></p><h2 id="什么是jsDelive"><a href="#什么是jsDelive" class="headerlink" title="什么是jsDelive"></a>什么是jsDelive</h2><p><em>jsDelivr 是一个免费开源的 CDN 解决方案，用于帮助开发者和站长。包含 JavaScript 库、jQuery 插件、CSS 框架、字体等等 Web 上常用的静态资源。</em></p><h2 id="jsDelive加载资源"><a href="#jsDelive加载资源" class="headerlink" title="jsDelive加载资源"></a>jsDelive加载资源</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;你的用户名&#x2F;你的仓库名@发布的版本号&#x2F;文件路径</span><br><span class="line"></span><br><span class="line">比如：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;加载js</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;yremp&#x2F;cdn@1.0&#x2F;js&#x2F;jquery.js</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;加载图片</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;yremp&#x2F;cdn@1.0&#x2F;images&#x2F;hb.png</span><br></pre></td></tr></table></figure><h2 id="现有的图床"><a href="#现有的图床" class="headerlink" title="现有的图床"></a>现有的图床</h2><h3 id="路过图床"><a href="#路过图床" class="headerlink" title="路过图床"></a>路过图床</h3><p><a href="https://imgchr.com/" target="_blank" rel="external nofollow noopener noreferrer">https://imgchr.com/</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">路过图床</span><br><span class="line">网站： https:&#x2F;&#x2F;imgchr.com&#x2F;</span><br><span class="line">简介：支持免注册上传图片，永久存储，支持HTTPS加密访 问和调用图片，提供多种图片链接格式，成立于2011年</span><br><span class="line">限制：最大10M</span><br></pre></td></tr></table></figure><h3 id="SM-MS"><a href="#SM-MS" class="headerlink" title="SM.MS"></a>SM.MS</h3><p><a href="https://sm.ms/" target="_blank" rel="external nofollow noopener noreferrer">https://sm.ms/</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">网站:https:&#x2F;&#x2F;sm.ms&#x2F;</span><br><span class="line">特点：永久存储免注册，图片链接支持https，可以删除上传 的图片，提供多种图片链接格式，建立于2015年，速度比路过图床慢</span><br><span class="line">图片上传限制：每个图片最大5M，每次最多上传10张</span><br></pre></td></tr></table></figure><h3 id="小贱贱图床"><a href="#小贱贱图床" class="headerlink" title="小贱贱图床"></a>小贱贱图床</h3><p><a href="http://pic.xiaojianjian.net" target="_blank" rel="external nofollow noopener noreferrer">http://pic.xiaojianjian.net</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">网站：http:&#x2F;&#x2F;pic.xiaojianjian.net</span><br><span class="line">需要注册，每日可以上传图片30张，上传后可以获取一个简单的外链，速度很快，但是图片清晰度会变低</span><br></pre></td></tr></table></figure><h3 id="聚合图床"><a href="#聚合图床" class="headerlink" title="聚合图床"></a>聚合图床</h3><p><a href="https://www.superbed.cn/" target="_blank" rel="external nofollow noopener noreferrer">https://www.superbed.cn</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">网站：https:&#x2F;&#x2F;www.superbed.cn</span><br><span class="line">简介：将图片分发到多处备份，借助其本身的CDN加速功能，节省服务器流量，并且不用担心图片被删除，即便其中某几个图床上的图片被删除了，还有其他备份，保证万无一失，支持匿名和注册管理</span><br><span class="line">图片上传限制：无</span><br></pre></td></tr></table></figure><h3 id="堆爱外链"><a href="#堆爱外链" class="headerlink" title="堆爱外链"></a>堆爱外链</h3><p><a href="http://pan.duiai.cc/" target="_blank" rel="external nofollow noopener noreferrer">http://pan.duiai.cc</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">地址：http:&#x2F;&#x2F;pan.duiai.cc</span><br><span class="line">简介：注册才可以上传，存在的时间比较久，除了图片还可以上传视频和音乐</span><br><span class="line">限制：视频体积过大不可，但具体限制不明</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><em>搭建图床的方法很多，就像<strong>七牛云</strong>30天后会回收测试域名，因此你必须要绑定自己的已经备案的域名，又拍云也需要绑定域名才可以使用，所有我不推荐大家使用。</em></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="hexo" scheme="http://tianyong.fun/categories/hexo/"/>
    
    
      <category term="hexo" scheme="http://tianyong.fun/tags/hexo/"/>
    
      <category term="图床" scheme="http://tianyong.fun/tags/%E5%9B%BE%E5%BA%8A/"/>
    
  </entry>
  
</feed>
