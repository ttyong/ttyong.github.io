<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2023-03-29T17:52:24.800Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>廖雪峰Java教程笔记-Java程序基础 数组类型</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.html</id>
    <published>2023-03-29T14:30:59.000Z</published>
    <updated>2023-03-29T17:52:24.800Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="数组类型"><a href="#数组类型" class="headerlink" title="数组类型"></a>数组类型</h1><h2 id="法一"><a href="#法一" class="headerlink" title="法一"></a>法一</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">使用数组来表示“一组”int类型。代码如下：</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // <span class="number">5</span>位同学的成绩:</span><br><span class="line">        int[] ns = new int[<span class="number">5</span>];</span><br><span class="line">        ns[<span class="number">0</span>] = <span class="number">68</span>;</span><br><span class="line">        ns[<span class="number">1</span>] = <span class="number">79</span>;</span><br><span class="line">        ns[<span class="number">2</span>] = <span class="number">91</span>;</span><br><span class="line">        ns[<span class="number">3</span>] = <span class="number">85</span>;</span><br><span class="line">        ns[<span class="number">4</span>] = <span class="number">62</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  定义一个数组类型的变量，使用数组类型“类型[]”，例如，int[]。和单个基本类型变量不同，数组变量初始化必须使用new int[<span class="number">5</span>]表示创建一个可容纳<span class="number">5</span>个int元素的数组。</span><br><span class="line">  </span><br><span class="line">Java的数组有几个特点：</span><br><span class="line">  数组所有元素初始化为默认值，整型都是<span class="number">0</span>，浮点型是<span class="number">0.0</span>，布尔型是false；</span><br><span class="line">  数组一旦创建后，大小就不可改变。</span><br><span class="line">  要访问数组中的某一个元素，需要使用索引。数组索引从<span class="number">0</span>开始，例如，<span class="number">5</span>个元素的数组，索引范围是<span class="number">0</span>~<span class="number">4</span>。</span><br><span class="line">  可以修改数组中的某一个元素，使用赋值语句，例如，ns[<span class="number">1</span>] = <span class="number">79</span>;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">可以用数组变量.length获取数组大小</span><br><span class="line">数组是引用类型，在使用索引访问数组元素时，如果索引超出范围，运行时将报错</span><br></pre></td></tr></table></figure><h2 id="法二"><a href="#法二" class="headerlink" title="法二"></a>法二</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">也可以在定义数组时直接指定初始化的元素，这样就不必写出数组大小，而是由编译器自动推算数组大小。例如：</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // <span class="number">5</span>位同学的成绩:</span><br><span class="line">        int[] ns = new int[] &#123; <span class="number">68</span>, <span class="number">79</span>, <span class="number">91</span>, <span class="number">85</span>, <span class="number">62</span> &#125;;</span><br><span class="line">        System.out.println(ns.length); // 编译器自动推算数组大小为<span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">还可以进一步简写为：</span><br><span class="line">int[] ns = &#123; <span class="number">68</span>, <span class="number">79</span>, <span class="number">91</span>, <span class="number">85</span>, <span class="number">62</span> &#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意数组是引用类型，并且数组大小不可变。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HUNHEt" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%5CHUNHEt.md.png" alt="HUNHEt.md.png"></a></p><h2 id="字符串数组"><a href="#字符串数组" class="headerlink" title="字符串数组"></a>字符串数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如果数组元素不是基本类型，而是一个引用类型，那么，修改数组元素会有哪些不同？</span><br><span class="line">字符串是引用类型，因此我们先定义一个字符串数组：</span><br><span class="line"></span><br><span class="line">String[] names = &#123;</span><br><span class="line">    <span class="string">"ABC"</span>, <span class="string">"XYZ"</span>, <span class="string">"zoo"</span></span><br><span class="line">&#125;;xxxxxxxxxx 如果数组元素不是基本类型，而是一个引用类型，那么，修改数组元素会有哪些不同？字符串是引用类型，因此我们先定义一个字符串数组：String[] names = &#123;    <span class="string">"ABC"</span>, <span class="string">"XYZ"</span>, <span class="string">"zoo"</span>&#125;;</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HUNHEt" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%5CHUNHEt.md-16801002864109.png" alt="HUNHEt.md.png"></a></p><p><a href="https://imgtu.com/i/HUUBPf" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%5CHUUBPf.md.png" alt="HUUBPf.md.png"></a></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数组是同一数据类型的集合，数组一旦创建后，大小就不可变；</span><br><span class="line"></span><br><span class="line">可以通过索引访问数组元素，但索引超出范围将报错；</span><br><span class="line"></span><br><span class="line">数组元素可以是值类型（如int）或引用类型（如String），但数组本身是引用类型；</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-Java程序基础 变量和数据类型</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html</id>
    <published>2023-03-29T14:24:52.000Z</published>
    <updated>2023-03-29T17:52:20.297Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="变量和数据类型"><a href="#变量和数据类型" class="headerlink" title="变量和数据类型"></a>变量和数据类型</h1><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Java中，变量分为两种：基本类型的变量和引用类型的变量。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">*先定义再应用*</span><br><span class="line">*可以一次性多个定义和赋值*</span><br><span class="line">*没有赋值，将自动赋默认值(基本数据类型)*</span><br><span class="line">*可以将一个基本数据类型变量赋值给另一个基本类型变量。不是指向同一个地址*</span><br></pre></td></tr></table></figure><h2 id="基本数据类型有"><a href="#基本数据类型有" class="headerlink" title="基本数据类型有"></a>基本数据类型有</h2><p><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5C202303292133989.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">~~~</span><br><span class="line">+ 整型 byte,short,int,long</span><br><span class="line">+ 浮点型 float,double</span><br><span class="line">   float要加上f或F，double可以省略</span><br><span class="line">+ 字符型 char</span><br><span class="line">  用单引号</span><br><span class="line">+ 布尔型 false,true</span><br><span class="line">~~~</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;不同的数据类型占用的字节数不一样。我们看一下Java基本数据类型占用的字节数</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HtXtqe" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5CHtXtqe.png" alt="HtXtqe.png"></a></p><h3 id="整型"><a href="#整型" class="headerlink" title="整型"></a>整型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对于整型类型，Java只定义了带符号的整型，因此，最高位的bit表示符号位（<span class="number">0</span>表示正数，<span class="number">1</span>表示负数）。各种整型能表示的最大范围如下</span><br><span class="line"></span><br><span class="line">byte：<span class="number">-128</span> ~ <span class="number">127</span></span><br><span class="line">short: <span class="number">-32768</span> ~ <span class="number">32767</span></span><br><span class="line">int: <span class="number">-2147483648</span> ~ <span class="number">2147483647</span></span><br><span class="line">long: <span class="number">-9223372036854775808</span> ~ <span class="number">9223372036854775807</span></span><br><span class="line"></span><br><span class="line">对于float类型，需要加上f后缀。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int i = <span class="number">2147483647</span>;</span><br><span class="line">        int i2 = <span class="number">-2147483648</span>;</span><br><span class="line">        int i3 = <span class="number">2</span>_000_000_000; // 加下划线更容易识别</span><br><span class="line">        int i4 = <span class="number">0xff0000</span>; // 十六进制表示的<span class="number">16711680</span></span><br><span class="line">        int i5 = <span class="number">0b1000000000</span>; // 二进制表示的<span class="number">512</span></span><br><span class="line">        long l = <span class="number">9000000000000000000L</span>; // long型的结尾需要加L</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">特别注意：同一个数的不同进制的表示是完全相同的，例如<span class="number">15</span>=<span class="number">0xf</span>＝<span class="number">0b1111</span></span><br></pre></td></tr></table></figure><h3 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a>浮点型</h3><p><code>因为小数用科学计数法表示的时候，小数点是可以“浮动”的,所以称为浮点数</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">float f1 = <span class="number">3.14</span>f;</span><br><span class="line">float f2 = <span class="number">3.14e38</span>f; // 科学计数法表示的<span class="number">3.14</span>x10^<span class="number">38</span></span><br><span class="line">double d = <span class="number">1.79e308</span>;</span><br><span class="line">double d2 = <span class="number">-1.79e308</span>;</span><br><span class="line">double d3 = <span class="number">4.9e-324</span>; // 科学计数法表示的<span class="number">4.9</span>x10^<span class="number">-324</span></span><br></pre></td></tr></table></figure><h3 id="布尔类型"><a href="#布尔类型" class="headerlink" title="布尔类型"></a>布尔类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">布尔类型boolean只有true和false两个值，布尔类型总是关系运算的计算结果</span><br><span class="line"></span><br><span class="line">Java语言对布尔类型的存储并没有做规定，因为理论上存储布尔类型只需要1 bit，但是通常JVM内部会把boolean表示为4字节整数</span><br></pre></td></tr></table></figure><h3 id="字符类型"><a href="#字符类型" class="headerlink" title="字符类型"></a>字符类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">字符类型char表示一个字符。Java的char类型除了可表示标准的ASCII外，还可以表示一个Unicode字符：</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        char a = <span class="string">'A'</span>;</span><br><span class="line">        char zh = <span class="string">'中'</span>;</span><br><span class="line">        System.out.println(a);</span><br><span class="line">        System.out.println(zh);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意char类型使用单引号&#39;，且仅有一个字符，要和双引号&quot;的字符串类型区分开。</span><br></pre></td></tr></table></figure><h2 id="引用类型"><a href="#引用类型" class="headerlink" title="引用类型"></a>引用类型</h2><p><em>除了上述基本类型的变量，剩下的都是引用类型</em></p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">引用类型最常用的就是String字符串：</span><br><span class="line"></span><br><span class="line">String s &#x3D; &quot;hello&quot;;</span><br><span class="line"></span><br><span class="line">引用类型的变量类似于C语言的指针，它内部存储一个“地址”，指向某个对象在内存的位置，后续我们介绍类的概念时会详细讨论。</span><br></pre></td></tr></table></figure><h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">定义变量的时候，如果加上final修饰符，这个变量就变成了常量</span><br><span class="line"></span><br><span class="line">final double PI = <span class="number">3.14</span>; // PI是一个常量</span><br><span class="line">double r = <span class="number">5.0</span>;</span><br><span class="line">double area = PI * r * r;</span><br><span class="line">PI = <span class="number">300</span>; // compile error!</span><br><span class="line"></span><br><span class="line">常量在定义时进行初始化后就不可再次赋值，再次赋值会导致编译错误。</span><br><span class="line">根据习惯，常量名通常全部大写。</span><br></pre></td></tr></table></figure><h3 id="var关键字"><a href="#var关键字" class="headerlink" title="var关键字"></a>var关键字</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">定义变量时，变量类型太长，可以用var</span><br><span class="line">StringBuilder sb = new StringBuilder();</span><br><span class="line"></span><br><span class="line">这个时候，如果想省略变量类型，可以使用var关键字:</span><br><span class="line">var sb = new StringBuilder();</span><br><span class="line">编译器会根据赋值语句自动推断出变量sb的类型是StringBuilder</span><br></pre></td></tr></table></figure><h2 id="变量的作用范围"><a href="#变量的作用范围" class="headerlink" title="变量的作用范围"></a>变量的作用范围</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">定义变量时，要遵循作用域最小化原则，尽量将变量定义在尽可能小的作用域，并且，不要重复使用变量名。</span><br></pre></td></tr></table></figure><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Java提供了两种变量类型：基本类型和引用类型</span><br><span class="line">基本类型包括整型，浮点型，布尔型，字符型。</span><br><span class="line">变量可重新赋值，等号是赋值语句，不是数学意义的等号。</span><br><span class="line">常量在初始化后不可重新赋值，使用常量便于理解程序意图。</span><br></pre></td></tr></table></figure><h2 id="整数运算"><a href="#整数运算" class="headerlink" title="整数运算"></a>整数运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">整数的数值表示不但是精确的，而且整数运算永远是精确的，即使是除法也是精确的，因为两个整数相除只能得到结果的整数部分</span><br><span class="line"></span><br><span class="line">int x = <span class="number">12345</span> / <span class="number">67</span>; // <span class="number">184</span></span><br><span class="line">求余运算使用%：</span><br><span class="line"></span><br><span class="line">int y = <span class="number">12345</span> % <span class="number">67</span>; // <span class="number">12345</span>÷<span class="number">67</span>的余数是<span class="number">17</span></span><br><span class="line">特别注意：整数的除法对于除数为<span class="number">0</span>时运行时将报错，但编译不会报错</span><br></pre></td></tr></table></figure><h3 id="溢出"><a href="#溢出" class="headerlink" title="溢出"></a>溢出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要特别注意，整数由于存在范围限制，如果计算结果超出了范围，就会产生溢出，而溢出不会出错，却会得到一个奇怪的结果</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int x = <span class="number">2147483640</span>;</span><br><span class="line">        int y = <span class="number">15</span>;</span><br><span class="line">        int sum = x + y;</span><br><span class="line">        System.out.println(sum); // <span class="number">-2147483641</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="number">0111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1000</span></span><br><span class="line">+ <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">1111</span></span><br><span class="line">-----------------------------------------</span><br><span class="line">  <span class="number">1000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0111</span></span><br><span class="line">由于最高位计算结果为<span class="number">1</span>，因此，加法结果变成了一个负数</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">还有一种简写的运算符，即+=，-=，*=，/=，它们的使用方法如下：</span><br><span class="line"></span><br><span class="line">n += <span class="number">100</span>; // <span class="number">3409</span>, 相当于 n = n + <span class="number">100</span>;</span><br><span class="line">n -= <span class="number">100</span>; // <span class="number">3309</span>, 相当于 n = n - <span class="number">100</span>;</span><br></pre></td></tr></table></figure><h3 id="自增-自减"><a href="#自增-自减" class="headerlink" title="自增/自减"></a>自增/自减</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">++</span><br><span class="line"></span><br><span class="line">--</span><br><span class="line"></span><br><span class="line">*写在变量前面和后面是不同的，前面(先加减在运算)，后面(先运算再加减)*</span><br></pre></td></tr></table></figure><h3 id="移位运算符"><a href="#移位运算符" class="headerlink" title="移位运算符"></a>移位运算符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">*数值的最高位是一个符号位*</span><br><span class="line">1.</span><br><span class="line">&gt;&gt;: 右位移</span><br><span class="line">int n &#x3D; 7;       &#x2F;&#x2F; 00000000 00000000 00000000 00000111 &#x3D; 7</span><br><span class="line">int a &#x3D; n &gt;&gt; 1;  &#x2F;&#x2F; 00000000 00000000 00000000 00000011 &#x3D; 3</span><br><span class="line">int b &#x3D; n &gt;&gt; 2;  &#x2F;&#x2F; 00000000 00000000 00000000 00000001 &#x3D; 1</span><br><span class="line">int c &#x3D; n &gt;&gt; 3;  &#x2F;&#x2F; 00000000 00000000 00000000 00000000 &#x3D; 0</span><br><span class="line">如果对一个负数进行右移，最高位的1不动，结果仍然是一个负数：</span><br><span class="line">int n &#x3D; -536870912;</span><br><span class="line">int a &#x3D; n &gt;&gt; 1;  &#x2F;&#x2F; 11110000 00000000 00000000 00000000 &#x3D; -268435456</span><br><span class="line">int b &#x3D; n &gt;&gt; 2;  &#x2F;&#x2F; 11111000 00000000 00000000 00000000 &#x3D; -134217728</span><br><span class="line">int c &#x3D; n &gt;&gt; 28; &#x2F;&#x2F; 11111111 11111111 11111111 11111110 &#x3D; -2</span><br><span class="line">int d &#x3D; n &gt;&gt; 29; &#x2F;&#x2F; 11111111 11111111 11111111 11111111 &#x3D; -1</span><br><span class="line"></span><br><span class="line">2. </span><br><span class="line">&lt;&lt;: 左位移</span><br><span class="line">int n &#x3D; 7;       &#x2F;&#x2F; 00000000 00000000 00000000 00000111 &#x3D; 7</span><br><span class="line">int a &#x3D; n &lt;&lt; 1;  &#x2F;&#x2F; 00000000 00000000 00000000 00001110 &#x3D; 14</span><br><span class="line">int b &#x3D; n &lt;&lt; 2;  &#x2F;&#x2F; 00000000 00000000 00000000 00011100 &#x3D; 28</span><br><span class="line">int c &#x3D; n &lt;&lt; 28; &#x2F;&#x2F; 01110000 00000000 00000000 00000000 &#x3D; 1879048192</span><br><span class="line">int d &#x3D; n &lt;&lt; 29; &#x2F;&#x2F; 11100000 00000000 00000000 00000000 &#x3D; -536870912</span><br><span class="line"></span><br><span class="line">*上面两种不会改变符号位*</span><br><span class="line"></span><br><span class="line">3.无符号的右移运算</span><br><span class="line">使用&gt;&gt;&gt;，它的特点是不管符号位，右移后高位总是补0，因此，对一个负数进行&gt;&gt;&gt;右移，它会变成正数，原因是最高位的1变成了0</span><br><span class="line">int n &#x3D; -536870912;</span><br><span class="line">int a &#x3D; n &gt;&gt;&gt; 1;  &#x2F;&#x2F; 01110000 00000000 00000000 00000000 &#x3D; 1879048192</span><br><span class="line">int b &#x3D; n &gt;&gt;&gt; 2;  &#x2F;&#x2F; 00111000 00000000 00000000 00000000 &#x3D; 939524096</span><br><span class="line">int c &#x3D; n &gt;&gt;&gt; 29; &#x2F;&#x2F; 00000000 00000000 00000000 00000111 &#x3D; 7</span><br><span class="line">int d &#x3D; n &gt;&gt;&gt; 31; &#x2F;&#x2F; 00000000 00000000 00000000 00000001 &#x3D; 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">对byte和short类型进行移位时，会首先转换为int再进行位移。</span><br><span class="line">仔细观察可发现，左移实际上就是不断地×2，右移实际上就是不断地÷2</span><br></pre></td></tr></table></figure><h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">位运算是按位进行与、或、非和异或的运算</span><br><span class="line"></span><br><span class="line">&amp;(与): 同1才为1</span><br><span class="line"></span><br><span class="line">|(或): 有1则为0</span><br><span class="line"></span><br><span class="line">~(非)： 01互换</span><br><span class="line"></span><br><span class="line">^(异或): 不同才为1</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对两个整数进行位运算，实际上就是按位对齐，然后依次对每一位进行运算。</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int i = <span class="number">167776589</span>; // <span class="number">00001010</span> <span class="number">00000000</span> <span class="number">00010001</span> <span class="number">01001101</span></span><br><span class="line">        int n = <span class="number">167776512</span>; // <span class="number">00001010</span> <span class="number">00000000</span> <span class="number">00010001</span> <span class="number">00000000</span></span><br><span class="line">        System.out.println(i &amp; n); // <span class="number">167776512</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运算优先级"><a href="#运算优先级" class="headerlink" title="运算优先级"></a>运算优先级</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在Java的计算表达式中，运算优先级从高到低依次是：</span><br><span class="line"></span><br><span class="line">()</span><br><span class="line">! ~ ++ --</span><br><span class="line">* / %</span><br><span class="line">+ -</span><br><span class="line">&lt;&lt; &gt;&gt; &gt;&gt;&gt;</span><br><span class="line">&amp;</span><br><span class="line">|</span><br><span class="line">+= -= *= /=</span><br><span class="line"></span><br><span class="line">记不住也没关系，只需要加括号就可以保证运算的优先级正确</span><br></pre></td></tr></table></figure><h3 id="类型自动提升-整与整"><a href="#类型自动提升-整与整" class="headerlink" title="类型自动提升(整与整)"></a>类型自动提升(整与整)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在运算过程中，如果参与运算的两个数类型不一致，那么计算结果为较大类型的整型。例如，short和int计算，结果总是int，原因是short首先自动被转型为int</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        short s = <span class="number">1234</span>;</span><br><span class="line">        int i = <span class="number">123456</span>;</span><br><span class="line">        int x = s + i; // s自动转型为int</span><br><span class="line">        short y = s + i; // 编译错误!</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="强制转换-整与整"><a href="#强制转换-整与整" class="headerlink" title="强制转换(整与整)"></a>强制转换(整与整)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">也可以将结果强制转型，即将大范围的整数转型为小范围的整数。强制转型使用(类型)，例如，将int强制转型为short：</span><br><span class="line"></span><br><span class="line">int i &#x3D; 12345;</span><br><span class="line">short s &#x3D; (short) i; &#x2F;&#x2F; 12345</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">要注意，超出范围的强制转型会得到错误的结果，原因是转型时，int的两个高位字节直接被扔掉，仅保留了低位的两个字节</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int i1 = <span class="number">1234567</span>;</span><br><span class="line">        short s1 = (short) i1; // <span class="number">-10617</span></span><br><span class="line">        System.out.println(s1);</span><br><span class="line">        int i2 = <span class="number">12345678</span>; //short <span class="number">32767</span></span><br><span class="line">        short s2 = (short) i2; // <span class="number">24910</span></span><br><span class="line">        System.out.println(s2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="number">-10617</span></span><br><span class="line"><span class="number">24910</span></span><br><span class="line"></span><br><span class="line">// 因此，强制转型的结果很可能是错的</span><br></pre></td></tr></table></figure><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">整数运算的结果永远是精确的；</span><br><span class="line">运算结果会自动提升；</span><br><span class="line">可以强制转型，但超出范围的强制转型会得到错误的结果；</span><br><span class="line">应该选择合适范围的整型（int或long），没有必要为了节省内存而使用byte和short进行整数运算。</span><br></pre></td></tr></table></figure><h2 id="浮点数计算"><a href="#浮点数计算" class="headerlink" title="浮点数计算"></a>浮点数计算</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">*无法精确表示数值，不能做移位和位运算*</span><br><span class="line">由于浮点数存在运算误差，所以比较两个浮点数是否相等常常会出现错误的结果。正确的比较方法是判断两个浮点数之差的绝对值是否小于一个很小的数</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 比较x和y是否相等，先计算其差的绝对值:</span><br><span class="line">double r = Math.abs(x - y);</span><br><span class="line">// 再判断绝对值是否足够小:</span><br><span class="line"><span class="keyword">if</span> (r &lt; <span class="number">0.00001</span>) &#123;</span><br><span class="line">    // 可以认为相等</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    // 不相等</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="类型提升"><a href="#类型提升" class="headerlink" title="类型提升"></a>类型提升</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果参与运算的两个数其中一个是整型，那么整型可以自动提升到浮点型</span><br></pre></td></tr></table></figure><h3 id="溢出-1"><a href="#溢出-1" class="headerlink" title="溢出"></a>溢出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">*整数在除零时编译时不出错，运行时出错*</span><br><span class="line">*浮点数除零不会报错，会返回特殊值：*</span><br><span class="line"></span><br><span class="line">NaN:<span class="keyword">not</span> a number</span><br><span class="line">Infinity:无穷大</span><br><span class="line">-Infinity:负无穷大</span><br><span class="line"></span><br><span class="line">double d1 = <span class="number">0.0</span> / <span class="number">0</span>; // NaN</span><br><span class="line">double d2 = <span class="number">1.0</span> / <span class="number">0</span>; // Infinity</span><br><span class="line">double d3 = <span class="number">-1.0</span> / <span class="number">0</span>; // -Infinity</span><br><span class="line"></span><br><span class="line">这三种特殊值在实际运算中很少碰到，我们只需要了解即可</span><br></pre></td></tr></table></figure><h3 id="强制转换-整与浮"><a href="#强制转换-整与浮" class="headerlink" title="强制转换(整与浮)"></a>强制转换(整与浮)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以将浮点数强制转型为整数。在转型时，浮点数的小数部分会被丢掉。如果转型后超过了整型能表示的最大范围，将返回整型的最大值。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int n1 = (int) <span class="number">12.3</span>; // <span class="number">12</span></span><br><span class="line">int n2 = (int) <span class="number">12.7</span>; // <span class="number">12</span></span><br><span class="line">int n2 = (int) <span class="number">-12.7</span>; // <span class="number">-12</span></span><br><span class="line">int n3 = (int) (<span class="number">12.7</span> + <span class="number">0.5</span>); // <span class="number">13</span></span><br><span class="line">int n4 = (int) <span class="number">1.2e20</span>; // <span class="number">2147483647</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">如果要进行四舍五入，可以对浮点数加上<span class="number">0.5</span>再强制转型</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        double d = <span class="number">2.6</span>;</span><br><span class="line">        int n = (int) (d + <span class="number">0.5</span>);</span><br><span class="line">        System.out.println(n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">浮点数常常无法精确表示，并且浮点数的运算结果可能有误差；</span><br><span class="line">比较两个浮点数通常比较它们的差的绝对值是否小于一个特定值；</span><br><span class="line">整型和浮点型运算时，整型会自动提升为浮点型；</span><br><span class="line">可以将浮点型强制转为整型，但超出范围后将始终返回整型的最大值。</span><br></pre></td></tr></table></figure><h2 id="布尔运算"><a href="#布尔运算" class="headerlink" title="布尔运算"></a>布尔运算</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">对于布尔类型boolean，永远只有true和false两个值。</span><br><span class="line">布尔运算是一种关系运算，包括以下几类：</span><br><span class="line"></span><br><span class="line">比较运算符：&gt;，&gt;&#x3D;，&lt;，&lt;&#x3D;，&#x3D;&#x3D;，!&#x3D;</span><br><span class="line">与运算 &amp;&amp;</span><br><span class="line">或运算 ||</span><br><span class="line">非运算 !</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">boolean isGreater = <span class="number">5</span> &gt; <span class="number">3</span>; // true</span><br><span class="line">int age = <span class="number">12</span>;</span><br><span class="line">boolean isZero = age == <span class="number">0</span>; // false</span><br><span class="line">boolean isNonZero = !isZero; // true</span><br><span class="line">boolean isAdult = age &gt;= <span class="number">18</span>; // false</span><br><span class="line">boolean isTeenager = age &gt;<span class="number">6</span> &amp;&amp; age &lt;<span class="number">18</span>; // true</span><br></pre></td></tr></table></figure><h3 id="关系运算符的优先级"><a href="#关系运算符的优先级" class="headerlink" title="关系运算符的优先级"></a>关系运算符的优先级</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">!</span><br><span class="line">&gt;，&gt;&#x3D;，&lt;，&lt;&#x3D;</span><br><span class="line">&#x3D;&#x3D;，!&#x3D;</span><br><span class="line">&amp;&amp;</span><br><span class="line">||</span><br></pre></td></tr></table></figure><h3 id="短路运算"><a href="#短路运算" class="headerlink" title="短路运算"></a>短路运算</h3><h4 id="true-amp-amp-任意"><a href="#true-amp-amp-任意" class="headerlink" title="true&amp;&amp;任意"></a>true&amp;&amp;任意</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">布尔运算的一个重要特点是短路运算。如果一个布尔运算的表达式能提前确定结果，则后续的计算不再执行，直接返回结果。</span><br><span class="line"></span><br><span class="line">因为false &amp;&amp; x的结果总是false，无论x是true还是false，因此，与运算在确定第一个值为false后，不再继续计算，而是直接返回false</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        boolean b &#x3D; 5 &lt; 3;</span><br><span class="line">        boolean result &#x3D; b &amp;&amp; (5 &#x2F; 0 &gt; 0);</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">false</span><br><span class="line"></span><br><span class="line">如果没有短路运算，&amp;&amp;后面的表达式会由于除数为0而报错，但实际上该语句并未报错，原因在于与运算是短路运算符，提前计算出了结果false</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HNRoOx" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5CHNRoOx.png" alt="HNRoOx.png"></a></p><h4 id="true-任意"><a href="#true-任意" class="headerlink" title="true || 任意"></a>true || 任意</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">类似的，对于||运算，只要能确定第一个值为true，后续计算也不再进行，而是直接返回true：</span><br><span class="line"></span><br><span class="line">boolean result &#x3D; true || (5 &#x2F; 0 &gt; 0); &#x2F;&#x2F; true</span><br></pre></td></tr></table></figure><h4 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Java还提供一个三元运算符b ? x : y，它根据第一个布尔表达式的结果，分别返回后续两个表达式之一的计算结果</span><br><span class="line"></span><br><span class="line">b为true返回x; b为false返回y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意到三元运算b ? x : y会首先计算b，如果b为true，则只计算x，否则，只计算y。此外，x和y的类型必须相同，因为返回值不是boolean，而是x和y之一。</span><br></pre></td></tr></table></figure><h4 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">与运算和或运算是短路运算；</span><br><span class="line">三元运算b ? x : y后面的类型必须相同，三元运算也是“短路运算”，只计算x或y。</span><br></pre></td></tr></table></figure><h2 id="字符和字符串"><a href="#字符和字符串" class="headerlink" title="字符和字符串"></a>字符和字符串</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Java中，字符和字符串是两个不同的类型</span><br></pre></td></tr></table></figure><h3 id="字符"><a href="#字符" class="headerlink" title="字符"></a>字符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">字符类型char是基本数据类型，它是character的缩写。一个char保存一个Unicode字符</span><br><span class="line"></span><br><span class="line">因为Java在内存中总是使用Unicode表示字符，所以，一个英文字符和一个中文字符都用一个char类型表示，它们都占用两个字节。要显示一个字符的Unicode编码，只需将char类型直接赋值给int类型即可：</span><br><span class="line"></span><br><span class="line">int n1 &#x3D; &#39;A&#39;; &#x2F;&#x2F; 字母“A”的Unicodde编码是65</span><br><span class="line">int n2 &#x3D; &#39;中&#39;; &#x2F;&#x2F; 汉字“中”的Unicode编码是20013</span><br><span class="line">还可以直接用转义字符\u+Unicode编码来表示一个字符：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 注意是十六进制:</span><br><span class="line">char c3 &#x3D; &#39;\u0041&#39;; &#x2F;&#x2F; &#39;A&#39;，因为十六进制0041 &#x3D; 十进制65</span><br><span class="line">char c4 &#x3D; &#39;\u4e2d&#39;; &#x2F;&#x2F; &#39;中&#39;，因为十六进制4e2d &#x3D; 十进制20013</span><br></pre></td></tr></table></figure><h3 id="字符串-1"><a href="#字符串-1" class="headerlink" title="字符串"></a>字符串</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">和char类型不同，字符串类型String是引用类型，我们用双引号<span class="string">"..."</span>表示字符串。一个字符串可以存储<span class="number">0</span>个到任意个字符：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">""</span>; // 空字符串，包含<span class="number">0</span>个字符</span><br><span class="line">String s1 = <span class="string">"A"</span>; // 包含一个字符</span><br><span class="line">String s2 = <span class="string">"ABC"</span>; // 包含<span class="number">3</span>个字符</span><br><span class="line">String s3 = <span class="string">"中文 ABC"</span>; // 包含<span class="number">6</span>个字符，其中有一个空格</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">因为字符串使用双引号<span class="string">"..."</span>表示开始和结束，那如果字符串本身恰好包含一个<span class="string">"字符怎么表示？例如，"</span>abc<span class="string">"xyz"</span>，编译器就无法判断中间的引号究竟是字符串的一部分还是表示字符串结束。这个时候，我们需要借助转义字符\：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"abc\"xyz"</span>; // 包含<span class="number">7</span>个字符: a, b, c, <span class="string">", x, y, z</span></span><br><span class="line"><span class="string">因为\是转义字符，所以，两个\\表示一个\字符：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">String s = "</span>abc\\xyz<span class="string">"; // 包含7个字符: a, b, c, \, x, y, z</span></span><br><span class="line"><span class="string">常见的转义字符包括：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">\" 表示字符"</span></span><br><span class="line">\<span class="string">' 表示字符'</span></span><br><span class="line">\\ 表示字符\</span><br><span class="line">\n 表示换行符</span><br><span class="line">\r 表示回车符</span><br><span class="line">\t 表示Tab</span><br><span class="line">\u<span class="comment">#### 表示一个Unicode编码的字符</span></span><br><span class="line">例如：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"ABC\n\u4e2d\u6587"</span>; // 包含<span class="number">6</span>个字符: A, B, C, 换行符, 中, 文</span><br></pre></td></tr></table></figure><h3 id="字符串连接"><a href="#字符串连接" class="headerlink" title="字符串连接"></a>字符串连接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Java的编译器对字符串做了特殊照顾，可以使用+连接任意字符串和其他数据类型，这样极大地方便了字符串的处理。例如：</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        String s1 = <span class="string">"Hello"</span>;</span><br><span class="line">        String s2 = <span class="string">"world"</span>;</span><br><span class="line">        String s = s1 + <span class="string">" "</span> + s2 + <span class="string">"!"</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">如果用+连接字符串和其他数据类型，会将其他数据类型先自动转型为字符串，再连接：</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int age = <span class="number">25</span>;</span><br><span class="line">        String s = <span class="string">"age is "</span> + age;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // 请将下面一组int值视为字符的Unicode码，把它们拼成一个字符串：</span><br><span class="line">        int a = <span class="number">72</span>;</span><br><span class="line">        int b = <span class="number">105</span>;</span><br><span class="line">        int c = <span class="number">65281</span>;</span><br><span class="line">        // FIXME</span><br><span class="line"> String s = <span class="string">""</span>+<span class="string">'\u0048'</span> +<span class="string">'\u0069'</span>  + <span class="string">'\uff01'</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class array &#123;    </span><br><span class="line">  public static void main(String[] args)&#123;        </span><br><span class="line">    int a = <span class="number">72</span>;        </span><br><span class="line">    int b = <span class="number">105</span>;        </span><br><span class="line">    int c = <span class="number">65281</span>;        </span><br><span class="line">    // FIXME:        </span><br><span class="line">    String s = <span class="string">""</span>+(char)a + (char)b + (char)c;        </span><br><span class="line">    System.out.println(s);    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="多行字符串"><a href="#多行字符串" class="headerlink" title="多行字符串"></a>多行字符串</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">如果我们要表示多行字符串，使用+号连接会非常不方便：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"first line \n"</span></span><br><span class="line">         + <span class="string">"second line \n"</span></span><br><span class="line">         + <span class="string">"end"</span>;</span><br><span class="line">从Java <span class="number">13</span>开始，字符串可以用<span class="string">"""..."""</span>表示多行字符串（Text Blocks）了</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        String s = <span class="string">"""</span></span><br><span class="line"><span class="string">                   SELECT * FROM</span></span><br><span class="line"><span class="string">                     users</span></span><br><span class="line"><span class="string">                   WHERE id &gt; 100</span></span><br><span class="line"><span class="string">                   ORDER BY name DESC</span></span><br><span class="line"><span class="string">                   """</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">上述多行字符串实际上是<span class="number">5</span>行，在最后一个DESC后面还有一个\n。如果我们不想在字符串末尾加一个\n，就需要这么写：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">""" </span></span><br><span class="line"><span class="string">           SELECT * FROM</span></span><br><span class="line"><span class="string">             users</span></span><br><span class="line"><span class="string">           WHERE id &gt; 100</span></span><br><span class="line"><span class="string">           ORDER BY name DESC"""</span>;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">还需要注意到，多行字符串前面共同的空格会被去掉，即：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"""</span></span><br><span class="line"><span class="string">...........SELECT * FROM</span></span><br><span class="line"><span class="string">...........  users</span></span><br><span class="line"><span class="string">...........WHERE id &gt; 100</span></span><br><span class="line"><span class="string">...........ORDER BY name DESC</span></span><br><span class="line"><span class="string">..........."""</span>;</span><br><span class="line">用.标注的空格都会被去掉。</span><br><span class="line"></span><br><span class="line">如果多行字符串的排版不规则，那么，去掉的空格就会变成这样：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"""</span></span><br><span class="line"><span class="string">.........  SELECT * FROM</span></span><br><span class="line"><span class="string">.........    users</span></span><br><span class="line"><span class="string">.........WHERE id &gt; 100</span></span><br><span class="line"><span class="string">.........  ORDER BY name DESC</span></span><br><span class="line"><span class="string">.........  """</span>;</span><br><span class="line">即总是以最短的行首空格为基准。</span><br></pre></td></tr></table></figure><h3 id="不可变特性"><a href="#不可变特性" class="headerlink" title="不可变特性"></a>不可变特性</h3><p><a href="https://imgtu.com/i/HUnuM8" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5CHUnuM8.md.png" alt="HUnuM8.md.png"></a></p><h3 id="空值"><a href="#空值" class="headerlink" title="空值"></a>空值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">引用类型的变量可以指向一个空值null，它表示不存在，即该变量不指向任何对象。例如：</span><br><span class="line"></span><br><span class="line">String s1 = null; // s1是null</span><br><span class="line">String s2; // 没有赋初值值，s2也是null</span><br><span class="line">String s3 = s1; // s3也是null</span><br><span class="line">String s4 = <span class="string">""</span>; // s4指向空字符串，不是null</span><br><span class="line">注意要区分空值null和空字符串<span class="string">""</span>，空字符串是一个有效的字符串对象，它不等于null</span><br></pre></td></tr></table></figure><h3 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Java的字符类型char是基本类型，字符串类型String是引用类型；</span><br><span class="line">基本类型的变量是“持有”某个数值，引用类型的变量是“指向”某个对象；</span><br><span class="line">引用类型的变量可以是空值null；</span><br><span class="line">要区分空值null和空字符串<span class="string">""</span></span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-Java程序基础 Java程序基本结构</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.html</id>
    <published>2023-03-29T14:19:48.000Z</published>
    <updated>2023-03-29T17:52:28.727Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="java程序基本结构"><a href="#java程序基本结构" class="headerlink" title="java程序基本结构"></a>java程序基本结构</h1><h2 id="类名规范"><a href="#类名规范" class="headerlink" title="类名规范"></a>类名规范</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">+ 首字母大写</span><br><span class="line">+ 字母开头，数字，下划线组合</span><br></pre></td></tr></table></figure><h2 id="方法名规范"><a href="#方法名规范" class="headerlink" title="方法名规范"></a>方法名规范</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命名和class一样，但是首字母小写</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里的方法名是main，返回值是void，表示没有任何返回值。</span><br><span class="line"></span><br><span class="line">我们注意到public除了可以修饰class外，也可以修饰方法。而关键字static是另一个修饰符，它表示静态方法，后面我们会讲解方法的类型，目前，我们只需要知道，Java入口程序规定的方法必须是静态方法，方法名必须为main，括号内的参数必须是String数组。</span><br><span class="line"></span><br><span class="line">每一行语句，分号结尾</span><br></pre></td></tr></table></figure><h2 id="注释方法"><a href="#注释方法" class="headerlink" title="注释方法"></a>注释方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">+ &#x2F;&#x2F;</span><br><span class="line">+ &#x2F;*... *&#x2F;</span><br><span class="line">+ &#x2F;**... *&#x2F;  这是一种特殊注释方法，用在类和方法的定义出，用于自动创建文档</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Java程序对格式没有明确的要求，多几个空格或者回车不影响程序的正确性，但是我们要养成良好的编程习惯</span><br><span class="line">*对于eclipse可以用快捷键ctrl+shift+f，快速格式化代码*</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-泛型</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-%E6%B3%9B%E5%9E%8B.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-%E6%B3%9B%E5%9E%8B.html</id>
    <published>2023-03-29T10:03:11.000Z</published>
    <updated>2023-03-29T17:52:04.932Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h1><h2 id="什么是泛型"><a href="#什么是泛型" class="headerlink" title="什么是泛型"></a>什么是泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ArrayList是一种可变长数组，其内部使用的是Object类型</span><br><span class="line"></span><br><span class="line">public class ArrayList &#123;</span><br><span class="line">    private Object[] array;</span><br><span class="line">    private int size;</span><br><span class="line">    public void add(Object e) &#123;...&#125;</span><br><span class="line">    public void remove(int index) &#123;...&#125;</span><br><span class="line">    public Object get(int index) &#123;...&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果用上述ArrayList存储String类型，会有这么几个缺点：</span><br><span class="line">需要强制转型；</span><br><span class="line">不方便，易出错。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ArrayList list &#x3D; new ArrayList();</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">&#x2F;&#x2F; 获取到Object，必须强制转型为String:</span><br><span class="line">String first &#x3D; (String) list.get(0);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">很容易出现ClassCastException，因为容易“误转型”：</span><br><span class="line">list.add(new Integer(123));</span><br><span class="line">&#x2F;&#x2F; ERROR: ClassCastException:</span><br><span class="line">String second &#x3D; (String) list.get(1);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">要解决上述问题，我们可以为String单独编写一种ArrayList：</span><br><span class="line">public class StringArrayList &#123;</span><br><span class="line">    private String[] array;</span><br><span class="line">    private int size;</span><br><span class="line">    public void add(String e) &#123;...&#125;</span><br><span class="line">    public void remove(int index) &#123;...&#125;</span><br><span class="line">    public String get(int index) &#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这样一来，存入的必须是String，取出的也一定是String，不需要强制转型，因为编译器会强制检查放入的类型：</span><br><span class="line">StringArrayList list &#x3D; new StringArrayList();</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">String first &#x3D; list.get(0);</span><br><span class="line">&#x2F;&#x2F; 编译错误: 不允许放入非String类型:</span><br><span class="line">list.add(new Integer(123));</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">问题暂时解决。</span><br><span class="line"></span><br><span class="line">然而，新的问题是，如果要存储Integer，还需要为Integer单独编写一种ArrayList：</span><br><span class="line">实际上，还需要为其他所有class单独编写一种ArrayList：</span><br><span class="line"></span><br><span class="line">LongArrayList</span><br><span class="line">DoubleArrayList</span><br><span class="line">PersonArrayList</span><br><span class="line">...</span><br><span class="line">这是不可能的，JDK的class就有上千个，而且它还不知道其他人编写的class。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">为了解决新的问题，我们必须把ArrayList变成一种模板：ArrayList&lt;T&gt;，代码如下：</span><br><span class="line">public class ArrayList&lt;T&gt; &#123;</span><br><span class="line">    private T[] array;</span><br><span class="line">    private int size;</span><br><span class="line">    public void add(T e) &#123;...&#125;</span><br><span class="line">    public void remove(int index) &#123;...&#125;</span><br><span class="line">    public T get(int index) &#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">T可以是任何class。这样一来，我们就实现了：编写一次模版，可以创建任意类型的ArrayList：</span><br><span class="line">&#x2F;&#x2F; 创建可以存储String的ArrayList:</span><br><span class="line">ArrayList&lt;String&gt; strList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">&#x2F;&#x2F; 创建可以存储Float的ArrayList:</span><br><span class="line">ArrayList&lt;Float&gt; floatList &#x3D; new ArrayList&lt;Float&gt;();</span><br><span class="line">&#x2F;&#x2F; 创建可以存储Person的ArrayList:</span><br><span class="line">ArrayList&lt;Person&gt; personList &#x3D; new ArrayList&lt;Person&gt;();</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">因此，泛型就是定义一种模板，例如ArrayList&lt;T&gt;，然后在代码中为用到的类创建对应的ArrayList&lt;类型&gt;：</span><br><span class="line">ArrayList&lt;String&gt; strList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">由编译器针对类型作检查：</span><br><span class="line"></span><br><span class="line">strList.add(&quot;hello&quot;); &#x2F;&#x2F; OK</span><br><span class="line">String s &#x3D; strList.get(0); &#x2F;&#x2F; OK</span><br><span class="line">strList.add(new Integer(123)); &#x2F;&#x2F; compile error!</span><br><span class="line">Integer n &#x3D; strList.get(0); &#x2F;&#x2F; compile error!</span><br><span class="line"></span><br><span class="line">这样一来，既实现了编写一次，万能匹配，又通过编译器保证了类型安全：这就是泛型。</span><br></pre></td></tr></table></figure><h3 id="向上转型"><a href="#向上转型" class="headerlink" title="向上转型"></a>向上转型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在Java标准库中的ArrayList&lt;T&gt;实现了List&lt;T&gt;接口，它可以向上转型为List&lt;T&gt;：</span><br><span class="line"></span><br><span class="line">public class ArrayList&lt;T&gt; implements List&lt;T&gt; &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">即类型ArrayList&lt;T&gt;可以向上转型为List&lt;T&gt;。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">要特别注意：不能把ArrayList&lt;Integer&gt;向上转型为ArrayList&lt;Number&gt;或List&lt;Number&gt;。</span><br><span class="line"></span><br><span class="line">这是为什么呢？假设ArrayList&lt;Integer&gt;可以向上转型为ArrayList&lt;Number&gt;，观察一下代码：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 创建ArrayList&lt;Integer&gt;类型：</span><br><span class="line">ArrayList&lt;Integer&gt; integerList &#x3D; new ArrayList&lt;Integer&gt;();</span><br><span class="line">&#x2F;&#x2F; 添加一个Integer：</span><br><span class="line">integerList.add(new Integer(123));</span><br><span class="line">&#x2F;&#x2F; “向上转型”为ArrayList&lt;Number&gt;：</span><br><span class="line">ArrayList&lt;Number&gt; numberList &#x3D; integerList;</span><br><span class="line">&#x2F;&#x2F; 添加一个Float，因为Float也是Number：</span><br><span class="line">numberList.add(new Float(12.34));</span><br><span class="line">&#x2F;&#x2F; 从ArrayList&lt;Integer&gt;获取索引为1的元素（即添加的Float）：</span><br><span class="line">Integer n &#x3D; integerList.get(1); &#x2F;&#x2F; ClassCastException!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们把一个ArrayList&lt;Integer&gt;转型为ArrayList&lt;Number&gt;类型后，这个ArrayList&lt;Number&gt;就可以接受Float类型，因为Float是Number的子类。但是，ArrayList&lt;Number&gt;实际上和ArrayList&lt;Integer&gt;是同一个对象，也就是ArrayList&lt;Integer&gt;类型，它不可能接受Float类型， 所以在获取Integer的时候将产生ClassCastException。</span><br><span class="line"></span><br><span class="line">实际上，编译器为了避免这种错误，根本就不允许把ArrayList&lt;Integer&gt;转型为ArrayList&lt;Number&gt;。</span><br><span class="line"></span><br><span class="line"> ArrayList&lt;Integer&gt;和ArrayList&lt;Number&gt;两者完全没有继承关系。</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">泛型就是编写模板代码来适应任意类型；</span><br><span class="line"></span><br><span class="line">泛型的好处是使用时不必对类型进行强制转换，它通过编译器对类型进行检查；</span><br><span class="line"></span><br><span class="line">注意泛型的继承关系：可以把&#96;ArrayList&lt;Integer&gt;&#96;向上转型为&#96;List&lt;Integer&gt;&#96;（&#96;T&#96;不能变！），但不能把&#96;ArrayList&lt;Integer&gt;&#96;向上转型为&#96;ArrayList&lt;Number&gt;&#96;（&#96;T&#96;不能变成父类）。</span><br></pre></td></tr></table></figure><h2 id="使用泛型"><a href="#使用泛型" class="headerlink" title="使用泛型"></a>使用泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">使用ArrayList时，如果不定义泛型类型时，泛型类型实际上就是Object：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 编译器警告:</span><br><span class="line">List list &#x3D; new ArrayList(); &#x2F;&#x2F;这里吧ArrayList向上转型为List</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">list.add(&quot;World&quot;);</span><br><span class="line">String first &#x3D; (String) list.get(0);</span><br><span class="line">String second &#x3D; (String) list.get(1);</span><br><span class="line">此时，只能把&lt;T&gt;当作Object使用，没有发挥泛型的优势。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当我们定义泛型类型&lt;String&gt;后，List&lt;T&gt;的泛型接口变为强类型List&lt;String&gt;：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 无编译器警告:</span><br><span class="line">List&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">list.add(&quot;World&quot;);</span><br><span class="line">&#x2F;&#x2F; 无强制转型:</span><br><span class="line">String first &#x3D; list.get(0);</span><br><span class="line">String second &#x3D; list.get(1);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当我们定义泛型类型&lt;Number&gt;后，List&lt;T&gt;的泛型接口变为强类型List&lt;Number&gt;：</span><br><span class="line"></span><br><span class="line">List&lt;Number&gt; list &#x3D; new ArrayList&lt;Number&gt;();</span><br><span class="line">list.add(new Integer(123));</span><br><span class="line">list.add(new Double(12.34));</span><br><span class="line">Number first &#x3D; list.get(0);</span><br><span class="line">Number second &#x3D; list.get(1);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">编译器如果能自动推断出泛型类型，就可以省略后面的泛型类型。例如，对于下面的代码：</span><br><span class="line"></span><br><span class="line">List&lt;Number&gt; list &#x3D; new ArrayList&lt;Number&gt;();</span><br><span class="line">编译器看到泛型类型List&lt;Number&gt;就可以自动推断出后面的ArrayList&lt;T&gt;的泛型类型必须是ArrayList&lt;Number&gt;，因此，可以把代码简写为：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 可以省略后面的Number，编译器可以自动推断泛型类型：</span><br><span class="line">List&lt;Number&gt; list &#x3D; new ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><h3 id="泛型接口"><a href="#泛型接口" class="headerlink" title="泛型接口"></a>泛型接口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">除了ArrayList&lt;T&gt;使用了泛型，还可以在接口中使用泛型。例如，Arrays.sort(Object[])可以对任意数组进行排序，但待排序的元素必须实现Comparable&lt;T&gt;这个泛型接口：</span><br><span class="line"></span><br><span class="line">public interface Comparable&lt;T&gt; &#123;</span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 返回负数: 当前实例比参数o小</span><br><span class="line">     * 返回0: 当前实例与参数o相等</span><br><span class="line">     * 返回正数: 当前实例比参数o大</span><br><span class="line">     *&#x2F;</span><br><span class="line">    int compareTo(T o);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">可以直接对String数组进行排序：</span><br><span class="line">&#x2F;&#x2F; sort</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">String[] ss &#x3D; new String[] &#123; &quot;Orange&quot;, &quot;Apple&quot;, &quot;Pear&quot; &#125;;</span><br><span class="line">        Arrays.sort(ss);</span><br><span class="line">        System.out.println(Arrays.toString(ss));</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">这是因为String本身已经实现了Comparable&lt;String&gt;接口。如果换成我们自定义的Person类型试试：</span><br><span class="line">&#x2F;&#x2F; sort</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">    Person[] ps &#x3D; new Person[] &#123;</span><br><span class="line">            new Person(&quot;Bob&quot;, 61),</span><br><span class="line">            new Person(&quot;Alice&quot;, 88),</span><br><span class="line">            new Person(&quot;Lily&quot;, 75),</span><br><span class="line">        &#125;;</span><br><span class="line">        Arrays.sort(ps);</span><br><span class="line">        System.out.println(Arrays.toString(ps));</span><br><span class="line"></span><br><span class="line">class Person &#123;</span><br><span class="line">    String name;</span><br><span class="line">    int score;</span><br><span class="line">    Person(String name, int score) &#123;</span><br><span class="line">        this.name &#x3D; name;</span><br><span class="line">        this.score &#x3D; score;</span><br><span class="line">    &#125;</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return this.name + &quot;,&quot; + this.score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">运行程序，我们会得到ClassCastException，即无法将Person转型为Comparable。我们修改代码，让Person实现Comparable&lt;T&gt;接口：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; sort</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Person[] ps &#x3D; new Person[] &#123;</span><br><span class="line">            new Person(&quot;Bob&quot;, 61),</span><br><span class="line">            new Person(&quot;Alice&quot;, 88),</span><br><span class="line">            new Person(&quot;Lily&quot;, 75),</span><br><span class="line">        &#125;;</span><br><span class="line">        Arrays.sort(ps);</span><br><span class="line">        System.out.println(Arrays.toString(ps));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Person implements Comparable&lt;Person&gt; &#123;</span><br><span class="line">    String name;</span><br><span class="line">    int score;</span><br><span class="line">    Person(String name, int score) &#123;</span><br><span class="line">        this.name &#x3D; name;</span><br><span class="line">        this.score &#x3D; score;</span><br><span class="line">    &#125;</span><br><span class="line">    public int compareTo(Person other) &#123;</span><br><span class="line">        return this.name.compareTo(other.name);</span><br><span class="line">    &#125;</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return this.name + &quot;,&quot; + this.score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行上述代码，可以正确实现按name进行排序。</span><br><span class="line"></span><br><span class="line">也可以修改比较逻辑，例如，按score从高到低排序。请自行修改测试。</span><br></pre></td></tr></table></figure><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">使用泛型时，把泛型参数&lt;T&gt;替换为需要的class类型，例如：ArrayList&lt;String&gt;，ArrayList&lt;Number&gt;等；</span><br><span class="line"></span><br><span class="line">可以省略编译器能自动推断出的类型，例如：List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();；</span><br><span class="line"></span><br><span class="line">不指定泛型参数类型时，编译器会给出警告，且只能将&lt;T&gt;视为Object类型；</span><br><span class="line"></span><br><span class="line">可以在接口中定义泛型类型，实现此接口的类必须实现正确的泛型类型。</span><br></pre></td></tr></table></figure><h2 id="编写泛型"><a href="#编写泛型" class="headerlink" title="编写泛型"></a>编写泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">编写泛型类比普通类要复杂。通常来说，泛型类一般用在集合类中，例如ArrayList&lt;T&gt;，我们很少需要编写泛型类。</span><br><span class="line"></span><br><span class="line">如果我们确实需要编写一个泛型类，那么，应该如何编写它？</span><br><span class="line">可以按照以下步骤来编写一个泛型类。</span><br><span class="line">首先，按照某种类型，例如：String，来编写类：</span><br><span class="line"></span><br><span class="line">public class Pair &#123;</span><br><span class="line">    private String first;</span><br><span class="line">    private String last;</span><br><span class="line">    public Pair(String first, String last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">然后，标记所有的特定类型，这里是String：</span><br><span class="line"></span><br><span class="line">public class Pair &#123;</span><br><span class="line">    private String first;</span><br><span class="line">    private String last;</span><br><span class="line">    public Pair(String first, String last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">最后，把特定类型String替换为T，并申明&lt;T&gt;：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">熟练后即可直接从T开始编写。</span><br></pre></td></tr></table></figure><h3 id="静态方法"><a href="#静态方法" class="headerlink" title="静态方法"></a>静态方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">编写泛型类时，要特别注意，泛型类型&lt;T&gt;不能用于静态方法。例如：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 对静态方法使用&lt;T&gt;:</span><br><span class="line">    public static Pair&lt;T&gt; create(T first, T last) &#123;</span><br><span class="line">        return new Pair&lt;T&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">上述代码会导致编译错误，我们无法在静态方法create()的方法参数和返回类型上使用泛型类型T。</span><br><span class="line"></span><br><span class="line">有些同学在网上搜索发现，可以在static修饰符后面加一个&lt;T&gt;，编译就能通过：</span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 可以编译通过:</span><br><span class="line">    public static &lt;T&gt; Pair&lt;T&gt; create(T first, T last) &#123;</span><br><span class="line">        return new Pair&lt;T&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">但实际上，这个&lt;T&gt;和Pair&lt;T&gt;类型的&lt;T&gt;已经没有任何关系了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">对于静态方法，我们可以单独改写为“泛型”方法，只需要使用另一个类型即可。对于上面的create()静态方法，我们应该把它改为另一种泛型类型，例如，&lt;K&gt;：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 静态泛型方法应该使用其他类型区分:</span><br><span class="line">    public static &lt;K&gt; Pair&lt;K&gt; create(K first, K last) &#123;</span><br><span class="line">        return new Pair&lt;K&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这样才能清楚地将静态方法的泛型类型和实例类型的泛型类型区分开。</span><br></pre></td></tr></table></figure><h3 id="多个泛型类型"><a href="#多个泛型类型" class="headerlink" title="多个泛型类型"></a>多个泛型类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">泛型还可以定义多种类型。例如，我们希望Pair不总是存储两个类型一样的对象，就可以使用类型&lt;T, K&gt;：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T, K&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private K last;</span><br><span class="line">    public Pair(T first, K last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public K getLast() &#123; ... &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">使用的时候，需要指出两种类型：</span><br><span class="line">Pair&lt;String, Integer&gt; p &#x3D; new Pair&lt;&gt;(&quot;test&quot;, 123);</span><br><span class="line"></span><br><span class="line">Java标准库的Map&lt;K, V&gt;就是使用两种泛型类型的例子。它对Key使用一种类型，对Value使用另一种类型。</span><br></pre></td></tr></table></figure><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">编写泛型时，需要定义泛型类型&lt;T&gt;；</span><br><span class="line"></span><br><span class="line">静态方法不能引用泛型类型&lt;T&gt;，必须定义其他类型（例如&lt;K&gt;）来实现静态泛型方法；</span><br><span class="line"></span><br><span class="line">泛型可以同时定义多种类型，例如Map&lt;K, V&gt;。</span><br></pre></td></tr></table></figure><h2 id="擦拭法"><a href="#擦拭法" class="headerlink" title="擦拭法"></a>擦拭法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">泛型是一种类似”模板代码“的技术，不同语言的泛型实现方式不一定相同。</span><br><span class="line">Java语言的泛型实现方式是擦拭法（Type Erasure）。</span><br><span class="line">所谓擦拭法是指，虚拟机对泛型其实一无所知，所有的工作都是编译器做的。</span><br><span class="line"></span><br><span class="line">例如，我们编写了一个泛型类Pair&lt;T&gt;，这是编译器看到的代码：</span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">而虚拟机根本不知道泛型。这是虚拟机执行的代码：</span><br><span class="line"></span><br><span class="line">public class Pair &#123;</span><br><span class="line">    private Object first;</span><br><span class="line">    private Object last;</span><br><span class="line">    public Pair(Object first, Object last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public Object getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public Object getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">因此，Java使用擦拭法实现泛型，导致了：</span><br><span class="line"></span><br><span class="line">编译器把类型&lt;T&gt;视为Object；</span><br><span class="line">编译器根据&lt;T&gt;实现安全的强制转型。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">使用泛型的时候，我们编写的代码也是编译器看到的代码：</span><br><span class="line"></span><br><span class="line">Pair&lt;String&gt; p &#x3D; new Pair&lt;&gt;(&quot;Hello&quot;, &quot;world&quot;);</span><br><span class="line">String first &#x3D; p.getFirst();</span><br><span class="line">String last &#x3D; p.getLast();</span><br><span class="line">而虚拟机执行的代码并没有泛型：</span><br><span class="line"></span><br><span class="line">Pair p &#x3D; new Pair(&quot;Hello&quot;, &quot;world&quot;);</span><br><span class="line">String first &#x3D; (String) p.getFirst();</span><br><span class="line">String last &#x3D; (String) p.getLast();</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">所以，Java的泛型是由编译器在编译时实行的，编译器内部永远把所有类型T视为Object处理，但是，在需要转型的时候，编译器会根据T的类型自动为我们实行安全地强制转型。</span><br><span class="line"></span><br><span class="line">了解了Java泛型的实现方式——擦拭法，我们就知道了Java泛型的局限：</span><br><span class="line">局限一：&lt;T&gt;不能是基本类型，例如int，因为实际类型是Object，Object类型无法持有基本类型：</span><br><span class="line"></span><br><span class="line">Pair&lt;int&gt; p &#x3D; new Pair&lt;&gt;(1, 2); &#x2F;&#x2F; compile error!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">局限二：无法取得带泛型的Class。观察以下代码：</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">Pair&lt;String&gt; p1 &#x3D; new Pair&lt;&gt;(&quot;Hello&quot;, &quot;world&quot;);</span><br><span class="line">        Pair&lt;Integer&gt; p2 &#x3D; new Pair&lt;&gt;(123, 456);</span><br><span class="line">        Class c1 &#x3D; p1.getClass();</span><br><span class="line">        Class c2 &#x3D; p2.getClass();</span><br><span class="line">        System.out.println(c1&#x3D;&#x3D;c2); &#x2F;&#x2F; true</span><br><span class="line">        System.out.println(c1&#x3D;&#x3D;Pair.class); &#x2F;&#x2F; true</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">因为T是Object，我们对Pair&lt;String&gt;和Pair&lt;Integer&gt;类型获取Class时，获取到的是同一个Class，也就是Pair类的Class。</span><br><span class="line"></span><br><span class="line">换句话说，所有泛型实例，无论T的类型是什么，getClass()返回同一个Class实例，因为编译后它们全部都是Pair&lt;Object&gt;。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">局限三：无法判断带泛型的类型：</span><br><span class="line"></span><br><span class="line">Pair&lt;Integer&gt; p &#x3D; new Pair&lt;&gt;(123, 456);</span><br><span class="line">&#x2F;&#x2F; Compile error:</span><br><span class="line">if (p instanceof Pair&lt;String&gt;) &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">原因和前面一样，并不存在Pair&lt;String&gt;.class，而是只有唯一的Pair.class。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">局限四：不能实例化T类型：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair() &#123;</span><br><span class="line">        &#x2F;&#x2F; Compile error:</span><br><span class="line">        first &#x3D; new T();</span><br><span class="line">        last &#x3D; new T();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">上述代码无法通过编译，因为构造方法的两行语句：</span><br><span class="line"></span><br><span class="line">first &#x3D; new T();</span><br><span class="line">last &#x3D; new T();</span><br><span class="line"></span><br><span class="line">擦拭后实际上变成了：</span><br><span class="line"></span><br><span class="line">first &#x3D; new Object();</span><br><span class="line">last &#x3D; new Object();</span><br><span class="line"></span><br><span class="line">这样一来，创建new Pair&lt;String&gt;()和创建new Pair&lt;Integer&gt;()就全部成了Object，显然编译器要阻止这种类型不对的代码。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">要实例化T类型，我们必须借助额外的Class&lt;T&gt;参数：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(Class&lt;T&gt; clazz) &#123;</span><br><span class="line">        first &#x3D; clazz.newInstance();</span><br><span class="line">        last &#x3D; clazz.newInstance();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">上述代码借助Class&lt;T&gt;参数并通过反射来实例化T类型，使用的时候，也必须传入Class&lt;T&gt;。例如：</span><br><span class="line"></span><br><span class="line">Pair&lt;String&gt; pair &#x3D; new Pair&lt;&gt;(String.class);</span><br><span class="line">因为传入了Class&lt;String&gt;的实例，所以我们借助String.class就可以实例化String类型。</span><br></pre></td></tr></table></figure><h3 id="不恰当的覆写方法"><a href="#不恰当的覆写方法" class="headerlink" title="不恰当的覆写方法"></a>不恰当的覆写方法</h3><h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 SparkSql</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-SparkSql.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-SparkSql.html</id>
    <published>2023-03-28T07:23:48.000Z</published>
    <updated>2023-03-29T08:00:59.233Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-SparkSql-极速上手SparkSql"><a href="#第十一周-SparkSql-极速上手SparkSql" class="headerlink" title="第十一周 SparkSql-极速上手SparkSql"></a>第十一周 SparkSql-极速上手SparkSql</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">之前已学完，spark core，离线数据计算</span><br></pre></td></tr></table></figure><h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL和我们之前讲Hive的时候说的hive on spark是不一样的。</span><br><span class="line">hive on spark是表示把底层的mapreduce引擎替换为spark引擎。</span><br><span class="line">而Spark SQL是Spark自己实现的一套SQL处理引擎。</span><br><span class="line"></span><br><span class="line">Spark SQL是Spark中的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象，就是DataFrame。</span><br><span class="line"></span><br><span class="line">DataFrame&#x3D;RDD+Schema 。</span><br><span class="line">它其实和关系型数据库中的表非常类似，RDD可以认为是表中的数据，Schema是表结构信息。</span><br><span class="line">DataFrame可以通过很多来源进行构建，包括：结构化的数据文件，Hive中的表，外部的关系型数据库，以及RDD</span><br><span class="line">Spark1.3出现的DataFrame ，Spark1.6出现了DataSet，在Spark2.0中两者统一，DataFrame等于DataSet[Row]</span><br></pre></td></tr></table></figure><h2 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">要使用Spark SQL，首先需要创建一个SpakSession对象</span><br><span class="line">SparkSession中包含了SparkContext和SqlContext</span><br><span class="line">所以说想通过SparkSession来操作RDD的话需要先通过它来获取SparkContext</span><br><span class="line"></span><br><span class="line">这个SqlContext是使用sparkSQL操作hive的时候会用到的。</span><br></pre></td></tr></table></figure><h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">使用SparkSession，可以从RDD、HIve表或者其它数据源创建DataFrame</span><br><span class="line">那下面我们来使用JSON文件来创建一个DataFrame</span><br><span class="line">想要使用spark-sql需要先添加spark-sql的依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在项目中添加sql这个包名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281536605.png" alt="image-20230328153653400"></p><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281542225.png" alt="image-20230328154207572"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.sql</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：使用json文件创建DataFrame</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object SqlDemoScala&#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">         val conf &#x3D; new SparkConf()</span><br><span class="line">         .setMaster(&quot;local&quot;)</span><br><span class="line">         &#x2F;&#x2F;创建SparkSession对象，里面包含SparkContext和SqlContext</span><br><span class="line">         val sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">         .appName(&quot;SqlDemoScala&quot;)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         &#x2F;&#x2F;读取json文件，获取DataFrame</span><br><span class="line">         val stuDf &#x3D; sparkSession.read.json(&quot;D:\\student.json&quot;)</span><br><span class="line">         &#x2F;&#x2F;查看DataFrame中的数据</span><br><span class="line">         stuDf.show()</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281546823.png" alt="image-20230328154309744"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用json文件创建DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SqlDemoJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"SqlDemoJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//读取json文件，获取Dataset&lt;Row&gt;</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.read().json(<span class="string">"D:\\student.json"</span>); <span class="comment">// 返回的dataset其实和dataframe一样的，新版本合并了</span></span><br><span class="line">         stuDf.show();</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281546440.png" alt="image-20230328154617041"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于DataFrame等于DataSet[Row]，它们两个可以互相转换，所以创建哪个都是一样的</span><br><span class="line">咱们前面的scala代码默认创建的是DataFrame，java代码默认创建的是DataSet</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">尝试对他们进行转换</span><br><span class="line">在Scala代码中将DataFrame转换为DataSet[Row]，对后面的操作没有影响</span><br><span class="line">&#x2F;&#x2F;将DataFrame转换为DataSet[Row]</span><br><span class="line">val stuDf &#x3D; sparkSession.read.json(&quot;D:\\student.json&quot;).as(&quot;stu&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在Java代码中将DataSet[Row]转换为DataFrame</span><br><span class="line">&#x2F;&#x2F;将Dataset&lt;Row&gt;转换为DataFrame</span><br><span class="line">Dataset&lt;Row&gt; stuDf &#x3D; sparkSession.read().json(&quot;D:\\student.json&quot;).toDF();</span><br></pre></td></tr></table></figure><h3 id="DataFrame常见算子操作"><a href="#DataFrame常见算子操作" class="headerlink" title="DataFrame常见算子操作"></a>DataFrame常见算子操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下Spark sql中针对DataFrame常见的算子操作</span><br><span class="line">先看一下官方文档</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281552610.png" alt="image-20230328155250497"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">printSchema()</span><br><span class="line">show()</span><br><span class="line">select()</span><br><span class="line">filter()、where()</span><br><span class="line">groupBy()</span><br><span class="line">count()</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.sql</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：DataFrame常见操作</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object DataFrameOpScala &#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">         val conf &#x3D; new SparkConf()</span><br><span class="line">         .setMaster(&quot;local&quot;)</span><br><span class="line">         &#x2F;&#x2F;创建SparkSession对象，里面包含SparkContext和SqlContext</span><br><span class="line">         val sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">         .appName(&quot;DataFrameOpScala&quot;)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">        val stuDf &#x3D; sparkSession.read.json(&quot;D:\\student.json&quot;)</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;打印schema信息</span><br><span class="line">         stuDf.printSchema()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;默认显示所有数据，可以通过参数控制显示多少条</span><br><span class="line">         stuDf.show(2)</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;查询数据中的指定字段信息</span><br><span class="line">         stuDf.select(&quot;name&quot;,&quot;age&quot;).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;在使用select的时候可以对数据做一些操作，需要添加隐式转换函数，否则语法报错</span><br><span class="line">         import sparkSession.implicits._</span><br><span class="line">         stuDf.select($&quot;name&quot;,$&quot;age&quot; + 1).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;对数据进行过滤，需要添加隐式转换函数，否则语法报错</span><br><span class="line">         stuDf.filter($&quot;age&quot;&gt;18).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;where底层调用的就是filter</span><br><span class="line">        stuDf.where($&quot;age&quot;&gt;18).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;对数据进行分组求和</span><br><span class="line">         stuDf.groupBy(&quot;age&quot;).count().show()</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281556810.png" alt="image-20230328155651098"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281557659.png" alt="image-20230328155732191"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281558631.png" alt="image-20230328155839255"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281601278.png" alt="image-20230328160101149"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281602257.png" alt="image-20230328160159987"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281602257.png" alt></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281603534.png" alt="image-20230328160345238"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.java.sql;</span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import static org.apache.spark.sql.functions.col;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：DataFrame常见操作</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class DataFrameOpJava &#123;</span><br><span class="line">     public static void main(String[] args) &#123;</span><br><span class="line">         SparkConf conf &#x3D; new SparkConf();</span><br><span class="line">         conf.setMaster(&quot;local&quot;);</span><br><span class="line">         &#x2F;&#x2F;创建SparkSession对象，里面包含SparkContext和SqlContext</span><br><span class="line">         SparkSession sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">         .appName(&quot;DataFrameOpJava&quot;)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         Dataset&lt;Row&gt; stuDf &#x3D; sparkSession.read().json(&quot;D:\\student.json&quot;);</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;打印schema信息</span><br><span class="line">         stuDf.printSchema();</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;默认显示所有数据，可以通过参数控制显示多少条</span><br><span class="line">         stuDf.show(2);</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;查询数据中的指定字段信息</span><br><span class="line">         stuDf.select(&quot;name&quot;,&quot;age&quot;).show();</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;在select的时候可以对数据做一些操作,需要引入import static org.apache.spa</span><br><span class="line">         stuDf.select(col(&quot;name&quot;),col(&quot;age&quot;).plus(1)).show();</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;对数据进行过滤</span><br><span class="line">         stuDf.filter(col(&quot;age&quot;).gt(18)).show();</span><br><span class="line">         stuDf.where(col(&quot;age&quot;).gt(18)).show();</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;对数据进行分组求和</span><br><span class="line">         stuDf.groupBy(&quot;age&quot;).count().show();</span><br><span class="line">         sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这些就是针对DataFrame的一些常见的操作。</span><br><span class="line">但是现在这种方式其实用起来还是不方便，只是提供了一些类似于可以操作表的算子，很对一些简单的查询还是可以的，但是针对一些复杂的操作，使用算子写起来就很麻烦了，所以我们希望能够直接支持用sql的方式执行，Spark SQL也是支持的</span><br></pre></td></tr></table></figure><h3 id="DataFrame的sql操作"><a href="#DataFrame的sql操作" class="headerlink" title="DataFrame的sql操作"></a>DataFrame的sql操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">想要实现直接支持sql语句查询DataFrame中的数据</span><br><span class="line">需要两步操作</span><br><span class="line">1. 先将DataFrame注册为一个临时表</span><br><span class="line">2. 使用sparkSession中的sql函数执行sql语句</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用sql操作DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameSqlScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"DataFrameSqlScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="keyword">val</span> stuDf = sparkSession.read.json(<span class="string">"D:\\student.json"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//将DataFrame注册为一个临时表</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">         <span class="comment">//使用sql查询临时表中的数据</span></span><br><span class="line">         sparkSession.sql(<span class="string">"select age,count(*) as num from student group by age"</span>)</span><br><span class="line">         .show()</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303282324292.png" alt="image-20230328232442914"></p><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用sql操作DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataFrameSqlJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"DataFrameSqlJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.read().json(<span class="string">"D:\\student.json"</span>);</span><br><span class="line">         <span class="comment">//将Dataset&lt;Row&gt;注册为一个临时表</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>);</span><br><span class="line">         <span class="comment">//使用sql查询临时表中的数据</span></span><br><span class="line">         sparkSession.sql(<span class="string">"select age,count(*) as num from student group by ag</span></span><br><span class="line"><span class="string">         .show();</span></span><br><span class="line"><span class="string">         sparkSession.stop();</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="RDD转换为DataFrame-包含DataFrame转RDD"><a href="#RDD转换为DataFrame-包含DataFrame转RDD" class="headerlink" title="RDD转换为DataFrame(包含DataFrame转RDD)"></a>RDD转换为DataFrame(包含DataFrame转RDD)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">为什么要将RDD转换为DataFrame?</span><br><span class="line"></span><br><span class="line">在实际工作中我们可能会先把hdfs上的一些日志数据加载进来，然后进行一些处理，最终变成结构化的数据，希望对这些数据做一些统计分析，当然了我们可以使用spark中提供的transformation算子来实现，只不过会有一些麻烦，毕竟是需要写代码的，如果能够使用sql实现，其实是更加方便的。</span><br><span class="line"></span><br><span class="line">所以可以针对我们前面创建的RDD，将它转换为DataFrame，这样就可以使用dataFrame中的一些算子或者直接写sql来操作数据了。</span><br><span class="line">Spark SQL支持这两种方式将RDD转换为DataFrame</span><br><span class="line">1. 反射方式</span><br><span class="line">2. 编程方式</span><br></pre></td></tr></table></figure><h4 id="反射方式"><a href="#反射方式" class="headerlink" title="反射方式"></a>反射方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下反射方式：</span><br><span class="line">这种方式是使用反射来推断RDD中的元数据。</span><br><span class="line">基于反射的方式，代码比较简洁，也就是说当你在写代码的时候，已经知道了RDD中的元数据(已经知道RDD里面的数据长什么样子)，这样的话使用反射这种方式是一种非常不错的选择。</span><br><span class="line">Scala具有隐式转换的特性，所以spark sql的scala接口是支持自动将包含了case class的RDD转换为DataFrame的</span><br><span class="line"></span><br><span class="line">下面来举一个例子</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><h5 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用反射方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RddToDataFrameByReflectScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByReflectScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="keyword">val</span> sc = sparkSession.sparkContext</span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>((<span class="string">"jack"</span>,<span class="number">18</span>),(<span class="string">"tom"</span>,<span class="number">20</span>),(<span class="string">"jessic"</span>,<span class="number">30</span>)))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//基于反射直接将包含Student对象的dataRDD转换为DataFrame</span></span><br><span class="line">         <span class="comment">//需要导入隐式转换</span></span><br><span class="line">         <span class="keyword">import</span> sparkSession.implicits._ <span class="comment">//不导入，toDF()不能用</span></span><br><span class="line">         <span class="keyword">val</span> stuDf = dataRDD.map(tup=&gt;<span class="type">Student</span>(tup._1,tup._2)).toDF()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//下面就可以通过DataFrame的方式操作dataRDD中的数据了</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         <span class="keyword">val</span> resDf = sparkSession.sql(<span class="string">"select name,age from student where age &gt; 18"</span>)</span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD，或者直接在这里show()也行</span></span><br><span class="line">         <span class="keyword">val</span> resRDD = resDf.rdd</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//从row中取数据 ，封装成student，打印到控制台</span></span><br><span class="line">     resRDD.map(row=&gt;<span class="type">Student</span>(row(<span class="number">0</span>).toString,row(<span class="number">1</span>).toString.toInt))</span><br><span class="line">         .collect()</span><br><span class="line">         .foreach(println(_))</span><br><span class="line">         <span class="comment">//使用row的getAs()方法，获取指定列名的值</span></span><br><span class="line">         resRDD.map(row=&gt;<span class="type">Student</span>(row.getAs[<span class="type">String</span>](<span class="string">"name"</span>),row.getAs[<span class="type">Int</span>](<span class="string">"age"</span>)))</span><br><span class="line">         .collect()</span><br><span class="line">         .foreach(println(_))</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//定义一个Student</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">name: <span class="type">String</span>,age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303282345692.png" alt="image-20230328234459220"></p><h5 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.RDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用反射方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RddToDataFrameByReflectJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByReflectJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="comment">//从sparkSession中获取的是scala中的sparkContext，所以需要转换成java中的sparkContext</span></span><br><span class="line">         JavaSparkContext sc = JavaSparkContext.fromSparkContext(sparkSession.sparkContext());</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t1 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jack"</span>, <span class="number">18</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t2 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"tom"</span>, <span class="number">20</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t3 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jessic"</span>, <span class="number">30</span>);</span><br><span class="line">         JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; dataRDD = sc.parallelize(Arrays.asList(t1,t2,t3));</span><br><span class="line">         JavaRDD&lt;Student&gt; stuRDD = dataRDD.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;String, Integer&gt;, Student&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Student <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new <span class="title">Student</span><span class="params">(tup._1, tup._2)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//注意：Student这个类必须声明为public(一个文件里只能有一个public class，所以只能在另一个文件里创建)，并且必须实现序列化</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.createDataFrame(stuRDD, Student<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">         </span><br><span class="line">                                                                      stuDf.createOrReplaceTempView(<span class="string">"student"</span>);</span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         Dataset&lt;Row&gt; resDf = sparkSession.sql(<span class="string">"select name,age from student where age&gt;18"</span>);</span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD，注意：这里需要转为JavaRDD</span></span><br><span class="line">         JavaRDD&lt;Row&gt; resRDD = resDf.javaRDD();</span><br><span class="line">         <span class="comment">//从row中取数据，封装成student，打印到控制台</span></span><br><span class="line">         List&lt;Student&gt; resList = resRDD.map(<span class="keyword">new</span> Function&lt;Row, Student&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Student <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">//return new Student(row.getString(0), row.getInt(1));</span></span><br><span class="line">         <span class="comment">//通过getAs获取数据</span></span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Student(row.getAs(<span class="string">"name"</span>).toString(), Integer.parseInt(row.getAs(<span class="string">"age"</span>).toString());</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).collect();</span><br><span class="line">         <span class="keyword">for</span>(Student stu : resList)&#123;</span><br><span class="line">         System.out.println(stu);</span><br><span class="line">        sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">student</span> <span class="keyword">implements</span> <span class="title">Serializable</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(String name,<span class="keyword">int</span> age)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name=name;</span><br><span class="line">        <span class="keyword">this</span>.age=age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span></span>&#123;</span><br><span class="line"><span class="keyword">this</span>.name=name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getAge</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">this</span>.age=age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Student&#123;"</span>+</span><br><span class="line">            <span class="string">"name='"</span>+name+<span class="string">'\''</span>+</span><br><span class="line">            <span class="string">",age="</span>+age+</span><br><span class="line">            <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java类中快速为字段生成get,set方法，右键-&gt;generate-&gt;...</span><br><span class="line">    快速生成构造方法 右键-&gt;generate-&gt;constructor</span><br><span class="line">    快速生成toString()方法 右键-&gt;generate-&gt;toString</span><br></pre></td></tr></table></figure><h4 id="编程方式"><a href="#编程方式" class="headerlink" title="编程方式"></a>编程方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来是编程的方式</span><br><span class="line">这种方式是通过编程接口来创建DataFrame，你可以在程序运行时动态构建一份元数据，就是Schema，然后将其应用到已经存在的RDD上。这种方式的代码比较冗长，但是如果在编写程序时，还不知道RDD的元数据，只有在程序运行时，才能动态得知其元数据，那么只能通过这种动态构建元数据的方式。</span><br><span class="line"></span><br><span class="line">也就是说当case calss中的字段无法预先定义的时候，就只能用编程方式动态指定元数据了</span><br></pre></td></tr></table></figure><h5 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">Stru</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用编程方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RddToDataFrameByProgramScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByProgramScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="keyword">val</span> sc = sparkSession.sparkContext</span><br><span class="line">         <span class="comment">// 假设这里不知道RDD的数据结构</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>((<span class="string">"jack"</span>,<span class="number">18</span>),(<span class="string">"tom"</span>,<span class="number">20</span>),(<span class="string">"jessic"</span>,<span class="number">30</span>)))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//组装rowRDD</span></span><br><span class="line">         <span class="keyword">val</span> rowRDD = dataRDD.map(tup=&gt;<span class="type">Row</span>(tup._1,tup._2))</span><br><span class="line">         <span class="comment">// dataframe=rdd+schema</span></span><br><span class="line">         <span class="comment">//指定元数据信息【这个元数据信息就可以动态从外部获取了，比较灵活】</span></span><br><span class="line">         <span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"name"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"age"</span>,<span class="type">IntegerType</span>,<span class="literal">true</span>)</span><br><span class="line">         ))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//组装DataFrame</span></span><br><span class="line">         <span class="keyword">val</span> stuDf = sparkSession.createDataFrame(rowRDD,schema)</span><br><span class="line">         <span class="comment">//下面就可以通过DataFrame的方式操作dataRDD中的数据了</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         <span class="keyword">val</span> resDf = sparkSession.sql(<span class="string">"select name,age from student where age &gt; 18"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD</span></span><br><span class="line">         <span class="keyword">val</span> resRDD = resDf.rdd </span><br><span class="line">         resRDD.map(row=&gt;(row(<span class="number">0</span>).toString,row(<span class="number">1</span>).toString.toInt))</span><br><span class="line">         .collect()</span><br><span class="line">         .foreach(println(_))</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IDEA编程时使用一个未知的东西，alt+回车,会提示需要导入的东西</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303290043493.png" alt="image-20230329004355828"></p><h5 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.RowFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用编程方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RddToDataFrameByProgramJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByProgramJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="comment">//从sparkSession中获取的是scala中的sparkContext，所以需要转换成java中的sp</span></span><br><span class="line">         JavaSparkContext sc = JavaSparkContext.fromSparkContext(sparkSession.</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t1 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jack"</span>, <span class="number">18</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t2 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"tom"</span>, <span class="number">20</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t3 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jessic"</span>, <span class="number">30</span>);</span><br><span class="line">         JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; dataRDD = sc.parallelize(Arrays.asList(t1,t2,t3));</span><br><span class="line">         <span class="comment">//组装rowRDD</span></span><br><span class="line">         JavaRDD&lt;Row&gt; rowRDD = dataRDD.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;String, Integer&gt;,Row&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> RowFactory.create(tup._1, tup._2);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//指定元数据信息</span></span><br><span class="line">        ArrayList&lt;StructField&gt; structFieldList = <span class="keyword">new</span> ArrayList&lt;StructField&gt;();</span><br><span class="line">         structFieldList.add(DataTypes.createStructField(<span class="string">"name"</span>, DataTypes.StringType,<span class="keyword">true</span>));</span><br><span class="line">         structFieldList.add(DataTypes.createStructField(<span class="string">"age"</span>, DataTypes.IntegerType,True));</span><br><span class="line">         StructType schema = DataTypes.createStructType(structFieldList);</span><br><span class="line">         <span class="comment">//构建DataFrame</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.createDataFrame(rowRDD, schema);</span><br><span class="line"></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>);</span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         Dataset&lt;Row&gt; resDf = sparkSession.sql(<span class="string">"select name,age from student where age&gt;18"</span>);</span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD，注意：这里需要转为JavaRDD</span></span><br><span class="line">         JavaRDD&lt;Row&gt; resRDD = resDf.javaRDD();</span><br><span class="line">         List&lt;Tuple2&lt;String, Integer&gt;&gt; resList = resRDD.map(<span class="keyword">new</span> Function&lt;Row, Tuple2&lt;String,Intege&gt;&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(row.getString(<span class="number">0</span>), row.getInt(<span class="number">1</span>));</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).collect();</span><br><span class="line">         </span><br><span class="line">                                                                      <span class="keyword">for</span>(Tuple2&lt;String,Integer&gt; tup : resList)&#123;</span><br><span class="line">         System.out.println(tup);</span><br><span class="line">         &#125;</span><br><span class="line">         sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="load和save操作"><a href="#load和save操作" class="headerlink" title="load和save操作"></a>load和save操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对于Spark SQL的DataFrame来说，无论是从什么数据源创建出来的DataFrame，都有一些共同的load和save操作。(不使用这两个方法，使用TextFile(),再转成DataFrame操作，再转RDD,saveAsText()也可以)</span><br><span class="line"></span><br><span class="line">load操作主要用于加载数据，创建出DataFrame；</span><br><span class="line">save操作，主要用于将DataFrame中的数据保存到文件中。</span><br><span class="line"></span><br><span class="line">我们前面操作json格式的数据的时候好像没有使用load方法，而是直接使用的json方法，这是什么特殊用法吗？</span><br><span class="line">查看json方法的源码会发现，它底层调用的是format和load方法</span><br><span class="line">def json(paths: String*): DataFrame &#x3D; format(&quot;json&quot;).load(paths : _*)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">我们如果使用原始的format和load方法加载数据，</span><br><span class="line">此时如果不指定format，则默认读取的数据源格式是parquet，也可以手动指定数据源格式。Spark SQL内置了一些常见的数据源类型，比如json, parquet, jdbc, orc, csv, text</span><br><span class="line"></span><br><span class="line">通过这个功能，就可以在不同类型的数据源之间进行转换了。</span><br></pre></td></tr></table></figure><h4 id="scala-5"><a href="#scala-5" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：load和save的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LoadAndSaveOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="comment">//读取数据(这种方式与前面讲的创建dataframe是一样的，sparkSession.read.json("xxx"))</span></span><br><span class="line">         <span class="keyword">val</span> stuDf = sparkSession.read</span><br><span class="line">         .format(<span class="string">"json"</span>)</span><br><span class="line">         .load(<span class="string">"D:\\student.json"</span>)</span><br><span class="line">         <span class="comment">//保存数据</span></span><br><span class="line">         stuDf.select(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">         .write</span><br><span class="line">         .format(<span class="string">"csv"</span>)</span><br><span class="line">         .save(<span class="string">"hdfs://bigdata01:9000/out-save001"</span>)</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291340241.png" alt="image-20230329134033719"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291340871.png" alt="image-20230329134053438"></p><h4 id="java-5"><a href="#java-5" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：load和save的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoadAndSaveOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//读取数据</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.read()</span><br><span class="line">         .format(<span class="string">"json"</span>)</span><br><span class="line">         .load(<span class="string">"D:\\student.json"</span>);</span><br><span class="line">         <span class="comment">//保存数据</span></span><br><span class="line">         stuDf.select(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">         .write()</span><br><span class="line">         .format(<span class="string">"csv"</span>)</span><br><span class="line">         .save(<span class="string">"hdfs://bigdata01:9000/out-save002"</span>);</span><br><span class="line">         sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="SaveMode"><a href="#SaveMode" class="headerlink" title="SaveMode"></a>SaveMode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL对于save操作，提供了不同的save mode。</span><br><span class="line">主要用来处理，当目标位置已经有数据时应该如何处理。save操作不会执行锁操作，并且也不是原子的，因此是有一定风险出现脏数据的。</span><br><span class="line"></span><br><span class="line">SaveMode 解释</span><br><span class="line">SaveMode.ErrorIfExists (默认) 如果目标位置已经存在数据，那么抛出一个异常</span><br><span class="line">SaveMode.Append 如果目标位置已经存在数据，那么将数据追加进去</span><br><span class="line">SaveMode.Overwrite 如果目标位置已经存在数据，那么就将已经存在的数据删除，用新数据进行覆盖</span><br><span class="line">SaveMode.Ignore 如果目标位置已经存在数据，那么就忽略，不做任何操作</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在LoadAndSaveOpScala中增加SaveMode的设置，重新执行，验证结果</span><br><span class="line">将SaveMode设置为Append，如果目标已存在，则追加</span><br><span class="line">stuDf.select(&quot;name&quot;,&quot;age&quot;)</span><br><span class="line"> .write</span><br><span class="line"> .format(&quot;csv&quot;)</span><br><span class="line"> .mode(SaveMode.Append)&#x2F;&#x2F;追加</span><br><span class="line"> .save(&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;out-save001&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291355158.png" alt="image-20230329135505416"></p><h4 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291356152.png" alt="image-20230329135637838"></p><h3 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Spark中提供了很多内置的函数，</span><br><span class="line">种类 函数</span><br><span class="line">聚合函数 avg, count, countDistinct, first, last, max, mean, min, sum, </span><br><span class="line"></span><br><span class="line">集合函数 array_contains, explode, size</span><br><span class="line"></span><br><span class="line">日期&#x2F;时间函数 datediff, date_add, date_sub, add_months, last_day, next_day, </span><br><span class="line"></span><br><span class="line">数学函数 abs, ceil, floor, round</span><br><span class="line"></span><br><span class="line">混合函数 if, isnull, md5, not, rand, when</span><br><span class="line"></span><br><span class="line">字符串函数 concat, get_json_object, length, reverse, split, upper</span><br><span class="line"></span><br><span class="line">窗口函数 denseRank, rank, rowNumber</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实这里面的函数和hive中的函数是类似的</span><br><span class="line">注意：SparkSQL中的SQL函数文档不全，其实在使用这些函数的时候，大家完全可以去查看hive中sql的文档，使用的时候都是一样的。</span><br></pre></td></tr></table></figure><h2 id="实战：TopN主播统计"><a href="#实战：TopN主播统计" class="headerlink" title="实战：TopN主播统计"></a>实战：TopN主播统计</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在前面讲Spark core的时候我们讲过一个案例，TopN主播统计，计算每个大区当天金币收入TopN的主播，之前我们使用spark中的transformation算子去计算，实现起来还是比较麻烦的，代码量相对来说比较多，下面我们就使用咱们刚学习的Spark sql去实现一下，你会发现，使用sql之后确实简单多了。</span><br><span class="line"></span><br><span class="line">回顾以下我们的两份原始数据，数据都是json格式的</span><br><span class="line">video_info.log 主播的开播记录，其中包含主播的id：uid、直播间id：vid 、大区：area、视频开播时长：length、增加粉丝数量：follow等信息</span><br><span class="line">gift_record.log 用户送礼记录，其中包含送礼人id：uid，直播间id：vid，礼物id：good_id，金币数量：gold 等信息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最终需要的结果是这样的</span><br><span class="line">US 8407173251015:180,8407173251012:70,8407173251001:60</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">分析一下具体步骤</span><br><span class="line">1. 直接使用SparkSession中的load方式加载json的数据</span><br><span class="line">2. 对这两份数据注册临时表</span><br><span class="line">3. 执行sql计算TopN主播</span><br><span class="line">4. 使用foreach将结果打印到控制台</span><br></pre></td></tr></table></figure><h3 id="scala-6"><a href="#scala-6" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：计算TopN主播</span></span><br><span class="line"><span class="comment"> * 1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line"><span class="comment"> * 2：对这两份数据注册临时表</span></span><br><span class="line"><span class="comment"> * 3：执行sql计算TopN主播</span></span><br><span class="line"><span class="comment"> * 4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopNAnchorScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line">         <span class="keyword">val</span> videoInfoDf = sparkSession.read.json(<span class="string">"D:\\video_info.log"</span>)</span><br><span class="line">         <span class="keyword">val</span> giftRecordDf = sparkSession.read.json(<span class="string">"D:\\gift_record.log"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//2：对这两份数据注册临时表</span></span><br><span class="line">         videoInfoDf.createOrReplaceTempView(<span class="string">"video_info"</span>)</span><br><span class="line">         giftRecordDf.createOrReplaceTempView(<span class="string">"gift_record"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//3：执行sql计算TopN主播</span></span><br><span class="line">         <span class="keyword">val</span> sql =<span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t4.area, "</span>+</span><br><span class="line">         <span class="string">"concat_ws(',',collect_list(t4.topn)) as topn_list "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t3.area,concat(t3.uid,':',cast(t3.gold_sum_all as int)) as topn "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t2.uid,t2.area,t2.gold_sum_all,row_number() over (partition by area orde</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>t1.uid,max(t1.area) as area,sum(t1.gold_sum) as gold_sum_all <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vi.uid,vi.vid,vi.area,gr.gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>video_info as vi <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>join <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>(select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vid,sum(gold) as gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>gift_record <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as gr <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>on vi.vid = gr.vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t1 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t1.uid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t2 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as t3 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>where t3.num &lt;=<span class="number">3</span> <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t4 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t4.area <span class="string">"</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         val resDf = sparkSession.sql(sql)</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         //4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="string">         resDf.rdd.foreach(row=&gt;println(row.getAs[String]("</span><span class="string">area")+"</span>\<span class="string">t"+row.getAs[String]))</span></span><br><span class="line"><span class="string">         sparkSession.stop()</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">t4.area,</span><br><span class="line"><span class="keyword">concat_ws</span>(<span class="string">','</span>,collect_list(t4.topn)) <span class="keyword">as</span> topn_list</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">t3.area,<span class="keyword">concat</span>(t3.uid,<span class="string">':'</span>,<span class="keyword">cast</span>(t3.gold_sum_all <span class="keyword">as</span> <span class="built_in">int</span>)) <span class="keyword">as</span> topn</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">t2.uid,t2.area,t2.gold_sum_all,row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> area <span class="keyword">order</span> <span class="keyword">by</span> gold_sum_all <span class="keyword">desc</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">t1.uid,<span class="keyword">max</span>(t1.area) <span class="keyword">as</span> area,<span class="keyword">sum</span>(t1.gold_sum) <span class="keyword">as</span> gold_sum_all</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">vi.uid,vi.vid,vi.area,gr.gold_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">video_info <span class="keyword">as</span> vi</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">vid,<span class="keyword">sum</span>(gold) <span class="keyword">as</span> gold_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">gift_record</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vid</span><br><span class="line">)<span class="keyword">as</span> gr</span><br><span class="line"><span class="keyword">on</span> vi.vid = gr.vid</span><br><span class="line">) <span class="keyword">as</span> t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t1.uid</span><br><span class="line">) <span class="keyword">as</span> t2</span><br><span class="line">)<span class="keyword">as</span> t3</span><br><span class="line"><span class="keyword">where</span> t3.num &lt;=<span class="number">3</span></span><br><span class="line">) <span class="keyword">as</span> t4</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t4.area</span><br></pre></td></tr></table></figure><h3 id="java-6"><a href="#java-6" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：计算TopN主播</span></span><br><span class="line"><span class="comment"> * 1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line"><span class="comment"> * 2：对这两份数据注册临时表</span></span><br><span class="line"><span class="comment"> * 3：执行sql计算TopN主播</span></span><br><span class="line"><span class="comment"> * 4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNAnchorJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"TopNAnchorJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line">         Dataset&lt;Row&gt; videoInfoDf = sparkSession.read().json(<span class="string">"D:\\video_info.log"</span>);</span><br><span class="line">         Dataset&lt;Row&gt; giftRecordDf = sparkSession.read().json(<span class="string">"D:\\gift_record.log"</span>);</span><br><span class="line">         <span class="comment">//2：对这两份数据注册临时表</span></span><br><span class="line">         videoInfoDf.createOrReplaceTempView(<span class="string">"video_info"</span>);</span><br><span class="line">         giftRecordDf.createOrReplaceTempView(<span class="string">"gift_record"</span>);</span><br><span class="line">         <span class="comment">//3：执行sql计算TopN主播</span></span><br><span class="line">         String sql = <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t4.area, "</span>+</span><br><span class="line">         <span class="string">"concat_ws(',',collect_list(t4.topn)) as topn_list "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t3.area,concat(t3.uid,':',cast(t3.gold_sum_all as int)) as t</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>t2.uid,t2.area,t2.gold_sum_all,row_number() over (partition </span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t1.uid,max(t1.area) as area,sum(t1.gold_sum) as gold_sum_all </span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vi.uid,vi.vid,vi.area,gr.gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>video_info as vi <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>join <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>(select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vid,sum(gold) as gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>gift_record <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as gr <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>on vi.vid = gr.vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t1 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t1.uid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t2 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as t3 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>where t3.num &lt;=<span class="number">3</span> <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t4 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t4.area <span class="string">";</span></span><br><span class="line"><span class="string">         Dataset&lt;Row&gt; resDf = sparkSession.sql(sql);</span></span><br><span class="line"><span class="string">         //4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="string">         resDf.javaRDD().foreach(new VoidFunction&lt;Row&gt;() &#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public void call(Row row) throws Exception &#123;</span></span><br><span class="line"><span class="string">         System.out.println(row.getString(0)+"</span>\t<span class="string">"+row.getString(1));</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         sparkSession.stop();</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">代码执行结果如下：</span><br><span class="line">CN 8407173251008:120,8407173251003:60,8407173251014:50</span><br><span class="line">ID 8407173251005:160,8407173251010:140,8407173251002:70</span><br><span class="line">US 8407173251015:180,8407173251012:70,8407173251001:60</span><br></pre></td></tr></table></figure><h3 id="集群执行"><a href="#集群执行" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">新建一个object：TopNAnchorClusterScala，修改代码，将任务的输出数据保存到hdfs上面</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：计算TopN主播</span></span><br><span class="line"><span class="comment"> * 1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line"><span class="comment"> * 2：对这两份数据注册临时表</span></span><br><span class="line"><span class="comment"> * 3：执行sql计算TopN主播</span></span><br><span class="line"><span class="comment"> * 4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopNAnchorClusterScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="comment">//1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line">         <span class="keyword">val</span> videoInfoDf = sparkSession.read.json(<span class="string">"hdfs://bigdata01:9000/video_inf</span></span><br><span class="line"><span class="string">         val giftRecordDf = sparkSession.read.json("</span>hdfs:<span class="comment">//bigdata01:9000/gift_rec</span></span><br><span class="line">         <span class="comment">//2：对这两份数据注册临时表</span></span><br><span class="line">         videoInfoDf.createOrReplaceTempView(<span class="string">"video_info"</span>)</span><br><span class="line">         giftRecordDf.createOrReplaceTempView(<span class="string">"gift_record"</span>)</span><br><span class="line">         <span class="comment">//3：执行sql计算TopN主播</span></span><br><span class="line">         <span class="keyword">val</span> sql =<span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t4.area, "</span>+</span><br><span class="line">         <span class="string">"concat_ws(',',collect_list(t4.topn)) as topn_list "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t3.area,concat(t3.uid,':',cast(t3.gold_sum_all as int)) as topn "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t2.uid,t2.area,t2.gold_sum_all,row_number() over (partition by area orde</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>t1.uid,max(t1.area) as area,sum(t1.gold_sum) as gold_sum_all <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vi.uid,vi.vid,vi.area,gr.gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>video_info as vi <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>join <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>(select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vid,sum(gold) as gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>gift_record <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as gr <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>on vi.vid = gr.vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t1 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t1.uid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t2 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as t3 <span class="string">"+</span></span><br><span class="line"><span class="string">        "</span>where t3.num &lt;=<span class="number">3</span> <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t4 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t4.area <span class="string">"</span></span><br><span class="line"><span class="string">         val resDf = sparkSession.sql(sql)</span></span><br><span class="line"><span class="string">         //4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="string">         resDf.rdd</span></span><br><span class="line"><span class="string">         .map(row=&gt;row.getAs[String]("</span><span class="string">area")+"</span>\<span class="string">t"+row.getAs[String]("</span>topn_<span class="string">list")</span></span><br><span class="line"><span class="string">         .saveAsTextFile("</span>hdfs:<span class="comment">//bigdata01:9000/out-topn")</span></span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">修改pom中依赖的配置，全部设置为provided</span><br><span class="line"></span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">         &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">         &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">         &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">         &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">         &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">         &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">         &lt;version&gt;1.2.68&lt;&#x2F;version&gt;</span><br><span class="line">         &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">         &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">         &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">         &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">         &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对spark-core和spark-sql在打包的时候是不需要的，针对fastjson有的spark job是需要的，不过建议</span><br><span class="line">在这设置为provided，打包的时候不要打进去，在具体使用的时候可以在spark-submit脚本中通过–jar</span><br><span class="line">来动态指定这个jar包，最好把这个jar包上传到hdfs上面统一管理和维护。</span><br><span class="line">编译打包，上传到bigdta04上的 &#x2F;data&#x2F;soft&#x2F;sparkjars 目录</span><br><span class="line">创建spark-submit脚本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 sparkjars]# vi topnJob.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.sql.TopNAnchorClusterScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot; \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">提交任务</span><br><span class="line">[root@bigdata04 sparkjars]# sh -x topnJob.sh</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291557223.png" alt="image-20230329155709778"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E7%AE%97%E5%AD%90%E4%BC%98%E5%8C%96.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E7%AE%97%E5%AD%90%E4%BC%98%E5%8C%96.html</id>
    <published>2023-03-27T17:50:58.000Z</published>
    <updated>2023-03-28T07:20:39.448Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-算子优化"><a href="#第十一周-Spark性能优化的道与术-算子优化" class="headerlink" title="第十一周 Spark性能优化的道与术-算子优化"></a>第十一周 Spark性能优化的道与术-算子优化</h1><h2 id="map-vs-mapPartitions"><a href="#map-vs-mapPartitions" class="headerlink" title="map vs mapPartitions"></a>map vs mapPartitions</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">map操作：对RDD 中的每个元素进行操作，一次处理一条数据</span><br><span class="line"></span><br><span class="line">mapPartitions操作(transformation操作)：对RDD中每个partition进行操作，一次处理一个分区的数据</span><br><span class="line"></span><br><span class="line">所以：</span><br><span class="line">map操作：执行1次map算子只处理1个元素，如果partition中元素较多，假设当前已经处理了1000个元素，在内存不足的情况下，Spark可以通过GC等方法回收内存（比如将已处理掉的</span><br><span class="line">1000个元素从内存中回收）。因此，map操作通常不会导致OOM异常；</span><br><span class="line"></span><br><span class="line">mapPartitions操作：执行1次map算子需要接收该partition 中的所有元素，因此一旦元素很多而内存不足，就容易导致OOM的异常，也不是说一定就会产生OOM异常，只是和map算子对比的话，</span><br><span class="line">相对来说容易产生OOM异常(OOM，java.lang.OutOfMemoryError 错误，也就是java内存溢出错误。)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">不过一般情况下，mapPartitions的性能更高；初始化操作、数据库链接等操作适合使用mapPartitions操作</span><br><span class="line"></span><br><span class="line">这是因为：</span><br><span class="line">假设需要将RDD中的每个元素写入数据库中，这时候就应该把创建数据库链接的操作放置在mapPartitions中，创建数据库链接这个操作本身就是个比较耗时的，如果该操作放在map中执行，将会频繁执行，比较耗时且影响数据库的稳定性。</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：mapPartitons的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapPartitionsOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"MapPartitionsOpScala"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="comment">//设置分区数量为2</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>),<span class="number">2</span>)</span><br><span class="line">         <span class="comment">//map算子一次处理一条数据</span></span><br><span class="line">         <span class="comment">/*val sum = dataRDD.map(item=&gt;&#123;</span></span><br><span class="line"><span class="comment">         println("==============")</span></span><br><span class="line"><span class="comment">         item * 2</span></span><br><span class="line"><span class="comment">         &#125;).reduce( _ + _)*/</span></span><br><span class="line">        <span class="comment">//mapPartitions算子一次处理一个分区的数据</span></span><br><span class="line">         <span class="keyword">val</span> sum = dataRDD.mapPartitions(it=&gt;&#123;</span><br><span class="line">         <span class="comment">//建议针对初始化链接之类的操作，使用mapPartitions，放在mapPartitions内部</span></span><br><span class="line">         <span class="comment">//例如：创建数据库链接，使用mapPartitions可以减少链接创建的次数，提高性能</span></span><br><span class="line">         <span class="comment">//注意：创建数据库链接的代码建议放在次数，不要放在Driver端或者it.foreach内部</span></span><br><span class="line">         <span class="comment">//数据库链接放在Driver端会导致链接无法序列化，无法传递到对应的task中执行，所以</span></span><br><span class="line">         <span class="comment">//数据库链接放在it.foreach()内部还是会创建多个链接，和使用map算子的效果是一样</span></span><br><span class="line">         println(<span class="string">"=================="</span>)</span><br><span class="line">         <span class="keyword">val</span> result = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">         <span class="comment">//这个foreach是调用的scala里面的函数</span></span><br><span class="line">         it.foreach(item=&gt;&#123;</span><br><span class="line">         result.+=(item * <span class="number">2</span>)</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">         result.toIterator</span><br><span class="line">         &#125;).reduce(_ + _)</span><br><span class="line">         println(<span class="string">"sum:"</span>+sum)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：mapPartitons的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapPartitionsOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"MapPartitionsOpJava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>));</span><br><span class="line">         Integer sum = dataRDD.mapPartitions(<span class="keyword">new</span> FlatMapFunction&lt;Iterator&lt;Inte</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterator&lt;Integer&gt; <span class="title">call</span><span class="params">(Iterator&lt;Integer&gt; it)</span> <span class="keyword">throws</span> Except</span></span><br><span class="line"><span class="function">         <span class="comment">//数据库链接的代码需要放在这个位置</span></span></span><br><span class="line"><span class="function">         ArrayList&lt;Integer&gt; list </span>= <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">         list.add(it.next() * <span class="number">2</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">        &#125;</span><br><span class="line">         &#125;).reduce(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> i1 + i2;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         System.out.println(<span class="string">"sum:"</span>+sum);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="foreach-vs-foreachPartition"><a href="#foreach-vs-foreachPartition" class="headerlink" title="foreach vs foreachPartition"></a>foreach vs foreachPartition</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">foreach：一次处理一条数据</span><br><span class="line">foreachPartition：一次处理一个分区的数据</span><br><span class="line">foreachPartition的特性和mapPartitions的特性是一样的，唯一的区别就是mapPartitions是transformation操作（不会立即执行），foreachPartition是action操作（会立即执行）</span><br></pre></td></tr></table></figure><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：foreachPartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ForeachPartitionOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"ForeachPartitionOpScala"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="comment">//设置分区数量为2</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>),<span class="number">2</span>)</span><br><span class="line">         <span class="comment">//foreachPartition：一次处理一个分区的数据，作用和mapPartitions类似</span></span><br><span class="line">         <span class="comment">//唯一的区是mapPartitions是transformation算子，foreachPartition是action算子</span></span><br><span class="line">         dataRDD.foreachPartition(it=&gt;&#123;</span><br><span class="line">         <span class="comment">//在此处获取数据库链接</span></span><br><span class="line">         println(<span class="string">"==============="</span>)</span><br><span class="line">         it.foreach(item=&gt;&#123; <span class="comment">// 和对RDD处理的foreach不一样</span></span><br><span class="line">             <span class="comment">//在这里使用数据库链接</span></span><br><span class="line">             println(item)</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">         &#125;)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281441692.png" alt="image-20230328144113035"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281440127.png" alt="image-20230328144052865"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：foreachPartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ForeachPartitionOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"ForeachPartitionOpJava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span></span><br><span class="line">         dataRDD.foreachPartition(<span class="keyword">new</span> VoidFunction&lt;Iterator&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Iterator&lt;Integer&gt; it)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         System.out.println(<span class="string">"============="</span>);</span><br><span class="line">         <span class="comment">//在此处获取数据库链接</span></span><br><span class="line">         <span class="keyword">while</span> (it.hasNext())&#123;</span><br><span class="line">         <span class="comment">//在这里使用数据库链接</span></span><br><span class="line">         System.out.println(it.next());</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="repartition的使用"><a href="#repartition的使用" class="headerlink" title="repartition的使用"></a>repartition的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">对RDD进行重分区，repartition主要有两个应用场景：</span><br><span class="line">1. 可以调整RDD的并行度</span><br><span class="line">针对个别RDD，如果感觉分区数量不合适，想要调整，可以通过repartition进行调整，分区调整了之后，对应的并行度也就可以调整了</span><br><span class="line">2. 可以解决RDD中数据倾斜的问题</span><br><span class="line">如果RDD中不同分区之间的数据出现了数据倾斜，可以通过repartition实现数据重新分发，可以均匀分发到不同分区中</span><br></pre></td></tr></table></figure><h3 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：repartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RepartitionOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"RepartitionOpScala"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="comment">//设置分区数量为2</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>),<span class="number">2</span>)</span><br><span class="line">         <span class="comment">//重新设置RDD的分区数量为3，这个操作会产生shuffle</span></span><br><span class="line">         <span class="comment">//也可以解决RDD中数据倾斜的问题</span></span><br><span class="line">         dataRDD.repartition(<span class="number">3</span>)</span><br><span class="line">         .foreachPartition(it=&gt;&#123;</span><br><span class="line">         println(<span class="string">"========="</span>) <span class="comment">// 运行程序会输出3次</span></span><br><span class="line">         it.foreach(println(_))</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//通过repartition可以控制输出数据产生的文件个数</span></span><br><span class="line">         dataRDD.saveAsTextFile(<span class="string">"hdfs://bigdata01:9000/rep-001"</span>) <span class="comment">// 生成3个文件</span></span><br><span class="line">                dataRDD.repartition(<span class="number">1</span>).saveAsTextFile(<span class="string">"hdfs://bigdata01:9000/rep-002"</span>) <span class="comment">// 生成1个文件 </span></span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：repartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RepartitionOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"RepartitionOpJava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>),<span class="number">2</span>);</span><br><span class="line">         dataRDD.repartition(<span class="number">3</span>)</span><br><span class="line">         .foreachPartition(<span class="keyword">new</span> VoidFunction&lt;Iterator&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Iterator&lt;Integer&gt; it)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         System.out.println(<span class="string">"=============="</span>);</span><br><span class="line">         <span class="keyword">while</span> (it.hasNext())&#123;</span><br><span class="line">         System.out.println(it.next());</span><br><span class="line">         &#125;</span><br><span class="line">             &#125;);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="reduceByKey和groupByKey的区别"><a href="#reduceByKey和groupByKey的区别" class="headerlink" title="reduceByKey和groupByKey的区别"></a>reduceByKey和groupByKey的区别</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">在实现分组聚合功能时这两个算子有什么区别？</span><br><span class="line">看这两行代码</span><br><span class="line">val counts &#x3D; wordCountRDD.reduceByKey(_ + _)</span><br><span class="line">val counts &#x3D; wordCountRDD.groupByKey().map(wc &#x3D;&gt; (wc._1, wc._2.sum))</span><br><span class="line"></span><br><span class="line">这两行代码的最终效果是一样的，都是对wordCountRDD中每个单词出现的次数进行聚合统计</span><br><span class="line">那这两种方式在原理层面有什么区别吗？</span><br><span class="line">首先这两个算子在执行的时候都会产生shuffle</span><br><span class="line">但是：</span><br><span class="line">1：当采用reduceByKey时，数据在进行shuffle之前会先进行局部聚合</span><br><span class="line">2：当使用groupByKey时，数据在shuffle之间不会进行局部聚合，会原样进行shuffle</span><br><span class="line"></span><br><span class="line">这样的话reduceByKey就减少了shuffle的数据传送，所以效率会高一些。</span><br><span class="line">如果能用reduceByKey，就用reduceByKey，因为它会在map端，先进行本地combine，可以大大减少要传输到reduce端的数据量，减少网络传输的开销</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面来看这个图，加深一下理解</span><br><span class="line"></span><br><span class="line">从图中可以看出来reduceByKey在shuffle之前会先对数据进行局部聚合，而groupByKey不会，所以在实现分组聚合的需求中，reduceByKey性能略胜一筹。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281513504.png" alt="image-20230328151345043"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html</id>
    <published>2023-03-27T11:31:29.000Z</published>
    <updated>2023-03-29T13:01:48.123Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-企业级最佳实践"><a href="#第十一周-Spark性能优化的道与术-企业级最佳实践" class="headerlink" title="第十一周 Spark性能优化的道与术-企业级最佳实践"></a>第十一周 Spark性能优化的道与术-企业级最佳实践</h1><h2 id="性能优化分析"><a href="#性能优化分析" class="headerlink" title="性能优化分析"></a>性能优化分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一个计算任务的执行主要依赖于CPU、内存、带宽</span><br><span class="line">Spark是一个基于内存的计算引擎，所以对它来说，影响最大的可能就是内存，一般我们的任务遇到了性能瓶颈大概率都是内存的问题，当然了CPU和带宽也可能会影响程序的性能，这个情况也不是没有的，只是比较少。</span><br><span class="line">Spark性能优化，其实主要就是在于对内存的使用进行调优。通常情况下，如果你的Spark程序计算的数据量比较小，并且你的内存足够使用，那么只要网络不至于卡死，一般是不会有大的性能问题的。但是Spark程序的性能问题往往出现在针对大数据量进行计算（比如上亿条数的数据，或者上T规模的数据），这个时候如果内存分配不合理就会比较慢，所以，Spark性能优化，主要是对内存进行优化。</span><br></pre></td></tr></table></figure><h2 id="内存都去哪了"><a href="#内存都去哪了" class="headerlink" title="内存都去哪了"></a>内存都去哪了</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 每个Java对象，都有一个对象头，会占用16个字节，主要是包括了一些对象的元信息，比如指向它的类的指针。如果一个对象本身很小，比如就包括了一个int类型的field，那么它的对象头实际上比对象自身还要大。</span><br><span class="line">2. Java的String对象的对象头，会比它内部的原始数据，要多出40个字节。因为它内部使用char数组来保存内部的字符序列，并且还要保存数组长度之类的信息。</span><br><span class="line">3. Java中的集合类型，比如HashMap和LinkedList，内部使用的是链表数据结构，所以对链表中的每一个数据，都使用了Entry对象来包装。Entry对象不光有对象头，还有指向下一个Entry的指针，通常占用8个字节。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">所以把原始文件中的数据转化为内存中的对象之后，占用的内存会比原始文件中的数据要大</span><br><span class="line"></span><br><span class="line">那我如何预估程序会消耗多少内存呢？</span><br><span class="line">通过cache方法，可以看到RDD中的数据cache到内存中之后占用多少内存，这样就能看出了</span><br><span class="line">代码如下：这个测试代码就只写一个scala版本的了</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：测试内存占用情况</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestMemoryScala</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">     conf.setAppName(<span class="string">"TestMemoryScala"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">     <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_10000000.dat"</span>).cache()</span><br><span class="line">     <span class="keyword">val</span> count = dataRDD.count()</span><br><span class="line">         println(count)</span><br><span class="line">     <span class="comment">//while循环是为了保证程序不结束，方便在本地查看4040页面(在本地运行时的spark web页面)中的storage信息</span></span><br><span class="line">     <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">     ;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271945127.png" alt="image-20230327194520543"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271944394.png" alt="image-20230327194456852"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">执行代码，访问localhost的4040端口界面</span><br><span class="line">这个界面其实就是spark的任务界面，在本地运行任务的话可以直接访问4040界面查看</span><br><span class="line">点击stages可以看到任务的原始输入数据是多大</span><br><span class="line"></span><br><span class="line">点击storage可以看到将数据加载到内存，生成RDD之后的大小</span><br><span class="line"></span><br><span class="line">这样我们就能知道这一份数据在RDD中会占用多少内存了，这样在使用的时候，如果想要把数据全部都加载进内存，就需要给这个任务分配这么多内存了，当然了你分配少一些也可以，只不过这样计算效率会变低，因为RDD中的部分数据内存放不下就会放到磁盘了。</span><br></pre></td></tr></table></figure><h2 id="性能优化方案"><a href="#性能优化方案" class="headerlink" title="性能优化方案"></a>性能优化方案</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面我们通过这几个方式来实现对Spark程序的性能优化</span><br><span class="line">    高性能序列化类库</span><br><span class="line">    持久化或者checkpoint</span><br><span class="line">    JVM垃圾回收调优</span><br><span class="line">    提高并行度</span><br><span class="line">    数据本地化</span><br><span class="line">    算子优化</span><br></pre></td></tr></table></figure><h3 id="高性能序列化类库"><a href="#高性能序列化类库" class="headerlink" title="高性能序列化类库"></a>高性能序列化类库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在任何分布式系统中，序列化都是扮演着一个重要的角色的。</span><br><span class="line">如果使用的序列化技术，在执行序列化操作的时候很慢，或者是序列化后的数据还是很大，那么会让分布式应用程序的性能下降很多。所以，进行Spark性能优化的第一步，就是进行序列化的性能优化。</span><br><span class="line"></span><br><span class="line">Spark默认会在一些地方对数据进行序列化，如果我们的算子函数使用到了外部的数据（比如Java中的自定义类型），那么也需要让其可序列化，否则程序在执行的时候是会报错的，提示没有实现序列化，这个一定要注意。</span><br><span class="line"></span><br><span class="line">原因是这样的：</span><br><span class="line">因为Spark的初始化工作是在Driver进程中进行的，但是实际执行是在Worker节点的Executor进程中进行的；当Executor端需要用到Driver端封装的对象时，就需要把Driver端的对象通过序列化传输到Executor端，这个对象就需要实现序列化。</span><br><span class="line">否则会报错，提示对象没有实现序列化</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，其实遇到这种没有实现序列化的对象，解决方法有两种</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 如果此对象可以支持序列化，则将其实现Serializable接口，让它支持序列化</span><br><span class="line">2. 如果此对象不支持序列化，针对一些数据库连接之类的对象，这种对象是不支持序列化的，所以可以把这个代码放到算子内部，这样就不会通过driver端传过去了，它会直接在executor中执行。</span><br><span class="line">Spark对于序列化的便捷性和性能进行了一个取舍和权衡。默认情况下，Spark倾向于序列化的便捷性，使用了Java自身提供的序列化机制——基于ObjectInputStream和 ObjectOutputStream的序列化机制，因为这种方式是Java原生提供的，使用起来比较方便，但是Java序列化机制的性能并不高。序列化的速度相对较慢，而且序列化以后的数据，相对来说还是比较大，比较占空间。所以，如果你的Spark应用程序对内存很敏感，那默认的Java序列化机制并不是最好的选择。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark实际上提供了两种序列化机制：</span><br><span class="line">Java序列化机制和Kryo序列化机制</span><br><span class="line">Spark只是默认使用了java这种序列化机制</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Java序列化机制：默认情况下，Spark使用Java自身的ObjectInputStream和ObjectOutputStream机制进行对象的序列化。只要你的类实现了Serializable接口，那么都是可以序列化的。Java序列化机制的速度比较慢，而且序列化后的数据占用的内存空间比较大，这是它的缺点</span><br><span class="line"></span><br><span class="line">Kryo序列化机制：Spark也支持使用Kryo序列化。Kryo序列化机制比Java序列化机制更快，而且序列化后的数据占用的空间更小，通常比Java序列化的数据占用的空间要小10倍左右。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Kryo序列化机制之所以不是默认序列化机制的原因：</span><br><span class="line"></span><br><span class="line">第一点：因为有些类型虽然实现了Seriralizable接口，但是它也不一定能够被Kryo进行序列化；</span><br><span class="line">第二点：如果你要得到最佳的性能，Kryo还要求你在Spark应用程序中，对所有你需要序列化的类型都进行手工注册，这样就比较麻烦了</span><br><span class="line"></span><br><span class="line">如果要使用Kryo序列化机制</span><br><span class="line">首先要用SparkConf设置spark.serializer的值为 org.apache.spark.serializer.KryoSerializer，就是将Spark的序列化器设置为KryoSerializer。这样，Spark在进行序列化时，就会使用Kryo进行序列化</span><br><span class="line">了。使用Kryo时针对需要序列化的类，需要预先进行注册，这样才能获得最佳性能——如果不注册的话，Kryo也能正常工作，只是Kryo必须时刻保存类型的全类名，反而占用不少内存。</span><br><span class="line">Spark默认对Scala中常用的类型在Kryo中做了注册，但是，如果在自己的算子中，使用了外部的自定义类型的对象，那么还是需要对其进行注册。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注册自定义的数据类型格式：</span><br><span class="line">conf.registerKryoClasses(...)</span><br><span class="line"></span><br><span class="line">注意：如果要序列化的自定义的类型，字段特别多，此时就需要对Kryo本身进行优化，因为Kryo内部的缓存可能不够存放那么大的class对象</span><br><span class="line"></span><br><span class="line">需要调用SparkConf.set()方法，设置spark.kryoserializer.buffer.mb参数的值，将其调大，默认值为2，单位是MB ，也就是说最大能缓存2M的对象，然后进行序列化。可以在必要时将其调大。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">什么场景下适合使用Kryo序列化？</span><br><span class="line"></span><br><span class="line">一般是针对一些自定义的对象，例如我们自己定义了一个对象，这个对象里面包含了几十M，或者上百M的数据，然后在算子函数内部，使用到了这个外部的大对象</span><br><span class="line">如果默认情况下，让Spark用java序列化机制来序列化这种外部的大对象，那么就会导致序列化速度比较慢，并且序列化以后的数据还是比较大。</span><br><span class="line"></span><br><span class="line">所以，在这种情况下，比较适合使用Kryo序列化类库，来对外部的大对象进行序列化，提高序列化速度，减少序列化后的内存空间占用。</span><br><span class="line">用代码实现一个案例：</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><h4 id="使用kryo实现序列化"><a href="#使用kryo实现序列化" class="headerlink" title="使用kryo实现序列化"></a>使用kryo实现序列化</h4><h5 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.<span class="type">Kryo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.<span class="type">KryoRegistrator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Kryo序列化的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KryoSerScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">     conf.setAppName(<span class="string">"KryoSerScala"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     <span class="comment">//指定使用kryo序列化机制，注意：如果使用了registerKryoClasses，其实这一行设置可以省略的</span></span><br><span class="line">     .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">     .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Person</span>]))<span class="comment">//注册自定义的数据类型(Array里也可以传多个)，注册了性能高一些</span></span><br><span class="line">     <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">     <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="string">"hello you"</span>,<span class="string">"hello me"</span>))</span><br><span class="line">     <span class="keyword">val</span> wordsRDD = dataRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">     <span class="keyword">val</span> personRDD = wordsRDD.map(word=&gt;<span class="type">Person</span>(word,<span class="number">18</span>)).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>)</span><br><span class="line">     personRDD.foreach(println(_))</span><br><span class="line">     <span class="comment">//while循环是为了保证程序不结束，方便在本地查看4040页面中的storage信息</span></span><br><span class="line">     <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">     ;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">                                                                 <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>,age: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span></span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行任务，然后访问localhost的4040界面</span><br><span class="line">在界面中可以看到cache的数据大小是 31 字节。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272100115.png" alt="image-20230327210029655"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那我们把kryo序列化设置去掉，使用默认的java序列化看一下效果</span><br><span class="line">修改代码，注释掉这两行代码即可</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">&#x2F;&#x2F;.registerKryoClasses(Array(classOf[Person]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行任务，再访问4040界面</span><br><span class="line">发现此时占用的内存空间是 138 字节，比使用kryo的方式内存空间多占用了将近5倍。</span><br><span class="line">所以从这可以看出来，使用 kryo 序列化方式对内存的占用会降低很多。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272101635.png" alt="image-20230327210118413"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：如果我们只是将spark的序列化机制改为了kryo序列化，但是没有对使用到的自定义类型手工进行注册，那么此时内存的占用会介于前面两种情况之间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改代码，只注释掉registerKryoClasses这一行代码</span><br><span class="line"></span><br><span class="line">.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">&#x2F;&#x2F;.registerKryoClasses(Array(classOf[Person]))&#x2F;&#x2F;注册自定义的数据类型</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行任务，再访问4040界面</span><br><span class="line">发现此时的内存占用为123字节，介于前面的31字节和138字节之间。</span><br><span class="line">所以从这可以看出来，在使用kryo序列化的时候，针对自定义的类型最好是手工注册一下，否则就算开启了kryo序列化，性能的提升也是有限的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272103931.png" alt="image-20230327210309583"></p><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.Kryo;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.KryoRegistrator;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.StorageLevel;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Kryo序列化的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KryoSerjava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//创建SparkContext：</span></span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"KryoSerjava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         .set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSer</span></span><br><span class="line"><span class="string">         .set("</span>spark.kryo.classesToRegister<span class="string">", "</span>com.imooc.java.Person<span class="string">");</span></span><br><span class="line"><span class="string">         JavaSparkContext sc = new JavaSparkContext(conf);</span></span><br><span class="line"><span class="string">         JavaRDD&lt;String&gt; dataRDD = sc.parallelize(Arrays.asList("</span>hello you<span class="string">", "</span>hello me<span class="string">"));</span></span><br><span class="line"><span class="string">         JavaRDD&lt;String&gt; wordsRDD = dataRDD.flatMap(new FlatMapFunction&lt;String,String&gt;()&#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span></span><br><span class="line"><span class="string">         return Arrays.asList(line.split("</span> <span class="string">")).iterator();</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         JavaRDD&lt;Person&gt; personRDD = wordsRDD.map(new Function&lt;String, Person&gt;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public Person call(String word) throws Exception &#123;</span></span><br><span class="line"><span class="string">         return new Person(word, 18);</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;).persist(StorageLevel.MEMORY_ONLY_SER());</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         personRDD.foreach(new VoidFunction&lt;Person&gt;() &#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public void call(Person person) throws Exception &#123;</span></span><br><span class="line"><span class="string">         System.out.println(person);</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         while (true)&#123;</span></span><br><span class="line"><span class="string">         ;</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">class Person implements Serializable&#123;</span></span><br><span class="line"><span class="string">    private String name;</span></span><br><span class="line"><span class="string">    private int age;</span></span><br><span class="line"><span class="string">    Person(String name,int age)&#123; // 这里讲可以通过什么自动生成，没听清</span></span><br><span class="line"><span class="string">        this.name = name;</span></span><br><span class="line"><span class="string">        this.age = age;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    @Override</span></span><br><span class="line"><span class="string">    public String toString() &#123; // alt+shift+s,右键generate-&gt;toString</span></span><br><span class="line"><span class="string">        return "</span>Person&#123;<span class="string">" +</span></span><br><span class="line"><span class="string">            "</span>name=<span class="string">'" + name + '</span>\<span class="string">''</span> +</span><br><span class="line">            <span class="string">", age="</span> + age +</span><br><span class="line">            <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="持久化或者checkpoint"><a href="#持久化或者checkpoint" class="headerlink" title="持久化或者checkpoint"></a>持久化或者checkpoint</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对程序中多次被transformation或者action操作的RDD进行持久化操作，避免对一个RDD反复进行计算，再进一步优化，使用Kryo序列化的持久化级别，减少内存占用</span><br><span class="line">为了保证RDD持久化数据在可能丢失的情况下还能实现高可靠，则需要对RDD执行Checkpoint操作</span><br><span class="line">这两个操作我们前面讲过了，在这就不再演示了</span><br></pre></td></tr></table></figure><h3 id="JVM垃圾回收调优"><a href="#JVM垃圾回收调优" class="headerlink" title="JVM垃圾回收调优"></a>JVM垃圾回收调优</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">由于Spark是基于内存的计算引擎，RDD缓存的数据，以及算子执行期间创建的对象都是放在内存中的，所以针对Spark任务如果内存设置不合理会导致大部分时间都消耗在垃圾回收上</span><br><span class="line"></span><br><span class="line">对于垃圾回收来说，最重要的就是调节RDD缓存占用的内存空间，和算子执行时创建的对象占用的内存空间的比例。</span><br><span class="line"></span><br><span class="line">默认情况下，Spark使用每个executor 60%的内存空间来缓存RDD，那么只有40%的内存空间来存放算子执行期间创建的对象</span><br><span class="line">在这种情况下，可能由于内存空间的不足，并且算子对应的task任务在运行时创建的对象过大，那么一旦发现40%的内存空间不够用了，就会触发Java虚拟机的垃圾回收操作。</span><br><span class="line"></span><br><span class="line">因此在极端情况下，垃圾回收操作可能会被频繁触发。</span><br><span class="line">在这种情况下，如果发现垃圾回收频繁发生。那么就需要对这个比例进行调优了， spark.storage.memoryFraction参数的值默认是0.6。</span><br><span class="line">使用SparkConf().set(&quot;spark.storage.memoryFraction&quot;, &quot;0.5&quot;) 可以进行修改，就是将RDD缓存占用内存空间的比例降低为 50% ，从而提供更多的内存空间来保存task运行时创建的对象。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">因此，对于RDD持久化而言，完全可以使用Kryo序列化，加上降低其executor内存占比的方式，来减少其内存消耗。给task提供更多的内存，从而避免task在执行时频繁触发垃圾回收。</span><br><span class="line">我们可以对task的垃圾回收进行监测，在spark的任务执行界面，可以查看每个task执行消耗的时间，以及task gc消耗的时间。</span><br><span class="line"></span><br><span class="line">重新向集群中提交checkpoint的代码，查看spark任务的task指标信息</span><br><span class="line">确保Hadoop集群、yarn的historyserver进程以及spark的historyserver进程是正常运行的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">删除checkpoint任务的输出目录</span><br><span class="line">[root@bigdata04 sparkjars]# hdfs dfs -rm -r &#x2F;out-chk001</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# sh -x checkPointJob.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击生成的第一个job，再点击进去查看这个job的stage，进入第一个stage，查看task的执行情况，看这里面的GC time的数值会不会比较大，最直观的就是如果gc time这里标红了，则说明gc时间过长。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272230037.png" alt="image-20230327223033714"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上面这个是分任务查看，其实还可以查看全局的，看Executor进程中整个任务执行总时间和gc的消耗时间。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272232315.png" alt="image-20230327223148318"></p><h4 id="java-GC"><a href="#java-GC" class="headerlink" title="java GC"></a>java GC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">既然说到了Java中的GC，那我们就需要说道说道了。</span><br><span class="line">Java堆空间被划分成了两块空间：一个是年轻代，一个是老年代。</span><br><span class="line">年轻代放的是短时间存活的对象</span><br><span class="line">老年代放的是长时间存活的对象。</span><br><span class="line">年轻代又被划分了三块空间， Eden、Survivor1、Survivor2</span><br><span class="line"></span><br><span class="line">来看一下这个内存划分比例图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272234159.png" alt="image-20230327223408841"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">年轻代占堆内存的1&#x2F;3，老年代占堆内存的2&#x2F;3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">其中年轻代又被划分了三块， Eden，Survivor1，Survivor2 的比例为 8:1:1</span><br><span class="line">Eden区域和Survivor1区域用于存放对象，Survivor2区域备用。</span><br><span class="line"></span><br><span class="line">我们创建的对象，首先会放入Eden区域，如果Eden区域满了，那么就会触发一次Minor GC，进行年轻代的垃圾回收(其实就是回收Eden区域内没有人使用的对象)，然后将存活的对象存入Survivor1区域，再创建对象的时候继续放入Eden区域。</span><br><span class="line"></span><br><span class="line">第二次Eden区域满了，那么Eden和Survivor1区域中存活的对象，会一块被移动到Survivor2区域中。然后Survivor1和Survivor2的角色调换，Survivor1变成了备用。</span><br><span class="line"></span><br><span class="line">当第三次Eden区域再满了的时候，Eden和Survivor2区域中存活的对象，会一块被移动到Survivor1区域中，按照这个规律进行循环</span><br><span class="line"></span><br><span class="line">如果一个对象，在年轻代中，撑过了多次垃圾回收(默认是15次)，都没有被回收掉，那么会被认为是长时间存活的，此时就会被移入老年代。此外，如果在将Eden和Survivor1中的存活对象，尝试放入Survivor2中时，发现Survivor2放满了，那么会直接放入老年代。此时就出现了，短时间存活的对象，也会进入老年代的问题。</span><br><span class="line"></span><br><span class="line">如果老年代的空间满了，那么就会触发Full GC，进行老年代的垃圾回收操作，如果执行Full GC也释放不了内存空间，就会报内存溢出的错误了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，Full GC是一个重量级的垃圾回收，Full GC执行的时候，程序是处于暂停状态的，这样会非常影响性能。</span><br></pre></td></tr></table></figure><h4 id="spark-GC调优方案"><a href="#spark-GC调优方案" class="headerlink" title="spark GC调优方案"></a>spark GC调优方案</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark中，垃圾回收调优的目标就是，只有真正长时间存活的对象，才能进入老年代，短时间存活的对象，只能呆在年轻代。不能因为某个Survivor区域空间不够，在Minor GC时，就进入了老年代，从而造成短时间存活的对象，长期呆在老年代中占据了空间，这样Full GC时要回收大量的短时间存活的对象，导致Full GC速度缓慢。</span><br><span class="line"></span><br><span class="line">如果发现，在task执行期间，大量full gc发生了，那么说明，年轻代的Eden区域，给的空间不够大。此时可以执行一些操作来优化垃圾回收行为</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1：最直接的就是提高Executor的内存</span><br><span class="line">在spark-submit中通过参数指定executor的内存</span><br><span class="line">--executor-memory 1G </span><br><span class="line"></span><br><span class="line">2：调整Eden与s1和s2的比值【一般情况下不建议调整这块的比值】</span><br><span class="line">-XX:NewRatio&#x3D;4：设置年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代).设置为4,则年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1&#x2F;5</span><br><span class="line">-XX:SurvivorRatio&#x3D;4：设置年轻代中Eden区与Survivor区的大小比值.设置为4,则两个Survivor区与一个Eden区的比值为2:4,一个Survivor区占整个年轻代的1&#x2F;6</span><br><span class="line">具体使用的时候在 spark-submit 脚本中通过 --conf 参数设置即可</span><br><span class="line">--conf &quot;spark.executor.extraJavaOptions&#x3D; -XX:SurvivorRatio&#x3D;4 -XX:NewRatio&#x3D;4&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">其实最直接的就是增加Executor的内存，如果这个内存上不去，其它的修改都是徒劳。</span><br><span class="line">举个例子就是说，一个20岁的成年人和一个3岁的小孩</span><br><span class="line">3岁的小孩掌握再多的格斗技巧都没有用，在绝对的实力面前一切都是花架子。</span><br><span class="line">所以说我们一般很少需要去调整Eden、s1、s2的比值，一般都是直接增加Executor的内存比较靠谱。</span><br></pre></td></tr></table></figure><h3 id="提高并行度"><a href="#提高并行度" class="headerlink" title="提高并行度"></a>提高并行度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实际上Spark集群的资源并不一定会被充分利用到，所以要尽量设置合理的并行度，来充分地利用集群的资源，这样才能提高Spark程序的性能。</span><br><span class="line"></span><br><span class="line">Spark会自动设置以文件作为输入源的RDD的并行度，依据其大小，比如HDFS，就会给每一个block创建一个partition，也依据这个设置并行度。对于reduceByKey等会发生shuffle操作的算子，会使用并行度最大的父RDD的并行度</span><br><span class="line"></span><br><span class="line">可以手动使用textFile()、parallelize()等方法的第二个参数来设置并行度(只针对这一个RDD)；也可以使用spark.default.parallelism参数(全局)，来设置统一的并行度。Spark官方的推荐是，给集群中的每个cpu core设置2~3个task。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">下面来举个例子</span><br><span class="line">我在spark-submit 脚本中给任务设置了5个executor，每个executor，设置了2个cpu core</span><br><span class="line">spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \   &#x2F;&#x2F; 为job设置5个executor</span><br><span class="line">--executor-cores 2 \   &#x2F;&#x2F; 分配两个CPU</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时，如果我在代码中设置了默认并行度为5</span><br><span class="line">conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br><span class="line"></span><br><span class="line">这个参数设置完了以后，也就意味着所有RDD的partition都被设置成了5个，针对RDD的每一个partition，spark会启动一个task来进行计算，所以对于所有的算子操作，都只会创建5个task来处理对应的RDD中的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">但是注意了，我们前面在spark-submit脚本中设置了5个executor，每个executor 2个cpu core，所以这个时候spark其实会向yarn集群申请10个cpu core，但是我们在代码中设置了默认并行度为5，只会产生5个task，一个task使用一个cpu core，那也就意味着有5个cpu core是空闲的，这样申请的资源就浪费了一半。</span><br><span class="line"></span><br><span class="line">其实最好的情况，就是每个cpu core都不闲着，一直在运行，这样可以达到资源的最大使用率，其实让一个cpu core运行一个task都是有点浪费的，官方也建议让每个cpu core运行2~3个task，这样可以充分压榨CPU的性能</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">为什么这样说呢？</span><br><span class="line"></span><br><span class="line">是这样的，因为每个task执行的顺序和执行结束的时间很大概率是不一样的，如果正好有10个cpu，运行10个taks，那么某个task可能很快就执行完了，那么这个CPU就空闲下来了，这样资源就浪费了。</span><br><span class="line">所以说官方推荐，给每个cpu分配2~3个task是比较合理的，可以充分利用CPU资源，发挥它最大的价值。</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">下面我们来实际写个案例看一下效果</span><br><span class="line">Scala代码如下：</span><br><span class="line"></span><br><span class="line">package com.imooc.scala</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：设置并行度</span><br><span class="line"> * 1：可以在textFile或者parallelize等方法的第二个参数中设置并行度</span><br><span class="line"> * 2：或者通过spark.default.parallelism参数统一设置并行度</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object MoreParallelismScala &#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">     val conf &#x3D; new SparkConf()</span><br><span class="line">     conf.setAppName(&quot;MoreParallelismScala&quot;)</span><br><span class="line">     &#x2F;&#x2F;.setMaster(&quot;local&quot;)</span><br><span class="line">    &#x2F;&#x2F;设置全局并行度</span><br><span class="line">     conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br><span class="line">     val sc &#x3D; new SparkContext(conf)</span><br><span class="line">     val dataRDD &#x3D; sc.parallelize(Array(&quot;hello&quot;,&quot;you&quot;,&quot;hello&quot;,&quot;me&quot;,&quot;hehe&quot;,&quot;hello&quot;,&quot;you&quot;,&quot;hello&quot;,&quot;me&quot;,&quot;hehe&quot;))</span><br><span class="line">     dataRDD.map((_,1))</span><br><span class="line">     .reduceByKey(_ + _)</span><br><span class="line">     .foreach(println(_))</span><br><span class="line">     sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：设置并行度</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MoreParallelismJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">     conf.setAppName(<span class="string">"MoreParallelismJava"</span>);</span><br><span class="line">     <span class="comment">//设置全局并行度</span></span><br><span class="line">     conf.set(<span class="string">"spark.default.parallelism"</span>,<span class="string">"5"</span>);</span><br><span class="line">     JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">     JavaRDD&lt;String&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="string">"hello"</span>,<span class="string">"you"</span>,<span class="string">"hello"</span>,<span class="string">"me"</span>,<span class="string">"hehe"</span>,<span class="string">"hello"</span>,<span class="string">"you"</span>,<span class="string">"hello"</span>,<span class="string">"me"</span>,<span class="string">"hehe"</span>));</span><br><span class="line">     dataRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">     return new Tuple2&lt;String, Integer&gt;<span class="params">(word,<span class="number">1</span>)</span></span>;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;).reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     <span class="keyword">return</span> i1 + i2;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;).foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     System.out.println(tup);</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;);</span><br><span class="line">     sc.stop();                                                      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">对代码编译打包</span><br><span class="line">spark-submit脚本内容如下：</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# vi moreParallelismJob.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.MoreParallelismScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280039960.png" alt="image-20230328003937453"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">任务提交到集群运行之后，查看spark的任务界面(job)</span><br><span class="line">先看executors，这里显示了4个executor和1个driver进程，为什么不是5个executor进程呢？</span><br><span class="line"></span><br><span class="line">是因为我们现在使用的是yarn-cluster模式，driver进程运行在集群内部，所以它占了一个executor，如果使用的是yarn-client模式，就会产生5个executor和1个单独的driver进程。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280042154.png" alt="image-20230328004230697"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后去看satges界面，两个Stage都是5个task并行执行，这5个task会使用5个cpu，但是我们给这个任务申请了10个cpu，所以就有5个是空闲的了(这里没考虑driver的占用)。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280057687.png" alt="image-20230328005706629"></p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280054889.png" alt="image-20230328005414569"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当在sparkContext生成对象后，再设置默认并行度会出现问题</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280055569.png" alt="image-20230328005531042"></p><h4 id="提高性能"><a href="#提高性能" class="headerlink" title="提高性能"></a>提高性能</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果想要最大限度利用CPU的性能，至少将spark.default.parallelism的值设置为10，这样可以实现一个cpu运行一个task，其实官方推荐是设置为20或者30。</span><br><span class="line">其实这个参数也可以在spark-submit脚本中动态设置，通过--conf参数设置，这样就比较灵活了。</span><br><span class="line"></span><br><span class="line">注意：此时需要将代码中设置spark.default.parallelism的配置注释掉</span><br><span class="line">&#x2F;&#x2F;conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">为了看起来更清晰，在这我们使用 yarn-client 模式，这样driver就不会占用我们的分配的executor了</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# vi moreParallelismJob2.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.MoreParallelismScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot; \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于修改了代码，所以需要重新编译，打包，执行</span><br><span class="line">执行结束后再来查看spark的任务界面，可以看到此时有10个task并行执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280117859.png" alt="image-20230328011738873"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280118186.png" alt="image-20230328011801559"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280116405.png" alt="image-20230328011600636"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是并行度相关的设置</span><br><span class="line">接下来我们来看一个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280126634.png" alt="image-20230328012605704"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个图中描述的就是刚才我们演示的两种情况下Executor和Task之间的关系</span><br></pre></td></tr></table></figure><h4 id="spark-submit常用参数"><a href="#spark-submit常用参数" class="headerlink" title="spark-submit常用参数"></a>spark-submit常用参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">最后我们来分析总结一下spark-submit脚本中经常配置的一些参数</span><br><span class="line"></span><br><span class="line">--name mySparkJobName：指定任务名称(代码里也可以设置)</span><br><span class="line">--class com.imooc.scala.xxxxx ：指定入口类</span><br><span class="line">--master yarn ：指定集群地址(standalone)，on yarn模式指定yarn</span><br><span class="line">--deploy-mode cluster ：client代表yarn-client，cluster代表yarn-cluster</span><br><span class="line">--executor-memory 1G ：executor进程的内存大小，实际工作中设置2~4G即可</span><br><span class="line">--num-executors 2 ：分配多少个executor进程</span><br><span class="line">--executor-cores 2 : 一个executor进程分配多少个cpu core</span><br><span class="line"></span><br><span class="line">--driver-cores 1 ：(如果使用yarn-cluster模式，这里不用设置也行，直接按executor的配置来)driver进程分配多少cpu core，默认为1即可</span><br><span class="line">--driver-memory 1G：(如果使用yarn-cluster模式，这里不用设置也行，直接按executor的配置来)driver进程的内存，如果需要使用类似于collect之类的action算子向Driver端拉取数据，则这里可以设置大一些</span><br><span class="line">--jars fastjson.jar,abc.jar 在这里可以设置job依赖的第三方jar包(pom里spark-core没提供的)【不建议把第三方依赖打入程序的jar包中，一方面会导致jar变大；另一方面，同一个项目组同事用的第三方依赖版本问题；还有这里可以使用本地路径，或者hdfs路径(建议使用hdfs路径，因为使用本地路径依赖，还是会读取到hdfs上)】</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot;：可以动态指定一些spark任务的参数，指定多个参</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">最后注意一点：针对 --num-executors 和 --executor-cores 的设置</span><br><span class="line">大家看这两种方式设置有什么区别：</span><br><span class="line">第一种方式：</span><br><span class="line">--num-executors 2</span><br><span class="line">--executor-cores 1</span><br><span class="line">第二种方式：</span><br><span class="line">--num-executors 1</span><br><span class="line">--executor-cores 2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这两种设置最终都会向集群申请2个cpu core，可以并行运行两个task，但是这两种设置方式有什么区别呢？</span><br><span class="line"></span><br><span class="line">第一种方法：多executor模式</span><br><span class="line">由于每个executor只分配了一个cpu core，我们将无法利用在同一个JVM中运行多个任务的优点。我们假设这两个executor是在两个节点中启动的，那么针对广播变量这种操作，将在两个节点的中都复制1份，最终会复制两份</span><br><span class="line"></span><br><span class="line">第二种方法：多core模式</span><br><span class="line">此时一个executor中会有2个cpu core，这样可以利用同一个JVM中运行多个任务的优点，并且针对广播变量的这种操作，只会在这个executor对应的节点中复制1份即可。</span><br><span class="line"></span><br><span class="line">那是不是我可以给一个executor分配很多的cpu core，也不是的，因为一个executor的内存大小是固定的，如果在里面运行过多的task可能会导致内存不够用，所以这块一般在工作中我们会给一个executor分配 2~4G 内存，对应的分配 2~4个cpu core。</span><br></pre></td></tr></table></figure><h3 id="数据本地化"><a href="#数据本地化" class="headerlink" title="数据本地化"></a>数据本地化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">数据本地化对于Spark Job性能有着巨大的影响。如果数据以及要计算它的代码是在一起的，那么性能当然会非常高。但是，如果数据和计算它的代码是分开的，那么其中之一必须到另外一方的机器上。通常来说，移动代码到其它节点，会比移动数据到代码所在的节点，速度要得多，因为代码比较小。Spark也正是基于这个数据本地化的原则来构建task调度算法的。</span><br><span class="line"></span><br><span class="line">数据本地化，指的是，数据离计算它的代码有多近。基于数据距离代码的距离，有几种数据本地化级别：</span><br><span class="line"></span><br><span class="line">数据本地化级别 解释</span><br><span class="line">PROCESS_LOCAL 进程本地化，性能最好：数据和计算它的代码在同一个JVM进程中</span><br><span class="line">NODE_LOCAL 节点本地化：数据和计算它的代码在一个节点上，但是不在一个JVM进程</span><br><span class="line">NO_PREF 数据从哪里过来，性能都是一样的，比如从数据库中获取数据，对于task而言在哪个机器上都是一样的</span><br><span class="line">RACK_LOCAL 数据和计算它的代码在一个机架上，数据需要通过网络在节点之间进行传输</span><br><span class="line">ANY 数据可能在任意地方，比如其它网络环境内，或者其它机架上，性能最差</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281058668.png" alt="image-20230328105831703"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Spark倾向使用最好的本地化级别调度task，但这是不现实的</span><br><span class="line">如果目前我们要处理的数据所在的executor上目前没有空闲的CPU，那么Spark就会放低本地化级别。这时有两个选择：</span><br><span class="line">第一，等待，直到executor上的cpu释放出来，那么就分配task过去；</span><br><span class="line">第二，立即在任意一个其它executor上启动一个task。</span><br><span class="line"></span><br><span class="line">Spark默认会等待指定时间，期望task要处理的数据所在的节点上的executor空闲出一个cpu，从而将task分配过去，只要超过了时间，那么Spark就会将task分配到其它任意一个空闲的executor上</span><br><span class="line"></span><br><span class="line">可以设置参数， spark.locality 系列参数，来调节Spark等待task可以进行数据本地化的时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait（3000毫秒）：默认等待3秒(通用的所有级别)</span><br><span class="line">spark.locality.wait.process：等待指定的时间看能否达到数据和计算它的代码在同一个JVM</span><br><span class="line">spark.locality.wait.node：等待指定的时间看能否达到数据和计算它的代码在一个节点上执行</span><br><span class="line">spark.locality.wait.rack：等待指定的时间看能否达到数据和计算它的代码在一个机架上</span><br><span class="line"></span><br><span class="line">看这个图里面的task，此时的数据本地化级别是最优的 PROCESS_LOCAL</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281108354.png" alt="image-20230328110840007"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-checkpoint.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-checkpoint.html</id>
    <published>2023-03-27T06:42:14.000Z</published>
    <updated>2023-03-28T06:52:48.630Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-2"><a href="#第十一周-Spark性能优化的道与术-2" class="headerlink" title="第十一周 Spark性能优化的道与术-2"></a>第十一周 Spark性能优化的道与术-2</h1><h2 id="checkpoint概述"><a href="#checkpoint概述" class="headerlink" title="checkpoint概述"></a>checkpoint概述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">checkpoint，是Spark提供的一个比较高级的功能。有时候，我们的Spark任务，比较复杂，从初始化RDD开始，到最后整个任务完成，有比较多的步骤，比如超过10个transformation算子。而且，整个任务运行的时间也特别长，比如通常要运行1~2个小时。在这种情况下，就比较适合使用checkpoint功能了。</span><br><span class="line"></span><br><span class="line">因为对于特别复杂的Spark任务，有很高的风险会出现某个要反复使用的RDD因为节点的故障导致丢失，虽然之前持久化过，但是还是导致数据丢失了。那么也就是说，出现失败的时候，没有容错机制，所以当后面的transformation算子，又要使用到该RDD时，就会发现数据丢失了，此时如果没有进行容错处理的话，那么就需要再重新计算一次数据了。</span><br><span class="line">所以针对这种Spark Job，如果我们担心某些关键的，在后面会反复使用的RDD，因为节点故障导致数据丢失，那么可以针对该RDD启动checkpoint机制，实现容错和高可用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那如何使用checkPoint呢？</span><br><span class="line">首先要调用SparkContext的setCheckpointDir()方法，设置一个容错的文件系统的目录，比如HDFS；然后，对RDD调用checkpoint()方法。</span><br><span class="line">最后，在RDD所在的job运行结束之后，会启动一个单独的job，将checkpoint设置过的RDD的数据写入之前设置的文件系统中。</span><br><span class="line"></span><br><span class="line">这是checkpoint使用的基本步骤，很简单，那我们下面先从理论层面分析一下当我们设置好checkpoint之后，Spark底层都做了哪些事情</span><br></pre></td></tr></table></figure><h2 id="RDD之checkpoint流程"><a href="#RDD之checkpoint流程" class="headerlink" title="RDD之checkpoint流程"></a>RDD之checkpoint流程</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271450477.png" alt="image-20230327144700422"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1：SparkContext设置checkpoint目录，用于存放checkpoint的数据；</span><br><span class="line">对RDD调用checkpoint方法，然后它就会被RDDCheckpointData对象进行管理，此时这个RDD的checkpoint状态会被设置为Initialized</span><br><span class="line">2：待RDD所在的job运行结束，会调用job中最后一个RDD的doCheckpoint方法，该方法沿着RDD的血缘关系向上查找被checkpoint()方法标记过的RDD， 并将其checkpoint状态从Initialized设置为CheckpointingInProgress</span><br><span class="line">3：启动一个单独的job，来将血缘关系中标记为CheckpointInProgress的RDD执行checkpoint操作，也就是将其数据写入checkpoint目录</span><br><span class="line">4：将RDD数据写入checkpoint目录之后，会将RDD状态改变Checkpointed；</span><br><span class="line">并且还会改变RDD的血缘关系，即会清除掉RDD所有依赖的RDD；最后还会设置其父RDD为新创建的CheckpointRDD</span><br></pre></td></tr></table></figure><h2 id="checkpoint与持久化的区别"><a href="#checkpoint与持久化的区别" class="headerlink" title="checkpoint与持久化的区别"></a>checkpoint与持久化的区别</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">那这里所说的checkpoint和我们之前讲的RDD持久化有什么区别吗？</span><br><span class="line">lineage是否发生改变linage（血缘关系）说的就是RDD之间的依赖关系</span><br><span class="line"></span><br><span class="line">持久化，只是将数据保存在内存中或者本地磁盘文件中，RDD的lineage(血缘关系)是不变的；</span><br><span class="line"></span><br><span class="line">Checkpoint执行之后，RDD就没有依赖的RDD了，也就是它的lineage改变了</span><br><span class="line">丢失数据的可能性持久化的数据丢失的可能性较大，如果采用persist把数据存在内存中的话，虽然速度最快但是也是最不可靠的，就算放在磁盘上也不是完全可靠的，因为磁盘也会损坏。Checkpoint的数据通常是保存在高可用文件系统中(HDFS),丢失的可能性很低</span><br><span class="line"></span><br><span class="line">建议：对需要checkpoint的RDD，先执行persist(StorageLevel.DISK_ONLY)</span><br><span class="line">为什么呢？</span><br><span class="line"></span><br><span class="line">因为默认情况下，如果某个RDD没有持久化，但是设置了checkpoint，那么这个时候，本来Spark任务已经执行结束了，但是由于中间的RDD没有持久化，在进行checkpoint的时候想要将这个RDD的数据写入外部存储系统的话，就需要重新计算这个RDD的数据，再将其checkpoint到外部存储系统中。</span><br><span class="line">如果对需要checkpoint的rdd进行了基于磁盘的持久化，那么后面进行checkpoint操作时，就会直接从磁盘上读取rdd的数据了，就不需要重新再计算一次了，这样效率就高了。</span><br><span class="line"></span><br><span class="line">那在这能不能使用基于内存的持久化呢？当然是可以的，不过没那个必要。</span><br></pre></td></tr></table></figure><h2 id="checkPoint的使用"><a href="#checkPoint的使用" class="headerlink" title="checkPoint的使用"></a>checkPoint的使用</h2><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们来演示一下：将一个RDD的数据持久化到HDFS上面</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：checkpoint的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CheckPointOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"CheckPointOpScala"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="keyword">if</span>(args.length==<span class="number">0</span>)&#123;</span><br><span class="line">         <span class="type">System</span>.exit(<span class="number">100</span>)</span><br><span class="line">     &#125;</span><br><span class="line">         <span class="keyword">val</span> outputPath = args(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//1：设置checkpint目录</span></span><br><span class="line">         sc.setCheckpointDir(<span class="string">"hdfs://bigdata01:9000/chk001"</span>)</span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_10000000.dat"</span>)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//2：对rdd执行checkpoint操作</span></span><br><span class="line">         dataRDD.checkpoint()</span><br><span class="line">         dataRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">         .map((_,<span class="number">1</span>))</span><br><span class="line">         .reduceByKey(_ + _)</span><br><span class="line">         .saveAsTextFile(outputPath)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.StorageLevel;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：checkpoint的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckPointOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"CheckPointOpJava"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         <span class="keyword">if</span>(args.length==<span class="number">0</span>)&#123;</span><br><span class="line">         System.exit(<span class="number">100</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         String outputPath = args[<span class="number">0</span>];</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//1：设置checkpint目录</span></span><br><span class="line">         sc.setCheckpointDir(<span class="string">"hdfs://bigdata01:9000/chk002"</span>);</span><br><span class="line">         JavaRDD&lt;String&gt; dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_100000.dat"</span>);</span><br><span class="line">         <span class="comment">//2: 对rdd执行checkpoint操作</span></span><br><span class="line">         dataRDD.checkpoint();</span><br><span class="line">         dataRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">          <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new Tuple2&lt;String, Integer&gt;<span class="params">(word,<span class="number">1</span>)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> i1 + i2;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).saveAsTextFile(outputPath);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="提交到集群执行"><a href="#提交到集群执行" class="headerlink" title="提交到集群执行"></a>提交到集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下面我们把这个任务打包提交到集群上运行一下，看一下效果。</span><br><span class="line">代码master部分注释掉</span><br><span class="line"></span><br><span class="line">先确保hadoop集群是正常运行的，以及hadoop中的historyserver进程和spark的historyserver进程也是正常运行的。</span><br><span class="line">测试数据之前已经上传到了hdfs上面，如果没有则需要上传</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271517969.png" alt="image-20230327151632675"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将pom.xml中的spark-core的依赖设置为provided，然后编译打包</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line"> &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">将打包的jar包上传到bigdata04的&#x2F;data&#x2F;soft&#x2F;sparkjars目录，创建一个新的spark-submit脚本</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271519124.png" alt="image-20230327151903625"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行成功之后可以到 setCheckpointDir 指定的目录中查看一下，可以看到目录中会生成对应的文件保存rdd中的数据，只不过生成的文件不是普通文本文件，直接查看文件中的内容显示为乱码。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271523654.png" alt="image-20230327152317696"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271523426.png" alt="image-20230327152338046"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271525661.png" alt="image-20230327152509680"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来进到YARN的8088界面查看</span><br><span class="line">点击Tracking UI进入spark的ui界面看第一个界面jobs</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271526703.png" alt="image-20230327152656935"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在这可以看出来产生了2个job，</span><br><span class="line">第一个job是我们正常的任务执行，执行了39s，一共产生了28个task任务</span><br><span class="line">第二个job是checkpoint启动的job，执行了35s，一共产生了14个task任务</span><br><span class="line"></span><br><span class="line">看第二个界面Stages，这里面的3个Stage是前面2个job产生的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">具体想知道哪些Stage属于哪个job任务的话，可以在任务界面，点击Description中的链接就可以看到job对应的Stage</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271528015.png" alt="image-20230327152845211"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个job其实就是我们实现的单词计数的功能，这个任务产生了两个stage，这两个stage具体是如何划分的呢？</span><br><span class="line">咱们前面讲过，stage的划分是由宽依赖决定的，在这个任务中reduceByKey这个过程会产生宽依赖，所以会产生2个Stage</span><br><span class="line">这里面显示的有这两个stage的一些基本信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271531710.png" alt="image-20230327153130946"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stage id：stage的编号，从0开始</span><br><span class="line">Duration：stage执行消耗的时间</span><br><span class="line">Tasks：Successed&#x2F;Total：task执行成功数量&#x2F;task总量</span><br><span class="line">Input：输入数据量</span><br><span class="line">ouput：输出数据量</span><br><span class="line">shuffle read&#x2F;shuffle read：shuffle过程传输数据量</span><br><span class="line">点击这个界面中的DAG Visualization可以看到当前这个任务stage的划分情况，可以看到每个Stage包含哪些算子</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271534605.png" alt="image-20230327153403130"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">进到Stage内部看一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271541155.png" alt="image-20230327154126374"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271542549.png" alt="image-20230327154248904"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面可以看到每个task的具体执行情况，执行状态，执行消耗的时间，GC消耗的时间，处理的数据量和数据条数、通过shuffle输出的数据量和数据条数</span><br><span class="line">其实从这里也可以看出来文件的每一个block块会产生一个task</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271546781.png" alt="image-20230327154559437"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是这个Stage执行的基本信息了。</span><br><span class="line">加下来看一下第二个Job，这个job是checkpoint启动的任务，查看它的stage的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271546799.png" alt="image-20230327154637511"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个job只会产生一个stage，因为我们只针对textFile的结果设置了checkpoint</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271547533.png" alt="image-20230327154722955"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个stage执行消耗了35s，说明这份数据是重新通过textFile读取过来的。</span><br><span class="line">针对Storage这块，显示的其实就是持久化的数据，如果对RDD做了持久化，那么在任务执行过程中能看到，任务执行结束就看不到了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271548520.png" alt="image-20230327154816099"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来验证一下在开启持久化的情况下执行checkpoint操作时的区别</span><br><span class="line">在代码中针对RDD开启持久化</span><br><span class="line">1：对比此时产生的两个job总的消耗的时间，以及job中的Stage消耗的时间</span><br><span class="line">其实你会发现开启持久化之后，checkpoint的那个job消耗的时间就变少了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271553264.png" alt="image-20230327155305626"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2：查看DAG Visualization，你会发现stage里面也会有有一些不一样的地方</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271556557.png" alt="image-20230327155658220"></p><h2 id="checkpoint源码分析"><a href="#checkpoint源码分析" class="headerlink" title="checkpoint源码分析"></a>checkpoint源码分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们通过理论层面分析了checkpoint的原理，以及演示了checkpoint的使用</span><br><span class="line">下面我们通过源码层面来对我们前面分析的理论进行验证</span><br><span class="line">先下载spark源码，下载流程和下载spark安装包的流程一样</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271953796.png" alt="image-20230327195356368"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">把下载的安装包解压到idea项目目录中</span><br><span class="line"></span><br><span class="line">打开spark-2.4.3源码目录，进入core目录，这个是spark的核心代码，我们要查看的checkpoint的源码就在这个项目中</span><br><span class="line">在idea中打开core这个子项目</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271954921.png" alt="image-20230327195430272"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面我们就来分析一下RDD的checkpoint功能：</span><br><span class="line">checkpoint功能可以分为两块</span><br><span class="line">1：checkpoint的写操作</span><br><span class="line">将指定RDD的数据通过checkpoint存储到指定外部存储中</span><br><span class="line">2：checkpoint的读操作</span><br><span class="line">任务中RDD数据在使用过程中丢失了，正好这个RDD之前做过checkpoint，所以这时就需要通过checkpoint来恢复数据</span><br></pre></td></tr></table></figure><h3 id="checkpoint的写操作"><a href="#checkpoint的写操作" class="headerlink" title="checkpoint的写操作"></a>checkpoint的写操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.1 ：当我们在自己开发的spark任务中先调用 sc.setCheckpointDir时，底层其实就会调用</span><br><span class="line">SparkContext中的setCheckpointDir方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def setCheckpointDir(directory: String) &#123;</span><br><span class="line">     &#x2F;&#x2F; If we are running on a cluster, log a warning if the directory is local.</span><br><span class="line">     &#x2F;&#x2F; Otherwise, the driver may attempt to reconstruct the checkpointed RDD fr</span><br><span class="line">     &#x2F;&#x2F; its own local file system, which is incorrect because the checkpoint fil</span><br><span class="line">     &#x2F;&#x2F; are actually on the executor machines.</span><br><span class="line">     if (!isLocal &amp;&amp; Utils.nonLocalPaths(directory).isEmpty) &#123;</span><br><span class="line">         logWarning(&quot;Spark is not running in local mode, therefore the checkpoint </span><br><span class="line">         s&quot;must not be on the local filesystem. Directory &#39;$directory&#39; &quot; +</span><br><span class="line">         &quot;appears to be on the local filesystem.&quot;)</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;根据我们传过来的目录，后面再拼上一个子目录，子目录使用一个UUID随机字符串</span><br><span class="line">     &#x2F;&#x2F;使用HDFS的javaAPI 在HDFS上创建目录</span><br><span class="line">     checkpointDir &#x3D; Option(directory).map &#123; dir &#x3D;&gt;</span><br><span class="line">     val path &#x3D; new Path(dir, UUID.randomUUID().toString)</span><br><span class="line">     val fs &#x3D; path.getFileSystem(hadoopConfiguration)</span><br><span class="line">     fs.mkdirs(path)</span><br><span class="line">     fs.getFileStatus(path).getPath.toString</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1.2：接着我们会调用RDD.checkpoint方法，此时会执行RDD这个class中的 checkpoint方法</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;这里相当于是checkpoint的一个标记，并没有真正执行checkpoint</span><br><span class="line">def checkpoint(): Unit &#x3D; RDDCheckpointData.synchronized &#123;</span><br><span class="line">     &#x2F;&#x2F; NOTE: we use a global lock here due to complexities downstream with ensu</span><br><span class="line">     &#x2F;&#x2F; children RDD partitions point to the correct parent partitions. In the f</span><br><span class="line">     &#x2F;&#x2F; we should revisit this consideration.</span><br><span class="line">     &#x2F;&#x2F;如果SparkContext没有设置checkpointDir，则抛出异常</span><br><span class="line">     if (context.checkpointDir.isEmpty) &#123;</span><br><span class="line">     throw new SparkException(&quot;Checkpoint directory has not been set in the Sp</span><br><span class="line">     &#125; else if (checkpointData.isEmpty) &#123;</span><br><span class="line">     &#x2F;&#x2F;如果设置了，则创建RDDCheckpointData的子类，这个子类主要负责管理RDD的checkpoi</span><br><span class="line">     &#x2F;&#x2F;并且会初始化checkpoint状态为Initialized</span><br><span class="line">     checkpointData &#x3D; Some(new ReliableRDDCheckpointData(this))</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这个checkpoint方法执行完成之后，这个流程就结束了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.3：剩下的就是在这个设置了checkpint的RDD所在的job执行结束之后，Spark会调用job中最后一个RDD的doCheckpoint方法</span><br><span class="line">这个逻辑是在SparkContext这个class的runJob方法中，当执行到Spark中的action算子时，这个runJob方法会被触发，开始执行任务。</span><br><span class="line">这个runJob的最后一行会调用rdd中的 doCheckpoint 方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;在有action动作时，会触发sparkcontext对runJob的调用</span><br><span class="line">def runJob[T, U: ClassTag](</span><br><span class="line">     rdd: RDD[T],</span><br><span class="line">     func: (TaskContext, Iterator[T]) &#x3D;&gt; U,</span><br><span class="line">     partitions: Seq[Int],</span><br><span class="line">     resultHandler: (Int, U) &#x3D;&gt; Unit): Unit &#x3D; &#123;</span><br><span class="line">     if (stopped.get()) &#123;</span><br><span class="line">     throw new IllegalStageException(&quot;SparkContext has been shutdown&quot;)</span><br><span class="line">     &#125;</span><br><span class="line">     val callSite &#x3D; getCallSite</span><br><span class="line">     val cleanedFunc &#x3D; clean(func)</span><br><span class="line">     logInfo(&quot;Starting job: &quot; + callSite.shortForm)</span><br><span class="line">     if (conf.getBoolean(&quot;spark.logLineage&quot;, false)) &#123;</span><br><span class="line">     logInfo(&quot;RDD&#39;s recursive dependencies:\n&quot; + rdd.toDebugString)</span><br><span class="line">     &#125;</span><br><span class="line">     dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, </span><br><span class="line">     progressBar.foreach(_.finishAll())</span><br><span class="line">     &#x2F;&#x2F;在这里会执行doCheckpoint()</span><br><span class="line">     rdd.doCheckpoint()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.4：接着会进入到RDD中的 doCheckpoint 方法</span><br><span class="line">这里面最终会调用 RDDCheckpointData 的 checkpoint 方法</span><br><span class="line">checkpointData.get.checkpoint()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def doCheckpoint(): Unit &#x3D; &#123;</span><br><span class="line">     RDDOperationScope.withScope(sc, &quot;checkpoint&quot;, allowNesting &#x3D; false, ignoreP</span><br><span class="line">     &#x2F;&#x2F;该rdd是否已经调用doCheckpoint，如果还没有，则开始处理</span><br><span class="line">     if (!doCheckpointCalled) &#123;</span><br><span class="line">     doCheckpointCalled &#x3D; true</span><br><span class="line">     &#x2F;&#x2F;若已经被checkpoint()标记过，则checkpointData.isDefined为true</span><br><span class="line">     if (checkpointData.isDefined) &#123;</span><br><span class="line">     &#x2F;&#x2F;查看是否需要把该rdd的所有依赖全部checkpoint</span><br><span class="line">     &#x2F;&#x2F;checkpointAllMarkedAncestors取自配置&quot;spark.checkpoint.checkpointAllM</span><br><span class="line">     &#x2F;&#x2F;默认不配时值为false</span><br><span class="line">     if (checkpointAllMarkedAncestors) &#123;</span><br><span class="line">     &#x2F;&#x2F; TODO We can collect all the RDDs that needs to be checkpointed, </span><br><span class="line">     &#x2F;&#x2F; them in parallel.</span><br><span class="line">     &#x2F;&#x2F; Checkpoint parents first because our lineage will be truncated af</span><br><span class="line">     &#x2F;&#x2F; checkpoint ourselves</span><br><span class="line">     &#x2F;&#x2F; 血缘上的每一个父rdd递归调用该方法</span><br><span class="line">     dependencies.foreach(_.rdd.doCheckpoint())</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;调用RDDCheckpointData的checkpoint方法</span><br><span class="line">     checkpointData.get.checkpoint()</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">     &#x2F;&#x2F;沿着rdd的血缘关系向上查找被checkpoint()标记过的RDD</span><br><span class="line">     dependencies.foreach(_.rdd.doCheckpoint())</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1.5：接下来进入到 RDDCheckpointData 的 checkpoint 方法中</span><br><span class="line">这里面会调用子类 ReliableCheckpointRDD 中的 doCheckpoint()方法</span><br><span class="line"></span><br><span class="line">final def checkpoint(): Unit &#x3D; &#123;</span><br><span class="line">     &#x2F;&#x2F; Guard against multiple threads checkpointing the same RDD by</span><br><span class="line">     &#x2F;&#x2F; atomically flipping the Stage of this RDDCheckpointData</span><br><span class="line">     &#x2F;&#x2F;&#x2F;&#x2F;将checkpoint的状态从Initialized置为CheckpointingInProgress</span><br><span class="line">     RDDCheckpointData.synchronized &#123;</span><br><span class="line">     if (cpStage &#x3D;&#x3D; Initialized) &#123;</span><br><span class="line">     cpStage &#x3D; CheckpointingInProgress</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">     return</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;调用子类的doCheckpoint，默认会使用ReliableCheckpointRDD子类，创建一个新的Chec</span><br><span class="line">     val newRDD &#x3D; doCheckpoint()</span><br><span class="line">     &#x2F;&#x2F; Update our Stage and truncate the RDD lineage</span><br><span class="line">     &#x2F;&#x2F;将checkpoint状态置为Checkpointed状态，并且改变rdd之前的依赖，设置父rdd为新创建</span><br><span class="line">     RDDCheckpointData.synchronized &#123;</span><br><span class="line">     cpRDD &#x3D; Some(newRDD)</span><br><span class="line">     cpStage &#x3D; Checkpointed</span><br><span class="line">     rdd.markCheckpointed()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.6：接着来进入 ReliableCheckpointRDD 中的 doCheckpoint() 方法</span><br><span class="line">这里面会调用 ReliableCheckpointRDD 中的 writeRDDToCheckpointDirectory 方法将rdd的数据写入HDFS</span><br><span class="line">中的 checkpoint 目录，并且返回创建的 CheckpointRDD</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">protected override def doCheckpoint(): CheckpointRDD[T] &#x3D; &#123;</span><br><span class="line">     &#x2F;&#x2F;将rdd的数据写入HDFS中的checkpoint目录，并且创建的CheckpointRDD</span><br><span class="line">     val newRDD &#x3D; ReliableCheckpointRDD.writeRDDToCheckpointDirectory(rdd, cpDir</span><br><span class="line">     &#x2F;&#x2F; Optionally clean our checkpoint files if the reference is out of scope</span><br><span class="line">     if (rdd.conf.getBoolean(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, </span><br><span class="line">     rdd.context.cleaner.foreach &#123; cleaner &#x3D;&gt;</span><br><span class="line">     cleaner.registerRDDCheckpointDataForCleanup(newRDD, rdd.id)</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line">logInfo(s&quot;Done checkpointing RDD $&#123;rdd.id&#125; to $cpDir, new parent is RDD $&#123;n</span><br><span class="line"> newRDD</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.7：接下来进入 ReliableCheckpointRDD 的 writeRDDToCheckpointDirectory 方法</span><br><span class="line">这里面最终会启动一个job，将checkpoint的数据写入到指定的HDFS目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;将rdd的数据写入HDFS中checkpoint目录，并且创建CheckpointRDD</span><br><span class="line">def writeRDDToCheckpointDirectory[T: ClassTag](</span><br><span class="line"> originalRDD: RDD[T],</span><br><span class="line"> checkpointDir: String,</span><br><span class="line"> blockSize: Int &#x3D; -1): ReliableCheckpointRDD[T] &#x3D; &#123;</span><br><span class="line"> val checkpointStartTimeNs &#x3D; System.nanoTime()</span><br><span class="line"> val sc &#x3D; originalRDD.sparkContext</span><br><span class="line"> &#x2F;&#x2F;Create the output path for the checkpoint</span><br><span class="line"> &#x2F;&#x2F;创建checkpoint输出目录</span><br><span class="line"> val checkpointDirPath &#x3D; new Path(checkpointDir)</span><br><span class="line"> &#x2F;&#x2F;获取HDFS文件系统API接口</span><br><span class="line"> val fs &#x3D; checkpointDirPath.getFileSystem(sc.hadoopConfiguration)</span><br><span class="line"> &#x2F;&#x2F;创建目录</span><br><span class="line"> if (!fs.mkdirs(checkpointDirPath)) &#123;</span><br><span class="line"> throw new SparkException(s&quot;Failed to create checkpoint path $checkpointDi</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Save to file, and reload it as an RDD</span><br><span class="line"> &#x2F;&#x2F;将Hadoop配置文件信息广播到所有节点</span><br><span class="line"> val broadcastedConf &#x3D; sc.broadcast(</span><br><span class="line"> new SerializableConfiguration(sc.hadoopConfiguration))</span><br><span class="line"> &#x2F;&#x2F; TODO: This is expensive because it computes the RDD again unnecessarily </span><br><span class="line"> &#x2F;&#x2F;这里强调了checkpoint是一个昂贵的操作，主要是说它昂贵在需要沿着血缘关系重新计算该</span><br><span class="line"> &#x2F;&#x2F;重新启动一个job,将rdd的分区数据写入HDFS</span><br><span class="line"> sc.runJob(originalRDD,</span><br><span class="line"> writePartitionToCheckpointFile[T](checkpointDirPath.toString, broadcasted</span><br><span class="line"> &#x2F;&#x2F;如果rdd的partitioner不为空，则将partitioner写入checkpoint目录</span><br><span class="line"> if (originalRDD.partitioner.nonEmpty) &#123;</span><br><span class="line"> writePartitionerToCheckpointDir(sc, originalRDD.partitioner.get, checkpoi</span><br><span class="line"> &#125;</span><br><span class="line"> val checkpointDurationMs &#x3D;</span><br><span class="line"> TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - checkpointStartTimeNs)</span><br><span class="line"> logInfo(s&quot;Checkpointing took $checkpointDurationMs ms.&quot;)</span><br><span class="line"> &#x2F;&#x2F;创建一个CheckpointRDD,该RDD的分区数目和原始的rdd的分区数是一样的</span><br><span class="line"> val newRDD &#x3D; new ReliableCheckpointRDD[T](</span><br><span class="line"> sc, checkpointDirPath.toString, originalRDD.partitioner)</span><br><span class="line"> if (newRDD.partitions.length !&#x3D; originalRDD.partitions.length) &#123;</span><br><span class="line"> throw new SparkException(</span><br><span class="line"> &quot;Checkpoint RDD has a different number of partitions from original RDD. </span><br><span class="line"> s&quot;RDD [ID: $&#123;originalRDD.id&#125;, num of partitions: $&#123;originalRDD.partit</span><br><span class="line"> s&quot;Checkpoint RDD [ID: $&#123;newRDD.id&#125;, num of partitions: &quot; +</span><br><span class="line"> s&quot;$&#123;newRDD.partitions.length&#125;].&quot;)</span><br><span class="line"> &#125;</span><br><span class="line"> newRDD</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行到这，其实调用过checkpoint方法的RDD就被保存到HDFS上了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：在这里通过checkpoint操作将RDD中的数据写入到HDFS中的时候，会调用RDD中的</span><br><span class="line">iterator方法，遍历RDD中所有分区的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们来分析一下这块的代码</span><br><span class="line">此时我们没有对RDD进行持久化，所以走else中的代码</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">进入 computeOrReadCheckpoint(split, context) 中</span><br><span class="line">此时这个RDD是将要进行checkpoint，还没有完成checkpoint，所以走 else ，会执行 compute 方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskCont</span><br><span class="line">&#123;</span><br><span class="line"> &#x2F;&#x2F;当前rdd是否已经checkpoint并且物化，如果已经checkpoint并且物化</span><br><span class="line"> &#x2F;&#x2F;则调用firstParent的iterator方法获取</span><br><span class="line"> if (isCheckpointedAndMaterialized) &#123;</span><br><span class="line"> &#x2F;&#x2F;注意：针对checpoint操作，它的血缘关系已经被切断了，那么它的firstParent就是前面</span><br><span class="line"> firstParent[T].iterator(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;如果没有，则表示持久化数据丢失，或者根本就没有持久化，</span><br><span class="line"> &#x2F;&#x2F;需要调用rdd的compute方法开始重新计算，返回一个Iterator对象</span><br><span class="line"> compute(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在这会执行RDD的子类 HadoopRDD 中的 compute 方法</span><br><span class="line">在这里会通过 RecordReader 获取RDD中指定分区的数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">override def compute(theSplit: Partition, context: TaskContext): Interruptibl</span><br><span class="line"> val iter &#x3D; new NextIterator[(K, V)] &#123;</span><br><span class="line"> private val split &#x3D; theSplit.asInstanceOf[HadoopPartition]</span><br><span class="line"> logInfo(&quot;Input split: &quot; + split.inputSplit)</span><br><span class="line"> private val jobConf &#x3D; getJobConf()</span><br><span class="line"> private val inputMetrics &#x3D; context.taskMetrics().inputMetrics</span><br><span class="line"> private val existingBytesRead &#x3D; inputMetrics.bytesRead</span><br><span class="line"> &#x2F;&#x2F; Sets InputFileBlockHolder for the file block&#39;s information</span><br><span class="line"> split.inputSplit.value match &#123;</span><br><span class="line"> case fs: FileSplit &#x3D;&gt;</span><br><span class="line"> InputFileBlockHolder.set(fs.getPath.toString, fs.getStart, fs.getLengt</span><br><span class="line"> case _ &#x3D;&gt;</span><br><span class="line"> InputFileBlockHolder.unset()</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Find a function that will return the FileSystem bytes read by this thr</span><br><span class="line"> &#x2F;&#x2F; creating RecordReader, because RecordReader&#39;s constructor might read s</span><br><span class="line"> private val getBytesReadCallback: Option[() &#x3D;&gt; Long] &#x3D; split.inputSplit.v</span><br><span class="line"> case _: FileSplit | _: CombineFileSplit &#x3D;&gt;</span><br><span class="line"> Some(SparkHadoopUtil.get.getFSBytesReadOnThreadCallback())</span><br><span class="line"> case _ &#x3D;&gt; None</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; We get our input bytes from thread-local Hadoop FileSystem statistics.</span><br><span class="line"> &#x2F;&#x2F; If we do a coalesce, however, we are likely to compute multiple partit</span><br><span class="line"> &#x2F;&#x2F; task and in the same thread, in which case we need to avoid override v</span><br><span class="line"> &#x2F;&#x2F; previous partitions (SPARK-13071).</span><br><span class="line"> private def updateBytesRead(): Unit &#x3D; &#123;</span><br><span class="line"> getBytesReadCallback.foreach &#123; getBytesRead &#x3D;&gt;</span><br><span class="line"> inputMetrics.setBytesRead(existingBytesRead + getBytesRead())</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> private var reader: RecordReader[K, V] &#x3D; null</span><br><span class="line"> private val inputFormat &#x3D; getInputFormat(jobConf)</span><br><span class="line"> HadoopRDD.addLocalConfiguration(</span><br><span class="line"> new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;, Locale.US).format(createTime),</span><br><span class="line"> context.stageId, theSplit.index, context.attemptNumber, jobConf)</span><br><span class="line"> reader &#x3D;</span><br><span class="line"> try &#123;</span><br><span class="line"> inputFormat.getRecordReader(split.inputSplit.value, jobConf, Reporter</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: FileNotFoundException if ignoreMissingFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped missing file: $&#123;split.inputSplit&#125;&quot;, e)</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> null</span><br><span class="line"> &#x2F;&#x2F; Throw FileNotFoundException even if &#96;ignoreCorruptFiles&#96; is true</span><br><span class="line"> case e: FileNotFoundException if !ignoreMissingFiles &#x3D;&gt; throw e</span><br><span class="line"> case e: IOException if ignoreCorruptFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped the rest content in the corrupted file: $&#123;split</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> null</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Register an on-task-completion callback to close the input stream.</span><br><span class="line"> context.addTaskCompletionListener[Unit] &#123; context &#x3D;&gt;</span><br><span class="line"> &#x2F;&#x2F; Update the bytes read before closing is to make sure lingering bytes</span><br><span class="line"> &#x2F;&#x2F; this thread get correctly added.</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> closeIfNeeded()</span><br><span class="line"> &#125;</span><br><span class="line"> private val key: K &#x3D; if (reader &#x3D;&#x3D; null) null.asInstanceOf[K] else reader</span><br><span class="line"> private val value: V &#x3D; if (reader &#x3D;&#x3D; null) null.asInstanceOf[V] else read</span><br><span class="line"> override def getNext(): (K, V) &#x3D; &#123;</span><br><span class="line"> try &#123;</span><br><span class="line"> finished &#x3D; !reader.next(key, value)</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: FileNotFoundException if ignoreMissingFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped missing file: $&#123;split.inputSplit&#125;&quot;, e)</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> &#x2F;&#x2F; Throw FileNotFoundException even if &#96;ignoreCorruptFiles&#96; is true</span><br><span class="line"> case e: FileNotFoundException if !ignoreMissingFiles &#x3D;&gt; throw e</span><br><span class="line"> case e: IOException if ignoreCorruptFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped the rest content in the corrupted file: $&#123;split</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> &#125;</span><br><span class="line"> if (!finished) &#123;</span><br><span class="line"> inputMetrics.incRecordsRead(1)</span><br><span class="line"> &#125;</span><br><span class="line"> if (inputMetrics.recordsRead % SparkHadoopUtil.UPDATE_INPUT_METRICS_INT</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> &#125;</span><br><span class="line"> (key, value)</span><br><span class="line"> &#125;</span><br><span class="line"> override def close(): Unit &#x3D; &#123;</span><br><span class="line"> if (reader !&#x3D; null) &#123;</span><br><span class="line"> InputFileBlockHolder.unset()</span><br><span class="line"> try &#123;</span><br><span class="line"> reader.close()</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: Exception &#x3D;&gt;</span><br><span class="line"> if (!ShutdownHookManager.inShutdown()) &#123;</span><br><span class="line"> logWarning(&quot;Exception in RecordReader.close()&quot;, e)</span><br><span class="line"> &#125;</span><br><span class="line"> &#125; finally &#123;</span><br><span class="line"> reader &#x3D; null</span><br><span class="line"> &#125;</span><br><span class="line"> if (getBytesReadCallback.isDefined) &#123;</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> &#125; else if (split.inputSplit.value.isInstanceOf[FileSplit] ||</span><br><span class="line"> split.inputSplit.value.isInstanceOf[CombineFileSplit]) &#123;</span><br><span class="line"> &#x2F;&#x2F; If we can&#39;t get the bytes read from the FS stats, fall back to t</span><br><span class="line"> &#x2F;&#x2F; which may be inaccurate.</span><br><span class="line"> try &#123;</span><br><span class="line"> inputMetrics.incBytesRead(split.inputSplit.value.getLength)</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: java.io.IOException &#x3D;&gt;</span><br><span class="line"> logWarning(&quot;Unable to get input size to set InputMetrics for ta</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> new InterruptibleIterator[(K, V)](context, iter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这样经过几次迭代之后就可以获取到RDD中所有分区的数据了，因为这个compute是一次获取一个分区</span><br><span class="line">的数据。获取到之后checkpoint就可以把这个RDD的数据存储到HDFS上了。</span><br><span class="line">这就是checkpoint的写操作</span><br></pre></td></tr></table></figure><h3 id="checkpoint的读操作"><a href="#checkpoint的读操作" class="headerlink" title="checkpoint的读操作"></a>checkpoint的读操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来分析一下checkpoint读数据这个操作</span><br><span class="line">当RDD中的数据丢失了以后，需要通过checkpoint读取存储在hdfs上的数据，</span><br><span class="line">2.1：这个时候还是会执行RDD中的iterator方法</span><br><span class="line">由于我们没有做持久化，只做了checkpoint，所以还是会走 else</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">进入 computeOrReadCheckpoint 方法</span><br><span class="line">此时rdd已经 checkpoint 并且物化，所以 if 分支满足</span><br><span class="line">执行 firstParent[T].iterator(split, context) 这行代码</span><br><span class="line"></span><br><span class="line">这行代码的意思是会找到当前这个RDD的父RDD，其实这个RDD执行过checkpoint之后，血缘关系已经</span><br><span class="line">被切断了，它的父RDD就是我们前面创建的那个 ReliableCheckpointRDD</span><br><span class="line">这个 ReliableCheckpointRDD 中没有覆盖 iterator 方法，所以在调用 iterator 的时候还是执行RDD这个</span><br><span class="line">父类中的 iterator ，重新进来之后再判断，这个 ReliableCheckpointRDD 再执行if判断的时候就不满足</span><br><span class="line">了，因为它的 checkpoint 属性不满足，所以会走 else ，执行 compute</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskCont</span><br><span class="line">&#123;</span><br><span class="line"> &#x2F;&#x2F;当前rdd是否已经checkpoint并且物化，如果已经checkpoint并且物化</span><br><span class="line"> &#x2F;&#x2F;则调用firstParent的iterator方法获取</span><br><span class="line"> if (isCheckpointedAndMaterialized) &#123;</span><br><span class="line"> &#x2F;&#x2F;注意：针对checpoint操作，它的血缘关系已经被切断了，那么它的firstParent就是前面</span><br><span class="line"> firstParent[T].iterator(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;如果没有，则表示持久化数据丢失，或者根本就没有持久化，</span><br><span class="line"> &#x2F;&#x2F;需要调用rdd的compute方法开始重新计算，返回一个Iterator对象</span><br><span class="line"> compute(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时会执行 ReliableCheckpointRDD 这个子类中的 compute 方法</span><br><span class="line">这里面就会找到之前checkpoint的文件，从HDFS上恢复RDD中的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">override def compute(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;获取checkpoint文件</span><br><span class="line"> val file &#x3D; new Path(checkpointPath, ReliableCheckpointRDD.checkpointFileNam</span><br><span class="line"> &#x2F;&#x2F;从HDFS上的checkpoint文件中读取checkpoint的数据</span><br><span class="line"> ReliableCheckpointRDD.readCheckpointFile(file, broadcastedConf, context)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这是从checkpoint中读取数据的流程</span><br><span class="line">咱们前面说过，建议对需要做checkpoint的数据先进行持久化，如果我们设置了持久化，针对</span><br><span class="line">checkpoint的写操作，在执行iterator方法的时候会是什么现象呢？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时在最后将RDD中的数据通过checkpoint存储到HDFS上的时候，会调用RDD的iterator方法，不过此</span><br><span class="line">时 storageLevel 就不为 null 了，因为我们对这个RDD做了基于磁盘的持久化，所以会走 if 分支，执行</span><br><span class="line">getOrCompute</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">进入 getOrCompute 方法</span><br><span class="line">由于这个RDD的数据已经做了持久化，所以在这就可以从 blockmanager 中读取数据了，就不需要重新从</span><br><span class="line">源头计算或者拉取数据了，所以会提高 checkpoint 的效率</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def getOrCompute(partition: Partition, context: TaskContext): </span><br><span class="line"> val blockId &#x3D; RDDBlockId(id, partition.index)</span><br><span class="line"> var readCachedBlock &#x3D; true</span><br><span class="line"> &#x2F;&#x2F; This method is called on executors, so we need call SparkEnv.get instead </span><br><span class="line"> SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementCla</span><br><span class="line"> readCachedBlock &#x3D; false</span><br><span class="line"> computeOrReadCheckpoint(partition, context)</span><br><span class="line"> &#125;) match &#123;</span><br><span class="line"> case Left(blockResult) &#x3D;&gt;</span><br><span class="line"> if (readCachedBlock) &#123;</span><br><span class="line"> val existingMetrics &#x3D; context.taskMetrics().inputMetrics</span><br><span class="line"> existingMetrics.incBytesRead(blockResult.bytes)</span><br><span class="line"> new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[It</span><br><span class="line"> override def next(): T &#x3D; &#123;</span><br><span class="line"> existingMetrics.incRecordsRead(1)</span><br><span class="line"> delegate.next()</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iter</span><br><span class="line"> &#125;</span><br><span class="line"> case Right(iter) &#x3D;&gt;</span><br><span class="line"> new InterruptibleIterator(context, iter.asInstanceOf[Iterator[T]])</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-1.html</id>
    <published>2023-03-27T03:33:38.000Z</published>
    <updated>2023-03-28T06:52:35.476Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术"><a href="#第十一周-Spark性能优化的道与术" class="headerlink" title="第十一周 Spark性能优化的道与术"></a>第十一周 Spark性能优化的道与术</h1><h2 id="宽依赖和窄依赖"><a href="#宽依赖和窄依赖" class="headerlink" title="宽依赖和窄依赖"></a>宽依赖和窄依赖</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">先看一下什么是窄依赖：</span><br><span class="line">窄依赖(Narrow Dependency)：指父RDD的每个分区只被子RDD的一个分区所使用，例如map、filter等这些算子</span><br><span class="line">一个RDD，对它的父RDD只有简单的一对一的关系，也就是说，RDD的每个partition仅仅依赖于父RDD中的一个partition，父RDD和子RDD的partition之间的对应关系，是一对一的。</span><br><span class="line"></span><br><span class="line">宽依赖(Shuffle Dependency)：父RDD的每个分区都可能被子RDD的多个分区使用，例如groupByKey、</span><br><span class="line">reduceByKey，sortBykey等算子，这些算子其实都会产生shuffle操作</span><br><span class="line">也就是说，每一个父RDD的partition中的数据都可能会传输一部分到下一个RDD的每个partition中。此时就会出现，父RDD和子RDD的partition之间，具有错综复杂的关系，那么，这种情况就叫做两个RDD之间是宽依赖，同时，他们之间会发生shuffle操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来看图具体分析一个案例</span><br><span class="line">以单词计数案例来分析</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271142255.png" alt="image-20230327114152987"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">最左侧是linesRDD，这个表示我们通过textFile读取文件中的数据之后获取的RDD</span><br><span class="line">接着是我们使用flatMap算子，对每一行数据按照空格切开，然后可以获取到第二个RDD，这个RDD中包含的是切开的每一个单词</span><br><span class="line">在这里这两个RDD就属于一个窄依赖，因为父RDD的每个分区只被子RDD的一个分区所使用，也就是说他们的分区是一对一的，这样就不需要经过shuffle了。</span><br><span class="line">接着是使用map算子，将每一个单词转换成(单词,1)这种形式，</span><br><span class="line">此时这两个RDD也是一个窄依赖的关系，父RDD的分区和子RDD的分区也是一对一的</span><br><span class="line">最后我们会调用reduceByKey算子，此时会对相同key的数据进行分组，分到一个分区里面，并且进行聚合操作，此时父RDD的每个分区都可能被子RDD的多个分区使用，那这两个RDD就属于宽依赖了。</span><br><span class="line"></span><br><span class="line">这就是宽窄依赖的区别，那我们在这区分宽窄依赖有什么意义吗？</span><br><span class="line">不要着急，往下面看</span><br></pre></td></tr></table></figure><h2 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark job是根据action算子触发的,遇到action算子就会起一个job</span><br><span class="line">Spark Job会被划分为多个Stage，每一个Stage是由一组并行的Task组成的</span><br><span class="line"></span><br><span class="line">注意：stage的划分依据就是看是否产生了shuflle(即宽依赖),遇到一个shuffle操作就划分为前后两个stage</span><br><span class="line">stage是由一组并行的task组成，stage会将一批task用TaskSet来封装，提交给TaskScheduler进行分配，最后发送到Executor执行</span><br><span class="line"></span><br><span class="line">下面来看一张图来具体分析一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271146961.png" alt="image-20230327114640278"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">为什么是从后往前呢？因为RDD之间是有血缘关系的，后面的RDD依赖前面的RDD，也就是说后面的RDD要等前面的RDD执行完,才会执行。</span><br><span class="line">所以从后往前遇到宽依赖就划分为两个stage，shuffle前一个,shuffle后一个。如果整个过程没有产生shuffle那就只会有一个stage</span><br><span class="line"></span><br><span class="line">看这个图</span><br><span class="line">RDD G往前推，到RDD B的时候，是窄依赖，所以不切分Stage，再往前到RDD A，此时产生了宽依赖，所以RDD A属于一个Stage、RDD B 和 G属于一个Stage</span><br><span class="line">再看下面，RDD G到RDD F，产生了宽依赖，所以RDD F属于一个Stage，因为RDD F和RDD C、D、E这几个RDD没有产生宽依赖，都是窄依赖，所以他们属于一个Stage。</span><br><span class="line">所以这个图中,RDD A单独一个stage1,RDD C、D、E、F被划分在stage2中,</span><br><span class="line">最后RDD B和RDD G划分在了stage3 里面。</span><br><span class="line"></span><br><span class="line">注意：Stage划分是从后往前划分，但是stage执行时从前往后的，这就是为什么后面先切割的stage为什么编号是3.</span><br></pre></td></tr></table></figure><h2 id="Spark-Job的三种提交模式"><a href="#Spark-Job的三种提交模式" class="headerlink" title="Spark Job的三种提交模式"></a>Spark Job的三种提交模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1. 第一种，standalone模式，基于Spark自己的standalone集群。</span><br><span class="line">指定spark-submit –master spark:&#x2F;&#x2F;bigdata01:7077</span><br><span class="line"></span><br><span class="line">2. 第二种，是基于YARN的client模式。</span><br><span class="line">指定–master yarn --deploy-mode client</span><br><span class="line">这种方式主要用于测试，查看日志方便一些，部分日志会直接打印到控制台上面，因为driver进程运行在本地客户端，就是提交Spark任务的那个客户端机器，driver负责调度job，会与yarn集群产生大量的通信，一般情况下Spark客户端机器和Hadoop集群的机器是无法内网通信，只能通过外网，这样在大量通信的情况下会影响通信效率，并且当我们执行一些action操作的时候数据也会返回给driver端，driver端机器的配置一般都不高，可能会导致内存溢出等问题。</span><br><span class="line"></span><br><span class="line">3. 第三种，是基于YARN的cluster模式。【推荐】</span><br><span class="line">指定–master yarn --deploy-mode cluster</span><br><span class="line">这种方式driver进程运行在集群中的某一台机器上，这样集群内部节点之间通信是可以通过内网通信的，并且集群内的机器的配置也会比普通的客户端机器配置高，所以就不存在yarn-client模式的一些问题了，只不过这个时候查看日志只能到集群上面看了，这倒没什么影响。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271206035.png" alt="image-20230327120631638"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">左边是standalone模式，现在我们使用的提交方式，driver进程是在客户端机器中的，其实针对standalone模式而言，这个Driver进程也是可以运行在集群中的</span><br><span class="line">来看一下官网文档，standalone模式也是支持的，通过指定deploy-mode 为cluster即可</span><br><span class="line"></span><br><span class="line">中间的值yarn client模式，由于是on yarn模式，所以里面是yarn集群的进程，此时driver进程就在提交spark任务的客户端机器上了</span><br><span class="line">最右边这个是yarn cluster模式，driver进程就会在集群中的某一个节点上面。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271210537.png" alt="image-20230327121044013"></p><h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">在MapReduce框架中，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce；而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I&#x2F;O。所以shuffle性能的高低也直接决定了整个程序的性能高低。Spark也会有自己</span><br><span class="line">的shuffle实现过程。</span><br><span class="line"></span><br><span class="line">我们首先来看一下</span><br><span class="line">在Spark中，什么情况下，会发生shuffle？</span><br><span class="line">reduceByKey、groupByKey、sortByKey、countByKey、join等操作都会产生shuffle。</span><br><span class="line"></span><br><span class="line">那下面我们来详细分析一下Spark中的shuffle过程。</span><br><span class="line">Spark的shuffle历经了几个过程</span><br><span class="line">1. Spark 0.8及以前使用Hash Based Shuffle</span><br><span class="line">2. Spark 0.8.1 为Hash Based Shuffle引入File Consolidation机制</span><br><span class="line">3. Spark1.6之后使用Sort-Base Shuffle，因为Hash Based Shuffle存在一些不足所以就把它替换掉了。</span><br><span class="line"></span><br><span class="line">所以Spark Shuffle 一共经历了这几个过程：</span><br><span class="line">1. 未优化的 Hash Based Shuffle</span><br><span class="line">2. 优化后的Hash Based Shuffle</span><br><span class="line">3. Sort-Based Shuffle</span><br></pre></td></tr></table></figure><h3 id="未优化的Hash-Based-Shuffle"><a href="#未优化的Hash-Based-Shuffle" class="headerlink" title="未优化的Hash Based Shuffle"></a>未优化的Hash Based Shuffle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">来看一个图，假设我们是在执行一个reduceByKey之类的操作，此时就会产生shuffle</span><br><span class="line">shuffle里面会有两种task，一种是shuffleMapTask，负责拉取前一个RDD中的数据，还有一个ResultTask，负责把拉取到的数据按照规则汇总起来</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271218160.png" alt="image-20230327121800239"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1：假设有1个节点，这个节点上有2个CPU，上面运行了4个ShuffleMapTask，这样的话其实同时只有2个ShuffleMapTask是并行执行的，因为一个cpu core同时只能执行一个ShuffleMapTask。</span><br><span class="line">2：每个ShuffleMapTask都会为每个ResultTask创建一份Bucket缓存，以及对应的ShuffleBlockFile磁盘文件</span><br><span class="line">这样的话，每一个ShuffleMapTask都会产生4份Bucket缓存和对应的4个ShuffleBlockFile文件，分别对应下面的4个ResultTask</span><br><span class="line"></span><br><span class="line">3：假设另一个节点上面运行了4个ResultTask现在等着获取ShuffleMapTask的输出数据，来完成比如ReduceByKey的操作。</span><br><span class="line">这是这个流程，注意了，如果有100个MapTask，100个ResultTask，那么会产生10000个本地磁盘文件，这样需要频繁的磁盘IO，是比较影响性能的。</span><br><span class="line"></span><br><span class="line">注意，那个bucket缓存是非常重要的，ShuffleMapTask会把所有的数据都写入Bucket缓存之后，才会刷写到对应的磁盘文件中，但是这就有一个问题，如果map端数据过多，那么很容易造成内存溢出，所以spark在优化后的Hash Based Shuffle中对这个问题进行了优化，默认这个内存缓存是100kb，当Bucket中的数据达到了阈值之后，就会将数据一点一点地刷写到对应的ShuffleBlockFile磁盘中了。这种操作的优点，是不容易发生内存溢出。缺点在于，如果内存缓存过小的话，那么可能发生过多的磁盘io操作。所以，这里的内存缓存大小，是可以根据实际的业务情况进行优化的。</span><br></pre></td></tr></table></figure><h3 id="优化后的Hash-Based-Shuffle"><a href="#优化后的Hash-Based-Shuffle" class="headerlink" title="优化后的Hash Based Shuffle"></a>优化后的Hash Based Shuffle</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271226135.png" alt="image-20230327122626812"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">看这个优化后的shuffle流程</span><br><span class="line">1：假设机器上有2个cpu，4个shuffleMaptask，这样同时只有2个在并行执行</span><br><span class="line">2：在这个版本中，Spark引入了consolidation机制，一个ShuffleMapTask将数据写入ResultTask数量的本地文件中，这个是不变的，但是当下一个ShuffleMapTask运行的时候，可以直接将数据写入之前产生的本地文件中，相当于对多个ShuffleMapTask的输出进行了合并，从而大大减少了本地磁盘中文件的数量。</span><br><span class="line">此时文件的数量变成了CPU core数量 * ResultTask数量，比如每个节点上有2个CPU，有100个ResultTask，那么每个节点上会产生200个文件</span><br><span class="line">这个时候文件数量就变得少多了。</span><br><span class="line"></span><br><span class="line">但是如果 ResultTask端的并行任务过多的话则 CPU core * Result Task 依旧过大，也会产生很多小文件</span><br></pre></td></tr></table></figure><h3 id="Sort-Based-Shuffle"><a href="#Sort-Based-Shuffle" class="headerlink" title="Sort-Based Shuffle"></a>Sort-Based Shuffle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">引入Consolidation机制虽然在一定程度上减少了磁盘文件数量，但是不足以有效提高Shuffle的性能，这种情况只适合中小型数据规模的数据处理。</span><br><span class="line">为了让Spark能在更大规模的集群上高性能处理大规模的数据，因此Spark引入了 Sort-Based Shuffle。</span><br><span class="line"></span><br><span class="line">该机制针对每一个ShuffleMapTask都只创建一个文件，将所有的 ShuffleMapTask的数据都写入同一个文件，并且对应生成一个索引文件。</span><br><span class="line">以前的数据是放在内存中，等到数据写完了再刷写到磁盘，现在为了减少内存的使用，在内存不够用的时候，可以将内存中的数据溢写到磁盘，结束的时候，再将这些溢写的文件联合内存中的数据一起进行归并，从而减少内存的使用量。一方面文件数量显著减少，另一方面减少缓存所占用的内存大小，而且同时避免GC的风险和频率。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271434463.png" alt="image-20230327143450475"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>IDEA使用经验积累</title>
    <link href="http://tianyong.fun/IDEA%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E7%A7%AF%E7%B4%AF.html"/>
    <id>http://tianyong.fun/IDEA%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E7%A7%AF%E7%B4%AF.html</id>
    <published>2023-03-24T15:07:36.000Z</published>
    <updated>2023-03-24T16:04:07.886Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="IDEA使用经验积累"><a href="#IDEA使用经验积累" class="headerlink" title="IDEA使用经验积累"></a>IDEA使用经验积累</h1><h2 id="运行程序报错内存不够"><a href="#运行程序报错内存不够" class="headerlink" title="运行程序报错内存不够"></a>运行程序报错内存不够</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">run-&gt;edit configuration-&gt;vm option</span><br><span class="line">-Xms1024m -Xmx1024m</span><br></pre></td></tr></table></figure><h2 id="Maven创建项目"><a href="#Maven创建项目" class="headerlink" title="Maven创建项目"></a>Maven创建项目</h2><h3 id="编辑spark程序时"><a href="#编辑spark程序时" class="headerlink" title="编辑spark程序时"></a>编辑spark程序时</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">本地运行</span><br><span class="line">&lt;!--&lt;scope&gt;provided&lt;&#x2F;scope&gt;--&gt;</span><br><span class="line">这个表示使用相关spark依赖</span><br><span class="line"></span><br><span class="line">在spark集群运行时</span><br><span class="line">要把注释去掉，表示不使用相关spark依赖</span><br></pre></td></tr></table></figure><h2 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctrl+alt+v:快速创建返回类型</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第16章 spark</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC16%E7%AB%A0-spark.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC16%E7%AB%A0-spark.html</id>
    <published>2023-02-28T14:35:14.000Z</published>
    <updated>2023-03-03T16:51:35.890Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第15章 hadoop架构再探讨</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC15%E7%AB%A0-hadoop%E6%9E%B6%E6%9E%84%E5%86%8D%E6%8E%A2%E8%AE%A8.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC15%E7%AB%A0-hadoop%E6%9E%B6%E6%9E%84%E5%86%8D%E6%8E%A2%E8%AE%A8.html</id>
    <published>2023-02-28T14:34:54.000Z</published>
    <updated>2023-03-03T16:51:17.421Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第14章 基于hadoop的数据仓库Hive</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC14%E7%AB%A0-%E5%9F%BA%E4%BA%8Ehadoop%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC14%E7%AB%A0-%E5%9F%BA%E4%BA%8Ehadoop%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive.html</id>
    <published>2023-02-28T14:34:22.000Z</published>
    <updated>2023-03-03T16:50:57.288Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第11章 大数据在互联网中的应用</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC11%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9C%A8%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC11%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9C%A8%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.html</id>
    <published>2023-02-28T14:33:23.000Z</published>
    <updated>2023-02-28T14:33:23.315Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第10章 数据可视化</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC10%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC10%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html</id>
    <published>2023-02-28T14:32:57.000Z</published>
    <updated>2023-03-03T16:50:39.828Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第9章 图计算</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.html</id>
    <published>2023-02-28T14:32:32.000Z</published>
    <updated>2023-03-09T15:04:20.844Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第9章-图计算"><a href="#第9章-图计算" class="headerlink" title="第9章 图计算"></a>第9章 图计算</h1><h2 id="图计算简介"><a href="#图计算简介" class="headerlink" title="图计算简介"></a>图计算简介</h2><h3 id="图结构数据"><a href="#图结构数据" class="headerlink" title="图结构数据"></a>图结构数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•许多大数据都是以大规模图或网络的形式呈现，如社交网络、传染病传播途径、交通事故对路网的影响</span><br><span class="line">•许多非图结构的大数据，也常常会被转换为图模型后进行分析</span><br><span class="line">•图数据结构很好地表达了数据之间的关联性</span><br><span class="line">•关联性计算是大数据计算的核心——通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用的信息–比如，通过为购物者之间的关系建模，就能很快找到口味相似的用户，并为之推荐商品</span><br><span class="line">–或者在社交网络中，通过传播关系发现意见领袖</span><br></pre></td></tr></table></figure><h3 id="传统图计算解决方案的不足之处"><a href="#传统图计算解决方案的不足之处" class="headerlink" title="传统图计算解决方案的不足之处"></a>传统图计算解决方案的不足之处</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">很多传统的图计算算法都存在以下几个典型问题：</span><br><span class="line">（1）常常表现出比较差的内存访问局部性</span><br><span class="line">（2）针对单个顶点的处理工作过少</span><br><span class="line">（3）计算过程中伴随着并行度的改变</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">针对大型图（比如社交网络和网络图）的计算问题，可能的解决方案及其不足之处具体如下：</span><br><span class="line">•（1）为特定的图应用定制相应的分布式实现：通用性不好</span><br><span class="line">•（2）基于现有的分布式计算平台进行图计算：在性能和易用性方面往往无法达到最优</span><br><span class="line">•现有的并行计算框架像MapReduce还无法满足复杂的关联性计算</span><br><span class="line">•MapReduce作为单输入、两阶段、粗粒度数据并行的分布式计算框架，在表达多迭代、稀疏结构和细粒度数据时，力不从心</span><br><span class="line">•比如，有公司利用MapReduce进行社交用户推荐，对于5000万注册用户，50亿关系对，利用10台机器的集群，需要超过10个小时的计算</span><br><span class="line">•（3）使用单机的图算法库：比如BGL、LEAD、NetworkX、JDSL、Standford GraphBase和FGL等，但是，在可以解决的问题的规模方面具有很大的局限性</span><br><span class="line">•（4）使用已有的并行图计算系统：比如，Parallel BGL和CGM Graph，实现了很多并行图算法，但是，对大规模分布式系统非常重要的一些方面（比如容错），无法提供较好的支持</span><br></pre></td></tr></table></figure><h3 id="图计算通用软件"><a href="#图计算通用软件" class="headerlink" title="图计算通用软件"></a>图计算通用软件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对大型图的计算，目前通用的图计算软件主要包括两种：</span><br><span class="line">第一种主要是基于遍历算法的、实时的图数据库，如Neo4j、OrientDB、DEX和Infinite Graph</span><br><span class="line">第二种是以图顶点为中心的、基于消息传递批处理的并行引擎，如GoldenOrb、Giraph、Pregel和Hama，这些图处理软件主要是基于BSP模型实现的并行图处理系统</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">一次BSP(整体同步并行计算模型、又称大同步模型)计算过程包括一系列全局超步（所谓的超步就是计算中的一次迭代），每个超步主要包括三个组件：</span><br><span class="line">•局部计算：每个参与的处理器都有自身的计算任务，它们只读取存储在本地内存中的值，不同处理器的计算任务都是异步并且独立的</span><br><span class="line"></span><br><span class="line">•通讯：处理器群相互交换数据，交换的形式是，由一方发起推送(put)和获取(get)操作</span><br><span class="line"></span><br><span class="line">•栅栏同步(Barrier Synchronization)：当一个处理器遇到“路障”（或栅栏），会等到其他所有处理器完成它们的计算步骤；每一次同步也是一个超步的完成和下一个超步的开始。图9-1是一个超步的垂直结构图</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308202813506.png" alt="image-20230308202813506" style="zoom:67%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082212630.png" alt="image-20230308221246547"></p><h2 id="Pregel简介"><a href="#Pregel简介" class="headerlink" title="Pregel简介"></a>Pregel简介</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•谷歌公司在2003年到2004年公布了GFS、MapReduce和BigTable，成为后来云计算和Hadoop项目的重要基石</span><br><span class="line"></span><br><span class="line">•谷歌在后Hadoop时代的新“三驾马车”——Caffeine(大规模网页索引的建立)、Dremel(实时交互式分析产品)和Pregel，再一次影响着圈子与大数据技术的发展潮流</span><br><span class="line"></span><br><span class="line">•Pregel是一种基于BSP模型实现的并行图处理系统</span><br><span class="line">•为了解决大型图的分布式计算问题，Pregel搭建了一套可扩展的、有容错机制的平台，该平台提供了一套非常灵活的API，可以描述各种各样的图计算</span><br><span class="line">•Pregel作为分布式图计算的计算框架，主要用于图遍历、最短路径、PageRank计算等等</span><br></pre></td></tr></table></figure><h2 id="Pregel图计算模型"><a href="#Pregel图计算模型" class="headerlink" title="Pregel图计算模型"></a>Pregel图计算模型</h2><h3 id="有向图和顶点"><a href="#有向图和顶点" class="headerlink" title="有向图和顶点"></a>有向图和顶点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•Pregel计算模型以有向图作为输入，有向图的每个顶点都有一个String类型的顶点ID，每个顶点都有一个可修改的用户自定义值与之关联，每条有向边都和其源顶点关联，并记录了其目标顶点ID，边上有一个可修改的用户自定义值与之关联</span><br><span class="line">•在每个超步S中，图中的所有顶点都会并行执行相同的用户自定义函数。每个顶点可以接收前一个超步(S-1)中发送给它的消息，修改其自身及其出射边的状态，并发送消息给其他顶点，甚至是修改整个图的拓扑结构。需要指出的是，在这种计算模式中，边并不是核心对象，在边上面不会运行相应的计算，只有顶点才会执行用户自定义函数进行相应计算</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091325684.png" alt="image-20230309132510569"></p><h3 id="顶点之间的消息传递"><a href="#顶点之间的消息传递" class="headerlink" title="顶点之间的消息传递"></a>顶点之间的消息传递</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">采用消息传递模型主要基于以下两个原因：</span><br><span class="line">（1）消息传递具有足够的表达能力，没有必要使用远程读取或共享内存的方式</span><br><span class="line">（2）有助于提升系统整体性能。大型图计算通常是由一个集群完成的，集群环境中执行远程数据读取会有较高的延迟；Pregel的消息模式采用异步和</span><br><span class="line">批量的方式传递消息，因此可以缓解远程读取的延迟</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308203709811.png" alt="image-20230308203709811" style="zoom:67%;"><h3 id="Pregel的计算过程"><a href="#Pregel的计算过程" class="headerlink" title="Pregel的计算过程"></a>Pregel的计算过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•Pregel的计算过程是由一系列被称为“超步”的迭代组成的。</span><br><span class="line"></span><br><span class="line">在每个超步中，每个顶点上面都会并行执行用户自定义的函数，该函数描述了一个顶点V在一个超步S中需要执行的操作。</span><br><span class="line"></span><br><span class="line">该函数可以读取前一个超步(S-1)中其他顶点发送给顶点V的消息，执行相应计算后，修改顶点V及其出射边的状态，然后沿着顶点V的出射边发送消息给其他顶点，而且，一个消息可能经过多条边的传递后被发送到任意已知ID的目标顶点上去。</span><br><span class="line"></span><br><span class="line">这些消息将会在下一个超步(S+1)中被目标顶点接收，然后像上述过程一样开始下一个超步(S+1)的迭代过程</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•在Pregel计算过程中，一个算法什么时候可以结束，是由所有顶点的状态决定的</span><br><span class="line">•在第0个超步，所有顶点处于活跃状态，都会参与该超步的计算过程</span><br><span class="line">•当一个顶点不需要继续执行进一步的计算时，就会把自己的状态设置为“停机”，进入非活跃状态</span><br><span class="line">•一旦一个顶点进入非活跃状态，后续超步中就不会再在该顶点上执行计算，除非其他顶点给该顶点发送消息把它再次激活</span><br><span class="line">•当一个处于非活跃状态的顶点收到来自其他顶点的消息时，Pregel计算框架必须根据条件判断来决定是否将其显式唤醒进入活跃状态</span><br><span class="line">•当图中所有的顶点都已经标识其自身达到“非活跃（inactive）”状态，并且没有消息在传送的时候，算法就可以停止运行</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308204037030.png" alt="image-20230308204037030" style="zoom:67%;"><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308204100764.png" alt="image-20230308204100764" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">发送出去的消息，会在下一个超步执行，每一个顶点都有一个输入队列，队列中的值来源于上一个超步，每一个超步执行时去取自己输入队列中的值</span><br></pre></td></tr></table></figure><h2 id="Pregel的C-API"><a href="#Pregel的C-API" class="headerlink" title="Pregel的C++ API"></a>Pregel的C++ API</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pregel已经预先定义好一个基类——Vertex类：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">template &lt;typename VertexValue, typename EdgeValue, typename MessageValue&gt;</span><br><span class="line">class Vertex &#123;</span><br><span class="line"> public:</span><br><span class="line">    virtual void Compute(MessageIterator* msgs) &#x3D; 0;</span><br><span class="line">    const string&amp; vertex_id() const; &#x2F;&#x2F; 顶点ID</span><br><span class="line">    int64 superstep() const; &#x2F;&#x2F; 超步是第几步</span><br><span class="line">    const VertexValue&amp; GetValue(); &#x2F;&#x2F; 获得顶点的值</span><br><span class="line">    VertexValue* MutableValue(); &#x2F;&#x2F; 修改顶点值</span><br><span class="line">    OutEdgeIterator GetOutEdgeIterator();&#x2F;&#x2F; 获得该顶点出射边</span><br><span class="line">    void SendMessageTo(const string&amp; dest_vertex, const MessageValue&amp; message);</span><br><span class="line">    void VoteToHalt();&#x2F;&#x2F; 修改状态</span><br><span class="line"> &#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">•在Vetex类中，定义了三个值类型参数，分别表示顶点、边和消息。每一个顶点都有一个给定类型的值与之对应</span><br><span class="line">•编写Pregel程序时，需要继承Vertex类，并且覆写Vertex类的虚函数Compute() </span><br><span class="line"></span><br><span class="line">•在Pregel执行计算过程时，在每个超步中都会并行调用每个顶点上定义的Compute()函数</span><br><span class="line">•允许Compute()方法查询当前顶点及其边的信息，以及发送消息到其他的顶点</span><br><span class="line">    –Compute()方法可以调用GetValue()方法来获取当前顶点的值</span><br><span class="line">    –调用MutableValue()方法来修改当前顶点的值</span><br><span class="line">    –通过由出射边的迭代器提供的方法来查看、修改出射边对应的值</span><br><span class="line">•对状态的修改，对于被修改的顶点而言是可以立即被看见的，但是，对于其他顶点而言是不可见的，因此，不同顶点并发进行的数据访问是不存在竞争关系的</span><br><span class="line">整个过程中，唯一需要在超步之间持久化的顶点级状态，是顶点和其对</span><br><span class="line">应的边所关联的值，因而，Pregel计算框架所需要管理的图状态就只包括顶点和边所关联的值，这种做法大大简化了计算流程，同时，也有利于图的分布和故障恢复</span><br></pre></td></tr></table></figure><h3 id="消息传递机制"><a href="#消息传递机制" class="headerlink" title="消息传递机制"></a>消息传递机制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 顶点之间的通讯是借助于消息传递机制来实现的，每条消息都包含了消息值和需要到达的目标顶点ID。用户可以通过Vertex类的模板参数来设定消息值的数据类型</span><br><span class="line">• 在一个超步S中，一个顶点可以发送任意数量的消息，这些消息将在下一个超步（S+1）中被其他顶点接收</span><br><span class="line">• 一个顶点V通过与之关联的出射边向外发送消息，并且，消息要到达的目标顶点并不一定是与顶点V相邻的顶点，一个消息可以连续经过多条连通的边到达某个与顶点V不相邻的顶点U，U可以从接收的消息中获取到与其不相邻的顶点V的ID</span><br></pre></td></tr></table></figure><h3 id="Combiner"><a href="#Combiner" class="headerlink" title="Combiner"></a>Combiner</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• Pregel计算框架在消息发出去之前，Combiner可以将发往同一个顶点的多个整型值进行求和得到一个值，只需向外发送这个“求和结果”，从而实现了由多个消息合并成一个消息，大大减少了传输和缓存的开销</span><br><span class="line">• 在默认情况下，Pregel计算框架并不会开启Combiner功能，因为，通常很难找到一种对所有顶点的Compute()函数都合适的Combiner</span><br><span class="line">• 当用户打算开启Combiner功能时，可以继承Combiner类并覆写虚函数Combine()</span><br><span class="line">• 此外，通常只对那些满足交换律和结合律的操作才可以去开启Combiner功能，因为，Pregel计算框架无法保证哪些消息会被合并，也无法保证消息传递给 Combine()的顺序和合并操作执行的顺序</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308204959085.png" alt="image-20230308204959085" style="zoom:67%;"><h3 id="Aggregator"><a href="#Aggregator" class="headerlink" title="Aggregator"></a>Aggregator</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• Aggregator提供了一种全局通信、监控和数据查看的机制</span><br><span class="line">• 在一个超步S中，每一个顶点都可以向一个Aggregator提供一个数据，Pregel计算框架会对这些值进行聚合操作产生一个值，在下一个超步（S+1）中，图中的所有顶点都可以看见这个值</span><br><span class="line">• Aggregator的聚合功能，允许在整型和字符串类型上执行最大值、最小值、求和操作，比如，可以定义一个“Sum”Aggregator来统计每个顶点的出射边数量，最后相加可以得到整个图的边的数量</span><br><span class="line">• Aggregator还可以实现全局协同的功能，比如，可以设计“and” Aggregator来决定在某个超步中Compute()函数是否执行某些逻辑分支，只有当“and” Aggregator显示所有顶点都满足了某条件时，才去执行这些逻辑分支</span><br></pre></td></tr></table></figure><h3 id="拓扑改变"><a href="#拓扑改变" class="headerlink" title="拓扑改变"></a>拓扑改变</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Pregel计算框架允许用户在自定义函数Compute()中定义操作，修改图的拓扑结构，比如在图中增加（或删除）边或顶点</span><br><span class="line">• 对于全局拓扑改变，Pregel采用了惰性协调机制，在改变请求发出时，Pregel不会对这些操作进行协调，只有当这些改变请求的消息到达目标顶点并被执行时，Pregel才会对这些操作进行协调，这样，所有针对某个顶点V的拓扑修改操作所引发的冲突，都会由V自己来处理</span><br><span class="line">• 对于本地的局部拓扑改变，是不会引发冲突的，顶点或边的本地增减能够立即生效，很大程度上简化了分布式编程</span><br></pre></td></tr></table></figure><h3 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 在Pregel计算框架中，图的保存格式多种多样，包括文本文件、关系数据库或键值数据库等</span><br><span class="line">• 在Pregel中，“从输入文件生成得到图结构”和“执行图计算”这两个过程是分离的，从而不会限制输入文件的格式</span><br><span class="line">• 对于输出，Pregel也采用了灵活的方式，可以以多种方式进行输出</span><br></pre></td></tr></table></figure><h2 id="Pregel的体系结构"><a href="#Pregel的体系结构" class="headerlink" title="Pregel的体系结构"></a>Pregel的体系结构</h2><h3 id="Pregel的执行过程"><a href="#Pregel的执行过程" class="headerlink" title="Pregel的执行过程"></a>Pregel的执行过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•在Pregel计算框架中，一个大型图会被划分成许多个分区，每个分区都包含了一部分顶点以及以其为起点的边</span><br><span class="line">•一个顶点应该被分配到哪个分区上，是由一个函数决定的，系统默认函数为hash(ID) mod N，其中，N为所有分区总数，ID是这个顶点的标识符；当然，用户也可以自己定义这个函数</span><br><span class="line">•这样，无论在哪台机器上，都可以简单根据顶点ID判断出该顶点属于哪个分区，即使该顶点可能已经不存在了</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308210115360.png" alt="image-20230308210115360" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在理想的情况下（不发生任何错误），一个Pregel用户程序的执行过程如下：</span><br><span class="line">（1）选择集群中的多台机器执行图计算任务，每台机器上运行用户程序的一个副本，其中，有一台机器会被选为Master，其他机器作为Worker。Master只负责协调多个Worker执行任务，系统不会把图的任何分区分配给它。Worker借助于名称服务系统可以定位到Master的位置，并向Master发送自己的注册信息。</span><br><span class="line">（2）Master把一个图分成多个分区，并把分区分配到多个Worker。一个Worker会领到一个或多个分区，每个</span><br><span class="line">Worker知道所有其他Worker所分配到的分区情况。每个</span><br><span class="line">Worker负责维护分配给自己的那些分区的状态(顶点及边</span><br><span class="line">的增删)，对分配给自己的分区中的顶点执行Compute()函</span><br><span class="line">数，向外发送消息，并管理接收到的消息。</span><br><span class="line">（3）Master会把用户输入划分成多个部分，通常是基于文件边界进行划分。划分后，每个部分都是一系列记录的集合，每条记录都包含一定数量的顶点和边。然后，Master会为每个Worker分配用户输入的一部分。如果一个Worker从输入内容中加载到的顶点，刚好是自己所分配到的分区中的顶点，就会立即更新相应的数据结构。否则，该Worker会根据加载到的顶点的ID，把它发送到其所属的分区所在的Worker上。当所有的输入都被加载后，图中的所有顶点都会被标记为“活跃”状态。</span><br><span class="line">（4）Master向每个Worker发送指令，Worker收到指令后，开始运行一个超步。Worker会为自己管辖的每个分区分配一个线程，对于分区中的每个顶点，Worker会把来自上一个超步的、发给该顶点的消息传递给它，并调用处于“活跃”状态的顶点上的Compute()函数，在执行计算过程中，顶点可以对外发送消息，但是，所有消息的发送工作必须在本超步结束之前完成。当所有这些工作都完成以后，Worker会通知Master，并把自己在下一个超步还处于“活跃”状态的顶点的数量报告给Master。上述步骤会被不断重复，直到所有顶点都不再活跃并且系统中不会有任何消息在传输，这时，执行过程才会结束。</span><br><span class="line">（5）计算过程结束后，Master会给所有的Worker发送指令，通知每个Worker对自己的计算结果进行持久化存储。</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308210550499.png" alt="image-20230308210550499" style="zoom:67%;"><h3 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Pregel采用检查点机制来实现容错。在每个超步的开始，Master会通知所有的Worker把自己管辖的分区的状态（包括顶点值、边值以及接收到的消息），写入到持久化存储设备</span><br><span class="line">• Master会周期性地向每个Worker发送ping消息，Worker收到ping消息后会给Master发送反馈消息。如果Master在指定时间间隔内没有收到某个Worker的反馈消息，就会把该Worker标记为“失效”。同样地，如果一个Worker在指定的时间间隔内没有收到来自Master的ping消息，该Worker也会停止工作</span><br><span class="line">• 每个Worker上都保存了一个或多个分区的状态信息，当一个Worker发生故障时，它所负责维护的分区的当前状态信息就会丢失。Master监测到一个Worker发生故障“失效”后，会把失效Worker所分配到的分区，重新分配到其他处于正常工作状态的Worker集合上，然后，所有这些分区会从最近的某超步S开始时写出的检查点中，重新加载状态信息。很显然，这个超步S可能会比失效Worker上最后运行的超步S1要早好几个阶段，因此，为了恢复到最新的正确状态，需要重新执行从超步S到超步S1的所有操作</span><br></pre></td></tr></table></figure><h3 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在一个Worker中，它所管辖的分区的状态信息是保存在内存中的。分区中的顶点的状态信息包括：</span><br><span class="line">•顶点的当前值</span><br><span class="line">•以该顶点为起点的出射边列表，每条出射边包含了目标顶点ID和边的值</span><br><span class="line">•消息队列，包含了所有接收到的、发送给该顶点的消息</span><br><span class="line">•标志位，用来标记顶点是否处于活跃状态</span><br><span class="line"></span><br><span class="line">在每个超步中，Worker会对自己所管辖的分区中的每个顶点进行遍历，并调用顶点上的Compute()函数，在调用时，会把以下三个参数传递进去：</span><br><span class="line">•该顶点的当前值</span><br><span class="line">•一个接收到的消息的迭代器</span><br><span class="line">•一个出射边的迭代器</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">•在Pregel中，为了获得更好的性能，“标志位”和输入消息</span><br><span class="line">队列是分开保存的</span><br><span class="line">•对于每个顶点而言，Pregel只保存一份顶点值和边值，但是，会保存两份“标志位”和输入消息队列，分别用于当前超步和下一个超步</span><br><span class="line">•在超步S中，当一个Worker在进行顶点处理时，用于当前超步的消息会被处理，同时，它在处理过程中还会接收到来自其他Worker的消息，这些消息会在下一个超步S+1中被处理，因此，需要两个消息队列用于存放作用于当前超步S的消息和作用于下一个超步S+1的消息</span><br><span class="line">•如果一个顶点V在超步S接收到消息，那么，它表示V将会在下一个超步S+1中（而不是当前超步S中）处于“活跃”状态</span><br><span class="line">•当一个Worker上的一个顶点V需要发送消息到其他顶点U时，该Worker会首先判断目标顶点U是否位于自己机器上</span><br><span class="line">•如果目标顶点U在自己的机器上，就直接把消息放入到与目标顶点U对应的输入消息队列中</span><br><span class="line">•如果发现目标顶点U在远程机器上，这个消息就会被暂时缓存到本地，当缓存中的消息数目达到一个事先设定的阈值时，这些缓存消息会被批量异步发送出去，传输到目标顶点所在的Worker上</span><br><span class="line">•如果存在用户自定义的Combiner操作，那么，当消息被加入到输出队列或者到达输入队列时，就可以对消息执行合并操作，这样可以节省存储空间和网络传输开销</span><br></pre></td></tr></table></figure><h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">•Master主要负责协调各个Worker执行任务，每个Worker会借助于名称服务系统定位到Master的位置，并向Master发送自己的注册信息，Master会为每个Worker分配一个唯一的ID</span><br><span class="line">•Master维护着关于当前处于“有效”状态的所有Worker的各种信息，包括每个Worker的ID和地址信息，以及每个Worker被分配到的分区信息</span><br><span class="line">•虽然在集群中只有一个Master，但是，它仍然能够承担起一个大规模图计算的协调任务，这是因为Master中保存这些信息的数据结构的大小，只与分区的数量有关，而与顶点和边的数量无关</span><br><span class="line">•一个大规模图计算任务会被Master分解到多个Worker去执行，在每个超步开始时，Master都会向所有处于“有效”状态的Worker发送相同的指令，然后等待这些Worker的回应</span><br><span class="line">•如果在指定时间内收不到某个Worker的反馈，Master就认为这个Worker失效</span><br><span class="line">•如果参与任务执行的多个Worker中的任意一个发生了故障失效，Master就会进入恢复模式</span><br><span class="line">•在每个超步中，图计算的各种工作，比如输入、输出、计算、保存和从检查点中恢复，都会在“路障（barrier）”之前结束</span><br><span class="line">•如果路障同步成功，说明一个超步顺利结束，Master就会进入下一个处理阶段，图计算进入下一个超步的执行</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•Master在内部运行了一个HTTP服务器来显示图计算过程的各种信息</span><br><span class="line">•用户可以通过网页随时监控图计算执行过程各个细节</span><br><span class="line">•图的大小</span><br><span class="line">•关于出度分布的柱状图</span><br><span class="line">•处于活跃状态的顶点数量</span><br><span class="line">•在当前超步的时间信息和消息流量</span><br><span class="line">•所有用户自定义Aggregator的值</span><br></pre></td></tr></table></figure><h3 id="Aggregator-1"><a href="#Aggregator-1" class="headerlink" title="Aggregator"></a>Aggregator</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 每个用户自定义的Aggregator都会采用聚合函数对一个值集合进行聚合计算得到一个全局值</span><br><span class="line">• 每个Worker都保存了一个Aggregator的实例集，其中的每个实例都是由类型名称和实例名称来标识的</span><br><span class="line">• 在执行图计算过程的某个超步S中，每个Worker会利用一个Aggregator对当前本地分区中包含的所有顶点的值进行归约，得到一个本地的局部归约值</span><br><span class="line">• 在超步S结束时，所有Worker会将所有包含局部归约值的Aggregator的值进行最后的汇总，得到全局值，然后提交给Master</span><br><span class="line">• 在下一个超步S+1开始时，Master就会将Aggregator的全局值发送给每个Worker</span><br></pre></td></tr></table></figure><h2 id="Pregel的应用实例"><a href="#Pregel的应用实例" class="headerlink" title="Pregel的应用实例"></a>Pregel的应用实例</h2><h3 id="单源最短路径"><a href="#单源最短路径" class="headerlink" title="单源最短路径"></a>单源最短路径</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303090019737.png" alt="image-20230309001948663"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class ShortestPathVertex</span><br><span class="line"> : public Vertex&lt;int, int, int&gt; &#123;</span><br><span class="line"> void Compute(MessageIterator* msgs) &#123;</span><br><span class="line">     int mindist &#x3D; IsSource(vertex_id()) ? 0 : INF;</span><br><span class="line">     for (; !msgs-&gt;Done(); msgs-&gt;Next())</span><br><span class="line">     mindist &#x3D; min(mindist, msgs-&gt;Value());</span><br><span class="line">     if (mindist &lt; GetValue()) &#123;</span><br><span class="line">     *MutableValue() &#x3D; mindist;</span><br><span class="line">     OutEdgeIterator iter &#x3D; GetOutEdgeIterator();</span><br><span class="line">     for (; !iter.Done(); iter.Next())</span><br><span class="line">     SendMessageTo(iter.Target(),</span><br><span class="line">     mindist + iter.GetValue());</span><br><span class="line"> &#125;</span><br><span class="line"> VoteToHalt();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1 class ShortestPathVertex</span><br><span class="line">2 : public Vertex&lt;int, int, int&gt; &#123;</span><br><span class="line">3 void Compute(MessageIterator* msgs) &#123;</span><br><span class="line">4 int mindist &#x3D; IsSource(vertex_id()) ? 0 : INF;</span><br><span class="line">5 for (; !msgs-&gt;Done(); msgs-&gt;Next())</span><br><span class="line">6 mindist&#x3D; min(mindist, msgs-&gt;Value());</span><br><span class="line">7 if (mindist &lt; GetValue()) &#123;</span><br><span class="line">8 *MutableValue() &#x3D; mindist;</span><br><span class="line">9 OutEdgeIteratoriter &#x3D; GetOutEdgeIterator();</span><br><span class="line">10 for (; !iter.Done(); iter.Next())</span><br><span class="line">11 SendMessageTo(iter.Target(),</span><br><span class="line">12 mindist+ iter.GetValue());</span><br><span class="line">13 &#125;</span><br><span class="line">14 VoteToHalt();</span><br><span class="line">15 &#125;</span><br><span class="line">16 &#125;;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091405728.png" alt="image-20230309140534677"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091405163.png" alt="image-20230309140546109"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091414778.png" alt="image-20230309141445727"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">超步1：</span><br><span class="line">•顶点0：没有收到消息，依然非活跃</span><br><span class="line">•顶点1：收到消息100（唯一消息），被显式唤醒，执行计算，mindist变为100，小于顶点值INF，顶点值修改为100，没有出射边，不需要发送消息，最后变为非活跃</span><br><span class="line">•顶点2：收到消息30，被显式唤醒，执行计算， mindist变为30，小于顶点值INF，顶点值修改为30，有两条出射边，向顶点3发送消息90（即：30+60），向顶点1发送消息90（即：30+60），最后变为非活跃</span><br><span class="line">•顶点3：没有收到消息，依然非活跃</span><br><span class="line">•顶点4：收到消息10，被显式唤醒，执行计算， mindist变为10，小于顶点值INF，顶点值修改为10，向顶点3发送消息60（即：10+50），最后变为非活跃</span><br><span class="line">剩余超步省略……</span><br><span class="line">当所有顶点非活跃，并且没有消息传递，就结束</span><br></pre></td></tr></table></figure><h3 id="二分匹配"><a href="#二分匹配" class="headerlink" title="二分匹配"></a>二分匹配</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">程序的执行过程是由四个阶段组成的多个循环组成的，当程序执行到超步S时，S mod 4就可以得到当前超步处于循环的哪个阶段。每个循环的四个阶段如下：</span><br><span class="line"> （1）阶段0：对于左集合中的任意顶点V，如果V还没有被匹配，就发送消息给它的每个邻居顶点请求匹配，然后，顶点V会调用VoteToHalt()进入“非活跃”状态。如果顶点V已经找到了匹配，或者V没有找到匹配但是没有出射边，那么，顶点V就不会发送消息。当顶点V没有发送消息，或者顶点V发送了消息但是所有的消息接收者都已经被匹配，那么，该顶点就不会再变为“活跃（active）”状态</span><br><span class="line"> （2）阶段1：对于右集合中的任意顶点U，如果它还没有被匹配，则会随机选择它接收到的消息中的其中一个，并向左集合中的消息发送者发送消息表示接受该匹配请求，然后给左集合中的其他请求者发送拒绝消息；然后，顶点U会调用VoteToHalt()进入“非活跃”状态</span><br><span class="line"> （3）阶段2：左集合中那些还未被匹配的顶点，会从它所收到的、右集合发送过来的接受请求中，选择其中一个给予确认，并发送一个确认消息。对于左集合中已经匹配的顶点而言，因为它们在阶段0不会向右集合发送任何匹配请求消息，因而也不会接收到任何来自右集合的匹配接受消息，因此，是不会执行阶段2的</span><br><span class="line"> （4）阶段3：右集合中还未被匹配的任意顶点U，会收到来自左集合的匹配确认消息，但是，每个未匹配的顶点U，最多会收到一个确认消息。然后，顶点U会调VoteToHalt()进入“非活跃”状态，完成它自身的匹配工作</span><br></pre></td></tr></table></figure><h2 id="Pregel和MapReduce实现PageRank算法的对比"><a href="#Pregel和MapReduce实现PageRank算法的对比" class="headerlink" title="Pregel和MapReduce实现PageRank算法的对比"></a>Pregel和MapReduce实现PageRank算法的对比</h2><h3 id="PageRank算法"><a href="#PageRank算法" class="headerlink" title="PageRank算法"></a>PageRank算法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• PageRank是一个函数，它为网络中每个网页赋一个权值。通过该权值来判断该网页的重要性</span><br><span class="line">• 该权值分配的方法并不是固定的，对PageRank算法的一些简单变形都会改变网页的相对PageRank值（PR值）</span><br><span class="line">• PageRank作为谷歌的网页链接排名算法，基本公式如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082125420.png" alt="image-20230308212003706"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 对于任意一个网页链接，其PR值为链入到该链接的源链接的PR值对该链接的贡献和，其中，N表示该网络中所有网页的数量，Ni为第i个源链接的链出度，PRi表示第i个源链接的PR值</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 网络链接之间的关系可以用一个连通图来表示，下图就是四个网页（A,B,C,D）互相链入链出组成的连通图，从中可以看出，网页A中包含指向网页B、C和D的外链，网页B和D是网页A的源链接</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082125422.png" alt="image-20230308212407285"></p><h3 id="PageRank算法在Pregel中的实现"><a href="#PageRank算法在Pregel中的实现" class="headerlink" title="PageRank算法在Pregel中的实现"></a>PageRank算法在Pregel中的实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 在Pregel计算模型中，图中的每个顶点会对应一个计算单元，每个计算单元包含三个成员变量：</span><br><span class="line">顶点值（Vertex value）：顶点对应的PR值</span><br><span class="line">出射边（Out edge）：只需要表示一条边，可以不取值</span><br><span class="line">消息（Message）：传递的消息，因为需要将本顶点对其它顶点的PR贡献值，传递给目标顶点</span><br><span class="line">• 每个计算单元包含一个成员函数Compute()，该函数定义了顶点上的运算，包括该顶点的PR值计算，以及从该顶点发送消息到其链出顶点</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class PageRankVertex: public Vertex&lt;double, void, double&gt; &#123;</span><br><span class="line">public:</span><br><span class="line">     virtual void Compute(MessageIterator* msgs) &#123;</span><br><span class="line">        if (superstep() &gt;&#x3D; 1) &#123;</span><br><span class="line">            double sum &#x3D; 0;</span><br><span class="line">            for (;!msgs-&gt;Done(); msgs-&gt;Next())</span><br><span class="line">            sum +&#x3D; msgs-&gt;Value();</span><br><span class="line">            *MutableValue() &#x3D;</span><br><span class="line">            0.15 &#x2F; NumVertices() + 0.85 * sum;</span><br><span class="line">        &#125;</span><br><span class="line">        if (superstep() &lt; 30) &#123;</span><br><span class="line">             const int64 n &#x3D; GetOutEdgeIterator().size();</span><br><span class="line">             SendMessageToAllNeighbors(GetValue()&#x2F; n);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">        VoteToHalt();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• PageRankVertex继承自Vertex类，顶点值类型是double，用来保存PageRank中间值，消息类型也是double，用来传输PageRank值，边的value类型是void，因为不需要存储任何信息</span><br><span class="line">• 这里假设在第0个超步时，图中各顶点值被初始化为1&#x2F;NumVertices()，其中，NumVertices()表示顶点数目</span><br><span class="line">• 在前30个超步中，每个顶点都会沿着它的出射边，发送它的PageRank值除以出射边数目以后的结果值。从第1个超步开始，每个顶点会将到达的消息中的值加到sum值中，同时将它的PageRank值设为0.15&#x2F;NumVertices()+0.85*sum</span><br><span class="line">• 到了第30个超步后，就没有需要发送的消息了，同时所有的顶点停止计算，得到最终结果</span><br></pre></td></tr></table></figure><h3 id="PageRank算法在MapReduce中的实现"><a href="#PageRank算法在MapReduce中的实现" class="headerlink" title="PageRank算法在MapReduce中的实现"></a>PageRank算法在MapReduce中的实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• MapReduce也是谷歌公司提出的一种计算模型，它是为全量计算而设计</span><br><span class="line">• 采用MapReduce实现PageRank的计算过程包括三个阶段：</span><br><span class="line">     第一阶段：解析网页</span><br><span class="line">     第二阶段：PageRank分配</span><br><span class="line">     第三阶段：收敛阶段</span><br></pre></td></tr></table></figure><h4 id="阶段1：解析网页"><a href="#阶段1：解析网页" class="headerlink" title="阶段1：解析网页"></a>阶段1：解析网页</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 该阶段的任务就是分析一个页面的链接数并赋初值。</span><br><span class="line">• 一个网页可以表示为由网址和内容构成的键值对&lt; URL，page content&gt;，作为Map任务的输入。阶段1的Map任务把&lt;URL，page content&gt;映射为&lt;URL，&lt;PRinit，url_list&gt;&gt;后进行输出，其中，PRinit是该URL页面对应的PageRank初始值，url_list包含了该URL页面中的外链所指向的所有URL。Reduce任务只是恒等函数，输入和输出相同。</span><br><span class="line">• 对右图，每个网页的初始PageRank值为1&#x2F;4。它在该阶段中:</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082138941.png" alt="image-20230308213807879" style="zoom:67%;"><h4 id="阶段2：PageRank分配"><a href="#阶段2：PageRank分配" class="headerlink" title="阶段2：PageRank分配"></a>阶段2：PageRank分配</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• 该阶段的任务就是多次迭代计算页面的PageRank值。</span><br><span class="line">• 在该阶段中，Map任务的输入是&lt;URL，&lt;cur_rank，url_list&gt;&gt;，其中，cur_rank是该</span><br><span class="line">URL页面对应的PageRank当前值，url_list包含了该URL页面中的外链所指向的所有</span><br><span class="line">URL。</span><br><span class="line">• 对于url_list中的每个元素u，Map任务输出&lt;u，&lt;URL, cur_rank&#x2F;|url_list|&gt;&gt;（其中，|url_list|表示外链的个数），并输出链接关系&lt;URL，url_list&gt;。</span><br><span class="line">• 每个页面的PageRank当前值被平均分配给了它们的每个外链。Map任务的输出会作为下面Reduce任务的输入。对下图第一次迭代Map任务的输入输出如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082143692.png" alt="image-20230308214310636"></p><h4 id="阶段3：收敛阶段"><a href="#阶段3：收敛阶段" class="headerlink" title="阶段3：收敛阶段"></a>阶段3：收敛阶段</h4><h3 id="PageRank算法在Pregel和MapReduce中实现的比较"><a href="#PageRank算法在Pregel和MapReduce中实现的比较" class="headerlink" title="PageRank算法在Pregel和MapReduce中实现的比较"></a>PageRank算法在Pregel和MapReduce中实现的比较</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第8章 流计算</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC8%E7%AB%A0-%E6%B5%81%E8%AE%A1%E7%AE%97.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC8%E7%AB%A0-%E6%B5%81%E8%AE%A1%E7%AE%97.html</id>
    <published>2023-02-28T14:32:22.000Z</published>
    <updated>2023-03-08T12:14:57.695Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第8章-流计算"><a href="#第8章-流计算" class="headerlink" title="第8章 流计算"></a>第8章 流计算</h1><h2 id="流计算概述"><a href="#流计算概述" class="headerlink" title="流计算概述"></a>流计算概述</h2><h3 id="静态数据和流数据"><a href="#静态数据和流数据" class="headerlink" title="静态数据和流数据"></a>静态数据和流数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">很多企业为了支持决策分析而构建的数据仓库系统，其中存放的大量历史数据就是静态数据。技术人员可以利用数据挖掘和OLAP（OnLine Analytical Processing）分析工具从静态数据中找到对企业有价值的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081458289.png" alt="image-20230308145838198"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 近年来，在Web应用、网络监控、传感监测等领域，兴起了一种新的数据密集型应用——流数据，即数据以大量、快速、时变的流形式持续到达</span><br><span class="line">• 流数据具有如下特征：</span><br><span class="line">– 数据快速持续到达，潜在大小也许是无穷无尽的</span><br><span class="line">– 数据来源众多，格式复杂</span><br><span class="line">– 数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储</span><br><span class="line">– 注重数据的整体价值，不过分关注个别数据</span><br><span class="line">– 数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序</span><br></pre></td></tr></table></figure><h3 id="批量计算和实时计算"><a href="#批量计算和实时计算" class="headerlink" title="批量计算和实时计算"></a>批量计算和实时计算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 对静态数据和流数据的处理，对应着两种截然不同的计算模式：批量计算和实时计算</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081503278.png" alt="image-20230308150351231"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 批量计算以“静态数据”为对象，可在充裕的时间内对海量数据进行批量处理，计算得到有价值的信息。Hadoop是典型的批处理模型，由HDFS和HBase存放大量的静态数据，由MapReduce负责对海量数据执行批量计算</span><br><span class="line">• 流数据须采用实时计算。实时计算最重要的一个需求是能够实时得到计算结果，一般要求响应时间为秒级。当只需要处理少量数据时，实时计算并不是问题；但是，在大数据时代，数据格式复杂、来源众多、数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计算——流计算，应运而生</span><br></pre></td></tr></table></figure><h3 id="流计算概念"><a href="#流计算概念" class="headerlink" title="流计算概念"></a>流计算概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 流计算：实时获取来自不同数据源的海量数据，经过实时分析处理，获得有价值的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081512277.png" alt="image-20230308151223180"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">• 流计算秉承一个基本理念，即数据的价值随着时间的流逝而降低。因此，当事件出现时就应该立即进行处理，而不是缓存起来进行批量处理。为了及时处理流数据，就需要一个低延迟、可扩展、高可靠的处理引擎</span><br><span class="line">• 对于一个流计算系统来说，它应达到如下需求：</span><br><span class="line">– 高性能：处理大数据的基本要求，如每秒处理几十万条数据</span><br><span class="line">– 海量式：支持TB级甚至是PB级的数据规模</span><br><span class="line">– 实时性：保证较低的延迟时间，达到秒级别，甚至是毫秒级别</span><br><span class="line">– 分布式：支持大数据的基本架构，必须能够平滑扩展</span><br><span class="line">– 易用性：能够快速进行开发和部署</span><br><span class="line">– 可靠性：能可靠地处理流数据</span><br></pre></td></tr></table></figure><h3 id="流计算与Hadoop"><a href="#流计算与Hadoop" class="headerlink" title="流计算与Hadoop"></a>流计算与Hadoop</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Hadoop设计的初衷是面向大规模数据的批量处理，每台机器并行运行MapReduce任务，最后对结果进行汇总输出</span><br><span class="line">• MapReduce是专门面向静态数据的批量处理的，内部各种实现机制都为批处理做了高度优化，不适合用于处理持续到达的动态数据</span><br><span class="line">• 我们可能会想到一种“变通”的方案来降低批处理的时间延迟——将基于MapReduce的批量处理转为小批量处理，将输入数据切成小的片段，每隔一个周期就启动一次MapReduce作业。但这种方式也无法有效处理流数据</span><br></pre></td></tr></table></figure><h3 id="流计算框架"><a href="#流计算框架" class="headerlink" title="流计算框架"></a>流计算框架</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 当前业界诞生了许多专门的流数据实时计算系统来满足各自需求</span><br><span class="line">• 目前有三类常见的流计算框架和平台：商业级的流计算平台、开源流计算框架、公司为支持自身业务开发的流计算框架</span><br><span class="line">• 较为常见的是开源流计算框架，代表如下：</span><br><span class="line">– Twitter Storm：免费、开源的分布式实时计算系统，可简单、高效、可靠地处理大量的流数据</span><br><span class="line">– Yahoo! S4（Simple Scalable Streaming System）：开源流计算平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的流式系统</span><br></pre></td></tr></table></figure><h2 id="流计算处理流程"><a href="#流计算处理流程" class="headerlink" title="流计算处理流程"></a>流计算处理流程</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 传统的数据处理流程，需要先采集数据并存储在关系数据库等数据管理系统中，之后由用户通过查询操作和数据管理系统进行交互</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081522466.png" alt="image-20230308152245421"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 传统的数据处理流程隐含了两个前提：</span><br><span class="line">– 存储的数据是旧的。存储的静态数据是过去某一时刻的快照，这些数据在查询时可能已不具备时效性了</span><br><span class="line">– 需要用户主动发出查询来获取结果</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 流计算的处理流程一般包含三个阶段：数据实时采集、数据实时计算、实时查询服务</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081524733.png" alt="image-20230308152430685"></p><h3 id="数据实时采集"><a href="#数据实时采集" class="headerlink" title="数据实时采集"></a>数据实时采集</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 数据实时采集阶段通常采集多个数据源的海量数据，需要保证实时性、低延迟与稳定可靠</span><br><span class="line">• 以日志数据为例，由于分布式集群的广泛应用，数据分散存储在不同的机器上，因此需要实时汇总来自不同机器上的日志数据</span><br><span class="line">• 目前有许多互联网公司发布的开源分布式日志采集系统均可满足每秒数百MB的数据采集和传输需求，如：</span><br><span class="line">– Facebook的Scribe</span><br><span class="line">– LinkedIn的Kafka</span><br><span class="line">– 淘宝的Time Tunnel</span><br><span class="line">– 基于Hadoop的Chukwa和Flume</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 数据采集系统的基本架构一般有以下三个部分：</span><br><span class="line">– Agent：主动采集数据，并把数据推送到Collector部分</span><br><span class="line">– Collector：接收多个Agent的数据，并实现有序、可靠、高性能</span><br><span class="line">的转发</span><br><span class="line">– Store：存储Collector转发过来的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081541896.png" alt="image-20230308154137847"></p><h3 id="数据实时计算"><a href="#数据实时计算" class="headerlink" title="数据实时计算"></a>数据实时计算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 数据实时计算阶段对采集的数据进行实时的分析和计算，并反馈实时结果</span><br><span class="line">• 经流处理系统处理后的数据，可视情况进行存储，以便之后再进行分析计算。在时效性要求较高的场景中，处理之后的数据也可以直接丢弃</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081549997.png" alt="image-20230308154930940" style="zoom:80%;"><h3 id="实时查询服务"><a href="#实时查询服务" class="headerlink" title="实时查询服务"></a>实时查询服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存</span><br><span class="line">• 传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户</span><br><span class="line">• 虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存</span><br><span class="line">• 传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户</span><br><span class="line">• 虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• 可见，流处理系统与传统的数据处理系统有如下不同：</span><br><span class="line">– 流处理系统处理的是实时的数据，而传统的数据处理系统处理的是预先存储好的静态数据</span><br><span class="line">– 用户通过流处理系统获取的是实时结果，而通过传统的数据处理系统，获取的是过去某一时刻的结果</span><br><span class="line">– 流处理系统无需用户主动发出查询，实时查询服务可以主动将实时结果推送给用户</span><br></pre></td></tr></table></figure><h2 id="流计算的应用"><a href="#流计算的应用" class="headerlink" title="流计算的应用"></a>流计算的应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 流计算是针对流数据的实时计算，可以应用在多种场景中，如Web服务、机器翻译、广告投放、自然语言处理、气候模拟预测等</span><br><span class="line">• 如百度、淘宝等大型网站中，每天都会产生大量流数据，包括用户的搜索内容、用户的浏览记录等数据。采用流计算进行实时数据分析，可以了解每个时刻的流量变化情况，甚至可以分析用户的实时浏览轨迹，从而进行实时个性化内容推荐</span><br><span class="line">• 但是，并不是每个应用场景都需要用到流计算的。流计算适合于需要处理持续到达的流数据、对数据处理有较高实时性要求的场景</span><br></pre></td></tr></table></figure><h3 id="应用场景1-实时分析"><a href="#应用场景1-实时分析" class="headerlink" title="应用场景1: 实时分析"></a>应用场景1: 实时分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 传统的业务分析一般采用分布式离线计算的方式，即将数据全部保存起来，然后每隔一定的时间进行离线分析来得到结果。但这样会导致一定的延时，难以保证结果的实时性</span><br><span class="line">• 如淘宝网“双十一”、“双十二”的促销活动，商家需要根据广告效果来即使调整广告，这就需要对广告的受访情况进行分析。但以往采用分布式离线分析，需要几小时甚至一天的延时才能得到分析结果。而促销活动只持续一天，因此，隔天才能得到的分析结果便失去了价值</span><br><span class="line">• 虽然分布式离线分析带来的小时级的分析延时可以满足大部分商家的需求，但随着实时性要求越来越高，如何实现秒级别的实时分析响应成为业务分析的一大挑战</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 针对流数据，“量子恒道”开发了海量数据实时流计算框架Super Mario。通过该框架，量子恒道可处理每天TB级的实时流数据，并且从用户发出请求到数据展示，整个延时控制在2-3秒内，达到了实时性的要求</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081604482.png" alt="image-20230308160445437" style="zoom:67%;"><h3 id="应用场景2-实时交通"><a href="#应用场景2-实时交通" class="headerlink" title="应用场景2: 实时交通"></a>应用场景2: 实时交通</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 流计算不仅为互联网带来改变，也能改变我们的生活</span><br><span class="line">• 如提供导航路线，一般的导航路线并没有考虑实时的交通状况，即便在计算路线时有考虑交通状况，往往也只是使用了以往的交通状况数据。要达到根据实时交通状态进行导航的效果，就需要获取海量的实时交通数据并进行实时分析</span><br><span class="line">• 借助于流计算的实时特性，不仅可以根据交通情况制定路线，而且在行驶过程中，也可以根据交通情况的变化实时更新路线，始终为用户提供最佳的行驶路线</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• IBM的流计算平台InfoSphere Streams，广泛应用于制造、零售、交通运输、金融证券以及监管各行各业的解决方案之中，使得实时快速做出决策的理念得以实现</span><br><span class="line">• 以上述的实时交通为例，InfoSphere Streams应用于斯德哥尔摩的交通信息管理，通过结合来自不同源的实时数据，可以生成动态的、多方位的观察交通流量的方式，为城市规划者和乘客提供实时交通状况查询</span><br></pre></td></tr></table></figure><h2 id="流计算开源框架-–-Storm"><a href="#流计算开源框架-–-Storm" class="headerlink" title="流计算开源框架 – Storm"></a>流计算开源框架 – Storm</h2><h3 id="Storm简介"><a href="#Storm简介" class="headerlink" title="Storm简介"></a>Storm简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Twitter Storm是一个免费、开源的分布式实时计算系统，Storm对于实时计算的意义类似于Hadoop对于批处理的意义，Storm可以简单、高效、可靠地处理流数据，并支持多种编程语言</span><br><span class="line">• Storm框架可以方便地与数据库系统进行整合，从而开发出强大的实时计算系统</span><br><span class="line">• Twitter是全球访问量最大的社交网站之一，Twitter开发Storm流处理框架也是为了应对其不断增长的流数据实时处理需求</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• Twitter采用了由实时系统和批处理系统组成的分层数据处理架构，一方面由Hadoop和ElephantDB组成批处理系统，另一方面由Storm和Cassandra组成实时系统</span><br><span class="line">• 在计算查询时，该系统会同时查询批处理视图和实时视图，并把它们合并起来以得到最终的结果</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081613772.png" alt="image-20230308161306725" style="zoom:67%;"><h3 id="Storm的特点"><a href="#Storm的特点" class="headerlink" title="Storm的特点"></a>Storm的特点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">• Storm可用于许多领域中，如实时分析、在线机器学习、持续计算、远程RPC、数据提取加载转换等</span><br><span class="line">• Storm具有以下主要特点：</span><br><span class="line">– 整合性：Storm可方便地与队列系统和数据库系统进行整合</span><br><span class="line">– 简易的API：Storm的API在使用上即简单又方便</span><br><span class="line">– 可扩展性：Storm的并行特性使其可以运行在分布式集群中</span><br><span class="line">– 容错性：Storm可自动进行故障节点的重启、任务的重新分配</span><br><span class="line">– 可靠的消息处理：Storm保证每个消息都能完整处理</span><br><span class="line">– 支持各种编程语言：Storm支持使用各种编程语言来定义任务</span><br><span class="line">– 快速部署：Storm可以快速进行部署和使用</span><br><span class="line">– 免费、开源：Storm是一款开源框架，可以免费使用</span><br></pre></td></tr></table></figure><h3 id="Storm设计思想"><a href="#Storm设计思想" class="headerlink" title="Storm设计思想"></a>Storm设计思想</h3><h4 id="Streams"><a href="#Streams" class="headerlink" title="Streams"></a>Streams</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 要了解Storm，首先需要了解Storm的设计思想。Storm对一些设计思想进行了抽象化，其主要术语包括Streams、Spouts、Bolts、Topology和Stream Groupings</span><br><span class="line">• Streams：Storm将流数据Stream描述成一个无限的Tuple序列，这些Tuple序列会以分布式的方式并行地创建和处理</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081637226.png" alt="image-20230308163744184" style="zoom:67%;"><h4 id="Spouts"><a href="#Spouts" class="headerlink" title="Spouts"></a>Spouts</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Spouts：Storm认为每个Stream都有一个源头，并把这个源头抽象为Spouts。Spouts会从外部读取流数据并持续发出Tuple</span><br></pre></td></tr></table></figure><h4 id="Bolts"><a href="#Bolts" class="headerlink" title="Bolts"></a>Bolts</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Bolts：Storm将Streams的状态转换过程抽象为Bolts。Bolts即可以处理Tuple，也可以将处理后的Tuple作为新的Streams发送给其他Bolts。对Tuple的处理逻辑都被封装在Bolts中，可执行过滤、聚合、查询等操作</span><br></pre></td></tr></table></figure><h4 id="Topology"><a href="#Topology" class="headerlink" title="Topology"></a>Topology</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Topology：Storm将Spouts和Bolts组成的网络抽象成Topology，它可以被提交到Storm集群执行。Topology可视为流转换图，图中节点是一个Spout或Bolt，边则表示Bolt订阅了哪个Stream。当Spout或者Bolt发送元组时，它会把元组发送到每个订阅了该Stream的Bolt上进行处理</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081643689.png" alt="image-20230308164303642" style="zoom:67%;"><h4 id="Stream-Groupings"><a href="#Stream-Groupings" class="headerlink" title="Stream Groupings"></a>Stream Groupings</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Stream Groupings：Storm中的Stream Groupings用于告知Topology如何在两个组件间（如Spout和Bolt之间，或者不同的Bolt之间）进行Tuple的传送。每一个Spout和Bolt都可以有多个分布式任务，一个任务在什么时候、以什么方式发送Tuple就是由Stream Groupings来决定的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081645742.png" alt="image-20230308164524685"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 目前，Storm中的Stream Groupings有如下几种方式：</span><br><span class="line">– Shuffle Grouping：随机分组，随机分发Tuple</span><br><span class="line">– Fields Grouping：按字段分组，具有相同值的Tuple会被分发到对应的Bolt</span><br><span class="line">– All Grouping：广播分发，每个Tuple都会被分发到所有Bolt中</span><br><span class="line">– Global Grouping：全局分组，Tuple只会分发给一个Bolt</span><br><span class="line">– Non Grouping：不分组，与随机分组效果类似</span><br><span class="line">– Direct Grouping：直接分组，由Tuple的生产者来定义接收者</span><br></pre></td></tr></table></figure><h3 id="Storm框架设计"><a href="#Storm框架设计" class="headerlink" title="Storm框架设计"></a>Storm框架设计</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• Storm运行任务的方式与Hadoop类似：Hadoop运行的是MapReduce作业，而Storm运行的是“Topology”</span><br><span class="line">• 但两者的任务大不相同，主要的不同是：MapReduce作业最终会完成计算并结束运行，而Topology将持续处理消息（直到人为终止）</span><br><span class="line">• Storm集群采用“Master—Worker”的节点方式：</span><br><span class="line">– Master节点运行名为“Nimbus”的后台程序（类似Hadoop中的“JobTracker”），负责在集群范围内分发代码、为Worker分配任务和监测故障</span><br><span class="line">– Worker节点运行名为“Supervisor”的后台程序，负责监听分配给它所在机器的工作，即根据Nimbus分配的任务来决定启动或停止Worker进程</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Storm使用Zookeeper来作为分布式协调组件，负责Nimbus和多个Supervisor之间的所有协调工作。借助于Zookeeper，若Nimbus进程或Supervisor进程意外终止，重启时也能读取、恢复之前的状态并继续工作，使得Storm极其稳定</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081656820.png" alt="image-20230308165644767" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 基于这样的架构设计，Storm的工作流程如下图所示：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081657604.png" alt="image-20230308165745550"></p><h3 id="Storm实例"><a href="#Storm实例" class="headerlink" title="Storm实例"></a>Storm实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 我们以单词统计的实例来加深对Storm的认识</span><br><span class="line">• Storm的编程模型非常简单，如下代码即定义了整个单词统计</span><br><span class="line">Topology的整体逻辑</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081658383.png" alt="image-20230308165849326"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Topology中仅定义了整体的计算逻辑，还需要定义具体的处理函数</span><br><span class="line">。具体的处理函数可以使用任一编程语言来定义，甚至也可以结合多种编程语言来实现</span><br><span class="line">• 如SplitSentence()方法虽然是通过Java语言定义的，但具体的操作可通过Python脚本来完成</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081700391.png" alt="image-20230308170042334"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Python脚本splitsentence.py定义了一个简单的单词分割方法，即通过空格来分割单词。分割后的单词通过emit()方法以Tuple的形式发送给订阅了该Stream的Bolt进行接收和处理</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081701390.png" alt="image-20230308170113339"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 单词统计的具体逻辑：首先判断单词是否统计过，若未统计过，需先将count值置为0。若单词已统计过，则每出现一次该单词，count值就加1</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081701542.png" alt="image-20230308170135478"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• 基于Storm的单词统计在形式上与基于MapReduce的单词统计是类似的，MapReduce使用的是Map和Reduce的抽象，而Storm使用的是Soput和Bolt的抽象</span><br><span class="line">• 总结一下Storm进行单词统计的整个流程：</span><br><span class="line">– 从Spout中发送Stream（每个英文句子为一个Tuple）</span><br><span class="line">– 用于分割单词的Bolt将接收的句子分解为独立的单词，将单词作为Tuple的字段名发送出去</span><br><span class="line">– 用于计数的Bolt接收表示单词的Tuple，并对其进行统计</span><br><span class="line">– 输出每个单词以及单词出现过的次数</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081702343.png" alt="image-20230308170211283" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 上述虽然是一个简单的单词统计，但对其进行扩展，便可应用到许多场景中，如微博中的实时热门话题。Twitter也正是使用了Storm框架实现了实时热门话题</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081702971.png" alt="image-20230308170245909"></p><h3 id="哪些公司在使用Storm"><a href="#哪些公司在使用Storm" class="headerlink" title="哪些公司在使用Storm"></a>哪些公司在使用Storm</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Storm自2011年发布以来，凭借其优良的框架设计及开源特性，在流计算领域获得了广泛认可，已应用到许多大型互联网公司的实际项目中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081710751.png" alt="image-20230308171025682"></p><h2 id="Storm安装和运行实例"><a href="#Storm安装和运行实例" class="headerlink" title="Storm安装和运行实例"></a>Storm安装和运行实例</h2><p><a href="http://dblab.xmu.edu.cn/blog/install-storm/" target="_blank" rel="external nofollow noopener noreferrer">Storm安装教程_CentOS6.4/Storm0.9.6</a></p><h3 id="安装Storm的基本过程"><a href="#安装Storm的基本过程" class="headerlink" title="安装Storm的基本过程"></a>安装Storm的基本过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">本实例中Storm具体运行环境如下：</span><br><span class="line">•CentOS 6.4</span><br><span class="line">•Storm 0.9.6</span><br><span class="line">•Java JDK 1.7</span><br><span class="line">•ZooKeeper 3.4.6</span><br><span class="line">•Python 2.6</span><br><span class="line">备注：CentOS中已默认安装了Python 2.6，我们还需要安装 JDK 环境以及分布式应用程序协调服务 Zookeeper</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">安装Storm的基本过程如下：</span><br><span class="line">•第一步：安装Java环境</span><br><span class="line">•第二步：安装 Zookeeper</span><br><span class="line">•第三步：安装Storm（单机）</span><br><span class="line">•第四步：关闭Storm</span><br></pre></td></tr></table></figure><h4 id="第一步：安装Java环境"><a href="#第一步：安装Java环境" class="headerlink" title="第一步：安装Java环境"></a>第一步：安装Java环境</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">•Storm 运行需要 Java 环境，可选择 Oracle 的 JDK，或是 OpenJDK</span><br><span class="line">，现在一般 Linux 系统默认安装的基本是 OpenJDK，如 CentOS 6.4 </span><br><span class="line">就默认安装了 OpenJDK 1.7。但需要注意的是，CentOS 6.4 中默认安</span><br><span class="line">装的只是 Java JRE，而不是 JDK，为了开发方便，我们还是需要通过</span><br><span class="line">yum 进行安装 JDK</span><br><span class="line"></span><br><span class="line">$ sudo yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel</span><br><span class="line"></span><br><span class="line">•接着需要配置一下 JAVA_HOME 环境变量，为方便，可以在 ~&#x2F;.bashrc</span><br><span class="line">中进行设置</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081716201.png" alt="image-20230308171605152"></p><h4 id="第二步：安装Zookeeper"><a href="#第二步：安装Zookeeper" class="headerlink" title="第二步：安装Zookeeper"></a>第二步：安装Zookeeper</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到官网下载Zookeeper，比如下载 ―zookeeper-3.4.6.tar.gz‖</span><br><span class="line">下载后执行如下命令进行安装 zookeeper（将命令中 3.4.6 改为你下载的版本）：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081716354.png" alt="image-20230308171657313"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown命令让hadoop用户拥有zookeeper目录下的所有文件的权限</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接着执行如下命令进行zookeeper配置：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081719215.png" alt="image-20230308171903171"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将当中的 dataDir&#x3D;&#x2F;tmp&#x2F;zookeeper 更改为</span><br><span class="line">dataDir&#x3D;&#x2F;usr&#x2F;local&#x2F;zookeeper&#x2F;tmp 。接着执行：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081719069.png" alt="image-20230308171920028"></p><h4 id="第三步：安装Storm（单机）"><a href="#第三步：安装Storm（单机）" class="headerlink" title="第三步：安装Storm（单机）"></a>第三步：安装Storm（单机）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到官网下载Storm，比如Storm0.9.6</span><br><span class="line">下载后执行如下命令进行安装Storm：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081720726.png" alt="image-20230308172015682"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接着执行如下命令进行Storm配置:</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081721931.png" alt="image-20230308172103894"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">修改其中的 storm.zookeeper.servers 和 nimbus.host 两个配置项，即取消掉</span><br><span class="line">注释且都修改值为 127.0.0.1（我们只需要在单机上运行），如下图所示。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081721615.png" alt="image-20230308172131564" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">然后就可以启动 Storm 了。执行如下命令启动 nimbus 后台进程：</span><br><span class="line">$ .&#x2F;bin&#x2F;storm nimbus</span><br><span class="line"></span><br><span class="line">启动 nimbus 后，终端被该进程占用了，不能再继续执行其他命令了。因此</span><br><span class="line">我们需要另外开启一个终端，然后执行如下命令启动 supervisor 后台进程：</span><br><span class="line">$ # 需要另外开启一个终端</span><br><span class="line">$ &#x2F;usr&#x2F;local&#x2F;storm&#x2F;bin&#x2F;storm supervisor</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">同样的，启动 supervisor 后，我们还需要开启另外的终端才能执行其他命令</span><br><span class="line">。另外，我们可以使用 jps 命令 检查是否成功启动，若成功启动会显示</span><br><span class="line">nimbus、supervisor、QuorumPeeMain （QuorumPeeMain 是 zookeeper </span><br><span class="line">的后台进程，若显示 config_value 表明 nimbus 或 supervisor 还在启动中）</span><br><span class="line">，如下图所示。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081723654.png" alt="image-20230308172323611"></p><h4 id="第四步：关闭Storm"><a href="#第四步：关闭Storm" class="headerlink" title="第四步：关闭Storm"></a>第四步：关闭Storm</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">之前启动的 nimbus 和 supervisor 占用了两个终端窗口，切换到这两个终</span><br><span class="line">端窗口，按键盘的 Ctrl+C 可以终止进程，终止后，也就相当于关闭了Storm。</span><br></pre></td></tr></table></figure><h3 id="运行Storm实例"><a href="#运行Storm实例" class="headerlink" title="运行Storm实例"></a>运行Storm实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Storm中自带了一些例子，我们可以执行一下 WordCount 例子来感受一</span><br><span class="line">下 Storm 的执行流程。执行如下命令：</span><br><span class="line">$ &#x2F;usr&#x2F;local&#x2F;storm&#x2F;bin&#x2F;storm jar &#x2F;usr&#x2F;local&#x2F;storm&#x2F;examples&#x2F;stormstarter&#x2F;storm-starter-topologies-0.9.6.jar </span><br><span class="line">storm.starter.WordCountTopology</span><br><span class="line"></span><br><span class="line">该程序是不断地取如下四句英文句子中的一句作为数据源，然后发送给</span><br><span class="line">bolt 来统计单词出现的次数。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081724098.png" alt="image-20230308172457055"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第7章 mapreduce</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC7%E7%AB%A0-mapreduce.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC7%E7%AB%A0-mapreduce.html</id>
    <published>2023-02-28T14:31:52.000Z</published>
    <updated>2023-03-14T02:01:17.369Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第7章-mapreduce"><a href="#第7章-mapreduce" class="headerlink" title="第7章 mapreduce"></a>第7章 mapreduce</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="分布式并行编程"><a href="#分布式并行编程" class="headerlink" title="分布式并行编程"></a>分布式并行编程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•“摩尔定律”， CPU性能大约每隔18个月翻一番</span><br><span class="line">•从2005年开始摩尔定律逐渐失效(大数据摩尔定律：每年按50%增长) ，需要处理的数据量快速增加，人们开始借助于分布式并行编程来提高程序性能</span><br><span class="line">•分布式程序运行在大规模计算机集群上，可以并行执行大规模数据处理任务，从而获得海量的计算能力</span><br><span class="line">•谷歌公司最先提出了分布式并行编程模型MapReduce，Hadoop MapReduce是它的开源实现，后者比前者使用门槛低很多</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">问题：在MapReduce出现之前，已经有像MPI这样非常成熟的并行计算框架了，那么为什么Google还需要MapReduce？MapReduce相较于传统的并行计算框架有什么优势？</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031429645.png" alt="image-20230303142923582" style="zoom:67%;"><h3 id="MapReduce模型简介"><a href="#MapReduce模型简介" class="headerlink" title="MapReduce模型简介"></a>MapReduce模型简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•MapReduce将复杂的、运行于大规模集群上的并行计算过程高度地抽象到了两个函数：Map和Reduce</span><br><span class="line">•编程容易，不需要掌握分布式并行编程细节，也可以很容易把自己的程序运行在分布式系统上，完成海量数据的计算</span><br><span class="line">•MapReduce采用“分而治之”策略，一个存储在分布式文件系统中的大规模数据集，会被切分成许多独立的分片（split），这些分片可以被多个Map任务并行处理</span><br><span class="line">•MapReduce设计的一个理念就是“计算向数据靠拢”，而不是“数据向计算靠拢”，因为，移动数据需要大量的网络传输开销</span><br><span class="line">•MapReduce框架采用了Master&#x2F;Slave架构，包括一个Master和若干个Slave。Master上运行JobTracker，Slave上运行TaskTracker</span><br><span class="line">•Hadoop框架是用Java实现的，但是，MapReduce应用程序则不一定要用Java来写</span><br></pre></td></tr></table></figure><h3 id="Map和Reduce函数"><a href="#Map和Reduce函数" class="headerlink" title="Map和Reduce函数"></a>Map和Reduce函数</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031447655.png" alt="image-20230303144723603"></p><h2 id="MapReduce体系结构"><a href="#MapReduce体系结构" class="headerlink" title="MapReduce体系结构"></a>MapReduce体系结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MapReduce体系结构主要由四个部分组成，分别是：Client、JobTracker、TaskTracker以及Task</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031450101.png" alt="image-20230303145010046" style="zoom:67%;"><h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•用户编写的MapReduce程序通过Client提交到JobTracker端</span><br><span class="line">•用户可通过Client提供的一些接口查看作业运行状态</span><br></pre></td></tr></table></figure><h3 id="JobTracker"><a href="#JobTracker" class="headerlink" title="JobTracker"></a>JobTracker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•JobTracker负责资源监控和作业调度</span><br><span class="line">•JobTracker监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点</span><br><span class="line">•JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（Task Scheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031503481.png" alt="image-20230303150311444" style="zoom:67%;"><h3 id="TaskTracker"><a href="#TaskTracker" class="headerlink" title="TaskTracker"></a>TaskTracker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）</span><br><span class="line">•TaskTracker使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot分为Map slot 和Reduce slot两种，分别供MapTask和Reduce Task使用</span><br></pre></td></tr></table></figure><h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Task分为Map Task 和Reduce Task 两种，均由TaskTracker 启动</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031503074.png" alt="image-20230303150354031" style="zoom:67%;"><h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><h3 id="工作流程概述"><a href="#工作流程概述" class="headerlink" title="工作流程概述"></a>工作流程概述</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031531923.png" alt="image-20230303153107874"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•不同的Map任务之间不会进行通信</span><br><span class="line">•不同的Reduce任务之间也不会发生任何信息交换</span><br><span class="line">•用户不能显式地从一台机器向另一台机器发送消息</span><br><span class="line">•所有的数据交换都是通过MapReduce框架自身去实现的</span><br></pre></td></tr></table></figure><h3 id="MapReduce各个执行阶段"><a href="#MapReduce各个执行阶段" class="headerlink" title="MapReduce各个执行阶段"></a>MapReduce各个执行阶段</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031531102.png" alt="image-20230303153150037" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">split：是逻辑分割，如起点，长度</span><br><span class="line">RR（Record Reader):按照split切分的起点和长度读取</span><br><span class="line">shuffle：分区、排序、合并、归并过程</span><br></pre></td></tr></table></figure><h4 id="关于Split（分片）"><a href="#关于Split（分片）" class="headerlink" title="关于Split（分片）"></a>关于Split（分片）</h4><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031532479.png" alt="image-20230303153252421" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HDFS以固定大小的block为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。</span><br></pre></td></tr></table></figure><h4 id="Map任务的数量"><a href="#Map任务的数量" class="headerlink" title="Map任务的数量"></a>Map任务的数量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">•Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031533401.png" alt="image-20230303153342357"></p><h4 id="Reduce任务的数量"><a href="#Reduce任务的数量" class="headerlink" title="Reduce任务的数量"></a>Reduce任务的数量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目</span><br><span class="line">•通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误）</span><br></pre></td></tr></table></figure><h3 id="Shuffle过程详解"><a href="#Shuffle过程详解" class="headerlink" title="Shuffle过程详解"></a>Shuffle过程详解</h3><h4 id="Shuffle过程简介"><a href="#Shuffle过程简介" class="headerlink" title="Shuffle过程简介"></a>Shuffle过程简介</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031619938.png" alt="image-20230303161924888"></p><h4 id="Map端的Shuffle过程"><a href="#Map端的Shuffle过程" class="headerlink" title="Map端的Shuffle过程"></a>Map端的Shuffle过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">•每个Map任务分配一个缓存</span><br><span class="line">•MapReduce默认100MB缓存</span><br><span class="line">•设置溢写比例0.8</span><br><span class="line">•分区默认采用哈希函数</span><br><span class="line">•排序是默认的操作</span><br><span class="line">•排序后可以合并（Combine）(用户定义了就执行，不是必须的)</span><br><span class="line">•合并不能改变最终结果(注意事项) 如求和，最大值</span><br><span class="line">•在Map任务全部结束之前进行归并(对之前多次溢写生成的磁盘文件，合并成大文件，文件里的数据是分区，排序了的)</span><br><span class="line">•归并得到一个大的文件，放在本地磁盘</span><br><span class="line">•文件归并时，如果溢写文件数量大于预定值（默认是3）则可以再次启动Combiner，少于3不需要</span><br><span class="line">•JobTracker会一直监测Map任务的执行，并通知Reduce任务来领取数据</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031620847.png" alt="image-20230303162007805" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">合并（Combine）和归并（Merge）的区别：</span><br><span class="line">两个键值对&lt;“a”,1&gt;和&lt;“a”,1&gt;，如果合并，会得到&lt;“a”,2&gt;，如果归并，会得到&lt;“a”,&lt;1,1&gt;&gt;</span><br></pre></td></tr></table></figure><h4 id="Reduce端的Shuffle过程"><a href="#Reduce端的Shuffle过程" class="headerlink" title="Reduce端的Shuffle过程"></a>Reduce端的Shuffle过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Reduce任务通过RPC向JobTracker询问Map任务是否已经完成，若完成，则领取数据</span><br><span class="line">•Reduce领取数据先放入缓存，来自不同Map机器，先归并，再合并，写入磁盘</span><br><span class="line">•多个溢写文件归并成一个或多个大文件，文件中的键值对是排序的</span><br><span class="line">•当数据很少时，不需要溢写到磁盘，直接在缓存中归并，然后输出给Reduce</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031620059.png" alt="image-20230303162053011" style="zoom:67%;"><h3 id="MapReduce应用程序执行过程"><a href="#MapReduce应用程序执行过程" class="headerlink" title="MapReduce应用程序执行过程"></a>MapReduce应用程序执行过程</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031622836.png" alt="image-20230303162200768"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">中间的输出是输出到磁盘不是hdfs！！！</span><br></pre></td></tr></table></figure><h2 id="实例分析：WordCount"><a href="#实例分析：WordCount" class="headerlink" title="实例分析：WordCount"></a>实例分析：WordCount</h2><h3 id="WordCount程序任务"><a href="#WordCount程序任务" class="headerlink" title="WordCount程序任务"></a>WordCount程序任务</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032219049.png" alt="image-20230303221931969" style="zoom:67%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032219887.png" alt="image-20230303221943836" style="zoom:67%;"><h3 id="WordCount设计思路"><a href="#WordCount设计思路" class="headerlink" title="WordCount设计思路"></a>WordCount设计思路</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 首先，需要检查WordCount程序任务是否可以采用MapReduce来实现</span><br><span class="line">• 其次，确定MapReduce程序的设计思路</span><br><span class="line">• 最后，确定MapReduce程序的执行过程</span><br></pre></td></tr></table></figure><h3 id="一个WordCount执行过程的实例"><a href="#一个WordCount执行过程的实例" class="headerlink" title="一个WordCount执行过程的实例"></a>一个WordCount执行过程的实例</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC7%E7%AB%A0-mapreduce.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230303222506161.png" alt="image-20230303222506161" style="zoom:67%;"><h4 id="用户没有定义Combiner时"><a href="#用户没有定义Combiner时" class="headerlink" title="用户没有定义Combiner时"></a>用户没有定义Combiner时</h4><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC7%E7%AB%A0-mapreduce.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230303222631369.png" alt="image-20230303222631369" style="zoom:67%;"><h4 id="用户有定义Combiner时"><a href="#用户有定义Combiner时" class="headerlink" title="用户有定义Combiner时"></a>用户有定义Combiner时</h4><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032227607.png" alt="image-20230303222701557" style="zoom:67%;"><h2 id="MapReduce的具体应用"><a href="#MapReduce的具体应用" class="headerlink" title="MapReduce的具体应用"></a>MapReduce的具体应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MapReduce可以很好地应用于各种计算问题</span><br><span class="line">• 关系代数运算（选择、投影、并、交、差、连接）</span><br><span class="line">• 分组与聚合运算</span><br><span class="line">• 矩阵-向量乘法</span><br><span class="line">• 矩阵乘法</span><br></pre></td></tr></table></figure><h3 id="用MapReduce实现关系的自然连接"><a href="#用MapReduce实现关系的自然连接" class="headerlink" title="用MapReduce实现关系的自然连接"></a>用MapReduce实现关系的自然连接</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032243239.png" alt="image-20230303224301191"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• 假设有关系R(A，B)和S(B,C)，对二者进行自然连接操作</span><br><span class="line">• 使用Map过程，把来自R的每个元组&lt;a,b&gt;转换成一个键值对&lt;b, &lt;R,a&gt;&gt;，其中的键就是属性B的值。把关系R包含到值中，这样做使得我们可以在Reduce阶段，只把那些来自R的元组和来自S的元组进行匹配。类似地，使用Map过程，把来自S的每个元组&lt;b,c&gt;，转换成一个键值对&lt;b,&lt;S,c&gt;&gt;</span><br><span class="line">• 所有具有相同B值的元组被发送到同一个Reduce进程中，Reduce进程的任务是，把来自关系R和S的、具有相同属性B值的元组进行合并</span><br><span class="line">• Reduce进程的输出则是连接后的元组&lt;a,b,c&gt;，输出被写到一个单独的输出文件中</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303032243094.png" alt="image-20230303224345042" style="zoom:67%;"><h2 id="MapReduce编程实践"><a href="#MapReduce编程实践" class="headerlink" title="MapReduce编程实践"></a>MapReduce编程实践</h2><h3 id="任务要求"><a href="#任务要求" class="headerlink" title="任务要求"></a>任务要求</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文件A的内容如下：</span><br><span class="line">China is my motherland</span><br><span class="line">I love China</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">文件B的内容如下：</span><br><span class="line">I am from China</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">期望结果如右侧所示：</span><br><span class="line">I 2</span><br><span class="line">is 1</span><br><span class="line">China 3</span><br><span class="line">my 1</span><br><span class="line">love 1</span><br><span class="line">am 1</span><br><span class="line">from 1</span><br><span class="line">motherland 1</span><br></pre></td></tr></table></figure><h3 id="编写Map处理逻辑"><a href="#编写Map处理逻辑" class="headerlink" title="编写Map处理逻辑"></a>编写Map处理逻辑</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Map输入类型为&lt;key,value&gt;</span><br><span class="line">•期望的Map输出类型为&lt;单词，出现次数&gt;</span><br><span class="line">•Map输入类型最终确定为&lt;Object,Text&gt;</span><br><span class="line">•Map输出类型最终确定为&lt;Text,IntWritable&gt;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>); </span><br><span class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text(); </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> </span></span><br><span class="line"><span class="function">IOException,InterruptedException</span>&#123; </span><br><span class="line">    StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString()); </span><br><span class="line">    <span class="keyword">while</span> (itr.hasMoreTokens())</span><br><span class="line">    &#123; </span><br><span class="line">        word.set(itr.nextToken()); </span><br><span class="line">        context.write(word,one); </span><br><span class="line">    &#125; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="编写Reduce处理逻辑"><a href="#编写Reduce处理逻辑" class="headerlink" title="编写Reduce处理逻辑"></a>编写Reduce处理逻辑</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">•在Reduce处理数据之前，Map的结果首先通过Shuffle阶段进行整理</span><br><span class="line">•Reduce阶段的任务：对输入数字序列进行求和</span><br><span class="line">•Reduce的输入数据为&lt;key,Iterable容器&gt;</span><br><span class="line">Reduce任务的输入数据：</span><br><span class="line">&lt;”I”,&lt;1,1&gt;&gt;</span><br><span class="line">&lt;”is”,1&gt;</span><br><span class="line">……</span><br><span class="line">&lt;”from”,1&gt;</span><br><span class="line">&lt;”China”,&lt;1,1,1&gt;&gt;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> </span></span><br><span class="line"><span class="class"><span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123; </span><br><span class="line">    <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable(); </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; </span></span></span><br><span class="line"><span class="function"><span class="params">values, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123; </span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>; </span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) </span><br><span class="line">        &#123;</span><br><span class="line">        sum += val.get(); </span><br><span class="line">        &#125;</span><br><span class="line">        result.set(sum); </span><br><span class="line">        context.write(key,result); </span><br><span class="line">&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="编写main方法"><a href="#编写main方法" class="headerlink" title="编写main方法"></a>编写main方法</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration(); <span class="comment">//程序运行时参数</span></span><br><span class="line">String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf,args).getRemainingArgs();</span><br><span class="line"><span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>)</span><br><span class="line">&#123; System.err.println(<span class="string">"Usage: wordcount &lt;in&gt; &lt;out&gt;"</span>);</span><br><span class="line">System.exit(<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line">Job job = <span class="keyword">new</span> Job(conf,<span class="string">"word count"</span>); <span class="comment">//设置环境参数</span></span><br><span class="line">job.setJarByClass(WordCount<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//设置整个程序的类名</span></span><br><span class="line">job.setMapperClass(MyMapper<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//添加MyMapper类</span></span><br><span class="line">job.setReducerClass(MyReducer<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//添加MyReducer类</span></span><br><span class="line">job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//设置输出类型</span></span><br><span class="line">job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">//设置输出类型</span></span><br><span class="line">FileInputFormat.addInputPath(job,<span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>])); <span class="comment">//设置输入文件</span></span><br><span class="line">FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>])); <span class="comment">//设置输出文件</span></span><br><span class="line">System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException; </span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span></span>&#123; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123; </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>); </span><br><span class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text(); </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123; </span><br><span class="line">    StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString()); </span><br><span class="line">    <span class="keyword">while</span> (itr.hasMoreTokens())&#123; </span><br><span class="line">word.set(itr.nextToken()); </span><br><span class="line">context.write(word,one); </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123; </span><br><span class="line"><span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable(); </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123; </span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>; </span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) </span><br><span class="line">        &#123; </span><br><span class="line">        sum += val.get(); </span><br><span class="line">        &#125; </span><br><span class="line">        result.set(sum); </span><br><span class="line">        context.write(key,result); </span><br><span class="line">        &#125; </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123; </span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration(); </span><br><span class="line">String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf,args).getRemainingArgs(); </span><br><span class="line"><span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>) </span><br><span class="line">&#123; </span><br><span class="line">    System.err.println(<span class="string">"Usage: wordcount &lt;in&gt; &lt;out&gt;"</span>); </span><br><span class="line">    System.exit(<span class="number">2</span>); </span><br><span class="line">&#125; </span><br><span class="line">    Job job = <span class="keyword">new</span> Job(conf,<span class="string">"word count"</span>); </span><br><span class="line">    job.setJarByClass(WordCount<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    job.setMapperClass(MyMapper<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    job.setReducerClass(MyReducer<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>; </span><br><span class="line">    FileInputFormat.addInputPath(job,<span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>])); </span><br><span class="line">    FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>])); </span><br><span class="line">System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="编译打包代码以及运行程序"><a href="#编译打包代码以及运行程序" class="headerlink" title="编译打包代码以及运行程序"></a>编译打包代码以及运行程序</h3><ul><li><a href="https://dblab.xmu.edu.cn/blog/hadoop-build-project-by-shell/" target="_blank" rel="external nofollow noopener noreferrer">使用命令行编译打包运行自己的MapReduce程序</a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实验步骤：</span><br><span class="line">•使用java编译程序，生成.class文件</span><br><span class="line">•将.class文件打包为jar包</span><br><span class="line">•运行jar包（需要启动Hadoop）</span><br><span class="line">•查看结果</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Hadoop 2.x 版本中的依赖 jar</span><br><span class="line">Hadoop 2.x 版本中 jar 不再集中在一个 hadoop-core*.jar 中，而是分成多个 jar，如使用 Hadoop 2.6.0 运行 WordCount 实例至少需要如下三个</span><br><span class="line">jar:</span><br><span class="line">•$HADOOP_HOME&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;hadoop-common-2.6.0.jar</span><br><span class="line">•$HADOOP_HOME&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduceclient-core-2.6.0.jar</span><br><span class="line">•$HADOOP_HOME&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;commons-cli-1.2.jar</span><br><span class="line"></span><br><span class="line">通过命令 hadoop classpath 可以得到运行 Hadoop 程序所需的全部classpath信息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">将 Hadoop 的 classhpath 信息添加到 CLASSPATH 变量中，在 ~&#x2F;.bashrc </span><br><span class="line">中增加如下几行：</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop export </span><br><span class="line">CLASSPATH&#x3D;$($HADOOP_HOME&#x2F;bin&#x2F;hadoop classpath):$CLASSPATH</span><br><span class="line">执行 source ~&#x2F;.bashrc 使变量生效，接着就可以通过 javac 命令编译</span><br><span class="line">WordCount.java</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303040040026.png" alt="image-20230304004002960" style="zoom:67%;"><h4 id="如何使用Eclipse编译运行MapReduce程序？"><a href="#如何使用Eclipse编译运行MapReduce程序？" class="headerlink" title="如何使用Eclipse编译运行MapReduce程序？"></a>如何使用Eclipse编译运行MapReduce程序？</h4><ul><li><a href="https://dblab.xmu.edu.cn/blog/hadoop-build-project-using-eclipse/" target="_blank" rel="external nofollow noopener noreferrer">使用Eclipse编译运行MapReduce程序</a></li></ul><h3 id="Hadoop中执行MapReduce任务的几种方式"><a href="#Hadoop中执行MapReduce任务的几种方式" class="headerlink" title="Hadoop中执行MapReduce任务的几种方式"></a>Hadoop中执行MapReduce任务的几种方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•Hadoop jar</span><br><span class="line">•Pig</span><br><span class="line">•Hive</span><br><span class="line">•Python</span><br><span class="line">•Shell脚本</span><br><span class="line">在解决问题的过程中，开发效率、执行效率都是要考虑的因素，不要太局限于某一种方法</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第6章 云数据库</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC6%E7%AB%A0-%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC6%E7%AB%A0-%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93.html</id>
    <published>2023-02-28T14:31:29.000Z</published>
    <updated>2023-03-03T06:11:18.498Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第6章-云数据库"><a href="#第6章-云数据库" class="headerlink" title="第6章 云数据库"></a>第6章 云数据库</h1><h2 id="云数据库概述"><a href="#云数据库概述" class="headerlink" title="云数据库概述"></a>云数据库概述</h2><h3 id="云计算是云数据库兴起的基础"><a href="#云计算是云数据库兴起的基础" class="headerlink" title="云计算是云数据库兴起的基础"></a>云计算是云数据库兴起的基础</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022043143.png" alt="image-20230302204336773"></p><h3 id="云数据库概念"><a href="#云数据库概念" class="headerlink" title="云数据库概念"></a>云数据库概念</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022045849.png" alt="image-20230302204509707" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">云数据库是部署和虚拟化在云计算环境中的数据库。云数据库是在云计算的大背景下发展起来的一种新兴的共享基础架构的方法，它极大地增强了数据库的存储能力，消除了人员、硬件、软件的重复配置，让软、硬件升级变得更加容易。云数据库具有高可扩展性、高可用性、采用多租形式和支持资源有效分发等特点。</span><br></pre></td></tr></table></figure><h3 id="云数据库的特性"><a href="#云数据库的特性" class="headerlink" title="云数据库的特性"></a>云数据库的特性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">（1）动态可扩展</span><br><span class="line">（2）高可用性</span><br><span class="line">（3）较低的使用代价</span><br><span class="line">（4）易用性</span><br><span class="line">（5）高性能</span><br><span class="line">（6）免维护</span><br><span class="line">（7）安全</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022048023.png" alt="image-20230302204851887" style="zoom:67%;"><h3 id="云数据库是个性化数据存储需求的理想选择"><a href="#云数据库是个性化数据存储需求的理想选择" class="headerlink" title="云数据库是个性化数据存储需求的理想选择"></a>云数据库是个性化数据存储需求的理想选择</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">企业类型不同，对于存储的需求也千差万别，而云数据库可以很好地满足不同企业的个性化存储需求：</span><br><span class="line">•首先，云数据库可以满足大企业的海量数据存储需求</span><br><span class="line">•其次，云数据库可以满足中小企业的低成本数据存储需求</span><br><span class="line">•另外，云数据库可以满足企业动态变化的数据存储需求</span><br><span class="line"></span><br><span class="line">到底选择自建数据库还是选择云数据库，取决于企业自身的具体需求</span><br><span class="line">•对于一些大型企业，目前通常采用自建数据库</span><br><span class="line">•对于一些财力有限的中小企业而言，IT预算比较有限，云数据库这种前期零投入、后期免维护的数据库服务，可以很好满足它们的需求</span><br></pre></td></tr></table></figure><h3 id="云数据库与其他数据库的关系"><a href="#云数据库与其他数据库的关系" class="headerlink" title="云数据库与其他数据库的关系"></a>云数据库与其他数据库的关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•从数据模型的角度来说，云数据库并非一种全新的数据库技术，而只是以服务的方式提供数据库功能</span><br><span class="line">•云数据库并没有专属于自己的数据模型，云数据库所采用的数据模型可以是关系数据库所使用的关系模型（微软的SQL Azure云数据库、阿里云RDS都采用了关系模型），也可以是NoSQL数据库所使用的非关系模型（Amazon Dynamo云数据库采用的是“键&#x2F;值”存储）</span><br><span class="line">•同一个公司也可能提供采用不同数据模型的多种云数据库服务</span><br><span class="line">•许多公司在开发云数据库时，后端数据库都是直接使用现有的各种关系数据库或NoSQL数据库产品</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022054765.png" alt="image-20230302205402630" style="zoom:67%;"><h2 id="云数据库产品"><a href="#云数据库产品" class="headerlink" title="云数据库产品"></a>云数据库产品</h2><h3 id="云数据库厂商概述"><a href="#云数据库厂商概述" class="headerlink" title="云数据库厂商概述"></a>云数据库厂商概述</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022056265.png" alt="image-20230302205607114"></p><h3 id="Amazon的云数据库产品"><a href="#Amazon的云数据库产品" class="headerlink" title="Amazon的云数据库产品"></a>Amazon的云数据库产品</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Amazon是云数据库市场的先行者。Amazon除了提供著名的S3</span><br><span class="line">存储服务和EC2计算服务以外，还提供基于云的数据库服务：</span><br><span class="line">•Amazon RDS：云中的关系数据库</span><br><span class="line">•Amazon SimpleDB：云中的键值数据库</span><br><span class="line">•Amazon DynamoDB：云中的NoSQL数据库</span><br><span class="line">•Amazon Redshift：云中的数据仓库</span><br><span class="line">•Amazon ElastiCache：云中的分布式内存缓存</span><br></pre></td></tr></table></figure><h3 id="Google的云数据库产品"><a href="#Google的云数据库产品" class="headerlink" title="Google的云数据库产品"></a>Google的云数据库产品</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•Google Cloud SQL是谷歌公司推出的基于MySQL的云数据库</span><br><span class="line">•使用Cloud SQL，所有的事务都在云中，并由谷歌管理，用户不需要配置或者排查错误</span><br><span class="line">•谷歌还提供导入或导出服务，方便用户将数据库带进或带出云</span><br><span class="line">•谷歌使用用户非常熟悉的MySQL，带有JDBC支持（适用于基于Java的App Engine应用）和DB-API支持（适用于基于Python的App Engine应用）的传统MySQL数据库环境，因此，多数应用程序不需过多调试即可运行，数据格式对于大多数开发者和管理员来说也是非常熟悉的</span><br><span class="line">•Google Cloud SQL还有一个好处就是与Google App Engine集成</span><br></pre></td></tr></table></figure><h3 id="Microsoft的云数据库产品"><a href="#Microsoft的云数据库产品" class="headerlink" title="Microsoft的云数据库产品"></a>Microsoft的云数据库产品</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SQL Azure具有以下特性：</span><br><span class="line">•属于关系型数据库：支持使用TSQL（Transact Structured Query Language）来管理、创建和操作云数据库</span><br><span class="line">•支持存储过程：它的数据类型、存储过程和传统的SQL Server具有很大的相似性，因此，应用可以在本地进行开发，然后部署到云平台上</span><br><span class="line">•支持大量数据类型：包含了几乎所有典型的SQL Server 2008的数据类型</span><br><span class="line">•支持云中的事务：支持局部事务，但是不支持分布式事务</span><br></pre></td></tr></table></figure><h3 id="其他云数据库产品"><a href="#其他云数据库产品" class="headerlink" title="其他云数据库产品"></a>其他云数据库产品</h3><h2 id="云数据库系统架构"><a href="#云数据库系统架构" class="headerlink" title="云数据库系统架构"></a>云数据库系统架构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UMP（Unified MySQL Platform）是由阿里集团核心系统数据库团队设计与实现的，提供低成本和高性能的MySQL云数据服务。</span><br></pre></td></tr></table></figure><h3 id="UMP系统概述"><a href="#UMP系统概述" class="headerlink" title="UMP系统概述"></a>UMP系统概述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•UMP系统是低成本和高性能的MySQL云数据库方案</span><br><span class="line">总的来说，UMP系统架构设计遵循了以下原则：</span><br><span class="line">•保持单一的系统对外入口，并且为系统内部维护单一的资源池</span><br><span class="line">•消除单点故障，保证服务的高可用性</span><br><span class="line">•保证系统具有良好的可伸缩，能够动态地增加、删减计算与存储节点</span><br><span class="line">•保证分配给用户的资源也是弹性可伸缩的，资源之间相互隔离，确保应用和数据安全</span><br></pre></td></tr></table></figure><h3 id="UMP系统架构"><a href="#UMP系统架构" class="headerlink" title="UMP系统架构"></a>UMP系统架构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">UMP系统中的角 LVS 色包括：</span><br><span class="line">•Controller服务器</span><br><span class="line">•Proxy服务器</span><br><span class="line">•Agent服务器</span><br><span class="line">•Web控制台</span><br><span class="line">•日志分析服务器</span><br><span class="line">•信息统计服务器</span><br><span class="line">•愚公系统</span><br><span class="line"></span><br><span class="line">依赖的开源组件包括：</span><br><span class="line">•Mnesia</span><br><span class="line">•LVS</span><br><span class="line">•RabbitMQ</span><br><span class="line">•ZooKeeper</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303022103037.png" alt="image-20230302210343710" style="zoom:67%;"><h4 id="Mnesia"><a href="#Mnesia" class="headerlink" title="Mnesia"></a>Mnesia</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Mnesia是一个分布式数据库管理系统</span><br><span class="line">•Mnesia支持事务，支持透明的数据分片，利用两阶段锁实现分布式事务，可以线性扩展到至少50个节点</span><br><span class="line">•Mnesia的数据库模式(schema)可在运行时动态重配置，表能被迁移或复制到多个节点来改进容错性</span><br><span class="line">•Mnesia的这些特性，使其在开发云数据库时被用来提供分布式数据库服务</span><br></pre></td></tr></table></figure><h4 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•RabbitMQ是一个工业级的消息队列产品（功能类似于IBM公司的消息队列产品IBM Websphere MQ），作为消息传输中间件来使用，可以实现可靠的消息传送</span><br><span class="line">•UMP集群中各个节点之间的通信，不需要建立专门的连接，都是通过读写队列消息来实现的</span><br></pre></td></tr></table></figure><h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Zookeeper是高效和可靠的协同工作系统，提供分布式锁之类的基本服务（比如统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等），用于构建分布式应用，减轻分布式应用程序所承担的协调任务</span><br><span class="line"></span><br><span class="line">在UMP系统中，Zookeeper主要发挥三个作用：</span><br><span class="line">•作为全局的配置服务器</span><br><span class="line">•提供分布式锁（选出一个集群的“总管”）</span><br><span class="line">•监控所有MySQL实例</span><br></pre></td></tr></table></figure><h4 id="LVS"><a href="#LVS" class="headerlink" title="LVS"></a>LVS</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•LVS(Linux Virtual Server)即Linux虚拟服务器，是一个虚拟的服务器集群系统</span><br><span class="line">•UMP系统借助于LVS来实现集群内部的负载均衡</span><br><span class="line">•LVS集群采用IP负载均衡技术和基于内容请求分发技术</span><br><span class="line">•调度器是LVS集群系统的唯一入口点，调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器</span><br><span class="line">•整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序</span><br></pre></td></tr></table></figure><h4 id="Controller服务器"><a href="#Controller服务器" class="headerlink" title="Controller服务器"></a>Controller服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•Controller服务器向UMP集群提供各种管理服务，实现集群成员管理、元数据存储、MySQL实例管理、故障恢复、备份、迁移、扩容等功能</span><br><span class="line">•Controller服务器上运行了一组Mnesia分布式数据库服务，其中存储了各种系统元数据，主要包括集群成员、用户的配置和状态信息，以及用户名到后端MySQL实例地址的映射关系（或称为“路由表”）等</span><br><span class="line">•当其它服务器组件需要获取用户数据时，可以向Controller服务器发送请求获取数据</span><br><span class="line">•为了避免单点故障，保证系统的高可用性，UMP系统中部署了多台Controller服务器，然后，由Zookeeper的分布式锁功能来帮助选出一个“总管”，负责各种系统任务的调度和监控</span><br></pre></td></tr></table></figure><h4 id="Web控制台"><a href="#Web控制台" class="headerlink" title="Web控制台"></a>Web控制台</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Web控制台向用户提供系统管理界面</span><br></pre></td></tr></table></figure><h4 id="Proxy服务器"><a href="#Proxy服务器" class="headerlink" title="Proxy服务器"></a>Proxy服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Proxy服务器向用户提供访问MySQL数据库的服务，它完全实现了MySQL协议，用户可以使用已有的MySQL客户端连接到Proxy服务器，Proxy服务器通过用户名获取到用户的认证信息、资源配额的限制(例如QPS、IOPS（I&#x2F;O Per Second）、最大连接数等)，以及后台MySQL实例的地址，然后，用户的SQL查询请求会被转发到相应的MySQL实例上。</span><br><span class="line">除了数据路由的基本功能外，Proxy服务器中还实现了很多重要的功能，主要包括屏蔽MySQL实例故障、读写分离、分库分表、资源隔离、记录用户访问日志等</span><br></pre></td></tr></table></figure><h4 id="Agent服务器"><a href="#Agent服务器" class="headerlink" title="Agent服务器"></a>Agent服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Agent服务器部署在运行MySQL进程的机器上，用来管理每台物理机上的MySQL实例，执行主从切换、创建、删除、备份、迁移等操作，同时，还负责收集和分析MySQL进程的统计信息、慢查询日志（Slow Query Log）和bin-log</span><br></pre></td></tr></table></figure><h4 id="日志分析服务器"><a href="#日志分析服务器" class="headerlink" title="日志分析服务器"></a>日志分析服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">日志分析服务器存储和分析Proxy服务器传入的用户访问日志，并支持实时查询一段时间内的慢日志和统计报表</span><br></pre></td></tr></table></figure><h4 id="信息统计服务器"><a href="#信息统计服务器" class="headerlink" title="信息统计服务器"></a>信息统计服务器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">信息统计服务器定期将采集到的用户的连接数、QPS数值以及MySQL实例的进程状态用RRDtool进行统计，可以在 Web界面上可视化展示统计结果，也可以把统计结果作为今后实现弹性的资源分配和自动化的MySQL实例迁移的依据</span><br></pre></td></tr></table></figure><h4 id="愚公系统"><a href="#愚公系统" class="headerlink" title="愚公系统"></a>愚公系统</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">愚公系统是一个全量复制结合bin-log分析进行增量复制的工具，可以实现在不停机的情况下动态扩容、缩容和迁移</span><br></pre></td></tr></table></figure><h3 id="UMP系统功能"><a href="#UMP系统功能" class="headerlink" title="UMP系统功能"></a>UMP系统功能</h3><h2 id="Amazon-AWS和云数据库"><a href="#Amazon-AWS和云数据库" class="headerlink" title="Amazon AWS和云数据库"></a>Amazon AWS和云数据库</h2><h2 id="微软云数据库SQL-Azure"><a href="#微软云数据库SQL-Azure" class="headerlink" title="微软云数据库SQL Azure"></a>微软云数据库SQL Azure</h2><h2 id="云数据库实践"><a href="#云数据库实践" class="headerlink" title="云数据库实践"></a>云数据库实践</h2><h3 id="阿里云RDS简介"><a href="#阿里云RDS简介" class="headerlink" title="阿里云RDS简介"></a>阿里云RDS简介</h3><h3 id="RDS中的概念"><a href="#RDS中的概念" class="headerlink" title="RDS中的概念"></a>RDS中的概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RDS实例，是用户购买RDS服务的基本单位。在实例中：</span><br><span class="line">• 可以创建多个数据库</span><br><span class="line">• 可以使用常见的数据库客户端连接、管理及使用数据</span><br><span class="line">• 可以通过RDS管理控制台或OPEN API来创建、修改和删除数据库</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> RDS数据库，是用户在一个实例下创建的逻辑单元</span><br><span class="line">• 一个实例可以创建多个数据库，在实例内数据库命名唯一，所有数据库都会共享该实例下的资源，如CPU、内存、磁盘容量等</span><br><span class="line">• RDS不支持使用标准的SQL语句或客户端工具创建数据库，必须使用OPEN API或RDS管理控制台进行操作</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> 地域指的是用户所购买的RDS实例的服务器所处的地理位置RDS目前支持杭州、青岛、北京、深圳和香港五个地域，服务品质完全相同。用户可以在购买RDS实例时指定地域，购买实例后暂不支持更改</span><br><span class="line"> RDS可用区是指在同一地域下，电力、网络隔离的物理区域，可用区之间内网互通，可用区内网络延时更小，不同可用区之间故障隔离</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RDS可用区又分为单可用区和多可用区</span><br><span class="line">• 单可用区是指RDS实例的主备节点位于相同的可用区，它可以有效控制云产品间的网络延迟</span><br><span class="line">• 多可用区是指RDS实例的主备节点位于不同的可用区，当主节点所在可用区出现故障（如机房断电等），RDS进行主备切换后，会切换到备节点所在的可用区继续提供服务。多可用区的RDS轻松实现了同城容灾</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> 磁盘容量是用户购买RDS实例时，所选择购买的磁盘大小实例所占用的磁盘容量，除了存储表格数据外，还有实例正常运行所需要的空间，如系统数据库、数据库回滚日志、重做日志、索引等</span><br><span class="line"> RDS连接数，是应用程序可以同时连接到RDS实例的连接数量</span><br><span class="line">• 任意连接到RDS实例的连接均计算在内，与应用程序或者网站能够支持的最大用户数无关</span><br><span class="line">• 用户在购买RDS实例时所选择的内存大小决定了该实例的最大连接数</span><br></pre></td></tr></table></figure><h3 id="购买和使用RDS数据库"><a href="#购买和使用RDS数据库" class="headerlink" title="购买和使用RDS数据库"></a>购买和使用RDS数据库</h3><h4 id="购买RDS实例"><a href="#购买RDS实例" class="headerlink" title="购买RDS实例"></a>购买RDS实例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">进入RDS页面后，点击“立即购买”，即</span><br><span class="line">可跳到下图的购买页面</span><br><span class="line">如果已经购买阿里云服务器ECS（Elastic </span><br><span class="line">Compute Service），若选择和ECS所在地</span><br><span class="line">域相同，ECS和RDS之间可以以内网方式</span><br><span class="line">访问</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031348841.png" alt="image-20230303134824745"></p><h4 id="管理RDS"><a href="#管理RDS" class="headerlink" title="管理RDS"></a>管理RDS</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">购买RDS实例成功后，可以通过管理控制台，查看已开通的产品与服务。点击云数据库RDS 进入管理界面如下图。我们可以创建新实例、对已购买实例进行管理、续费和升级操作</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031350779.png" alt="image-20230303135035722"></p><h4 id="管理RDS实例"><a href="#管理RDS实例" class="headerlink" title="管理RDS实例"></a>管理RDS实例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">•点击已购买RDS实例的管理操作，可以查看该实例</span><br><span class="line">的基本信息如下图</span><br><span class="line">• 一个实例可以创建多个数据库，在实例内数据库</span><br><span class="line">命名唯一，所有数据库都会共享该实例下的资源</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031352019.png" alt="image-20230303135228962" style="zoom:67%;"><h4 id="新建RDS账号"><a href="#新建RDS账号" class="headerlink" title="新建RDS账号"></a>新建RDS账号</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 点击“创建新账号”按钮后，可创建新的RDS账号，并选定需绑定的数据库，以及输入账号密码和账号类型（读写权限）等信息</span><br><span class="line">• MySQL实例支持最多创建50个账号，SQL Server实例支持最多创建20个账号</span><br><span class="line">• 创建完RDS账号后，还可以对RDS账号进行重置密码和修改操作</span><br></pre></td></tr></table></figure><h4 id="新建RDS数据库"><a href="#新建RDS数据库" class="headerlink" title="新建RDS数据库"></a>新建RDS数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 点击“数据库管理”按钮后，可查看数据库基本信息，并可对数据库进行创建、删除（需至少有1个数据库）的操作</span><br><span class="line">• 点击“增加数据库”后，在弹出的界面中填写数据库相关信息，提交后即可生效</span><br><span class="line">• 此外，RDS数据库还可以是自建数据库迁移来的或是从其他RDS实例中迁入的</span><br></pre></td></tr></table></figure><h4 id="连接RDS数据库"><a href="#连接RDS数据库" class="headerlink" title="连接RDS数据库"></a>连接RDS数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 如果是在阿里云服务器ECS上连接RDS数据库，就选择内网模式；</span><br><span class="line">• 如果是在其他服务器上连接RDS使用，就选择外网模式，在控制台的右上角有切换方式</span><br><span class="line">• 从本地对云端的RDS数据库进行远程访问时使用外网模式，需要在“安全控制-&gt;白名单设置”位置填入本地机器的外网IP，从而让RDS数据库允许我们的本地机器访问</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> 使用客户端MySQL-Front 访问</span><br><span class="line"> 使用数据库管理工具Navicat_MySQL</span><br><span class="line"> 使用MySQL命令登录</span><br><span class="line">命令格式如下：</span><br><span class="line">mysql -u user_name -h yuqianli.mysql.rds.aliyuncs.com -P3306 -pxxxx</span><br><span class="line"> 使用阿里云控制台iDB Cloud访问</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031401865.png" alt="image-20230303140131813" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RDS连接地址以及端口不需要再输入，只需在“用户名”中输入数据库的账号，在“密码”栏中输入数据库账号的密码，便可以登录RDS。</span><br></pre></td></tr></table></figure><h4 id="操作RDS数据库"><a href="#操作RDS数据库" class="headerlink" title="操作RDS数据库"></a>操作RDS数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 连接RDS数据库后，对数据库的操作与直接对本机MySQL数据库操作无异。iDB Cloud登录数据库后的界面如下图所示：</span><br><span class="line">• 在“iDB Cloud登录数据库界面”（如右图）</span><br><span class="line">的顶端可以看到iDB Cloud提供以下三种创建</span><br><span class="line">表的方法：</span><br><span class="line">可视化界面</span><br><span class="line">SQL窗口</span><br><span class="line">命令窗口</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031403043.png" alt="image-20230303140347992"></p><h3 id="将本地数据库迁移到云端RDS数据库"><a href="#将本地数据库迁移到云端RDS数据库" class="headerlink" title="将本地数据库迁移到云端RDS数据库"></a>将本地数据库迁移到云端RDS数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• 假设我们有一个本地应用程序，它使用本地的MySQL数据库存取和管理数据。现在，我们打算把本地MySQL数据库中的数据全部迁移到远程的阿里云RDS数据库中，本地应用程序不迁移（依然运行在本地），但是，我们希望本地应用程序使用云端的RDS数据库服务进行数据存取和管</span><br><span class="line">理。为此，需要执行以下两步操作：</span><br><span class="line">第一步：把本地数据库迁移到云端的RDS数据库</span><br><span class="line">第二步：修改本地应用程序配置，使用RDS数据库服务</span><br></pre></td></tr></table></figure><h4 id="如何把本地数据库迁移到云端的RDS数据库？"><a href="#如何把本地数据库迁移到云端的RDS数据库？" class="headerlink" title="如何把本地数据库迁移到云端的RDS数据库？"></a>如何把本地数据库迁移到云端的RDS数据库？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第1步：在本地数据库中创建一个迁移账号</span><br><span class="line">第2步：设置迁移账号权限</span><br><span class="line">第3步：确认本地数据库中的配置文件是否正确，需要确认本地数据</span><br><span class="line">库中的MySQL配置文件my.cnf</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303031405291.png" alt="image-20230303140559230"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第4步：登录本地数据库，通过命令查看是否为“ROW”模式</span><br><span class="line">第5步：在RDS管理控制台对应的实例页面，点击“将数据迁移至RDS”按钮，在弹出的页面中，填写待迁移的本地数据库连接地址、数据库连接端口、数据库账号、数据库密码，即可完成从本地迁移到云端</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第5章 NoSQL数据库</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.html</id>
    <published>2023-02-28T14:31:10.000Z</published>
    <updated>2023-03-02T12:37:26.982Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第5章-NoSQL数据库"><a href="#第5章-NoSQL数据库" class="headerlink" title="第5章 NoSQL数据库"></a>第5章 NoSQL数据库</h1><h2 id="NoSQL简介"><a href="#NoSQL简介" class="headerlink" title="NoSQL简介"></a>NoSQL简介</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010035462.png" alt="image-20230301003541415"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">通常，NoSQL数据库具有以下几个特点：</span><br><span class="line">（1）灵活的可扩展性</span><br><span class="line">（2）灵活的数据模型</span><br><span class="line">（3）与云计算紧密融合</span><br></pre></td></tr></table></figure><h2 id="NoSQL兴起的原因"><a href="#NoSQL兴起的原因" class="headerlink" title="NoSQL兴起的原因"></a>NoSQL兴起的原因</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系数据库已经无法满足Web2.0的需求。主要表现在以下几个方面：</span><br><span class="line">（1）无法满足海量数据的管理需求</span><br><span class="line">（2）无法满足数据高并发的需求</span><br><span class="line">（3）无法满足高可扩展性和高可用性的需求</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（主从机制实现读写负载分离，同步或异步传输）--到---&gt;分库分表</span><br><span class="line">MySQL集群是否可以完全解决问题？</span><br><span class="line">•复杂性：部署、管理、配置很复杂</span><br><span class="line">•数据库复制：MySQL主备之间采用复制方式，只能是异步复制，当主库压力较大时可能产生较大延迟，主备切换可能会丢失最后一部分更新事务，这时往往需要人工介入，备份和恢复不方便</span><br><span class="line">•扩容问题：如果系统压力过大需要增加新的机器，这个过程涉及数据重新划分，整个过程比较复杂，且容易出错</span><br><span class="line">•动态数据迁移问题：如果某个数据库组压力过大，需要将其中部分数据迁移出去，迁移过程需要总控节点整体协调，以及数据库节点的配合。这个过程很难做到自动化</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010045076.png" alt="image-20230301004517003"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">“One size fits all”模式很难适用于截然不同的业务场景</span><br><span class="line">•关系模型作为统一的数据模型既被用于数据分析，也被用于在线业务。但这两者一个强调高吞吐，一个强调低延时，已经演化出完全不同的架构。用同一套模型来抽象显然是不合适的</span><br><span class="line">•Hadoop就是针对数据分析</span><br><span class="line">•MongoDB、Redis等是针对在线业务，两者都抛弃了关系模型</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系数据库的关键特性包括完善的事务机制和高效的查询机制。但是，关系数据库引以为傲的两个关键特性，到了Web2.0时代却成了鸡肋，主要表现在以下几个方面：</span><br><span class="line">（1）Web2.0网站系统通常不要求严格的数据库事务(银行交易)</span><br><span class="line">（2）Web2.0并不要求严格的读写实时性(微博发布后是否快速可见)</span><br><span class="line">（3）Web2.0通常不包含大量复杂的SQL查询（去结构化，存储空间换取更好的查询性能）</span><br></pre></td></tr></table></figure><h2 id="NoSQL与关系数据库的比较"><a href="#NoSQL与关系数据库的比较" class="headerlink" title="NoSQL与关系数据库的比较"></a>NoSQL与关系数据库的比较</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ACID，是指数据库管理系统（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RDBMS:关系型数据库管理系统</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228233539924.png" alt="image-20230228233539924" style="zoom:80%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282336998.png" alt="image-20230228233645936"></p><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228233700240.png" alt="image-20230228233700240" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">总结</span><br><span class="line">（1）关系数据库</span><br><span class="line">优势：以完善的关系代数理论作为基础，有严格的标准，支持事务ACID四性，借助索引机制可以实现高效的查询，技术成熟，有专业公司的技术支持</span><br><span class="line">劣势：可扩展性较差，无法较好支持海量数据存储，数据模型过于死板、无法较好支持Web2.0应用，事务机制影响了系统的整体性能等</span><br><span class="line">（2）NoSQL数据库</span><br><span class="line">优势：可以支持超大规模数据存储，灵活的数据模型可以很好地支持Web2.0应用，具有强大的横向扩展能力等</span><br><span class="line">劣势：缺乏数学理论基础，复杂查询性能不高，大都不能实现事务强一致性，很难实现数据完整性，技术尚不成熟，缺乏专业团队的技术支持，维护较困难等</span><br><span class="line"></span><br><span class="line">关系数据库和NoSQL数据库各有优缺点，彼此无法取代</span><br><span class="line">•关系数据库应用场景：电信、银行等领域的关键业务系统，需要保证强事务一致性</span><br><span class="line">•NoSQL数据库应用场景：互联网企业、传统企业的非关键业务（比如数据分析）</span><br><span class="line"></span><br><span class="line">采用混合架构</span><br><span class="line">•案例：亚马逊公司就使用不同类型的数据库来支撑它的电子商务应用</span><br><span class="line">•对于“购物篮”这种临时性数据，采用键值存储会更加高效</span><br><span class="line">•当前的产品和订单信息则适合存放在关系数据库中</span><br><span class="line">•大量的历史订单信息则适合保存在类似MongoDB的文档数据库中</span><br></pre></td></tr></table></figure><h2 id="NoSQL的四大类型"><a href="#NoSQL的四大类型" class="headerlink" title="NoSQL的四大类型"></a>NoSQL的四大类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NoSQL数据库虽然数量众多，但是，归结起来，典型的NoSQL数据库通常包括键值数据库、列族数据库、文档数据库和图形数据库</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228234144829.png" alt="image-20230228234144829" style="zoom:80%;"><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228234215303.png" alt="image-20230228234215303" style="zoom:80%;"><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228234306458.png" alt="image-20230228234306458" style="zoom:80%;"><h3 id="键值数据库"><a href="#键值数据库" class="headerlink" title="键值数据库"></a>键值数据库</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282344758.png" alt="image-20230228234416688" style="zoom:80%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282345799.png" alt="image-20230228234538745"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库管理系统（Relational Database Management System）RDBMS</span><br></pre></td></tr></table></figure><h3 id="列族数据库"><a href="#列族数据库" class="headerlink" title="列族数据库"></a>列族数据库</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228234918950.png" alt="image-20230228234918950" style="zoom:80%;"><h3 id="文档数据库"><a href="#文档数据库" class="headerlink" title="文档数据库"></a>文档数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">“文档”其实是一个数据记录，这个记录能够对包含的数据类型和内容进行“自我描述”。XML文档、HTML文档和JSON 文档就属于这一类。SequoiaDB就是使用JSON格式的文档数据库，它的存储的数据是这样的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282351530.png" alt="image-20230228235133485"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282352815.png" alt="image-20230228235234768"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系数据库：</span><br><span class="line">必须有schema信息才能理解数据的含义</span><br><span class="line">学生（学号，姓名，性别，年龄，系，年级）</span><br><span class="line">（1001，张三，男，20，计算机，2002）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•数据是不规则的，每一条记录包含了所有的有关“SequoiaDB”的信息而没有任何外部的引用，这条记录就是“自包含”的</span><br><span class="line">•这使得记录很容易完全移动到其他服务器，因为这条记录的所有信息都包含在里面了，不需要考虑还有信息在别的表没有一起迁移走</span><br><span class="line">•同时，因为在移动过程中，只有被移动的那一条记录（文档）需要操作，而不像关系型中每个有关联的表都需要锁住来保证一致性，这样一来ACID的保证就会变得更快速，读写的速度也会有很大的提升</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282357394.png" alt="image-20230228235722334"></p><h3 id="图形数据库"><a href="#图形数据库" class="headerlink" title="图形数据库"></a>图形数据库</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230228235744083.png" alt="image-20230228235744083" style="zoom:80%;"><h2 id="NoSQL的三大基石"><a href="#NoSQL的三大基石" class="headerlink" title="NoSQL的三大基石"></a>NoSQL的三大基石</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202302282359330.png" alt="image-20230228235943292"></p><h3 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">C（Consistency）：一致性，是指任何一个读操作总是能够读到之前完成的写操作的结果，也就是在分布式环境中，多点的数据是一致的，或者说，所有节点在同一时间具有相同的数据</span><br><span class="line">A:（Availability）：可用性，是指快速获取数据，可以在确定的时间内返回操作结果，保证每个请求不管成功或者失败都有响应；</span><br><span class="line">P（Tolerance of Network Partition）：分区容忍性，是指当出现网络分区的情况时（即系统中的一部分节点无法和其他节点进行通信），分离的系统也能够正常运行，也就是说，系统中任意信息的丢失或失败不会影响系统的继续运作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CAP理论告诉我们，一个分布式系统不可能同时满足一致性、可用性和分区容忍性这三个需求，最多只能同时满足其中两个，正所谓“鱼和熊掌不可兼得”。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010003922.png" alt="image-20230301000322877" style="zoom:67%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010123156.png" alt="image-20230301012357087" style="zoom: 50%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010124239.png" alt="image-20230301012418187" style="zoom:50%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010124061.png" alt="image-20230301012434006" style="zoom:50%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">当处理CAP的问题时，可以有几个明显的选择：</span><br><span class="line">1.CA：也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最</span><br><span class="line">简单的做法是把所有与事务相关的内容都放到同一台机器上。很显然，这种</span><br><span class="line">做法会严重影响系统的可扩展性。传统的关系数据库（MySQL、SQL Server</span><br><span class="line">和PostgreSQL），都采用了这种设计原则，因此，扩展性都比较差</span><br><span class="line">2.CP：也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当</span><br><span class="line">出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间</span><br><span class="line">就无法对外提供服务</span><br><span class="line">3.AP：也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允</span><br><span class="line">许系统返回不一致的数据</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010127593.png" alt="image-20230301012745550" style="zoom:50%;"><h3 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h3> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">说起BASE（Basically Availble, Soft-state, Eventual consistency），</span><br><span class="line">不得不谈到ACID。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303010131759.png" alt="image-20230301013119713" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">一个数据库事务具有ACID四性：</span><br><span class="line">A（Atomicity）：原子性，是指事务必须是原子工作单元，对于其数</span><br><span class="line">据修改，要么全都执行，要么全都不执行</span><br><span class="line">C（Consistency）：一致性，是指事务在完成时，必须使所有的数据</span><br><span class="line">都保持一致状态</span><br><span class="line">I（Isolation）：隔离性，是指由并发事务所做的修改必须与任何其它</span><br><span class="line">并发事务所做的修改隔离</span><br><span class="line">D（Durability）：持久性，是指事务完成之后，它对于系统的影响是</span><br><span class="line">永久性的，该修改即使出现致命的系统故障也将一直保持</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BASE的基本含义是基本可用（Basically Availble）、软状态（Soft state）和最终一致性（Eventual consistency）：</span><br><span class="line">  基本可用</span><br><span class="line">基本可用，是指一个分布式系统的一部分发生问题变得不可用时，其他部分仍然可以正常使用，也就是允许分区失败的情形出现</span><br><span class="line">  软状态</span><br><span class="line">“软状态（soft-state）”是与“硬状态（hard-state）”相对应的一种提法。数据库保存的数据是“硬状态”时，可以保证数据一致性，即保证数据一直是正确的。“软状态”是指状态可以有一段时间不同步，具有一定的滞后性</span><br><span class="line">  最终一致性</span><br><span class="line">一致性的类型包括强一致性和弱一致性，二者的主要区别在于高并发的数据访问操作下，后续操作是否能够获取最新的数据。对于强一致性而言，当执行完一次更新操作后，后续的其他读操作就可以保证读到更新后的最新数据；反之，如果不能保证后续访问读到的都是更新后的最新数据，那么就是弱一致性。而最终一致性只不过是弱一致性的一种特例，允许后续的访问操作可以暂时读不到更新后的数据，但是经过一段时间之后，必须最终读到更新后的数据。</span><br><span class="line">最常见的实现最终一致性的系统是DNS（域名系统）。一个域名更新操作根据配置的形式被分发出去，并结合有过期机制的缓存；最终所有的客户端可以看到最新的值。</span><br></pre></td></tr></table></figure><h3 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">最终一致性根据更新数据后各进程访问到数据的时间和方式的不同，</span><br><span class="line">  又可以区分为：</span><br><span class="line">  因果一致性：如果进程A通知进程B它已更新了一个数据项，那么进程B</span><br><span class="line">的后续访问将获得A写入的最新值。而与进程A无因果关系的进程C的访问</span><br><span class="line">，仍然遵守一般的最终一致性规则</span><br><span class="line">  “读己之所写”一致性：可以视为因果一致性的一个特例。当进程A自</span><br><span class="line">己执行一个更新操作之后，它自己总是可以访问到更新过的值，绝不会看</span><br><span class="line">到旧值</span><br><span class="line">  单调读一致性：如果进程已经看到过数据对象的某个值，那么任何后续</span><br><span class="line">访问都不会返回在那个值之前的值</span><br><span class="line">  会话一致性：它把访问存储系统的进程放到会话（session）的上下文中</span><br><span class="line">，只要会话还存在，系统就保证“读己之所写”一致性。如果由于某些失</span><br><span class="line">败情形令会话终止，就要建立新的会话，而且系统保证不会延续到新的会</span><br><span class="line">话</span><br><span class="line">  单调写一致性：系统保证来自同一个进程的写操作顺序执行。系统必须</span><br><span class="line">保证这种程度的一致性，否则就非常难以编程了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如何实现各种类型的一致性？</span><br><span class="line">对于分布式数据系统：</span><br><span class="line">•N — 数据复制的份数</span><br><span class="line">•W — 更新数据是需要保证写完成的节点数</span><br><span class="line">•R — 读取数据的时候需要读取的节点数</span><br><span class="line">如果W+R&gt;N，写的节点和读的节点重叠，则是强一致性。例如对于典型的一主</span><br><span class="line">一备同步复制的关系型数据库，N&#x3D;2,W&#x3D;2,R&#x3D;1，则不管读的是主库还是备库的</span><br><span class="line">数据，都是一致的。一般设定是R＋W &#x3D; N+1，这是保证强一致性的最小设定如果W+R&lt;&#x3D;N，则是弱一致性。例如对于一主一备异步复制的关系型数据库，N&#x3D;2,W&#x3D;1,R&#x3D;1，则如果读的是备库，就可能无法读取主库已经更新过的数据，所以是弱一致性</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">对于分布式系统，为了保证高可用性，一般设置N&gt;&#x3D;3。不同的N,W,R组合，是</span><br><span class="line">在可用性和一致性之间取一个平衡，以适应不同的应用场景。</span><br><span class="line">•如果N&#x3D;W,R&#x3D;1，任何一个写节点失效，都会导致写失败，因此可用性会降低，</span><br><span class="line">但是由于数据分布的N个节点是同步写入的，因此可 以保证强一致性。</span><br><span class="line">实例：HBase是借助其底层的HDFS来实现其数据冗余备份的。HDFS采用的就</span><br><span class="line">是强一致性保证。在数据没有完全同步到N个节点前，写操作是不会返回成功的</span><br><span class="line">。也就是说它的W＝N，而读操作只需要读到一个值即可，也就是说它R＝1。</span><br><span class="line">•像Voldemort，Cassandra和Riak这些类Dynamo的系统，通常都允许用户按需要设置N，R，W三个值，即使是设置成W＋R&lt;&#x3D; N也是可以的。也就是说他允</span><br><span class="line">许用户在强一致性和最终一致性之间自由选择。而在用户选择了最终一致性，或</span><br><span class="line">者是W&lt;N的强一致性时，则总会出现一段“各个节点数据不同步导致系统处理</span><br><span class="line">不一致的时间”。为了提供最终一致性的支持，这些系统会提供一些工具来使数</span><br><span class="line">据更新被最终同步到所有相关节点。</span><br></pre></td></tr></table></figure><h2 id="从NoSQL到NewSQL数据库"><a href="#从NoSQL到NewSQL数据库" class="headerlink" title="从NoSQL到NewSQL数据库"></a>从NoSQL到NewSQL数据库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库：适用事务型</span><br><span class="line">非关系型数据库：适用互联网</span><br><span class="line">newsql：适用数据分析</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021517991.png" alt="image-20230302151742886" style="zoom:80%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">newsql数据库同时具备关系型数据库和非关系型数据库的优点(事务和水平拓展)</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021518715.png" alt="image-20230302151829648" style="zoom:80%;"><h2 id="文档数据库MongoDB"><a href="#文档数据库MongoDB" class="headerlink" title="文档数据库MongoDB"></a>文档数据库MongoDB</h2><a href="/%E5%B4%94%E5%BA%86%E6%89%8Dpython3%E7%88%AC%E8%99%AB-pymongo.html" title="文章标题（可选）">文章标题（可选）</a><h3 id="MongoDB简介"><a href="#MongoDB简介" class="headerlink" title="MongoDB简介"></a>MongoDB简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据</span><br><span class="line">库系统。</span><br><span class="line">•在高负载的情况下，添加更多的节点，可以保证服务器性能。</span><br><span class="line">•MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。</span><br><span class="line">•MongoDB 将数据存储为一个文档，数据结构由键值(key&#x3D;&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021536435.png" alt="image-20230302153655381" style="zoom:67%;"><h4 id="主要特点"><a href="#主要特点" class="headerlink" title="主要特点"></a>主要特点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">•提供了一个面向文档存储，操作起来比较简单和容易</span><br><span class="line">•可以设置任何属性的索引来实现更快的排序</span><br><span class="line">•具有较好的水平可扩展性</span><br><span class="line">•支持丰富的查询表达式，可轻易查询文档中内嵌的对象及数组</span><br><span class="line">•可以实现替换完成的文档（数据）或者一些指定的数据字段</span><br><span class="line">•MongoDB中的Map&#x2F;Reduce主要是用来对数据进行批量处理和聚合操作</span><br><span class="line">•支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等语言</span><br><span class="line">•MongoDB安装简单</span><br></pre></td></tr></table></figure><h3 id="MongoDB概念解析"><a href="#MongoDB概念解析" class="headerlink" title="MongoDB概念解析"></a>MongoDB概念解析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在mongodb中基本的概念是文档、集合、数据库</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021538144.png" alt="image-20230302153842093" style="zoom:67%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021540254.png" alt="image-20230302154010180" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">举例2：在一个关系型数据库中，一篇博客（包含文章内容、评论、评论的投票</span><br><span class="line">）会被打散在多张数据表中。在文档数据库MongoDB中，能用一个文档来表示</span><br><span class="line">一篇博客， 评论与投票作为文档数组，放在正文主文档中。这样数据更易于管</span><br><span class="line">理，消除了传统关系型数据库中影响性能和水平扩展性的“JOIN”操作。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021542579.png" alt="image-20230302154214533"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">关系数据库中的其中一条记录，在文档数据库MongoDB中的存储方式类似如下：</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021542657.png" alt="image-20230302154247610" style="zoom:67%;"><h4 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•一个mongodb中可以建立多个数据库。</span><br><span class="line">•MongoDB的默认数据库为&quot;db&quot;，该数据库存储在data目录中。</span><br><span class="line">•MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。</span><br></pre></td></tr></table></figure><h4 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">文档是一个键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相</span><br><span class="line">同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021546083.png" alt="image-20230302154657033"></p><h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：</span><br><span class="line">Relational Database Management System)中的表格。</span><br><span class="line">•集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插</span><br><span class="line">入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。</span><br><span class="line">比如，我们可以将以下不同数据结构的文档插入到集合中：</span><br></pre></td></tr></table></figure><h4 id="MongoDB-数据类型"><a href="#MongoDB-数据类型" class="headerlink" title="MongoDB 数据类型"></a>MongoDB 数据类型</h4><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC5%E7%AB%A0-NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230302155125678.png" alt="image-20230302155125678" style="zoom:67%;"><h3 id="安装MongoDB"><a href="#安装MongoDB" class="headerlink" title="安装MongoDB"></a>安装MongoDB</h3><h4 id="Window平台安装-MongoDB"><a href="#Window平台安装-MongoDB" class="headerlink" title="Window平台安装 MongoDB"></a>Window平台安装 MongoDB</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MongoDB提供了可用于32位和64位系统的预编译二进制包，你可</span><br><span class="line">以从MongoDB官网下载安装，MongoDB预编译二进制包下载地址</span><br><span class="line">：http:&#x2F;&#x2F;www.mongodb.org&#x2F;downloads</span><br><span class="line">注意：在 MongoDB2.2 版本后已经不再支持 Windows XP 系统。</span><br></pre></td></tr></table></figure><h4 id="Linux平台安装MongoDB"><a href="#Linux平台安装MongoDB" class="headerlink" title="Linux平台安装MongoDB"></a>Linux平台安装MongoDB</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MongoDB提供了linux平台上32位和64位的安装包，你可以在官网下载安装包。</span><br><span class="line">下载地址：http:&#x2F;&#x2F;www.mongodb.org&#x2F;downloads</span><br><span class="line">启动 MongoDB服务</span><br><span class="line">只需要在MongoDB安装目录的bin目录下执行&#39;mongod&#39;即可</span><br></pre></td></tr></table></figure><h3 id="访问MongoDB"><a href="#访问MongoDB" class="headerlink" title="访问MongoDB"></a>访问MongoDB</h3><h4 id="使用-MongoDB-shell访问MongoDB"><a href="#使用-MongoDB-shell访问MongoDB" class="headerlink" title="使用 MongoDB shell访问MongoDB"></a>使用 MongoDB shell访问MongoDB</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mongodb:&#x2F;&#x2F;localhost</span><br><span class="line">•使用 MongoDB shell 来连接 MongoDB 服务器</span><br><span class="line">•使用用户名和密码连接登陆到指定数据库：</span><br><span class="line">mongodb:&#x2F;&#x2F;admin:123456@localhost&#x2F;test</span><br></pre></td></tr></table></figure><h5 id="MongoDB-创建数据库"><a href="#MongoDB-创建数据库" class="headerlink" title="MongoDB 创建数据库"></a>MongoDB 创建数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MongoDB 创建数据库的语法格式如下：</span><br><span class="line">use DATABASE_NAME</span><br><span class="line">如果数据库不存在，则创建数据库，否则切换到指定数据库。</span><br><span class="line">如果你想查看所有数据库，可以使用 show dbs 命令</span><br></pre></td></tr></table></figure><h5 id="创建集合"><a href="#创建集合" class="headerlink" title="创建集合"></a>创建集合</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MongoDB没有单独创建集合名的shell命令，在插入数据的时候，MongoDB会自动创建对应的集合。</span><br></pre></td></tr></table></figure><h5 id="MongoDB-插入文档"><a href="#MongoDB-插入文档" class="headerlink" title="MongoDB 插入文档"></a>MongoDB 插入文档</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">文档的数据结构和JSON基本一样。</span><br><span class="line">所有存储在集合中的数据都是BSON格式。</span><br><span class="line">BSON是一种类JSON的一种二进制形式的存储格式,简称Binary JSON。</span><br><span class="line">MongoDB 使用 insert() 或 save() 方法向集合中插入文档，语法如下：</span><br><span class="line">db.COLLECTION_NAME.insert(document)</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303021557873.png" alt="image-20230302155758827" style="zoom:50%;"><h4 id="使用Java程序访问-MongoDB"><a href="#使用Java程序访问-MongoDB" class="headerlink" title="使用Java程序访问 MongoDB"></a>使用Java程序访问 MongoDB</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">环境配置</span><br><span class="line">•在Java程序中如果要使用MongoDB，需要确保已经安装了Java环境及MongoDB JDBC 驱动。</span><br><span class="line">•首先必须下载mongo jar包，下载地址：</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;mongodb&#x2F;mongo-java-driver&#x2F;downloads, 请确保下载最新版本。</span><br><span class="line">•需要将mongo.jar包含在你的 classpath 中</span><br></pre></td></tr></table></figure><h5 id="（1）连接数据库"><a href="#（1）连接数据库" class="headerlink" title="（1）连接数据库"></a>（1）连接数据库</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.mongodb.MongoClient;</span><br><span class="line">……<span class="comment">//这里省略其他需要导入的包</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MongoDBJDBC</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String args[] )</span></span>&#123;</span><br><span class="line"><span class="keyword">try</span>&#123; </span><br><span class="line">    <span class="comment">// 连接到 mongodb 服务</span></span><br><span class="line">    MongoClient mongoClient = <span class="keyword">new</span> MongoClient( <span class="string">"localhost"</span> , <span class="number">27017</span> );</span><br><span class="line">    <span class="comment">// 连接到数据库</span></span><br><span class="line">    DB db = mongoClient.getDB( <span class="string">"test"</span> );</span><br><span class="line">    System.out.println(<span class="string">"Connect to database successfully"</span>);</span><br><span class="line">    <span class="keyword">boolean</span> auth = db.authenticate(myUserName, myPassword);</span><br><span class="line">    System.out.println(<span class="string">"Authentication: "</span>+auth);</span><br><span class="line">&#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">    System.err.println( e.getClass().getName() + <span class="string">": "</span> + e.getMessage() );</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="（2）创建集合"><a href="#（2）创建集合" class="headerlink" title="（2）创建集合"></a>（2）创建集合</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">可以使用com.mongodb.DB类中的createCollection()来创建集合</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MongoDBJDBC</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String args[] )</span></span>&#123;</span><br><span class="line"><span class="keyword">try</span>&#123; </span><br><span class="line">    <span class="comment">// 连接到 mongodb 服务</span></span><br><span class="line">    MongoClient mongoClient = <span class="keyword">new</span> MongoClient( <span class="string">"localhost"</span> , <span class="number">27017</span> );</span><br><span class="line">    <span class="comment">// 连接到数据库</span></span><br><span class="line">    DB db = mongoClient.getDB( <span class="string">"test"</span> );</span><br><span class="line">    System.out.println(<span class="string">"Connect to database successfully"</span>);</span><br><span class="line">    <span class="keyword">boolean</span> auth = db.authenticate(myUserName, myPassword);</span><br><span class="line">    System.out.println(<span class="string">"Authentication: "</span>+auth);</span><br><span class="line">    DBCollection coll = db.createCollection(<span class="string">"mycol"</span>);</span><br><span class="line">    System.out.println(<span class="string">"Collection created successfully"</span>);</span><br><span class="line">&#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">    System.err.println( e.getClass().getName() + <span class="string">": "</span> + e.getMessage() );</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="（3）插入文档"><a href="#（3）插入文档" class="headerlink" title="（3）插入文档"></a>（3）插入文档</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">可以使用com.mongodb.DBCollection类的 insert() 方法来插入一个文档</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MongoDBJDBC</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">( String args[] )</span></span>&#123;</span><br><span class="line"><span class="keyword">try</span>&#123; </span><br><span class="line">    <span class="comment">// 连接到 mongodb 服务</span></span><br><span class="line">    MongoClient mongoClient = <span class="keyword">new</span> MongoClient( <span class="string">"localhost"</span> , <span class="number">27017</span> );</span><br><span class="line">    <span class="comment">// 连接到数据库</span></span><br><span class="line">    DB db = mongoClient.getDB( <span class="string">"test"</span> );</span><br><span class="line">    System.out.println(<span class="string">"Connect to database successfully"</span>);</span><br><span class="line">    <span class="keyword">boolean</span> auth = db.authenticate(myUserName, myPassword);</span><br><span class="line">    System.out.println(<span class="string">"Authentication: "</span>+auth); </span><br><span class="line">    DBCollection coll = db.getCollection(<span class="string">"mycol"</span>);</span><br><span class="line">    System.out.println(<span class="string">"Collection mycol selected successfully"</span>);</span><br><span class="line">    BasicDBObject doc = <span class="keyword">new</span> BasicDBObject(<span class="string">"title"</span>, <span class="string">"MongoDB"</span>).</span><br><span class="line">    append(<span class="string">"description"</span>, <span class="string">"database"</span>).</span><br><span class="line">    append(<span class="string">"likes"</span>, <span class="number">100</span>).</span><br><span class="line">    append(<span class="string">"url"</span>, <span class="string">"http://www.w3cschool.cc/mongodb/"</span>).</span><br><span class="line">    append(<span class="string">"by"</span>, <span class="string">"w3cschool.cc"</span>);</span><br><span class="line">    coll.insert(doc);</span><br><span class="line">    System.out.println(<span class="string">"Document inserted successfully"</span>);</span><br><span class="line">&#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">System.err.println( e.getClass().getName() + <span class="string">": "</span> + e.getMessage() );</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
</feed>
