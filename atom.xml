<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2023-05-17T07:39:42.425Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v2.0-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-4.html</id>
    <published>2023-05-17T07:34:44.000Z</published>
    <updated>2023-05-17T07:39:42.425Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v2-0-4"><a href="#第十八周-直播平台三度关系推荐v2-0-4" class="headerlink" title="第十八周 直播平台三度关系推荐v2.0-4"></a>第十八周 直播平台三度关系推荐v2.0-4</h1><h2 id="数据加工总线之SparkSQL计算引擎开发"><a href="#数据加工总线之SparkSQL计算引擎开发" class="headerlink" title="数据加工总线之SparkSQL计算引擎开发"></a>数据加工总线之SparkSQL计算引擎开发</h2><h3 id="核心功能点梳理"><a href="#核心功能点梳理" class="headerlink" title="核心功能点梳理"></a>核心功能点梳理</h3><h3 id="开发基于SparkSQL的计算引擎"><a href="#开发基于SparkSQL的计算引擎" class="headerlink" title="开发基于SparkSQL的计算引擎"></a>开发基于SparkSQL的计算引擎</h3><h2 id="数据加工总线之FlinkSQL计算引擎开发"><a href="#数据加工总线之FlinkSQL计算引擎开发" class="headerlink" title="数据加工总线之FlinkSQL计算引擎开发"></a>数据加工总线之FlinkSQL计算引擎开发</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v2.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v2.0-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-3.html</id>
    <published>2023-05-03T09:35:36.000Z</published>
    <updated>2023-05-17T07:39:38.772Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v2-0-3"><a href="#第十八周-直播平台三度关系推荐v2-0-3" class="headerlink" title="第十八周 直播平台三度关系推荐v2.0-3"></a>第十八周 直播平台三度关系推荐v2.0-3</h1><h2 id="数据中台的前世今生"><a href="#数据中台的前世今生" class="headerlink" title="数据中台的前世今生"></a>数据中台的前世今生</h2><h3 id="什么是中台"><a href="#什么是中台" class="headerlink" title="什么是中台"></a>什么是中台</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">中台是2019年开始火起来的一个概念，它最早是由阿里在2015年提出的“大中台，小前台”战略中延伸出来的概念，灵感来源于一家芬兰的小公司Supercell——一家仅有300名员工，却接连推出爆款游戏，是全球最会赚钱的明星游戏公司。2015年年中,马云带领阿里巴巴集团高管,拜访了位于芬兰赫尔辛基的这家移动游戏公司，这家看似很小的公司，设置了一个强大的技术平台，来支持众多的小团队进行游戏研发。这样一来，他们就可以专心创新，不用担心基础却又至关重要的技术支撑问题。恰恰是这家小公司，开创了中台的“玩法”，并将其运用到了极致。</span><br><span class="line">下面我们举个例子，通过IT行业的发展来进一步理解什么是中台？为什么要出现中台？</span><br></pre></td></tr></table></figure><p>传统IT时代</p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162119743.png" alt="image-20230516211621053"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在传统IT时代，无论项目如何复杂，都可以分为 前台 和 后台 两部分，简单明了。</span><br><span class="line">每一个业务线负责维护自己的前台和后台</span><br><span class="line">这里的前台不仅仅包含前端页面，还包含提供的各种服务</span><br><span class="line">后台指的是底层的服务，例如我们提取的一些工具服务</span><br><span class="line">在当时，项目的发展相对稳定，并不需要像互联网时代那么快速的去迭代和试错，所以这种架构没有什么问题。</span><br></pre></td></tr></table></figure><h4 id="传统IT时代存在的问题"><a href="#传统IT时代存在的问题" class="headerlink" title="传统IT时代存在的问题"></a>传统IT时代存在的问题</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162119728.png" alt="image-20230516211851281"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">发展到现在这个时代，传统的前台+后台这种架构是存在一些问题的，每一个产品线之间都会有一些重复的内容，例如这里面的用户模块和支付模块，每一个产品线都需要，如果每一个产品线都是自己开发自己的，这样就会有三套用户模块和支付模块，对于集团公司而言，这就叫重复造轮子。如果后期又增加了新的产品线，还要重新再开发用户模块和支付模块。</span><br><span class="line"></span><br><span class="line">所以说为了提高开发效率，我们有必要抽取出一个中间组织，为所有的产品线提供一些公共资源，这个中间组织就是中台。</span><br><span class="line"></span><br><span class="line">下面来看一个引入了中台之后的案例。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162122689.png" alt="image-20230516212224583"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">本来是各个部门都建立了自己的数据采集，数仓，数据模型等内容，重复开发，浪费成本。各个部门的数据也没有打通，数据很难产生很大的价值。</span><br><span class="line">引入了中台之后，构建了统一的数据采集、统一的数据资产中心、统一的数据建模、分析与挖掘、统一的数据服务，最终向各部门统一提供数据支撑。</span><br></pre></td></tr></table></figure><h4 id="阿里”大中台小前台架构-”"><a href="#阿里”大中台小前台架构-”" class="headerlink" title="阿里”大中台小前台架构 ”"></a>阿里”大中台小前台架构 ”</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来这个是阿里的大中台 小前台架构</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162125389.png" alt="image-20230516212502449"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">阿里许多产品线的共通业务经过下沉，形成了中台的各种业务中心，为各大业务线提供支持。</span><br><span class="line">这样前台应用就会更加灵活，想要构建一个新的前台应用也是比较快速容易的。</span><br></pre></td></tr></table></figure><h4 id="中台架构主要解决的问题"><a href="#中台架构主要解决的问题" class="headerlink" title="中台架构主要解决的问题"></a>中台架构主要解决的问题</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面我们来总结一下中台这种架构主要解决的问题。</span><br><span class="line"></span><br><span class="line">信息获取成本高，之前是每一个产品线都需要单独维护自己的数据，成本比较高。</span><br><span class="line">服务具有不确定性，通过中台可以以不变应万变</span><br><span class="line">互联互通成本高，不同产品线的数据想要打通成本过高。</span><br><span class="line">低水平重复建设，不同产品线需要重复建设相同的模块。</span><br><span class="line">通过中台，可以很好的解决这些问题。</span><br></pre></td></tr></table></figure><h4 id="中台的延伸"><a href="#中台的延伸" class="headerlink" title="中台的延伸"></a>中台的延伸</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">中台是一个大而全的概念，基于中台延伸出了多个方向</span><br><span class="line">技术中台</span><br><span class="line">移动中台</span><br><span class="line">业务中台</span><br><span class="line">数据中台</span><br><span class="line">研发中台</span><br><span class="line">组织中台</span><br><span class="line">等等…</span><br><span class="line"></span><br><span class="line">在这里我们可以把中台理解为航空母舰，这些中台都是基于这个航空母舰延伸出来的</span><br><span class="line"></span><br><span class="line">技术中台提供了技术支撑能力，帮助我们解决了基础设施，分布式数据库等底层技术问题，为前台特种兵提供了精良的武器装备。</span><br><span class="line"></span><br><span class="line">移动中台提供了战场一线火力支援能力，帮助我们提供更加个性化的服务，增强用户体验，为战场提供了陆军支援能力，随机应变，所向披靡。</span><br><span class="line">注意：这里的移动中台并不是说这个中台会移动，这里的移动表示的是移动端的意思，就是手机端。</span><br><span class="line"></span><br><span class="line">业务中台提供重用服务，例如用户中心，订单中心之类的开箱即用可重用能力，为战场提供了强大的后台炮火支援能力，随叫随到，威力强大。</span><br><span class="line"></span><br><span class="line">数据中台提供了数据分析能力，帮助我们从数据中学习改进，调整方向，为战场提供了强大及时的雷达监测能力，帮助我们掌控战场。</span><br><span class="line"></span><br><span class="line">研发中台提供了技术实践支撑能力，帮助我们快速搭建项目，管理进度，测试，持续集成，持续交付，是前台特种兵的训练基地及快速送达战场的机动运输部队。</span><br><span class="line"></span><br><span class="line">组织中台为我们的项目提供投资管理、风险管理、资源调度等，是战场的指挥部，战争的大脑，指挥前线，调度后方。</span><br></pre></td></tr></table></figure><h4 id="阿里中台技术栈全景"><a href="#阿里中台技术栈全景" class="headerlink" title="阿里中台技术栈全景"></a>阿里中台技术栈全景</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下阿里的中台技术栈全景</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162133313.png" alt="image-20230516213309367"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">最下面是一些基础设施和基础中间件</span><br><span class="line">上层是业务中台和数据中台</span><br><span class="line">其中业务中台里面是以业务进行区分，抽取出来的一些公共组件，例如：会员中心，商品中心，交易中心、订单中心、支付中心、评价中心</span><br><span class="line">后期如果新增的产品线需要用到这些功能的时候可以从业务总台中直接开箱即用，提高效率。</span><br><span class="line">数据中台中包含大数据计算服务(包含离线和实时)、大数据开发套件(这里面包含的是一些小工具)、画像分析、数据可视化、数仓规则、数据服务等，可以实现数据的一站式接入和使用。</span><br><span class="line">移动中台包含了很多移动端的公共组件和功能。</span><br><span class="line">基于这些中台就可以快速为上层这些应用提供各种支持了。</span><br></pre></td></tr></table></figure><h3 id="什么是数据中台"><a href="#什么是数据中台" class="headerlink" title="什么是数据中台"></a>什么是数据中台</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">前面我们讲了什么是中台，中台其实是一个统称，基于中台也延伸出了很多分支。</span><br><span class="line">每一个分支深究起来都有很多内容，不过目前来说，在这些中台的分支里面，数据中台是最为火热的，因为数据是可以直接为企业决策提供支持，可以直接产生价值的。</span><br><span class="line"></span><br><span class="line">下面我们就来具体分析一下什么是数据中台</span><br><span class="line">针对数据中台的定义业内目前有很多种说法，没有官方的定义，不同的人有不同的理解。</span><br><span class="line"></span><br><span class="line">通俗来讲数据中台是指利用大数据技术，对海量数据统一进行采集、计算、存储，并且对外提供数据服务。</span><br><span class="line">数据中台的主要作用在于将企业内部所有数据统一处理形成标准化数据，挖掘出对企业最有价值的数据，构建企业数据资产库，对内对外提供一致的，高可用的大数据服务。</span><br><span class="line"></span><br><span class="line">正式一点来说，可以这样理解</span><br><span class="line">数据中台是一套可持续 ”让企业的数据用起来 ” 的机制</span><br><span class="line">通过数据中台把数据变为一种服务能力，既能提升决策水平，又能直接支撑企业业务</span><br><span class="line">数据中台不仅仅是技术，也不仅仅是产品，而是一套完整的让数据用起来的机制。</span><br><span class="line">数据中台不是单纯的技术叠加，不是一个技术化的大数据平台，二者有本质区别。</span><br><span class="line">大数据平台更关心技术层面的事情，包括研发效率，平台的大数据处理能力，针对的往往是技术人员</span><br><span class="line">而数据中台的核心是数据服务能力，数据中台不仅面向技术人员，更需要面向多个部门的业务人员。</span><br></pre></td></tr></table></figure><h3 id="数据中台的演进过程"><a href="#数据中台的演进过程" class="headerlink" title="数据中台的演进过程"></a>数据中台的演进过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">数据中台并不是直接就有的，也是根据时代的发展，企业的需求，一步一步演进出来的。</span><br><span class="line">下面我们就来看一下数据中台的演进过程。</span><br><span class="line"></span><br><span class="line">1：最开始是 数据库阶段，主要是OLTP（联机事务处理）的需求；</span><br><span class="line">以淘宝为例，最开始淘宝还只是一个简单的网站，淘宝的整个结构就是前端的一些页面，加上后端的数据库，只是个简单的OLTP系统，主要就是交易的事务处理。</span><br><span class="line"></span><br><span class="line">这个阶段，互联网黄页才刚刚出现，数据来源大部分还是传统商业的ERP&#x2F;CRM的结构化数据，数据量并不大，也就是GB的级别。简单的数据库就能满足需求。</span><br><span class="line"></span><br><span class="line">随着淘宝用户超过100万，分析需求的比重就越来越大。淘宝需要知道它的交易来自于哪些地区，来自于哪些人，谁在买淘宝的东西等等，于是，就进入了数据处理的第二个阶段：数据仓库阶段。</span><br><span class="line"></span><br><span class="line">2：数据仓库阶段，OLAP（联机分析处理）成为主要需求；</span><br><span class="line">OLTP和OLAP对数据存储和计算的需求是不一样的，OLTP处理的是结构化的交易数据，而OLAP对应的是互联网数据，而互联网里面数据量最大的是日志，90%以上的数据都是用户点击之类的非结构化的日志数据，而且数据量已经达到了TB的级别。</span><br><span class="line"></span><br><span class="line">针对分析需求，就诞生了数据仓库，数据仓库主要解决大量数据的存储和计算需求，也就是把非结构化的数据转化成结构化数据，存储下来。</span><br><span class="line"></span><br><span class="line">这个阶段，数据仓库支持的主要就是BI和报表需求。</span><br><span class="line"></span><br><span class="line">随着数据量越来越大，从TB进入了PB级别，原来的技术架构越来越不能支持海量数据处理，这时候就进入了第三个阶段：数据平台阶段。</span><br><span class="line"></span><br><span class="line">3：数据平台阶段，主要解决BI和报表需求的技术问题；</span><br><span class="line">这个阶段解决的还是BI和报表需求，但是主要是在解决底层的技术问题，也就是数据库架构设计的问题。</span><br><span class="line"></span><br><span class="line">这在数据库技术领域被概括为「Shared Everything、Shared Nothing、或Shared Disk」，说的就是数据库架构设计本身的不同技术思路之争。</span><br><span class="line"></span><br><span class="line">Shared Everything一般是针对单个主机，完全透明共享CPU&#x2F;MEMORY&#x2F;IO，并行处理能力是最差的，典型的代表SQLServer。</span><br><span class="line"></span><br><span class="line">Shared Disk的代表是Oracle RAC，用户访问RAC就像访问一个数据库，但是这背后是一个集群，RAC来保证这个集群的数据一致性。</span><br><span class="line"></span><br><span class="line">问题在于Oracle RAC(实时应用集群)是基于IOE架构的（使用IBM的小型机、Oracle数据库、EMC存储设备）。在海量数据处理上，IOE架构有天然的限制，不适合未来的发展。</span><br><span class="line"></span><br><span class="line">Shared Nothing的代表就是Hadoop。Hadoop的并行处理和扩展能力更好。</span><br><span class="line"></span><br><span class="line">Hadoop的好处是如果要增加数据处理的能力和容量，只需要增加服务器就好，成本不高，在海量数据处理和大规模并行处理上有很大优势。</span><br><span class="line"></span><br><span class="line">综上所述，第三阶段就是，建立Shared Nothing的海量数据处理平台来解决数据存储成本增长过快的问题。</span><br><span class="line"></span><br><span class="line">4：数据中台阶段，通过系统来对接OLTP（事务处理）和OLAP（报表分析）的需求，强调数据业务化的能力。</span><br><span class="line">这个阶段的特征是数据量呈现指数级增长，从PB迈向了EB级别，未来会到什么量级，谁也说不清楚。</span><br><span class="line"></span><br><span class="line">主要是因为，2015年之后，IOT（物联网）发展起来，带动了视频、图像、声音数据的增长，未来90%的数据可能都来自于视频、图像、声音这些非结构化数据，这些数据需要视觉计算技术、图像解析引擎+视频解析引擎+音频解析引擎来转换成结构化数据。5G技术的发展，可能会进一步放大视频、图像、声音数据的重要性。</span><br><span class="line"></span><br><span class="line">线下要想和线上一样，通过数据来改善业务，就要和线上一样能做到行为可监测，数据可收集，这是前提。线下最大量的就是视频、图像、声音数据，而这些数据靠人来手工收集，肯定是不靠谱的，依靠IOT（物联网）技术和算法的进步，最终会通过智能端来自动化获取数据。</span><br><span class="line"></span><br><span class="line">要使用这些数据，光有视觉算法和智能端也不行，要有云来存储和处理这些数据，以及打通其它领域的数据。</span><br><span class="line"></span><br><span class="line">目前的数据中台，最底层的数据平台还是偏技术的，是中台技术方案的其中一个组件，主要解决数据存储和计算的问题；在往上面就是一层数据服务层，数据服务层通过服务化API能够把数据和前台的业务层对接；数据中台里面都是系统去做对接，通过智能算法，能把前台的分析需求和交易需求去做对接，最终赋能业务。</span><br></pre></td></tr></table></figure><h3 id="数据中台-VS-数据仓库"><a href="#数据中台-VS-数据仓库" class="headerlink" title="数据中台 VS 数据仓库"></a>数据中台 VS 数据仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">数据仓库主要支持管理决策和业务分析</span><br><span class="line">数据中台是将数据服务化之后提供给业务系统，目的是将数据能力渗透到各个业务环节，不限于决策分析类场景</span><br><span class="line">数据中台建设包含数据体系建设，也就是数据中台包含数据仓库的完整内容</span><br><span class="line">所以说数据仓库阶段的成果是可以转化到数据中台阶段的，并不会全部推倒重做。</span><br></pre></td></tr></table></figure><h3 id="数据中台需要具备的四大能力"><a href="#数据中台需要具备的四大能力" class="headerlink" title="数据中台需要具备的四大能力"></a>数据中台需要具备的四大能力</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">根据我们前面对数据中台的分析，总结起来，数据中台需要具备以下能力：</span><br><span class="line">1：数据汇聚整合</span><br><span class="line">随着业务的发展，企业内部往往有多个信息部门和数据中心，大量系统、功能和应用重复建设，存在巨大的数据资源、计算资源和人力资源的浪费，同时组织壁垒也会导致数据孤岛的出现，使得内外部数据难以全局规划，数据中台需要对数据进行整合和完善。</span><br><span class="line"></span><br><span class="line">2：数据提纯加工</span><br><span class="line">数据就像石油，需要经过提纯加工才能使用，这个过程就是数据资产化。</span><br><span class="line">数据中台必须联通全域数据，通过统一的数据标准和质量体系，建设提纯加工后的标准数据资产体系，以满足企业业务对数据的需求。</span><br><span class="line"></span><br><span class="line">3：数据服务可视化</span><br><span class="line">为了尽快让数据用起来，数据中台必须提供快捷，快速的数据服务能力，让相关人员能够迅速开发数据应用，支持数据资产场景化能力的快速输出，以响应客户的动态需求。</span><br><span class="line"></span><br><span class="line">4：数据价值变现</span><br><span class="line">数据中台通过打通企业数据，提供以前单个部门无法提供的数据服务能力，以实现数据的更大价值变现。</span><br></pre></td></tr></table></figure><h2 id="数据中台架构"><a href="#数据中台架构" class="headerlink" title="数据中台架构"></a>数据中台架构</h2><h3 id="数据中台总体架构图"><a href="#数据中台总体架构图" class="headerlink" title="数据中台总体架构图"></a>数据中台总体架构图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面我们通过理论层面对数据中台有了一定的了解，下面我们通过架构层面来详细看一下数据中台的设计</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171109254.png" alt="image-20230517110950057"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">数据中台是位于底层存储计算平台与上层的数据应用之间的一整套体系。</span><br><span class="line">数据中台屏蔽掉底层存储平台的计算技术复杂性，降低对技术人才的需求，让数据的使用成本更低。</span><br><span class="line">通过数据中台的数据汇聚、数据开发模块建立企业数据资产。</span><br><span class="line">通过数据体系对数据进行分层存储</span><br><span class="line">通过资产管理、数据服务，把数据资产变为数据服务能力，服务于企业业务。</span><br><span class="line">数据安全管理、数据运营体系，保障数据中台可以长期健康、持续运转。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">数据汇聚</span><br><span class="line">数据汇聚是数据中台数据接入的入口，数据中台本身不产生数据，所有的数据来自于业务系统，数据库、日志、文件等，这些数据分散在不同的网络环境和存储平台中，难以利用，很难产生业务价值，所以需要统一汇聚。</span><br><span class="line"></span><br><span class="line">数据开发</span><br><span class="line">数据开发是一整套数据加工以及处理的工具，因为通过数据汇聚模块汇聚到中台的数据没有经过处理，基本是按照数据的原始状态堆砌在一起的，这样业务是很难直接使用的。所以需要通过数据开发模块实现对数据的加工处理，形成有价值的数据，提供给业务部门使用。</span><br><span class="line"></span><br><span class="line">数据体系</span><br><span class="line">通过数据汇聚、数据开发，中台就具备了构建数仓平台的基本能力，这一块其实就是将采集过来的各种数据按照数仓的标准进行建设。</span><br><span class="line"></span><br><span class="line">数据资产管理</span><br><span class="line">通过数仓建立起来的数据资产比较偏向于技术，业务人员比较难理解，资产管理是以业务人员更好理解的方式，把数据资产展现给企业的业务人员。</span><br><span class="line"></span><br><span class="line">数据服务体系</span><br><span class="line">数据服务体系就是把数据变为一种服务能力，通过数据服务让数据参与到业务，激活整个数据中台，数据服务体系是数据中台存在的价值所在。</span><br><span class="line"></span><br><span class="line">数据运营体系</span><br><span class="line">是数据中台得以健康、持续运转的基础</span><br><span class="line"></span><br><span class="line">数据安全管理</span><br><span class="line">是为了保证数据中台中的数据安全。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是一个典型的数据中台总体架构设计。</span><br></pre></td></tr></table></figure><h3 id="数据中台-四字箴言"><a href="#数据中台-四字箴言" class="headerlink" title="数据中台 四字箴言"></a>数据中台 四字箴言</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果大家之前没有工作过的话，可能对数据中台还是不好理解，所以在这我将数据中台的功能总结为四个字：采、存、通、用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">下面我们来详细分析一下这四字箴言</span><br><span class="line"></span><br><span class="line">采</span><br><span class="line">采：表示采集的意思，就是采集企业中的所有数据。</span><br><span class="line">随着互联网、移动互联网、物联网等技术的兴起，企业的业务形态开始多元化，数据的产生形式也是多样化的，对应的就需要有多种采集形式</span><br><span class="line"></span><br><span class="line">埋点采集、硬件采集、爬虫采集、数据库采集、日志采集</span><br><span class="line"></span><br><span class="line">埋点采集：一般是采集用户行为信息，例如用户在平台上的浏览、点击、停留等行为</span><br><span class="line">硬件采集：指的是物联网数据采集，例如通过无人机传感器来采集空气质量指标</span><br><span class="line">爬虫采集：指的是采集互联网上的公开数据，例如：电商平台竞品价格采集</span><br><span class="line">数据库采集：一般是采集企业内的业务数据，例如：用户交易数据、用户个人信息数据等</span><br><span class="line">日志采集：一般是采集软件运行时产生的日志</span><br><span class="line"></span><br><span class="line">这些是常见的采集形式</span><br><span class="line"></span><br><span class="line">从数据组织形式可以分为：结构化数据、半结构化数据、非结构化数据</span><br><span class="line">结构化数据：数据规则、完整、能够通过二维逻辑来表现的数据，严格遵守数据格式与长度规范，常见的有数据库中的数据、excel中的数据</span><br><span class="line">半结构化数据：数据规则、完整，同样严格遵守数据格式与长度规范，但无法通过二维关系来表现，常见的有JSON、XML等格式的数据</span><br><span class="line">非结构化数据：数据结构不规则或不完整，不方便用二维逻辑表来表现，需要经过复杂的逻辑处理才能提取其中的信息内容，常见的有word文档、图片、视频、音频等数据</span><br><span class="line"></span><br><span class="line">从数据的时效性上来划分，可以分为：离线数据、实时数据</span><br><span class="line">离线数据：主要用于大批量数据的周期性迁移，对时效性要求不高，一般采用分布式批量数据同步的形式，通过连接读取数据，读取数据过程中可以有全量、增量的方式，经过统一处理后写入到目标存储。</span><br><span class="line">实时数据：主要面向低延时的数据应用场景，一般通过实时监控的方式实现，例如通过读取数据库的binlog日志来实现数据库的实时数据采集。</span><br><span class="line"></span><br><span class="line">前面我们针对数据的采集形式、数据的组织形式、数据的时效性进行了分析，那这些数据在采集的时候具体应该使用什么类型的工具呢？</span><br><span class="line"></span><br><span class="line">常见的采集工具有：Flume、FileBeat、Logstash、Sqoop、Canal、DataX等</span><br><span class="line">其中Flume、FileBeat、Logstash适合采集日志数据，这三个组件的特性在前面项目课程中已经详细分析过了，在这不再赘述。</span><br><span class="line">sqoop是在结构化数据和HDFS之间进行批量数据迁移的工具，适合批量采集数据库中的数据，它的主要优势是，在特定场景下，数据交换过程会有很大的性能提升。主要缺点是处理过程定制程度较高，需要在脚本中调整配置参数实现，在用户的一些自定义逻辑和数据同步链路监控方面比较薄弱。</span><br><span class="line">DataX是阿里开源的一套数据采集工具，提供数据采集全链路的流量监控，将作业本身的状态，数据流量，数据速度，执行速度等信息进行展示，提供脏数据探测功能，支持传输过程中对传输报错进行策略化处理。由于它是基于进程内读写直连的方式，高并发数据采集场景下对机器内存要求比较高。不过DataX不支持非结构化数据的采集。</span><br><span class="line"></span><br><span class="line">这些单个工具都无法很好的满足企业复杂的数据采集场景，所以我们需要对已有的采集工具进行二次开发，以可视化配置的方式提供给用户，屏蔽底层工具的复杂性，要支持常见的数据源采集：关系型数据库、NoSQL数据库、MQ、文件系统等，并且支持增量同步、全量同步等方式。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">存</span><br><span class="line">将数据采集过来之后，就需要考虑数据存储了。</span><br><span class="line">在这里我们可以将数据分为两种：静态数据和动态数据</span><br><span class="line">其中静态数据：是以 HDFS 、S3等分布式文件系统作为存储引擎，适用于高吞吐量的离线大数据分析场景。这类存储的局限性是数据无法进行随机的读写。</span><br><span class="line">动态数据：是以 HBase、Cassandra等NoSQL数据库作为存储引擎，适用于大数据随机读写的场景。这类存储的局限性是批量读取吞吐量远不如HDFS，不适合用于批量数据分析的场景。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">通</span><br><span class="line">表示是对数据进行加工计算，构建企业级数据仓库，打通企业中的全域数据。</span><br><span class="line">针对数据的加工计算，可以分为两大块，离线计算和实时计算</span><br><span class="line">离线计算中的代表框架为：MapReduce、Hive、和Spark</span><br><span class="line">实时计算中的代表框架为：Storm、SparkStreaming和Flink，针对实时计算，现在主要是以Flink为主了。</span><br><span class="line">针对这些计算框架，如果每一个计算任务都需要开发代码的话，对使用人员就不友好了，特别是针对一些业务人员，他们不会写代码，只会写SQL，所以这时候我们就需要开发一套基于SQL的一站式开发平台，底层引擎使用Spark和Flink，支持离线数据计算和实时数据计算。</span><br><span class="line">让用户彻底规避掉繁重的底层代码开发工作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">用</span><br><span class="line">企业全域数据采集、存储，打通之后，就涉及到如何去用了。</span><br><span class="line">这里的”用” 包含很多层面。</span><br><span class="line">首先是包括数据资产管理，也可以称之为数据治理，其中包含数据元标准管理，数据标签管理，数据模型管理、元数据管理、数据质量管理等，保证数据中台里面数据的合理化和规范化，充分发挥数据的价值。</span><br><span class="line">对于数据的拥有者和管理者来说，通过对数据的合理管理和有效应用，能盘活并充分释放数据的巨大价值，但如果不能对数据进行有效管理，数据就用不起来，或者即使用起来也用不好，在这种情况下，堆积如山的无序数据给企业带来的是高昂的成本。</span><br><span class="line"></span><br><span class="line">在使用数据的时候还需要做好数据安全管理，随着大数据技术和应用的快速发展，数据所承载的多维度业务价值已被越来越多的挖掘和应用变现，随之而来的是数据安全和隐私已经成为世界性的关注点，上升到国家战略层面，最近闹得沸沸扬扬的特朗普要禁用国外版的抖音(TikTok)事件，特朗普的理由就是TikTok平台的数据对他们产生了威胁。</span><br><span class="line">所以说数据安全很有必要，整体的数据安全管理体系通过分层建设、分级防护，创造面向数据的安全管理体系系统框架，形成完整的数据安全管理体系。</span><br><span class="line">数据中台的建设，应该始终把数据安全管理放在最重要的位置上，通过设计完备的数据安全管理体系，多方面，多层次保障数据安全。</span><br><span class="line"></span><br><span class="line">最终我们需要把安全、有价值的数据快速方便的提供给上层应用，此时需要通过数据服务对外开放，也就是API接口的形式。</span><br><span class="line">举个例子，水是生命之源，是人们赖以生存和发展的重要物质资源，在日常生活中，可以通过不同的方式使用水，这也给我们的生活带来了巨大便利。</span><br><span class="line">在数据世界中，数据资产就好比日常生活中生命所需的水资源，无处不在且不可或缺。但是如果没有相应的水加工厂，运输管道，人们只能到水库打水喝，这明显会极大影响人们正常的生活和工作。因此，将数据封装成数据服务，以接口形式提供给上层应用，才能极大释放、提升数据资产的价值。</span><br><span class="line"></span><br><span class="line">最后总结一下，数据中台其实可以这样理解，采集企业全域数据，存储起来，通过加工计算打通数据之间的关系，最后以API接口的形式对外提供数据服务。这就是数据中台要做的事情。</span><br></pre></td></tr></table></figure><h2 id="什么样的企业适合建设数据中台"><a href="#什么样的企业适合建设数据中台" class="headerlink" title="什么样的企业适合建设数据中台"></a>什么样的企业适合建设数据中台</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前面我们分析了什么是数据中台，数据中台的好处，以及数据中台的架构，是不是所有的企业都需要构建数据中台呢？</span><br><span class="line">不是的，下面就来看一下到底什么样的企业适合建设数据中台</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171128064.png" alt="image-20230517112856557"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">看这个案例：</span><br><span class="line">某企业下面有多个科研实体，每个科研实体下面有多个研究中心</span><br><span class="line">类似于一个集团公司，下面有多个子公司，每个子公司里面还有多个产品线。</span><br><span class="line">这种类型的企业业务复杂，有丰富的数据维度和多个业务场景，比较适合建设数据中台。</span><br><span class="line">初创型企业没有必要搭建数据中台，首先要解决的是生存问题，甚至于连数据仓库都没必要搭建，需要等企业走上正轨进入快速发展期的时候才需要构建数据仓库、数据中台。</span><br></pre></td></tr></table></figure><h3 id="数据应用成熟度的四个阶段"><a href="#数据应用成熟度的四个阶段" class="headerlink" title="数据应用成熟度的四个阶段"></a>数据应用成熟度的四个阶段</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">当然了，评价一个企业是否适合建设数据中台，也是有一些量化指标的，可以根据企业中的数据应用成熟度来进行判断，我们可以把企业中数据应用成熟度分为四个阶段</span><br><span class="line"></span><br><span class="line">统计分析</span><br><span class="line">决策支持</span><br><span class="line">数据驱动</span><br><span class="line">运营优化</span><br><span class="line"></span><br><span class="line">针对不同的阶段，从企业战略定位、企业数据形态、数据应用场景、数据应用工具、企业组织架构等多个方面，不同特征维度进行参考判定，也就构成了数据应用成熟度评估模型。</span><br><span class="line">依据这四个阶段的划分标准，企业可以进行数据应用成熟度的自我评测。</span><br><span class="line">数据应用成熟度越高，则代表数据对业务的支撑能力越强，数据应用成熟度越低，则意味着业务对数据的依赖程度越低。</span><br><span class="line"></span><br><span class="line">来看一下具体的评估模型。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171132712.png" alt="image-20230517113246298"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">第一阶段：统计分析阶段</span><br><span class="line">统计分析阶段主要有以下特征</span><br><span class="line">1：在企业战略方面，该阶段的企业战略定位纯粹以业务为驱动，主要以满足企业业务需求，实现业务流程的流程化、自动化为导向。</span><br><span class="line">2：在企业数据形态方面，该阶段的企业可能有少量的业务数据积累，但没有以数据为导向积累数据，数据主要以业务系统依托的关系型数据库进行存储，数据无组织，各业务数据分散存储和管理，数据维度单一，无数据质量管控。</span><br><span class="line">3：在数据应用场景方面，该阶段的数据应用场景只针对业务系统中的关键数据和指标，进行简单的、单一维度的统计分析和管理，辅助业务总结，每次基于业务目标的数据统计都需要定制化开发，如周报、月报等。</span><br><span class="line">4：在数据应用工具方面，该阶段业务报表主要以系统内嵌报表以及Excel报表为主，模式相对单一。</span><br><span class="line">5：在企业组织架构发面，该阶段企业无专门的数据相关部门，主要以IT部门的数据库运维管理和业务部门的数据分析师为主，需要数据报表时，一般用系统中定制的统计报表或者由特定业务部门提供Excel报表。</span><br><span class="line"></span><br><span class="line">如果对数据的应用仅停留在单系统、单维度的统计分析上，只用于对历史业务开展情况进行简单分析，数据并没有发挥出应有的价值，数据只是辅助企业了解业务运转的情况。我们希望能通过数据为业务决策提供支撑，因此就出现了第二阶段</span><br><span class="line"></span><br><span class="line">第二阶段：决策支持阶段</span><br><span class="line">决策支持阶段主要有以下特征</span><br><span class="line">1：在企业战略方面，该阶段，企业开始具备通过数据支撑经营决策的思路，并在考虑通过数据可视化的方式实现数据与业务的融合，以解决业务问题和支撑管理决策。</span><br><span class="line">2：在企业数据形态方面，企业开始注重业务过程中的数据积累，开始对各业务环节的数据进行汇聚、管理、数据维度逐渐丰富。以面向业务主体的指标体系为形式进行数据组织，开始注重数据质量的管控，实施数据质量控制。</span><br><span class="line">3：在数据应用场景方面，该阶段的数据应用场景开始基于数据仓库进行各业务主题的数据收集、管理、分析，为企业管理人员提供决策支持。</span><br><span class="line">4：在数据应用工具方面，开始针对数据收集和管理 建立数据仓库，数据开发工具和专业可视化工具，进行系统化数据收集、管理和分析。</span><br><span class="line">5：在企业组织架构发面，开始出现数据分析师的岗位，可能会设立专门的数据挖掘或商业智能部门来支撑企业进行数据化决策。</span><br><span class="line"></span><br><span class="line">无论是在统计分析阶段还是决策支撑阶段，业务的运转和数据之间依然是相互隔离的。企业对数据的应用都还停留在对部分维度的业务数据进行分析得到结果后，再由人工对业务进行不同程度的干预，最终实现业务优化。而我们希望能够让数据直接驱动业务变得更精准，更有效。最典型的应用场景就是类似于头条、抖音里面的个性化推荐功能，通过数据直接驱动业务的优化。所以就出现了第三阶段</span><br><span class="line"></span><br><span class="line">第三阶段：数据驱动阶段</span><br><span class="line"></span><br><span class="line">数据驱动阶段主要有以下特征</span><br><span class="line">1：在企业战略方面，企业开始将数据作为重要资产和生产资料，通过大数据技术对企业相关数据进行汇聚、打通和分析挖掘，为业务应用提供数据服务，通过数据驱动业务发展。</span><br><span class="line">2：在企业数据形态方面，业务数据积累具备一定规模，对结构化数据、非结构化数据进行处理与应用，根据需求进行数据清洗加工和标准化处理。</span><br><span class="line">3：在数据应用场景方面，该阶段的数据应用场景主要以满足业务需求为主，主要是用数据提升现有业务能力，进行智能化升级。</span><br><span class="line">4：在数据应用工具方面，在该阶段，企业开始通过大数据生态体系中的批计算、流计算等大数据处理技术进行数据汇聚和开发，并最终为现有的业务场景赋能，以驱动业务升级。</span><br><span class="line">5：在企业组织架构发面，在该阶段，企业开始正式设立独立的大数据部门。</span><br><span class="line"></span><br><span class="line">数据驱动阶段，数据其实已经与业务紧密结合，数据在业务运转过程中直接产生价值，但是，由于数据应用都是独立建设的，没有从全局考虑，企业在数据应用的过程中，经常会遇到标准口径不一致，内容重复建设，各业务数据无法融合产生更大的价值，企业数据价值无法被业务快速应用等问题，因此，出现了第四阶段</span><br><span class="line"></span><br><span class="line">第四阶段：运营优化阶段</span><br><span class="line">运营优化阶段主要有以下特征</span><br><span class="line">1：在企业战略方面，该阶段，企业开始建设数据中台，数据中台定位是为企业未来5~10年发展提供数据能力支撑，在DT时代对企业进行智能化升级。</span><br><span class="line">2：在企业数据形态方面，在该阶段，企业数据伴随数据驱动的业务快速发展，数据量快速增长，通过建立企业体系化，标准化的数据采集、存储、实现企业数据的全面资产化。</span><br><span class="line">3：在数据应用场景方面，在该阶段，数据应用通过统一的数据资产体系，提供统一、标准化的数据服务能力，为企业各类快速变化的业务应用提供数据服务支撑。</span><br><span class="line">4：在数据应用工具方面，建立一套体系化的数据汇聚、加工、管理、服务及应用体系，逐渐实现大数据能力工具化，工具平台化，平台智能化。</span><br><span class="line">5：在企业组织架构方面，在该阶段，企业组织架构中开始在管理层设置数据管理委员会来负责数据机制的建设和管理，将数据变为企业的一种独特资产。同时也会成立专门的资产运营部门，保障数据资产应用的合理性和效率，将更多的数据服务消费者引入到数据平台之中。</span><br><span class="line"></span><br><span class="line">这就是数据应用成熟度的四个阶段，目前中大型企业大部分处于从决策支持阶段转向数据驱动阶段，一些一线大型互联网企业正在处于从数据驱动阶段转向运营优化阶段。</span><br><span class="line">目前数据中台正处于快速发展阶段，成熟的大型公司都在开始着手构建数据中台。</span><br></pre></td></tr></table></figure><h3 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">下面有几个小案例，我们来分析一下</span><br><span class="line"></span><br><span class="line">企业A：类似于”万能钥匙 ”之类的工具类APP，随着DAU的增加，需要给用户提供个性化推荐内容</span><br><span class="line">目前比较合适的是启动一个内容推荐类的算法项目，在可预见的将来，看不到更多的数据场景，所以不适合建设数据中台。</span><br><span class="line"></span><br><span class="line">企业B：类似于”百果园 ”之类的连锁店，门店数量比较多，需要用大数据来精细化运营用户和商品</span><br><span class="line">因为具备了一定的门店规模和数据规模，可以实现一些个性化营销推送，商品猜你喜欢等功能，比较适合建设数据中台。</span><br><span class="line"></span><br><span class="line">企业C：类似于”华为 ”这样的多业态集团公司，旗下有多个业务板块，各个业务板块都有自己的数仓和报表</span><br><span class="line"></span><br><span class="line">这种属于多业态集团公司，是最适合建设数据中台的。</span><br></pre></td></tr></table></figure><h2 id="数据中台企业级解决方案"><a href="#数据中台企业级解决方案" class="headerlink" title="数据中台企业级解决方案"></a>数据中台企业级解决方案</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面我们对数据中台的理论进行分析，下面我们来看一下，数据中台在一些大型企业中的落地方案</span><br></pre></td></tr></table></figure><h3 id="阿里数据中台"><a href="#阿里数据中台" class="headerlink" title="阿里数据中台"></a>阿里数据中台</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在国内，”中台”的概念是阿里带头喊出来的，所以我们先来看一下阿里的数据中台方案</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171137113.png" alt="image-20230517113733709"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">最底层是计算和存储平台</span><br><span class="line">往上面是垂直数据中心，负责全域数据采集与引入，以需求为驱动，以数据多样性的全域思想为指导，采集与引入全业务、多终端、多形态的数据；</span><br><span class="line">再往上面是公共数据中心，按照基础层、公共中间层、应用层的数据分层架构模式，通过数据指标结构化、规范化的方式实现指标口径统一</span><br><span class="line">再往上面是萃取数据中心，形成以业务核心对象为中心的连接和标签体系，深度萃取数据价值</span><br><span class="line">再往上面是统一主题式服务，通过构建服务元数据中心和数据服务查询引擎，面向业务统一数据出口与数据查询逻辑，屏蔽多数据源与多物理表；</span><br><span class="line">左侧是数据资产管理：通过资产分析、应用、优化、运营等方面实现降低数据管理成本、追踪数据价值。</span><br><span class="line"></span><br><span class="line">最上层的是不同的产品线应用，通过下面的数据中台提供一站式数据服务支撑。</span><br><span class="line"></span><br><span class="line">阿里数据中台有三大核心：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171138752.png" alt="image-20230517113808804"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">OneData（统一数据）：定义数据标准与建模标准，对离线数据、实时数据建立数据资产体系</span><br><span class="line">OneEntity（统一实体）：主数据的管理，实现全域产品体系主数据融合</span><br><span class="line">OneService（统一服务）：统一对外提供API与SDK服务</span><br><span class="line">这就是阿里数据中台的三大核心。其实就是将全域数据统一标准，然后打通，最终统一对外提供数据服务。</span><br></pre></td></tr></table></figure><h3 id="菜鸟数据中台"><a href="#菜鸟数据中台" class="headerlink" title="菜鸟数据中台"></a>菜鸟数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171138932.png" alt="image-20230517113839690"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">整体技术架构，分三层，底层是基础设施，基础平台，中间是中台，上面是前台。</span><br><span class="line">有些同学可能会有困惑“数据中台和大数据平台”的区别是什么？</span><br><span class="line">图中的基础平台就是我们常说的大数据平台，包含了数据的采集、计算、加工等。</span><br><span class="line">数据中台是构建在整个大数据平台之上的，它是围绕数据运营、分析、应用等场景去做的一套解决方案。</span><br><span class="line"></span><br><span class="line">数据中台分成两块，一个是数据层，一个是服务层。数据层就是我们前面说的“数仓“，这里边包含菜鸟的所有数据，沉淀的数据资产。</span><br><span class="line">再往上是服务层，这里划分成了几个套件，每个套件都是围绕数据使用的一个场景做的解决方案 。</span><br><span class="line"></span><br><span class="line">右侧的东西是数据管理套件，从数据的加工生产到使用，它从全链路的视角把数据给管理起来。</span><br><span class="line"></span><br><span class="line">最上层是前台业务。</span><br></pre></td></tr></table></figure><h3 id="滴滴数据中台"><a href="#滴滴数据中台" class="headerlink" title="滴滴数据中台"></a>滴滴数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171146305.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">最底层是数据架构：数据架构体系包含了当前大数据领域主流的技术</span><br><span class="line"></span><br><span class="line">再往上面是数据研发，数据中间件，实现数据的采集，计算和数据质量监控。</span><br><span class="line"></span><br><span class="line">再往上面是数据资产体系，构建了数据字典和数据图谱，然后通过数据赋能，提供各种自助查询和可视化分析。</span><br><span class="line"></span><br><span class="line">最上层是数据服务层，将数据服务化提供给各个产品使用。</span><br></pre></td></tr></table></figure><h3 id="苏宁数据中台"><a href="#苏宁数据中台" class="headerlink" title="苏宁数据中台"></a>苏宁数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171145463.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">最底层是大数据计算存储引擎</span><br><span class="line">上层是数据开发套件，负责加工计算数据，</span><br><span class="line">然后是数据仓库主题域，构建多维度的数据主题，</span><br><span class="line">接下来是数据治理套件，管理数据模型，保证数据质量。</span><br><span class="line"></span><br><span class="line">再往上层是数据应用引擎，这里面包含了可视化引擎，数据服务引擎，数据分析引擎和画像引擎，通过这几个引擎对外提供数据服务。</span><br><span class="line"></span><br><span class="line">最上层是数据应用层，主要是使用数据的。</span><br></pre></td></tr></table></figure><h3 id="华为云数据中台"><a href="#华为云数据中台" class="headerlink" title="华为云数据中台"></a>华为云数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171145118.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">华为云数据中台在这里可以划分为三块</span><br><span class="line"></span><br><span class="line">第一块是数据建设：其实就是把全域数据采集过来，对数据加工计算，基于各种维度构建数据主题。</span><br><span class="line">第二块是平台建设：这里面抽取出来了一些公共的功能组件，并且提供了数据服务。</span><br><span class="line"></span><br><span class="line">第三块是中台消费场景，这里面会有多种场景依靠数据中台提供数据支撑。</span><br></pre></td></tr></table></figure><h3 id="浙江移动数据中台"><a href="#浙江移动数据中台" class="headerlink" title="浙江移动数据中台"></a>浙江移动数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171144694.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">浙江移动打造的数据中台，是为了实现跨域数据整合并沉淀公共的数据能力，同时提供丰富的数据模型，标准化的数据服务，个性化的开发平台与工具，满足一线数据开放和智慧运营的要求。</span><br><span class="line">这个数据中台架构中主要包含了三块内容</span><br><span class="line">数据模型、数据服务和数据开发。</span><br><span class="line"></span><br><span class="line">数据模型：负责实现数据与数据模型打通。</span><br><span class="line">数据服务：负责封装标准数据服务，对外提供数据查询服务</span><br><span class="line">数据开发：针对各种个性化数据应用开发需求提供技术支持</span><br></pre></td></tr></table></figure><h3 id="某大数据服务商数据中台"><a href="#某大数据服务商数据中台" class="headerlink" title="某大数据服务商数据中台"></a>某大数据服务商数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171142475.png" alt="image-20230517114238211"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">底层是基础设施和计算层</span><br><span class="line">往上面是数据开发治理模块，里面涉及离线数据开发、实时数据开发和算法开发。</span><br><span class="line">再往上层是数据服务层，这里面会对数据进行体系化，最终对外提供服务。</span><br><span class="line">最上层是业务应用层，这里会通过数据服务提供数据支撑。</span><br></pre></td></tr></table></figure><h3 id="某企业数据大脑"><a href="#某企业数据大脑" class="headerlink" title="某企业数据大脑"></a>某企业数据大脑</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171141192.png" alt="image-20230517114128072"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这个是某企业的数据大脑总体设计，里面包含了数据中台。</span><br><span class="line">数据大脑主要是为了解决大数据系统建设中数据存储、连通、使用的共性问题，形成业务数据中台，包括数据资源的统一规划，数据整体建模和资产管理，数据标签化计算，形成不同行业的数据体系。</span><br><span class="line">将行业知识库和领域数据相结合，开发各类计算组件，构建统一数据加工平台，对数据进行加工整理支撑行业应用，形成相关领域行业的数据大脑。</span><br><span class="line"></span><br><span class="line">这里面的数据中台主要包含以下内容：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171141182.png" alt="image-20230517114145986"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">到这为止，我们分析了多个企业的数据中台，虽然这些企业的数据中台架构没有完全一样的，但是总结下来我们会发现，他们里面都会有一些共同的核心内容。</span><br><span class="line">数据采集存储、数据加工计算，整合打通各个维度数据，最后对外提供数据服务。</span><br><span class="line">其实精简之后就是我在前面给大家总结的数据中台四字箴言，采存通用。</span><br><span class="line"></span><br><span class="line">希望通过我们前面的学习能够然大家对数据中台有一个整体的认识。</span><br></pre></td></tr></table></figure><h2 id="数据中台之数据加工总线"><a href="#数据中台之数据加工总线" class="headerlink" title="数据中台之数据加工总线"></a>数据中台之数据加工总线</h2><h3 id="目前大数据领域实时计算的现状"><a href="#目前大数据领域实时计算的现状" class="headerlink" title="目前大数据领域实时计算的现状"></a>目前大数据领域实时计算的现状</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">随着大数据行业的整体发展，企业对实时计算的需求越来越多，特别是在构建实时数仓的时候，需要接入很多实时数据源，并且数仓还是分层的，针对每一层的数据都需要进行实时计算，此时就需要开发很多实时计算程序，实时计算程序的复用性很低，针对每一种类型的数据都需要开发对应的实时计算程序，开发成本高，并且对程序员也不友好，需要专门的大数据开发工程师，所以我们希望在实时计算领域能够提供类似HiveSQL的功能，直接写SQL就能实现实时计算任务，不需要每次都写一堆的代码，提高工作效率，尽可能让会只会SQL的普通开发人员也能轻松的开发实时计算任务。</span><br><span class="line"></span><br><span class="line">为了解决这个痛点，于是，我们研发了数据加工总线平台，也可以称之为数据实时流转平台。</span><br></pre></td></tr></table></figure><h3 id="什么是数据加工总线"><a href="#什么是数据加工总线" class="headerlink" title="什么是数据加工总线"></a>什么是数据加工总线</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">为了使实时数据的处理能够更加高效、简单，所以我们研发了一站式实时数据开发平台。只需要在页面选择数据源、目的地以及对应的SQL计算逻辑，就可以轻松实现海量实时数据计算任务的开发。</span><br><span class="line"></span><br><span class="line">这个平台主要的功能就是支持SQL实现实时数据计算任务的开发。</span><br><span class="line"></span><br><span class="line">我们期望达到的目标，通过这套平台，可以实现用SQL解决80%以上的实时数据计算需求。</span><br></pre></td></tr></table></figure><h3 id="数据加工总线原型图总览"><a href="#数据加工总线原型图总览" class="headerlink" title="数据加工总线原型图总览"></a>数据加工总线原型图总览</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于数据加工总线涉及前端和后端，在企业中前端代码有专门的同事负责开发，我们大数据部门只需要负责后台功能开发即可，所以在课程中不涉及前端页面代码，在这里通过原型图来演示一下数据加工总线具体的使用流程，加深大家的理解。</span><br><span class="line">注意：原型图只能在这里给大家演示一下，不能发出去，希望大家理解。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171152004.jpg" alt="图片描述"></p><h3 id="数据加工总线架构图V1-0"><a href="#数据加工总线架构图V1-0" class="headerlink" title="数据加工总线架构图V1.0"></a>数据加工总线架构图V1.0</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下数据加工总线的后台架构图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171154966.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">数据源和目的地都是Kafka，因为目前在大数据领域，实时数据一般都是用的Kafka。</span><br><span class="line">中间就是我们需要开发的核心计算引擎，基于SparkSQL封装的实时SQL计算引擎。</span><br><span class="line">为什么在这要选择使用SparkSQL？</span><br><span class="line">因为SparkSQL在我们公司已经广泛应用了很长时间了，并且Spark框架本身也迭代了很多版本了，比较稳定，整个生态圈也比较完善。所以前期在技术选型的时候优先考虑的是底层计算引擎的稳定性。</span><br><span class="line">还有就是秒级别的延时是可以满足业务需求的，所以当时SparkSQL+SparkStreaming是最好的方案。</span><br><span class="line">Flink当时版本还不是很稳定，FlinkSQL也是刚出现没多久，所以没有直接使用Flink。</span><br><span class="line">当时我们也考虑了，等第一个版本稳定了以后，后期再把FlinkSQL也增加进来，提供两套底层计算引擎，可以根据需求进行动态切换。</span><br><span class="line"></span><br><span class="line">针对这里面的数据字段和数据模型的概念做一下解释</span><br><span class="line">由于我们需要在SparkSQL中基于kafka的数据进行建表，kafka中的数据我们使用的是json格式的，json格式的数据只有字段名称，缺少字段类型信息，官方一点来说其实就是缺少元数据信息，所以需要针对kafka中的数据定义元数据，这样才能在SparkSQL中建表。</span><br><span class="line"></span><br><span class="line">注意：元数据的定义是在数据治理子系统中维护的。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v2.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v2.0-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-2.html</id>
    <published>2023-05-03T09:32:38.000Z</published>
    <updated>2023-05-20T12:50:47.753Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v2-0-2"><a href="#第十八周-直播平台三度关系推荐v2-0-2" class="headerlink" title="第十八周 直播平台三度关系推荐v2.0-2"></a>第十八周 直播平台三度关系推荐v2.0-2</h1><h2 id="每周一计算最近一周内主活主播的三度关系列表-任务六"><a href="#每周一计算最近一周内主活主播的三度关系列表-任务六" class="headerlink" title="每周一计算最近一周内主活主播的三度关系列表(任务六)"></a>每周一计算最近一周内主活主播的三度关系列表(任务六)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现每周一计算最近一周内主活主播的三度关系列表</span><br><span class="line">创建子module项目：get_recommend_list</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="子项目pom"><a href="#子项目pom" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;get_recommend_list&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line"></span><br><span class="line">注意：此时需要通过flink读取neo4j中的数据，但是针对DataSet是不支持addSource方法的，但是它里面有一个createInput，可以接收一个自定义的InputFormat，所以我就需要定义一个Neo4jInputFormat了</span><br></pre></td></tr></table></figure><h3 id="Neo4jInputFormat-scala"><a href="#Neo4jInputFormat-scala" class="headerlink" title="Neo4jInputFormat.scala"></a>Neo4jInputFormat.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">创建类：Neo4jInputFormat</span><br><span class="line">代码如下：</span><br><span class="line"></span><br><span class="line">注意：此代码中的输入组件只能使用单并行度执行，如果使用多并行度查询可能会出现重复数据</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.io.statistics.<span class="type">BaseStatistics</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.io.&#123;<span class="type">DefaultInputSplitAssigner</span>, <span class="type">NonParallelInput</span>, <span class="type">RichInputFormat</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.io.&#123;<span class="type">GenericInputSplit</span>, <span class="type">InputSplit</span>, <span class="type">InputSplitAssigner</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">Driver</span>, <span class="type">GraphDatabase</span>, <span class="type">Result</span>, <span class="type">Session</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从Neo4j中查询满足条件的主播</span></span><br><span class="line"><span class="comment"> * 一周内活跃过，并且主播等级大于4</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neo4jInputFormat</span> <span class="keyword">extends</span> <span class="title">RichInputFormat</span>[<span class="type">String</span>,<span class="type">InputSplit</span>] <span class="keyword">with</span> <span class="title">NonParallelInput</span></span>&#123; <span class="comment">// 这里是多继承</span></span><br><span class="line">  <span class="comment">//注意：with NonParallelInput 表示此组件不支持多并行度</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//保存neo4j相关的配置参数</span></span><br><span class="line">  <span class="keyword">var</span> param: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>] = <span class="type">Map</span>()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> driver: <span class="type">Driver</span> = _</span><br><span class="line">  <span class="keyword">var</span> session: <span class="type">Session</span> = _</span><br><span class="line">  <span class="keyword">var</span> result: <span class="type">Result</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 构造函数</span></span><br><span class="line"><span class="comment">   * 接收neo4j相关的配置参数</span></span><br><span class="line"><span class="comment">   * @param param</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(param: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>])&#123;</span><br><span class="line">    <span class="keyword">this</span>()</span><br><span class="line">    <span class="keyword">this</span>.param = param</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 配置此输入格式</span></span><br><span class="line"><span class="comment">   * @param parameters</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">configure</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 获取输入数据的基本统计信息</span></span><br><span class="line"><span class="comment">   * @param cachedStatistics</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getStatistics</span></span>(cachedStatistics: <span class="type">BaseStatistics</span>): <span class="type">BaseStatistics</span> = &#123;</span><br><span class="line">    cachedStatistics</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 对输入数据切分split</span></span><br><span class="line"><span class="comment">   * @param minNumSplits</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createInputSplits</span></span>(minNumSplits: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">InputSplit</span>] = &#123;</span><br><span class="line">    <span class="type">Array</span>(<span class="keyword">new</span> <span class="type">GenericInputSplit</span>(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 获取切分的split</span></span><br><span class="line"><span class="comment">   * @param inputSplits</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getInputSplitAssigner</span></span>(inputSplits: <span class="type">Array</span>[<span class="type">InputSplit</span>]): <span class="type">InputSplitAssigner</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DefaultInputSplitAssigner</span>(inputSplits)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 初始化方法：只执行一次</span></span><br><span class="line"><span class="comment">   * 获取neo4j连接，开启会话</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">openInputFormat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//初始化Neo4j连接</span></span><br><span class="line">    <span class="keyword">this</span>.driver = <span class="type">GraphDatabase</span>.driver(param(<span class="string">"boltUrl"</span>), <span class="type">AuthTokens</span>.basic(param(<span class="string">"userName"</span>), param(<span class="string">"passWord"</span>)))</span><br><span class="line">    <span class="comment">//开启会话</span></span><br><span class="line">    <span class="keyword">this</span>.session = driver.session()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 关闭Neo4j连接</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">closeInputFormat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(driver!=<span class="literal">null</span>)&#123;</span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 此方法也是只执行一次</span></span><br><span class="line"><span class="comment">   * @param split</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(split: <span class="type">InputSplit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.result = session.run(<span class="string">"match (a:User) where a.timestamp &gt;="</span>+param(<span class="string">"timestamp"</span>)+<span class="string">" and a.level &gt;= "</span>+param(<span class="string">"level"</span>)+<span class="string">" return a.uid"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 如果数据读取完毕号以后，需要返回true</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reachedEnd</span></span>(): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    !result.hasNext</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 读取结果数据，一次读取一条</span></span><br><span class="line"><span class="comment">   * @param reuse</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">nextRecord</span></span>(reuse: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> record = result.next()</span><br><span class="line">    <span class="keyword">val</span> uid = record.get(<span class="number">0</span>).asString()</span><br><span class="line">    uid</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 关闭会话</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(session!=<span class="literal">null</span>)&#123;</span><br><span class="line">      session.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="GetRecommendList-scala"><a href="#GetRecommendList-scala" class="headerlink" title="GetRecommendList.scala"></a>GetRecommendList.scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.&#123;<span class="type">ArrayBuffer</span>, <span class="type">ListBuffer</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务6：</span></span><br><span class="line"><span class="comment"> * 每周一计算最近一周内主活主播的三度关系列表</span></span><br><span class="line"><span class="comment"> * 注意：</span></span><br><span class="line"><span class="comment"> * 1：待推荐主播最近一周内活跃过</span></span><br><span class="line"><span class="comment"> * 2：待推荐主播等级&gt;4</span></span><br><span class="line"><span class="comment"> * 3：待推荐主播最近1个月视频评级满足3B+或2A+(flag=1)</span></span><br><span class="line"><span class="comment"> * 4：待推荐主播的粉丝列表关注重合度&gt;2</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GetRecommendListScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"GetRecommendListScala"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">var</span> timestamp = <span class="number">0</span>L <span class="comment">//过滤最近一周内是否活跃过</span></span><br><span class="line">    <span class="keyword">var</span> dumplicateNum = <span class="number">2</span> <span class="comment">//粉丝列表关注重合度</span></span><br><span class="line">    <span class="keyword">var</span> level = <span class="number">4</span> <span class="comment">//主播等级</span></span><br><span class="line">    <span class="keyword">var</span> outputPath = <span class="string">"hdfs://bigdata01:9000/data/recommend_data/20260201"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      appName = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">      timestamp = args(<span class="number">4</span>).toLong</span><br><span class="line">      dumplicateNum = args(<span class="number">5</span>).toInt</span><br><span class="line">      level = args(<span class="number">6</span>).toInt</span><br><span class="line">      outputPath = args(<span class="number">7</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span>  org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> param = <span class="type">Map</span>(<span class="string">"boltUrl"</span>-&gt;boltUrl,<span class="string">"userName"</span>-&gt;userName,<span class="string">"passWord"</span>-&gt;passWord,<span class="string">"timestamp"</span>-&gt;timestamp.toString,<span class="string">"level"</span>-&gt;level.toString)</span><br><span class="line">    <span class="comment">//获取一周内主活的主播 并且主播等级大于4的数据</span></span><br><span class="line">    <span class="keyword">val</span> uidSet = env.createInput(<span class="keyword">new</span> <span class="type">Neo4jInputFormat</span>(param))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一次处理一批</span></span><br><span class="line">    <span class="comment">//过滤出粉丝关注重合度&gt;2的数据，并且对关注重合度倒序排序</span></span><br><span class="line">    <span class="comment">//最终的数据格式是：主播id,待推荐的主播id</span></span><br><span class="line">    <span class="keyword">val</span> mapSet = uidSet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      <span class="comment">//保存计算出来的结果</span></span><br><span class="line">      <span class="keyword">val</span> resultArr = <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">      it.foreach(uid=&gt;&#123;</span><br><span class="line">        <span class="comment">//计算一个用户的三度关系(主播的三度关系)</span></span><br><span class="line">        <span class="comment">//注意：数据量打了之后，这个计算操作是非常耗时</span></span><br><span class="line">        <span class="keyword">val</span> result = session.run(<span class="string">"match (a:User &#123;uid:'"</span>+uid+<span class="string">"'&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt; (c:User) return a.uid as auid,c.uid as cuid,count(c.uid) as sum order by sum desc limit 30"</span>)</span><br><span class="line">        <span class="comment">//对b、c的主活时间进行过滤，以及对c的level和flag值进行过滤</span></span><br><span class="line">        <span class="comment">/*val result = session.run("match (a:User &#123;uid:'"+uid+"'&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt; (c:User)" +</span></span><br><span class="line"><span class="comment">          " where b.timestamp &gt;= "+timestamp+" and c.timestamp &gt;= "+timestamp+" and c.level &gt;= "+level +" and c.flag =1 " +</span></span><br><span class="line"><span class="comment">          " return a.uid as auid,c.uid as cuid,count(c.uid) as sum order by sum desc limit 30")*/</span></span><br><span class="line">        <span class="keyword">while</span>(result.hasNext)&#123;</span><br><span class="line">          <span class="keyword">val</span> record = result.next()</span><br><span class="line">          <span class="keyword">val</span> sum = record.get(<span class="string">"sum"</span>).asInt()</span><br><span class="line">          <span class="keyword">if</span>(sum &gt; dumplicateNum)&#123;</span><br><span class="line">            resultArr += record.get(<span class="string">"auid"</span>).asString()+<span class="string">"\t"</span>+record.get(<span class="string">"cuid"</span>).asString()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      resultArr.iterator</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把数据转成tupl2的形式</span></span><br><span class="line">    <span class="keyword">val</span> tup2Set = mapSet.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> splits = line.split(<span class="string">"\t"</span>)</span><br><span class="line">      (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//根据主播id进行分组，可以获取到这个主播的待推荐列表</span></span><br><span class="line">    <span class="keyword">val</span> reduceSet = tup2Set.groupBy(_._1).reduceGroup(it =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> list = it.toList</span><br><span class="line">      <span class="keyword">val</span> tmpList = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">      <span class="keyword">for</span> (l &lt;- list) &#123;</span><br><span class="line">        tmpList += l._2</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//把结果组装成这种形式 1001   1002,1003,1004</span></span><br><span class="line">      (list.head._1, tmpList.toList.mkString(<span class="string">","</span>))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//注意：writeAsCsv只能保存tuple类型的数据</span></span><br><span class="line">    <span class="comment">//writerAsText可以支持任何类型，如果是对象，会调用对象的toString方法写入到文件中</span></span><br><span class="line">    reduceSet.writeAsCsv(outputPath,<span class="string">"\n"</span>,<span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行任务</span></span><br><span class="line">    env.execute(appName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">其实我们也可以直接在这里将结果输入写入到redis中，不过为了整体看起来更加规范，在这就先把数据临时写到hdfs上面。</span><br><span class="line"></span><br><span class="line">在本地执行代码，然后到hdfs上面确认结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161459949.png" alt="image-20230516145733981"></p><h3 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来对程序编译打包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br></pre></td></tr></table></figure><h3 id="startGetRecommendList-sh"><a href="#startGetRecommendList-sh" class="headerlink" title="startGetRecommendList.sh"></a>startGetRecommendList.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取上周一的时间</span></span><br><span class="line">dt=`date -d "7 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">    dt=`date -d "7 days ago $1" +"%Y%m%d"`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="GetRecommendListScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"><span class="meta">#</span><span class="bash">获取上周一的时间戳(单位：毫秒)</span></span><br><span class="line">timestamp=`date --date="$&#123;dt&#125;" +%s`000</span><br><span class="line"><span class="meta">#</span><span class="bash">粉丝列表关注重合度</span></span><br><span class="line">dumplicateNum=2</span><br><span class="line"><span class="meta">#</span><span class="bash">主播等级</span></span><br><span class="line">level=4</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">输出结果数据路径</span></span><br><span class="line">outputPath="hdfs://bigdata01:9000/data/recommend_data/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.GetRecommendListScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/get_recommend_list-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;appName&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125; $&#123;timestamp&#125; $&#123;dumplicateNum&#125; $&#123;level&#125; $&#123;outputPath&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $8&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startGetRecommendList.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161504206.png" alt="image-20230516150429277"></p><h2 id="三度关系列表数据导出到Redis-任务七"><a href="#三度关系列表数据导出到Redis-任务七" class="headerlink" title="三度关系列表数据导出到Redis(任务七)"></a>三度关系列表数据导出到Redis(任务七)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现将三度关系列表数据导出到Redis</span><br><span class="line"></span><br><span class="line">注意：此任务每周执行一次，在任务6执行完毕以后执行这个。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">创建子module项目：export_data</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="pom-xml"><a href="#pom-xml" class="headerlink" title="pom.xml"></a>pom.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;export_data&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;redis.clients&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;jedis&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line">创建类：ExportDataScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><h3 id="ExportDataScala"><a href="#ExportDataScala" class="headerlink" title="ExportDataScala"></a>ExportDataScala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.<span class="type">Jedis</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务7：</span></span><br><span class="line"><span class="comment"> * 将三度列表关系数据导出到Redis</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExportDataScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/recommend_data/20260125"</span></span><br><span class="line">    <span class="keyword">var</span> redisHost = <span class="string">"bigdata04"</span></span><br><span class="line">    <span class="keyword">var</span> redisPort = <span class="number">6379</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      redisHost = args(<span class="number">1</span>)</span><br><span class="line">      redisPort = args(<span class="number">2</span>).toInt</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//读取hdfs中的数据</span></span><br><span class="line">    <span class="keyword">val</span> text = env.readTextFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    text.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取jedis连接</span></span><br><span class="line">      <span class="keyword">val</span> jedis = <span class="keyword">new</span> <span class="type">Jedis</span>(redisHost, redisPort)</span><br><span class="line">      <span class="comment">//开启管道(提高性能，不开启也没事)</span></span><br><span class="line">      <span class="keyword">val</span> pipeline = jedis.pipelined()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//获取uid</span></span><br><span class="line">        <span class="keyword">val</span> uid = fields(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//获取待推荐主播列表</span></span><br><span class="line">        <span class="keyword">val</span> recommend_uids = fields(<span class="number">1</span>).split(<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//注意：在这里给key起一个有意义的名字，l表示list类型、rec是recommend的简写(简写是因为key要放内存)</span></span><br><span class="line">        <span class="keyword">val</span> key = <span class="string">"l_rec_"</span>+uid</span><br><span class="line"></span><br><span class="line">        <span class="comment">//先删除(保证每周更新一次),pipeline中的删除操作在scala语言下使用有问题</span></span><br><span class="line">        jedis.del(key)</span><br><span class="line">        <span class="keyword">for</span>(r_uid &lt;- recommend_uids)&#123;</span><br><span class="line">          pipeline.rpush(key,r_uid)</span><br><span class="line">          <span class="comment">//给key设置一个有效时间，30天，如果30天数据没有更新，则删除此key</span></span><br><span class="line">          pipeline.expire(key,<span class="number">30</span>*<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//提交管道中的命令</span></span><br><span class="line">      pipeline.sync()</span><br><span class="line">      <span class="comment">//关闭jedis连接</span></span><br><span class="line">      jedis.close()</span><br><span class="line">      <span class="string">""</span></span><br><span class="line">    &#125;).print()</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在执行代码之前，需要先把redis服务启动起来</span><br></pre></td></tr></table></figure><h3 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在本地执行代码，到redis中验证效果。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161647908.png" alt="image-20230516164750844"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161638708.png" alt="image-20230516163851685"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161649565.png" alt="image-20230516164936168"></p><h3 id="打包-1"><a href="#打包-1" class="headerlink" title="打包"></a>打包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来对程序编译打包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br></pre></td></tr></table></figure><h3 id="startExportData-sh"><a href="#startExportData-sh" class="headerlink" title="startExportData.sh"></a>startExportData.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取上周一的时间</span></span><br><span class="line">dt=`date -d "7 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">    dt=`date -d "7 days ago $1" +"%Y%m%d"`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/recommend_data/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="ExportDataScala"`date +%s`</span><br><span class="line">redisHost="bigdata04"</span><br><span class="line">redisPort=6379</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.ExportDataScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/export_data-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;filePath&#125; $&#123;redisHost&#125; $&#123;redisPort&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $8&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="提交任务-1"><a href="#提交任务-1" class="headerlink" title="提交任务"></a>提交任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">向集群提交任务，先把redis中之前生成的数据删一下</span><br><span class="line"></span><br><span class="line">sh -x startExportData.sh 20260201</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">任务成功执行，验证redis中的结果也是正确的</span><br></pre></td></tr></table></figure><h2 id="数据接口定义及开发-java-web了解即可"><a href="#数据接口定义及开发-java-web了解即可" class="headerlink" title="数据接口定义及开发(java web了解即可)"></a>数据接口定义及开发(java web了解即可)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前面我们把结果数据计算好了，那接下来我们需要开发数据接口，对外提供数据。</span><br><span class="line">首先定义接口文档</span><br></pre></td></tr></table></figure><h3 id="数据接口文档定义"><a href="#数据接口文档定义" class="headerlink" title="数据接口文档定义"></a>数据接口文档定义</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">为了方便跨部门数据使用，我们需要定义接口文档，便于其他部门的同事使用我们的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161701517.png" alt="image-20230516170118438"></p><h3 id="数据接口代码开发"><a href="#数据接口代码开发" class="headerlink" title="数据接口代码开发"></a>数据接口代码开发</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">开发数据接口需要用到javaweb项目，在这给大家演示一下如何基于spring-boot搭建一个javaweb项目</span><br><span class="line">创建子module项目：data_server</span><br><span class="line">在pom.xml中添加依赖</span><br><span class="line">首先添加spring-boot的依赖，还有fastjson依赖，因为我们后面在传输数据的时候需要使用json格式</span><br></pre></td></tr></table></figure><h4 id="pom-xml-1"><a href="#pom-xml-1" class="headerlink" title="pom.xml"></a>pom.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;data_server&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;redis.clients&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;jedis&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.3.2&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spring-boot-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.1.1.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;mainClass&gt;com.imooc.Application&lt;&#x2F;mainClass&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;repackage&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在项目的resource目录中添加这两个文件</span><br><span class="line"></span><br><span class="line">application.properties</span><br><span class="line">logback.xml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">创建包：com.imooc</span><br><span class="line">然后把下面这几个文件夹及文件拷贝到com.imooc包里面</span><br><span class="line">controller</span><br><span class="line">Application.java</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161716013.png" alt="image-20230516171622873"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">直接在Application类中右键执行，就可以启动这个javaweb项目，项目内部已经集成了tomcta容器，监听的端口是8085</span><br><span class="line">验证项目是否可以正常访问。</span><br><span class="line">在浏览器中访问</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161726656.png" alt="image-20230516172612542"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">能看到结果数据说明此项目的基础框架是ok的，接下来我们就来开发一个接口</span><br><span class="line">由于在这我们需要操作redis，所以需要到pom.xml中增加jedis的依赖，以及把我们之前开发的RedisUtils工具类也拷贝过来</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在com.imooc下创建utils目录，把RedisUtils拷贝到里面</span><br><span class="line"></span><br><span class="line">接下来到DataController类中增加一个方法：getRecommendList</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><h4 id="DataController-java"><a href="#DataController-java" class="headerlink" title="DataController.java"></a>DataController.java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> com.imooc.utils.RedisUtils;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.*;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.Jedis;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 数据接口V1.0</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RestController</span><span class="comment">//控制器类</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/v1"</span>)<span class="comment">//映射路径</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataController</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(DataController<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 测试接口</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@RequestMapping</span>(value=<span class="string">"/t1"</span>,method = RequestMethod.GET)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">test</span><span class="params">(@RequestParam(<span class="string">"name"</span>)</span> String name) </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"hello,"</span>+name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据主播uid查询三度关系推荐列表数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 返回数据格式：</span></span><br><span class="line"><span class="comment">     * &#123;"flag":"success/error","msg":"错误信息","rec_uids":["1005","1004"]&#125;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> uid</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@RequestMapping</span>(value=<span class="string">"/get_recommend_list"</span>,method = RequestMethod.GET)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> JSONObject <span class="title">getRecommendList</span><span class="params">(@RequestParam(<span class="string">"uid"</span>)</span> String uid) </span>&#123;</span><br><span class="line">        JSONObject resobj = <span class="keyword">new</span> JSONObject();</span><br><span class="line">        String flag = <span class="string">"success"</span>;</span><br><span class="line">        String msg = <span class="string">"ok"</span>;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            Jedis jedis = RedisUtils.getJedis();</span><br><span class="line">            <span class="comment">//获取待推荐列表数据</span></span><br><span class="line">            List&lt;String&gt; uidList = jedis.lrange(<span class="string">"l_rec_"</span> + uid, <span class="number">0</span>, -<span class="number">1</span>);</span><br><span class="line">            String[] uidArr = uidList.toArray(<span class="keyword">new</span> String[<span class="number">0</span>]);</span><br><span class="line">            resobj.put(<span class="string">"rec_uids"</span>,uidArr);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            flag = <span class="string">"error"</span>;</span><br><span class="line">            msg = e.getMessage();</span><br><span class="line">            logger.error(msg);</span><br><span class="line">        &#125;</span><br><span class="line">        resobj.put(<span class="string">"flag"</span>,flag);</span><br><span class="line">        resobj.put(<span class="string">"msg"</span>,msg);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> resobj;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Application-java"><a href="#Application-java" class="headerlink" title="Application.java"></a>Application.java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spring boot 入口启动程序</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SpringBootApplication</span> <span class="comment">//定义springboot入口程序</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(Application<span class="class">.<span class="keyword">class</span>,<span class="title">args</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">重新启动data_server项目</span><br><span class="line">然后在浏览器中访问刚才开发的接口，能看到正常输出结果则说明此接口是正常的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161730743.png" alt="image-20230516173008401"></p><h3 id="打包-2"><a href="#打包-2" class="headerlink" title="打包"></a>打包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup java -jar data_server-1.0-SNAPSHOT.jar &amp;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个命令相当于模拟浏览器的请求</span><br><span class="line">curl -XGET &#39;http:&#x2F;&#x2F;bigdata04:8085&#x2F;v1&#x2F;get_recommend_list?uid&#x3D;1000&#39;</span><br><span class="line">&#123;&quot;msg&quot;:&quot;ok&quot;,&quot;flag&quot;:&quot;success&quot;,&quot;rec_uids&quot;:[&quot;1005&quot;,&quot;1004&quot;]&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161754559.png" alt="image-20230516175456652"></p><h2 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略略略</span><br></pre></td></tr></table></figure><h2 id="项目扩展优化"><a href="#项目扩展优化" class="headerlink" title="项目扩展优化"></a>项目扩展优化</h2><h3 id="如何保证在Neo4j中维护平台全量粉丝关注数据"><a href="#如何保证在Neo4j中维护平台全量粉丝关注数据" class="headerlink" title="如何保证在Neo4j中维护平台全量粉丝关注数据"></a>如何保证在Neo4j中维护平台全量粉丝关注数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对粉丝关注数据我们有两份</span><br><span class="line">第一份是历史粉丝关注数据</span><br><span class="line">第二份是实时粉丝关注数据</span><br><span class="line"></span><br><span class="line">如何通过这两份数据实现维护平台全量粉丝关注数据呢？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">背景是这样的</span><br><span class="line">历史粉丝数据是由服务端每天晚上0点的时候定时同步到mysql数据库中的，因为之前平台是把粉丝的关注数据，存储到了redis中，每天晚上定时向mysql中同步一次。</span><br><span class="line">实时粉丝数据在准备做这个项目之前通过日志采集工具把这些数据采集到kafka里面了</span><br><span class="line"></span><br><span class="line">基于此，假设我们是在2026年2月1日那天上午10点开始将mysql中的历史数据导出来，然后批量导入到neo4j中，mysql中的粉丝数据其实是截止到2026年2月1日0点的。</span><br><span class="line">这个导入过程当时耗时将近2天。</span><br><span class="line">也就是在2026年2月3日上午10点左右导入完毕的，此时neo4j中的粉丝关注数据是截止到2026年2月1日0点的。</span><br><span class="line"></span><br><span class="line">接下来我们需要通过kafka来将这两天内的粉丝关注数据读取出来，补充到neo4j中，如何实现呢？</span><br><span class="line">因为我们的kafka当时是保存3天的数据，所以说这里面保存的还有2026-01-31 10点左右开始的数据，所以说当时我们开发好SparkStreaming程序之后，使用一个新的消费者groupid，然后将auto.offset.reset设置为earlist，读取最早的数据，这样就可以将这个topic中前3天的数据都读出来，然后在neo4j中进行维护，这样其实会重复执行2026-01-31 10点到2月1 日0点之间的数据，但是对最终的结果是没有影响的。</span><br><span class="line">这样就可以实现在neo4j中全量维护粉丝关注数据了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162119832.png" alt="image-20230516210302781"></p><h3 id="如何解决数据乱序导致的粉丝关注关系不准确"><a href="#如何解决数据乱序导致的粉丝关注关系不准确" class="headerlink" title="如何解决数据乱序导致的粉丝关注关系不准确"></a>如何解决数据乱序导致的粉丝关注关系不准确</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">通过在sparkStreaming内部对读取到的一小批数据基于时间进行排序，按照时间顺序执行粉丝关注相关操作，这样可以从一定程度上解决数据乱序的问题</span><br><span class="line">在v2.0中，我们使用了Flink计算引擎，此时可以使用watermark+eventtime来解决数据乱序的问题。</span><br></pre></td></tr></table></figure><h3 id="如何优化三度关系推荐列表数据计算程序"><a href="#如何优化三度关系推荐列表数据计算程序" class="headerlink" title="如何优化三度关系推荐列表数据计算程序"></a>如何优化三度关系推荐列表数据计算程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">针对三度关系推荐列表数据计算程序：GetRecommendListScala</span><br><span class="line">这个任务在执行的时候需要执行20个小时左右，因为这里面会先查询出来满足条件的主播，然后挨个计算这些主播的三度关系数据，这里面需要和neo4j进行交互，主要慢在了neo4j这里，因为三度关系查询是比较复杂的，所以会比较耗时。</span><br><span class="line">这个任务在执行的时候我们会发现它有时候无缘无故的提示task丢失，进而导致任务失败，还得重新计算，代价太大，所以这样不太靠谱。</span><br><span class="line">后来发现是由于spark离线任务执行时间过长的时候会出现这种task丢失的问题。</span><br><span class="line">所以后来我们对这个程序又做了优化。</span><br><span class="line">针对第一步计算出来的主播列表，分成20份保存到hdfs上面</span><br></pre></td></tr></table></figure><h3 id="项目数据规模"><a href="#项目数据规模" class="headerlink" title="项目数据规模"></a>项目数据规模</h3><h3 id="集群资源规模-HDP集群"><a href="#集群资源规模-HDP集群" class="headerlink" title="集群资源规模(HDP集群)"></a>集群资源规模(HDP集群)</h3><h3 id="集群数据规模"><a href="#集群数据规模" class="headerlink" title="集群数据规模"></a>集群数据规模</h3><h3 id="Neo4j性能指标"><a href="#Neo4j性能指标" class="headerlink" title="Neo4j性能指标"></a>Neo4j性能指标</h3><h3 id="Neo4j核心参数修改"><a href="#Neo4j核心参数修改" class="headerlink" title="Neo4j核心参数修改"></a>Neo4j核心参数修改</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v2.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v2.0-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-1.html</id>
    <published>2023-04-24T07:37:38.000Z</published>
    <updated>2023-05-21T10:40:33.211Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v2-0-1"><a href="#第十八周-直播平台三度关系推荐v2-0-1" class="headerlink" title="第十八周 直播平台三度关系推荐v2.0-1"></a>第十八周 直播平台三度关系推荐v2.0-1</h1><h2 id="V1-0架构存在的问题"><a href="#V1-0架构存在的问题" class="headerlink" title="V1.0架构存在的问题"></a>V1.0架构存在的问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">V1.0这个架构里面其实存在三个主要的问题</span><br><span class="line"></span><br><span class="line">SparkStreaming程序的实时性不够</span><br><span class="line">其实说实话，针对目前的粉丝实时关注数据，使用SparkStreaming程序来维护问题也不大，但是我们程序猿是要有追求的，既然有更好的方案，那我们肯定不能使用差的，所以这块我们我们需要使用Flink来实现，它可以提供真正意义上的实时。</span><br><span class="line"></span><br><span class="line">三度关系推荐数据适合存储在缓存系统中(Redis)</span><br><span class="line">咱们前面把最终计算好的三度关系数据保存在了MySQL中。</span><br><span class="line">其实这种数据是比较适合存储到一些基于内存的缓存系统中的，对查询的性能要求比较高，并且这些数据也是有时效性的，需要定时更新和删除老数据，所以说此时，使用redis是比较合适的，redis的查询性能比较高，并且redis中可以给key设置一个生存时间，可以实现定时删除过期数据的效果。</span><br><span class="line">咱们这份数据是有2个字段，第一列是主播uid，第二列是待推荐的主播uid</span><br><span class="line">存储到redis里面的话value使用list类型即可，在list类型里面存储待推荐的主播列表</span><br><span class="line"></span><br><span class="line">为了规范数据使用，建议开发数据接口</span><br><span class="line">咱们前面把存储系统直接暴露给其他业务部门，是不太安全的，并且他们使用起来也不方便，在实际工作中，各个业务部门之间进行数据交互，正规流程都是提供接口。</span><br><span class="line">还有一个原因是这样的，我们在开发一个功能的时候，假设需要前端和后端同时开发，这个时候我们就需要提前把数据接口定义好，先提供假数据，这样前端和后端可以同时进行开发，前端在开发页面功能的时候就可以使用我们提供的数据接口了，当我们把后端功能搞定以后，修改数据接口的底层逻辑代码，接入真实的数据即可，这个时候对前端而言是没有任何影响的，就算后期我们修改表结构了，对前端也没有什么影响，只要接口没有变就行，这样也可以实现前端和后端的解耦。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020904879.png" alt="image-20230502090405428"></p><h2 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来再来回顾一下技术选型，看看在V2.0中有哪些变化</span><br><span class="line">还是这4大块</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020905225.png" alt="image-20230502090508445"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在数据采集这块，去掉了Sqoop，因为在这里我们需要将三度关系数据导出到redis中，sqoop是不支持的，所以我们需要开发flink程序实现数据导出功能。</span><br><span class="line"></span><br><span class="line">在数据存储这块，我们把MySQL改为了Redis</span><br><span class="line"></span><br><span class="line">在数据计算这块，我们把Spark计算引擎改为了Flink，在V2.0中，我们将之前使用Spark开发的代码都使用Flink重新实现一遍。</span><br><span class="line"></span><br><span class="line">数据展现模块没有变化。</span><br></pre></td></tr></table></figure><h2 id="整体架构设计"><a href="#整体架构设计" class="headerlink" title="整体架构设计"></a>整体架构设计</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020909294.png" alt="image-20230502090921788"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这是最新的架构图，在这里面，替换了Spark计算引擎和MySQL数据库，引入了Flink和Redis</span><br><span class="line">以及在这里引入了数据接口模块，通过接口对外提供数据。</span><br><span class="line">其它的地方没有变化。</span><br><span class="line"></span><br><span class="line">注意：其实针对离线计算使用Spark或者Flink没有多大区别，不过我们还是希望一个项目中的计算框架相对来说是统一的，这样好管理，也好维护，所以在V2.0架构中，不管是离线计算还是实时计算，都使用Flink实现。</span><br></pre></td></tr></table></figure><h2 id="数据采集模块开发"><a href="#数据采集模块开发" class="headerlink" title="数据采集模块开发"></a>数据采集模块开发</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据采集模块没有变化，所以在这就不再分析了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020911095.png" alt="image-20230502091109759"></p><h2 id="数据计算核心指标详细分析"><a href="#数据计算核心指标详细分析" class="headerlink" title="数据计算核心指标详细分析"></a>数据计算核心指标详细分析</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020913388.png" alt="image-20230502091324304"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这里面的数据源和计算指标都没有变化</span><br><span class="line">一共还是这7个步骤</span><br></pre></td></tr></table></figure><h2 id="历史粉丝关注数据初始化-任务一"><a href="#历史粉丝关注数据初始化-任务一" class="headerlink" title="历史粉丝关注数据初始化(任务一)"></a>历史粉丝关注数据初始化(任务一)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一步：历史粉丝关注数据初始化</span><br><span class="line">这块流程是没有变化的，使用load csv将我们之前导出的历史粉丝关注数据进行初始化即可。</span><br><span class="line">把neo4j中之前的数据清空一下，直接删除neo4j下面的data目录即可，然后启动neo4j</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">通过浏览器访问neo4j，重新设置密码</span><br><span class="line"></span><br><span class="line">然后我们使用neo4j的shell命令行执行下面命令。</span><br><span class="line"></span><br><span class="line">连接neo4j</span><br><span class="line">bin&#x2F;cypher-shell -a bolt:&#x2F;&#x2F;bigdata04:7687 -u neo4j -p admin</span><br><span class="line"></span><br><span class="line">建立索引</span><br><span class="line">CREATE CONSTRAINT ON (user:User) ASSERT user.uid IS UNIQUE;</span><br><span class="line"></span><br><span class="line">批量导入数据</span><br><span class="line">USING PERIODIC COMMIT 1000</span><br><span class="line">       LOAD CSV WITH HEADERS FROM &#39;file:&#x2F;&#x2F;&#x2F;follower_00.log&#39; AS line FIELDTERMINATOR &#39;\t&#39;</span><br><span class="line">       MERGE (viewer:User &#123; uid: toString(line.fuid)&#125;)</span><br><span class="line">       MERGE (anchor:User &#123; uid: toString(line.uid)&#125;)</span><br><span class="line">       MERGE (viewer)-[:follow]-&gt;(anchor);</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020919105.png" alt="image-20230502091913756"></p><h2 id="实时维护粉丝关注数据-任务二"><a href="#实时维护粉丝关注数据-任务二" class="headerlink" title="实时维护粉丝关注数据(任务二)"></a>实时维护粉丝关注数据(任务二)</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020920206.png" alt="image-20230502092054017"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实时维护Neo4j中粉丝关注数据</span><br><span class="line">先创建maven项目db_video_recommend_v2</span><br><span class="line">在pom.xml中添加项目需要用到的所有依赖</span><br><span class="line"></span><br><span class="line">再创建子module项目：real_time_follow</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="父项目pom"><a href="#父项目pom" class="headerlink" title="父项目pom"></a>父项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;packaging&gt;pom&lt;&#x2F;packaging&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;modules&gt;</span><br><span class="line">        &lt;module&gt;real_time_follow&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_user_level&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_user_active&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_video_info&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;get_recommend_list&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;export_data&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;data_server&lt;&#x2F;module&gt;</span><br><span class="line">    &lt;&#x2F;modules&gt;</span><br><span class="line">    &lt;dependencyManagement&gt;</span><br><span class="line">        &lt;dependencies&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.1.1.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.httpcomponents&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;httpclient&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;4.5.12&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;commons-dbutils&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;commons-dbutils&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;8.0.20&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.2.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.2.68&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- flink相关依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-streaming-java_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-connector-kafka_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- log4j的依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- neo4j相关依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;4.1.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- jedis依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;redis.clients&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;jedis&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.9.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!--spring-boot依赖--&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.1.1.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;&#x2F;dependencyManagement&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="子项目pom"><a href="#子项目pom" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;real_time_follow&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">    &lt;!-- log4j的依赖 --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;!-- flink的依赖 --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-streaming-java_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-connector-kafka_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;!-- neo4j的依赖 --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;!-- fastjson的依赖 --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="RealTimeFollowScala-scala"><a href="#RealTimeFollowScala-scala" class="headerlink" title="RealTimeFollowScala.scala"></a>RealTimeFollowScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line">创建类：RealTimeFollowScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务2：</span></span><br><span class="line"><span class="comment"> * 实时维护粉丝关注数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTimeFollowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"RealTimeFollowScala"</span></span><br><span class="line">    <span class="keyword">var</span> kafkaBrokers = <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span></span><br><span class="line">    <span class="keyword">var</span> groupId = <span class="string">"con_f_1"</span></span><br><span class="line">    <span class="keyword">var</span> topic = <span class="string">"user_follow"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      appName = args(<span class="number">0</span>)</span><br><span class="line">      kafkaBrokers = args(<span class="number">1</span>)</span><br><span class="line">      groupId = args(<span class="number">2</span>)</span><br><span class="line">      topic = args(<span class="number">3</span>)</span><br><span class="line">      boltUrl = args(<span class="number">4</span>)</span><br><span class="line">      userName = args(<span class="number">5</span>)</span><br><span class="line">      passWord = args(<span class="number">6</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,kafkaBrokers)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,groupId)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//解析json数据中的核心字段</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">      <span class="keyword">val</span> desc = jsonObj.getString(<span class="string">"desc"</span>)</span><br><span class="line">      <span class="keyword">val</span> followerUid = jsonObj.getString(<span class="string">"followeruid"</span>)</span><br><span class="line">      <span class="keyword">val</span> followUid = jsonObj.getString(<span class="string">"followuid"</span>)</span><br><span class="line">      (desc, followerUid, followUid)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用Neo4jSink维护粉丝关注数据</span></span><br><span class="line">    <span class="keyword">val</span> param = <span class="type">Map</span>(<span class="string">"boltUrl"</span>-&gt;boltUrl,<span class="string">"userName"</span>-&gt;userName,<span class="string">"passWord"</span>-&gt;passWord)</span><br><span class="line">    tupStream.addSink(<span class="keyword">new</span> <span class="type">Neo4jSink</span>(param))</span><br><span class="line"></span><br><span class="line">    env.execute(appName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">注意：由于flink中的实时计算是来一条数据计算一次，在StreamAPI中没有mapPartition方法，不支持一批一批的处理，如果每处理一条数据就获取一次Neo4j数据库连接，这样效率就太差了，所以我们需要实现一个自定义的sink组件，在sink组件内部有一个初始化函数可以获取一次连接，多次使用，这样就不需要频繁创建neo4j数据库连接了。</span><br><span class="line"></span><br><span class="line">实现自定义的sink需要实现SinkFunction接口或者继承RichSinkFunction</span><br><span class="line">具体实现逻辑可以参考已有connector中针对sink组件的实现</span><br><span class="line">例如：RedisSink</span><br><span class="line">源码在这里：</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;bahir-flink&#x2F;blob&#x2F;master&#x2F;flink-connector-redis&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;flink&#x2F;streaming&#x2F;connectors&#x2F;redis&#x2F;RedisSink.java</span><br><span class="line"></span><br><span class="line">这里面一共有三个主要的函数：</span><br><span class="line">1：open，是一个初始化方法，在Sink组件初始化的时候执行一次，适合在里面初始化一些资源连接</span><br><span class="line">2：invoke，会被频繁调用，sink接收到一条数据这个方法就会执行一次，具体的业务逻辑在这里实现</span><br><span class="line">3：close，当任务停止的时候，会先调用sink组件中的close方法，适合在里面做一些关闭资源的操作</span><br></pre></td></tr></table></figure><h3 id="Neo4jSink-scala"><a href="#Neo4jSink-scala" class="headerlink" title="Neo4jSink.scala"></a>Neo4jSink.scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.&#123;<span class="type">RichSinkFunction</span>, <span class="type">SinkFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">Driver</span>, <span class="type">GraphDatabase</span>, <span class="type">Transaction</span>, <span class="type">TransactionWork</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 维护粉丝数据在Neo4j中的关注关系</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neo4jSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[<span class="type">Tuple3</span>[<span class="type">String</span>,<span class="type">String</span>,<span class="type">String</span>]]</span>&#123;</span><br><span class="line">  <span class="comment">//保存neo4j相关的配置参数</span></span><br><span class="line">  <span class="keyword">var</span> param: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>] = <span class="type">Map</span>()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> driver: <span class="type">Driver</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 构造函数</span></span><br><span class="line"><span class="comment">   * 接收neo4j相关的配置参数</span></span><br><span class="line"><span class="comment">   * @param param</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(param: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>])&#123;</span><br><span class="line">    <span class="keyword">this</span>()</span><br><span class="line">    <span class="keyword">this</span>.param = param</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 初始化方法，只执行一次</span></span><br><span class="line"><span class="comment">   * 适合初始化资源连接</span></span><br><span class="line"><span class="comment">   * @param parameters</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.driver = <span class="type">GraphDatabase</span>.driver(param(<span class="string">"boltUrl"</span>), <span class="type">AuthTokens</span>.basic(param(<span class="string">"userName"</span>), param(<span class="string">"passWord"</span>)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 核心代码，来一条数据，此方法会执行一次</span></span><br><span class="line"><span class="comment">   * @param value</span></span><br><span class="line"><span class="comment">   * @param context</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(value: (<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>), context: <span class="type">SinkFunction</span>.<span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//开启会话</span></span><br><span class="line">    <span class="keyword">val</span> session = driver.session()</span><br><span class="line">    <span class="keyword">val</span> followType = value._1</span><br><span class="line">    <span class="keyword">val</span> followerUid = value._2</span><br><span class="line">    <span class="keyword">val</span> followUid = value._3</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(<span class="string">"follow"</span>.equals(followType))&#123;</span><br><span class="line">      <span class="comment">//添加关注：因为涉及多条命令，所以需要使用事务</span></span><br><span class="line">      session.writeTransaction(<span class="keyword">new</span> <span class="type">TransactionWork</span>[<span class="type">Unit</span>]()&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(tx: <span class="type">Transaction</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">try</span>&#123;</span><br><span class="line">            tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followerUid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">            tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followUid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">            tx.run(<span class="string">"match (a:User &#123;uid:'"</span>+followerUid+<span class="string">"'&#125;),(b:User &#123;uid:'"</span>+followUid+<span class="string">"'&#125;) merge (a) -[:follow]-&gt; (b)"</span>)</span><br><span class="line">            tx.commit()</span><br><span class="line">          &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; tx.rollback()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="comment">//取消关注</span></span><br><span class="line">      session.run(<span class="string">"match (:User &#123;uid:'"</span>+followerUid+<span class="string">"'&#125;) -[r:follow]-&gt; (:User &#123;uid:'"</span>+followUid+<span class="string">"'&#125;) delete r"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//关闭会话</span></span><br><span class="line">    session.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 任务停止的时候会先调用此方法</span></span><br><span class="line"><span class="comment">   * 适合关闭资源连接</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//关闭连接</span></span><br><span class="line">    <span class="keyword">if</span>(driver!=<span class="literal">null</span>)&#123;</span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：需要确保zookeeper、kafka服务是正常运行的。</span><br><span class="line"></span><br><span class="line">接下来需要产生测试数据，我们可以继续使用之前generate_data项目中的GenerateRealTimeFollowData产生数据，这种流程我们前面在v1.0中已经使用过了。</span><br><span class="line">下面给大家演示一种方便测试的方法</span><br><span class="line">其实我们可以通过kafka的基于console的生产者直接向user_follow这个topic生产数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关注数据</span><br><span class="line">&#123;&quot;followeruid&quot;:&quot;2004&quot;,&quot;followuid&quot;:&quot;2008&quot;,&quot;timestamp&quot;:1598771070069,&quot;type&quot;:&quot;user_follow&quot;,&quot;desc&quot;:&quot;follow&quot;&#125;</span><br><span class="line">取消关注数据</span><br><span class="line">&#123;&quot;followeruid&quot;:&quot;2004&quot;,&quot;followuid&quot;:&quot;2008&quot;,&quot;timestamp&quot;:1598771070069,&quot;type&quot;:&quot;user_follow&quot;,&quot;desc&quot;:&quot;unfollow&quot;&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305021217745.png" alt="image-20230502121745119"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305021218562.png" alt="image-20230502121838255"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">到neo4j中确认效果发现确实新增了一个关注关系</span><br><span class="line">再模拟产生一条粉丝取消关注的数据</span><br><span class="line"></span><br><span class="line">到neo4j中确认效果发现刚才新增的关注关系没有了。</span><br><span class="line">这样就说明我们自己定义的Neo4jSink是可以正常工作的。</span><br><span class="line"></span><br><span class="line">注意：在实际工作中，有时候为了方便测试代码是否可以正常运行，很多时候也会采用这种基于控制台的生产者直接模拟产生数据，这样不会经过中间商，没有差价！</span><br><span class="line">如果使用整个数据采集全链路流程的话，可能会由于中间某个环节出问题导致的最终看不到效果，此时我们还得排查到底是哪里出了问题，这样就乱套了，本来是要验证代码逻辑的，结果又要去排查其它地方的问题了。</span><br><span class="line">所以说针对流程比较复杂的，我们在测试的时候一块一块进行测试，先验证代码逻辑没问题，最后再跑一个全流程确认一下最终效果。</span><br></pre></td></tr></table></figure><h3 id="集群执行"><a href="#集群执行" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们希望把代码提交到集群上运行</span><br><span class="line">需要先调整代码，把参数提取出来</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">对项目打jar包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br><span class="line">这里面的scala版本指定的是2.12版本</span><br><span class="line">注意：flink官方建议把所有依赖都打进一个jar包，所以我们在这就把依赖打进一个jar包里面。</span><br><span class="line">在flink1.11的时候新增了一个特性，可以支持动态指定依赖的jar包，但是我测试了还是有bug，所以在这我们就只能把依赖都打进jar包里面，其实我内心是拒绝的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：针对log4j,flink相关的依赖在打包的时候不需要打进去，所以需要添加provided属性</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时我们就需要使用这个带有jar-with-dependencies的jar包了</span><br><span class="line">real_time_follow-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><h3 id="startRealTimeFollow-sh"><a href="#startRealTimeFollow-sh" class="headerlink" title="startRealTimeFollow.sh"></a>startRealTimeFollow.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发任务提交脚本</span><br><span class="line">startRealTimeFollow.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="RealTimeFollowScala"</span><br><span class="line">kafkaBrokers="bigdata01:9092,bigdata02:9092,bigdata03:9092"</span><br><span class="line">groupId="con_f_1"</span><br><span class="line">topic="user_follow"</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.RealTimeFollowScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/real_time_follow-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;appName&#125; $&#123;kafkaBrokers&#125; $&#123;groupId&#125; $&#123;topic&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在执行之前需要配置flink的环境变量，FLINK_HOME</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305021821488.png" alt="image-20230502182120542"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">向集群提交任务</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305021821591.png" alt="image-20230502182148965"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">通过kafka的console控制台生产者，模拟产生数据，到neo4j中确认效果，发现是没有问题的。</span><br><span class="line"></span><br><span class="line">注意：此时是存在数据乱序的问题的，前面在讲Flink的时候我们详细讲解过Flink中的乱序处理方案，在这里给大家留一个作业，对这个代码进行改造，解决数据乱序问题。</span><br></pre></td></tr></table></figure><h2 id="每天定时更新主播等级-任务三"><a href="#每天定时更新主播等级-任务三" class="headerlink" title="每天定时更新主播等级(任务三)"></a>每天定时更新主播等级(任务三)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现每天定时更新主播等级</span><br><span class="line">创建子module项目：update_user_level</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="子项目pom-1"><a href="#子项目pom-1" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_level&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br></pre></td></tr></table></figure><h3 id="UpdateUserLevelScala-scala"><a href="#UpdateUserLevelScala-scala" class="headerlink" title="UpdateUserLevelScala.scala"></a>UpdateUserLevelScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">创建类：UpdateUserLevelScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务3：</span></span><br><span class="line"><span class="comment"> * 每天定时更新主播等级</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateUserLevelScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/cl_level_user/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//读取hdfs中的数据</span></span><br><span class="line">    <span class="keyword">val</span> text = env.readTextFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//校验数据准确性</span></span><br><span class="line">    <span class="keyword">val</span> filterSet = text.filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length == <span class="number">8</span> &amp;&amp; !fields(<span class="number">0</span>).equals(<span class="string">"id"</span>)) &#123;</span><br><span class="line">        <span class="literal">true</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    filterSet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//添加等级</span></span><br><span class="line">        session.run(<span class="string">"merge (u:User &#123;uid:'"</span>+fields(<span class="number">1</span>).trim+<span class="string">"'&#125;) set u.level = "</span>+fields(<span class="number">3</span>).trim)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span> <span class="comment">// spark的foreachpartition不需要返回数据，flink dataset的mappartition需要返回数据</span></span><br><span class="line">    &#125;).print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-1"><a href="#本地执行-1" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用之前生成的这份数据</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;cl_level_user&#x2F;20260201</span><br><span class="line">在本地执行代码，到neo4j中确认节点中是否新增了level属性，如果有，就说明程序执行成功了。</span><br></pre></td></tr></table></figure><h3 id="集群执行-1"><a href="#集群执行-1" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来对程序编译打包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br></pre></td></tr></table></figure><h3 id="startUpdateUserLevel-sh"><a href="#startUpdateUserLevel-sh" class="headerlink" title="startUpdateUserLevel.sh"></a>startUpdateUserLevel.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/cl_level_user/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="UpdateUserLevelScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.UpdateUserLevelScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/update_user_level-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $8&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="每天定时更新用户活跃时间-任务四"><a href="#每天定时更新用户活跃时间-任务四" class="headerlink" title="每天定时更新用户活跃时间(任务四)"></a>每天定时更新用户活跃时间(任务四)</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031521064.png" alt="image-20230503152132924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现每天定时更新用户活跃时间</span><br><span class="line">创建子module项目：update_user_active</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="子项目pom-2"><a href="#子项目pom-2" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_active&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateUserActiveScala-scala"><a href="#UpdateUserActiveScala-scala" class="headerlink" title="UpdateUserActiveScala.scala"></a>UpdateUserActiveScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line">创建类：UpdateUserActiveScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务4：</span></span><br><span class="line"><span class="comment"> * 每天定时更新用户活跃时间</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateUserActiveScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/user_active/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//读取hdfs中的数据</span></span><br><span class="line">    <span class="keyword">val</span> text = env.readTextFile(filePath)</span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    text.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">        <span class="keyword">val</span> uid = jsonObj.getString(<span class="string">"uid"</span>)</span><br><span class="line">        <span class="keyword">val</span> timeStamp = jsonObj.getString(<span class="string">"UnixtimeStamp"</span>)</span><br><span class="line">        <span class="comment">//添加用户活跃时间</span></span><br><span class="line">        session.run(<span class="string">"merge (u:User &#123;uid:'"</span>+uid+<span class="string">"'&#125;) set u.timestamp = "</span>+timeStamp)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span> <span class="comment">// 和前面一个一样的道理</span></span><br><span class="line">    &#125;).print() <span class="comment">// 为了任务能够执行</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-2"><a href="#本地执行-2" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用之前生成的这份数据</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;user_active&#x2F;20260201</span><br><span class="line">在本地执行代码，到neo4j中确认节点中是否新增了timestamp属性，如果有，就说明程序执行成功了。</span><br></pre></td></tr></table></figure><h3 id="集群执行-2"><a href="#集群执行-2" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来对程序编译打包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br></pre></td></tr></table></figure><h3 id="startUpdateUserActive-sh"><a href="#startUpdateUserActive-sh" class="headerlink" title="startUpdateUserActive.sh"></a>startUpdateUserActive.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发任务脚本</span><br><span class="line">startUpdateUserActive.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/user_active/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="UpdateUserActiveScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.UpdateUserActiveScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/update_user_active-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $8&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="每周一计算最近一个月主播视频评级-任务五"><a href="#每周一计算最近一个月主播视频评级-任务五" class="headerlink" title="每周一计算最近一个月主播视频评级(任务五)"></a>每周一计算最近一个月主播视频评级(任务五)</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031537915.png" alt="image-20230503153659474"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现每周一计算最近一个月主播视频评级</span><br><span class="line">创建子module项目：update_video_info</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="子项目pom-3"><a href="#子项目pom-3" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_video_info&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateVideoInfoScala-scala"><a href="#UpdateVideoInfoScala-scala" class="headerlink" title="UpdateVideoInfoScala.scala"></a>UpdateVideoInfoScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line">创建类：UpdateVideoInfoScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.<span class="type">Order</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务5：</span></span><br><span class="line"><span class="comment"> * 每周一计算最近一个月主播视频评级</span></span><br><span class="line"><span class="comment"> * 把最近几次视频评级在3B+或2A+的主播，在neo4j中设置flag=1</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意：在执行程序之前，需要先把flag=1的重置为0</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateVideoInfoScala</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> logger = <span class="type">LoggerFactory</span>.getLogger(<span class="string">"UpdateVideoInfoScala"</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/video_info/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在Driver端执行此代码，将flag=1的值重置为0</span></span><br><span class="line">    <span class="comment">//获取neo4j的连接</span></span><br><span class="line">    <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">    <span class="comment">//开启一个会话</span></span><br><span class="line">    <span class="keyword">val</span> session = driver.session()</span><br><span class="line">    session.run(<span class="string">"match (a:User) where a.flag=1 set a.flag = 0"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//关闭会话</span></span><br><span class="line">    session.close()</span><br><span class="line">    <span class="comment">//关闭连接</span></span><br><span class="line">    driver.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//读取hdfs中的数据</span></span><br><span class="line">    <span class="keyword">val</span> text = env.readTextFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//解析数据中的uid、rating、timestamp</span></span><br><span class="line">    <span class="keyword">val</span> tup3Set = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">        <span class="keyword">val</span> uid = jsonObj.getString(<span class="string">"uid"</span>)</span><br><span class="line">        <span class="keyword">val</span> rating = jsonObj.getString(<span class="string">"rating"</span>)</span><br><span class="line">        <span class="keyword">val</span> timestamp: <span class="type">Long</span> = jsonObj.getLong(<span class="string">"timestamp"</span>) <span class="comment">//这里要显示指定类型，不然会报错</span></span><br><span class="line">        (uid, rating, timestamp)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; logger.error(<span class="string">"json数据解析失败："</span> + line)</span><br><span class="line">          (<span class="string">"0"</span>, <span class="string">"0"</span>, <span class="number">0</span>L)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤异常数据</span></span><br><span class="line">    <span class="keyword">val</span> filterSet = tup3Set.filter(_._2 != <span class="string">"0"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取用户最近3场直播(视频)的评级信息</span></span><br><span class="line">    <span class="keyword">val</span> top3Set = filterSet.groupBy(<span class="number">0</span>)</span><br><span class="line">      .sortGroup(<span class="number">2</span>, <span class="type">Order</span>.<span class="type">DESCENDING</span>)</span><br><span class="line">      .reduceGroup(it =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> list = it.toList</span><br><span class="line">        <span class="comment">//(2002,A,1769913940002)(2002,A,1769913940001)(2002,A,1769913940000)</span></span><br><span class="line">        <span class="comment">//uid,rating,timestamp \t uid,rating,timestamp \t uid,rating,timestamp</span></span><br><span class="line">        <span class="keyword">val</span> top3 = list.take(<span class="number">3</span>).mkString(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//(2002,(2002,A,1769913940002)(2002,A,1769913940001)(2002,A,1769913940000))</span></span><br><span class="line">        (list.head._1, top3)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤出来满足3场B+的数据</span></span><br><span class="line">    <span class="keyword">val</span> top3BSet = top3Set.filter(tup =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> flag = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">val</span> fields = tup._2.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length == <span class="number">3</span>) &#123;</span><br><span class="line">        <span class="comment">//3场B+，表示里面没有出现C和D</span></span><br><span class="line">        <span class="keyword">val</span> tmp_str = fields(<span class="number">0</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">1</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">2</span>).split(<span class="string">","</span>)(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (!tmp_str.contains(<span class="string">"C"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"D"</span>)) &#123;</span><br><span class="line">          flag = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把满足3场B+的数据更新到neo4j中，设置flag=1</span></span><br><span class="line">    top3BSet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(tup=&gt;&#123;</span><br><span class="line">        session.run(<span class="string">"match (a:User &#123;uid:'"</span>+tup._1+<span class="string">"'&#125;) where a.level &gt;=15 set a.flag = 1"</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤出来满足2场A+的数据</span></span><br><span class="line">    <span class="keyword">val</span>   = top3Set.filter(tup =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> flag = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">val</span> fields = tup._2.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="comment">//2场A+，获取最近两场直播评级，里面不能出现B、C、D</span></span><br><span class="line">        <span class="keyword">val</span> tmp_str = fields(<span class="number">0</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">1</span>).split(<span class="string">","</span>)(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (!tmp_str.contains(<span class="string">"B"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"C"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"D"</span>)) &#123;</span><br><span class="line">          flag = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把满足2场A+的数据更新到neo4j中，设置flag=1</span></span><br><span class="line">    top2ASet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(tup=&gt;&#123;</span><br><span class="line">        session.run(<span class="string">"match (a:User &#123;uid:'"</span>+tup._1+<span class="string">"'&#125;) where a.level &gt;=4 set a.flag = 1"</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span> <span class="comment">// 这里的道理和前面一样</span></span><br><span class="line">    &#125;).print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-3"><a href="#本地执行-3" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用之前生成的这份数据</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;video_info&#x2F;20260201</span><br><span class="line">在本地执行代码，到neo4j中确认节点中是否有flag属性的值。</span><br></pre></td></tr></table></figure><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031614511.png" alt="image-20230503161453968"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">代码中对uid进行分区那里没有显示指定类型，会报错。也是常见错误</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031610325.png" alt="image-20230503161026920"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上面代码mappartion中的数据库链接部分，没有创建session，但并没有报错，用的是外部driver端的session；数据库链接并不支持序列化，不支持从driver到节点的传输。这种异常很常见。</span><br></pre></td></tr></table></figure><h3 id="多个输入目录问题"><a href="#多个输入目录问题" class="headerlink" title="多个输入目录问题"></a>多个输入目录问题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：flink目前不支持直接读取多个hdfs目录，在spark中，我们可以将多个hdfs目录使用逗号拼接成一个输入路径，flink目前不支持这种用法。</span><br></pre></td></tr></table></figure><h4 id="UpdateVideoInfoMoreFileScala-scala"><a href="#UpdateVideoInfoMoreFileScala-scala" class="headerlink" title="UpdateVideoInfoMoreFileScala.scala"></a>UpdateVideoInfoMoreFileScala.scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那如何实现读取最近一个月的数据呢？</span><br><span class="line">我们可以使用flink中的union算子间接实现读取多个hdfs目录的效果</span><br><span class="line">复制一份代码，改名字为：UpdateVideoInfoMoreFileScala</span><br><span class="line"></span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.<span class="type">Order</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务5：</span></span><br><span class="line"><span class="comment"> * 每周一计算最近一个月主播视频评级</span></span><br><span class="line"><span class="comment"> * 把最近几次视频评级在3B+或2A+的主播，在neo4j中设置flag=1</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意：在执行程序之前，需要先把flag=1的重置为0</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateVideoInfoMoreFileScala</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> logger = <span class="type">LoggerFactory</span>.getLogger(<span class="string">"UpdateVideoInfoScala"</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/video_info/20260201,hdfs://bigdata01:9000/data/video_info/20260217"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*//在Driver端执行此代码，将flag=1的值重置为0</span></span><br><span class="line"><span class="comment">    //获取neo4j的连接</span></span><br><span class="line"><span class="comment">    val driver = GraphDatabase.driver(boltUrl, AuthTokens.basic(userName, passWord))</span></span><br><span class="line"><span class="comment">    //开启一个会话</span></span><br><span class="line"><span class="comment">    val session = driver.session()</span></span><br><span class="line"><span class="comment">    session.run("match (a:User) where a.flag=1 set a.flag = 0")</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    //关闭会话</span></span><br><span class="line"><span class="comment">    session.close()</span></span><br><span class="line"><span class="comment">    //关闭连接</span></span><br><span class="line"><span class="comment">    driver.close()*/</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用union实现读取多个hdfs目录中的数据</span></span><br><span class="line">    <span class="keyword">val</span> files = filePath.split(<span class="string">","</span>)</span><br><span class="line">    <span class="keyword">var</span> allText: <span class="type">DataSet</span>[<span class="type">String</span>] = env.fromElements(<span class="string">"123"</span>)</span><br><span class="line">    <span class="keyword">for</span>(file &lt;- files)&#123;</span><br><span class="line">      allText = allText.union(env.readTextFile(file)) <span class="comment">// 这里要赋值才符合逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"原始数据条数："</span>+allText.count())</span><br><span class="line"></span><br><span class="line">    <span class="comment">//解析数据中的uid、rating、timestamp</span></span><br><span class="line">    <span class="keyword">val</span> tup3Set = allText.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">        <span class="keyword">val</span> uid = jsonObj.getString(<span class="string">"uid"</span>)</span><br><span class="line">        <span class="keyword">val</span> rating = jsonObj.getString(<span class="string">"rating"</span>)</span><br><span class="line">        <span class="keyword">val</span> timestamp: <span class="type">Long</span> = jsonObj.getLong(<span class="string">"timestamp"</span>)</span><br><span class="line">        (uid, rating, timestamp)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; logger.error(<span class="string">"json数据解析失败："</span> + line)</span><br><span class="line">          (<span class="string">"0"</span>, <span class="string">"0"</span>, <span class="number">0</span>L)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤异常数据</span></span><br><span class="line">    <span class="keyword">val</span> filterSet = tup3Set.filter(_._2 != <span class="string">"0"</span>)</span><br><span class="line">    println(<span class="string">"过滤后的数据："</span>+filterSet.count())</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取用户最近3场直播(视频)的评级信息</span></span><br><span class="line">    <span class="keyword">val</span> top3Set = filterSet.groupBy(<span class="number">0</span>)</span><br><span class="line">      .sortGroup(<span class="number">2</span>, <span class="type">Order</span>.<span class="type">DESCENDING</span>)</span><br><span class="line">      .reduceGroup(it =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> list = it.toList</span><br><span class="line">        <span class="comment">//(2002,A,1769913940002)(2002,A,1769913940001)(2002,A,1769913940000)</span></span><br><span class="line">        <span class="comment">//uid,rating,timestamp \t uid,rating,timestamp \t uid,rating,timestamp</span></span><br><span class="line">        <span class="keyword">val</span> top3 = list.take(<span class="number">3</span>).mkString(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//(2002,(2002,A,1769913940002)(2002,A,1769913940001)(2002,A,1769913940000))</span></span><br><span class="line">        (list.head._1, top3)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤出来满足3场B+的数据</span></span><br><span class="line">    <span class="keyword">val</span> top3BSet = top3Set.filter(tup =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> flag = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">val</span> fields = tup._2.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length == <span class="number">3</span>) &#123;</span><br><span class="line">        <span class="comment">//3场B+，表示里面没有出现C和D</span></span><br><span class="line">        <span class="keyword">val</span> tmp_str = fields(<span class="number">0</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">1</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">2</span>).split(<span class="string">","</span>)(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (!tmp_str.contains(<span class="string">"C"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"D"</span>)) &#123;</span><br><span class="line">          flag = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把满足3场B+的数据更新到neo4j中，设置flag=1</span></span><br><span class="line">    top3BSet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(tup=&gt;&#123;</span><br><span class="line">        session.run(<span class="string">"match (a:User &#123;uid:'"</span>+tup._1+<span class="string">"'&#125;) where a.level &gt;=15 set a.flag = 1"</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤出来满足2场A+的数据</span></span><br><span class="line">    <span class="keyword">val</span> top2ASet = top3Set.filter(tup =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> flag = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">val</span> fields = tup._2.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="comment">//2场A+，获取最近两场直播评级，里面不能出现B、C、D</span></span><br><span class="line">        <span class="keyword">val</span> tmp_str = fields(<span class="number">0</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">1</span>).split(<span class="string">","</span>)(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (!tmp_str.contains(<span class="string">"B"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"C"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"D"</span>)) &#123;</span><br><span class="line">          flag = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把满足2场A+的数据更新到neo4j中，设置flag=1</span></span><br><span class="line">    top2ASet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(tup=&gt;&#123;</span><br><span class="line">        session.run(<span class="string">"match (a:User &#123;uid:'"</span>+tup._1+<span class="string">"'&#125;) where a.level &gt;=4 set a.flag = 1"</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span></span><br><span class="line">    &#125;).print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="集群执行-3"><a href="#集群执行-3" class="headerlink" title="集群执行"></a>集群执行</h3><h3 id="startUpdateVideoInfo-sh"><a href="#startUpdateVideoInfo-sh" class="headerlink" title="startUpdateVideoInfo.sh"></a>startUpdateVideoInfo.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发任务脚本</span><br><span class="line">startUpdateVideoInfo.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">获取最近一个月的文件目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash">filepath=<span class="string">""</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">for</span>((i=1;i&lt;=30;i++))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">do</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    filepath+=<span class="string">"hdfs://bigdata01:9000/data/video_info/"</span>`date -d <span class="string">"<span class="variable">$i</span> days ago"</span> +<span class="string">"%Y%m%d"</span>`,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">done</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：在使用的时候需要将最后面的逗号去掉 <span class="variable">$&#123;filePath:0:-1&#125;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/video_info/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="UpdateVideoInfoScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.UpdateVideoInfoScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/update_video_info-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;tmp=$8;getline;print tmp","$8&#125;'` // 这个程序提交到时候会有两个application，以前的判断方式不适合了</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED,SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时其实会发现产生了两个Flink任务。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031731219.png" alt="image-20230503173125131"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v2.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v1.0-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-3.html</id>
    <published>2023-04-24T07:37:33.000Z</published>
    <updated>2023-05-21T10:40:35.607Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v1-0-3"><a href="#第十八周-直播平台三度关系推荐v1-0-3" class="headerlink" title="第十八周 直播平台三度关系推荐v1.0-3"></a>第十八周 直播平台三度关系推荐v1.0-3</h1><h2 id="数据计算之实时维护粉丝关注-第二个任务"><a href="#数据计算之实时维护粉丝关注-第二个任务" class="headerlink" title="数据计算之实时维护粉丝关注(第二个任务)"></a>数据计算之实时维护粉丝关注(第二个任务)</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261212936.png" alt="image-20230426121205538"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下数据计算中的第二步，实时维护粉丝关注数据。我们的实时粉丝关注数据呢，来源于服务端日志，因为当用户在直播平台中对主播进行关注和取消关注的时候呢，会调用服务端接口。所以说服务端会记录这些操作日志。具体的数据格式呢，是这样。这是一个json格式，fuid就代表了粉丝。uid代表的是主播。好，这个timestamp，它表示这个具体你这个关注行为，或者你取消关注行为，它产生的时间。这个type呢，表示这个数据是什么类型的数据，它是粉丝关注相关的数据。那具体这条数据是关注还是取消关注，我们要根据这个desc这个参数来定。它里面这个值如果是follow就表示是关注，如果是UN follow，就表示取消关注。所以说后期我们在解析这个数据的时候呢，其实核心字段就是三个followeruid，还有这个followuid，还有这个desc。那接下来我们就需要使用sparkstreamming实时维护neo4j粉丝关注的相关数据。</span><br></pre></td></tr></table></figure><h3 id="父项目pom"><a href="#父项目pom" class="headerlink" title="父项目pom"></a>父项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;packaging&gt;pom&lt;&#x2F;packaging&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;modules&gt;</span><br><span class="line">        &lt;module&gt;generate_data&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;data_collect&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;server_inter&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;real_time_follow&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_user_level&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_user_active&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_video_info&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;get_recommend_list&lt;&#x2F;module&gt;</span><br><span class="line">    &lt;&#x2F;modules&gt;</span><br><span class="line">    &lt;dependencyManagement&gt;</span><br><span class="line">        &lt;dependencies&gt;</span><br><span class="line">            &lt;!-- log4j的依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.2.68&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.1.1.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.httpcomponents&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;httpclient&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;4.5.12&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;commons-dbutils&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;commons-dbutils&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;8.0.20&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.2.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- spark相关依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spark-streaming_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- neo4j相关依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;4.1.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;neo4j-contrib&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;neo4j-spark-connector&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.5-M1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;&#x2F;dependencyManagement&gt;</span><br><span class="line">    &lt;repositories&gt;</span><br><span class="line">        &lt;!-- list of other repositories --&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;SparkPackagesRepo&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;url&gt;http:&#x2F;&#x2F;dl.bintray.com&#x2F;spark-packages&#x2F;maven&lt;&#x2F;url&gt;</span><br><span class="line">        &lt;&#x2F;repository&gt;</span><br><span class="line">    &lt;&#x2F;repositories&gt;</span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以及添加一个repository，因为neo4j-spark-connector这个依赖在maven中央仓库中是没有的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意我在这个父项目创建module子项目，副项目这个pom文件里面啊，提前把相关的依赖啊都给它放进，我们可以看一下，主要是下面这些东西。这是Spark相关的依赖，Spark core、spark sql、 Spark streaming。还有这个spark streaming kafka对吧，因为你要读取kafka嘛。以及下面呢，是你后这相关的一些依赖。现在我提前把它拿过来，后面我们再具体用到的时候呢，我再去分析。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261215313.png" alt="image-20230426121552111"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261216414.png" alt="image-20230426121614568"></p><h3 id="子项目pom"><a href="#子项目pom" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;real_time_follow&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那针对我们这个实施项目，它里面都需要什么依赖呢？注意它里面呢，你首先需要这个spark streaming以及spark streaming kafka吧。以及用neo4j那个依赖。还有一个，因为我们的原始数据是json的，我们要解析json，所以说呢，还要用那个fastjson这个包。那所以说我们看一下啊。neo4j-java-driver就类似于我们要操作mysql一样，我们要添加mysql对应的一个connector。啊，这个是类似的啊。</span><br><span class="line"></span><br><span class="line">在这呢，你可以把这个版本号给它删掉就行。因为在这啊，我们是在这个父项目里面统一来维护管理这些依赖，其实主要是管理这些版本。因为这个最外层呢，你看它套了一个dependency manager里面呢是一个dependency。所以说你看和这个比的话，它外面是多了一个这个depend manager。所以说呢，在这统一管理这些依赖的版本，你后期这里面这些子项目，你需要用到哪些依赖，你把这些依赖拿过来就可以使用了，这样的话你不需要指定版本，它呢会读取你那个父项目里面pom里面指定的这个版本。如果说你有个性化的需求，你说你那个公共的版本不满足我的要求，我想使用某一个特殊的版本，那么你可以在这来指定你的版本就可以了。这样的话只针对你这个子项目有效。那把这几个依赖加过来就可以了，现在我们来看看这个啊，前面这几个就没什么好说的了，主要看看这个neo4j-java-driver。就类似我们操作mysql一样，我们需要找到neo4j它的一个驱动jar包给大家分析一下我是在哪找到的？官网，这个其实啊，我们直接到这个maven仓库里面去搜也行，你到仓库里面去搜这个new瑞杠Java杠driver也是可以找到的，是一样的。这是这个流程。</span><br><span class="line"></span><br><span class="line">好，那接下来呢，我们把这个项目它的一个技术环境，再给它配一下邮件。摩托塞定是。这样里面注意咱那个library，我们把这个scala2.11这个SDK给它加进来。因为针对Spark而言，我们用的是2.11。你在这呢，我们再建一个录入叫SC。把它设置为S。OK，注意前面我们在开发代码的时候，我们先用代码来开发。等最后的时候呢，我会把那个java代码具体的实现呢，也给大家提供过去。我们课上讲的时候以这个scala代码为主。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那接下来建一个包。com。点五克点Spark。在这里面，我们创建一个object。叫real time。follow。那在这我们先写点注释，这个呢是任务二了，就第二个任务。实施维护粉丝关注数据。你妈吗？对，在这我们一个streaming。context。它里面呢，需要接触一个SPA以及一个时间second。second。我们把这个周期呢设为五秒行吧。上面呢，我们来创建，你有一个Spark，这个都是固定的写法。said master。LOGO2。因为你是一个实时处理程序。set APP name。就是这个。提取的变量。好。好，这个呢，也提取一个变量有SSC吧。这个呢表示创建。context。</span><br><span class="line"></span><br><span class="line">那接下来呢，我们需要获取。消费卡夫卡的数据流。这个呢，也是一个固定的写法。搞不搞？第二数一。我们使用那个direct。泛型呢，都是spring这个我们前面讲过啊嗯。把这个SSC传过去。后面呢，穿这个location。street。这个后面呢，直接那个五月份。好，它下面还有一个参数叫consumer。一个订阅功能啊。这块泛型呢，也都是three。sweets。注意这里面我们需要给它指定两个参数，一个呢是topics对吧，指定topic。还有一个呢，是卡夫卡的一个参数。需要这两个参数。那我们这上面需要重建一下。嗯。在这呢，指定要读取的topic的名称。主要是两个。那我们先创建第一个。他其实就是这个。我们直接使用个map。object。直接在这块来处理啊。首先在里面使用卡夫卡的一个扑克地址。RI。101冒号9092。逗号。0203把这个改一下，0203。好，这样第一个参数就搞定了。我们还需要指定一下这个K的一个序列化类型。前面先空着，我先把后面那个写，写完之后呢，复制一下。嗯。glass of string。这样就可以了。那前面的话呢，其实就是K点把这个拿过来。he ser。小写。这个呢是value的序列化类型，嗯。把这个复制一下，再改一下。好，这样就可以了，嗯。那下面来指定这个消费者组ID。嗯嗯。给我点ID。con吧，行吧。这个就是你来自己来指定。下面是一个消费策略。就你第一次消费的时候如何来消费？auto ofetet。就是读取最新的数据。这个是自动提交offset。嗯嗯。enable。there auto。这个是醋啊。现在啊，你最好给他强制指定类型，要不然用的时候呢，可能会有问题。加点allow their words。对吧，这个就可以了，那既然我们指定我们需要消费的这个topic。它是一个，它可以接收多个。注意，接着我们应该消费那个user。对吧，这里面是实时的粉丝关注相关的数据。OK，那这块呢就可以了，我们就可以获取到这个消费卡不卡的一个数据流了。这个是一个卡夫卡。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>, <span class="type">Transaction</span>, <span class="type">TransactionWork</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务2：</span></span><br><span class="line"><span class="comment"> * 实时维护粉丝关注数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTimeFollowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"RealTimeFollowScala"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定Kafka的配置信息</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Object</span>](</span><br><span class="line">      <span class="comment">//kafka的broker地址信息</span></span><br><span class="line">      <span class="string">"bootstrap.servers"</span>-&gt;<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>,</span><br><span class="line">      <span class="comment">//key的序列化类型</span></span><br><span class="line">      <span class="string">"key.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//value的序列化类型</span></span><br><span class="line">      <span class="string">"value.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//消费者组id</span></span><br><span class="line">      <span class="string">"group.id"</span>-&gt;<span class="string">"con_1"</span>,</span><br><span class="line">      <span class="comment">//消费策略</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span>-&gt;<span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">//自动提交offset</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span>-&gt;(<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//指定要读取的topic的名称</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"user_follow"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">    <span class="keyword">val</span> kafkaDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    <span class="comment">//首先将kafkaDStream转换为rdd，然后就可以调用rdd中的foreachPartition了</span></span><br><span class="line">    kafkaDStream.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">      <span class="comment">//一次处理一个分区的数据</span></span><br><span class="line">      rdd.foreachPartition(it=&gt;&#123;</span><br><span class="line">        <span class="comment">//获取neo4j连接</span></span><br><span class="line">        <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(<span class="string">"bolt://bigdata04:7687"</span>, <span class="type">AuthTokens</span>.basic(<span class="string">"neo4j"</span>, <span class="string">"admin"</span>))</span><br><span class="line">        <span class="comment">//开启一个会话</span></span><br><span class="line">        <span class="keyword">val</span> session = driver.session()</span><br><span class="line">        it.foreach(record=&gt;&#123;</span><br><span class="line">          <span class="comment">//获取粉丝关注相关数据</span></span><br><span class="line">          <span class="comment">//&#123;"followeruid":"1001","followuid":"1002","timestamp":1798198304,"type":"user_follow","desc":"follow"&#125;</span></span><br><span class="line">          <span class="keyword">val</span> line = record.value()</span><br><span class="line">          <span class="comment">//解析数据</span></span><br><span class="line">          <span class="keyword">val</span> userFollowObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">          <span class="comment">//获取数据类型，关注 or  取消关注</span></span><br><span class="line">          <span class="keyword">val</span> followType = userFollowObj.getString(<span class="string">"desc"</span>)</span><br><span class="line">          <span class="comment">//获取followeruid</span></span><br><span class="line">          <span class="keyword">val</span> followeruid = userFollowObj.getString(<span class="string">"followeruid"</span>)</span><br><span class="line">          <span class="comment">//获取followuid</span></span><br><span class="line">          <span class="keyword">val</span> followuid = userFollowObj.getString(<span class="string">"followuid"</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span>(<span class="string">"follow"</span>.equals(followType))&#123;</span><br><span class="line">            <span class="comment">//添加关注：因为涉及多条命令，所以需要使用事务</span></span><br><span class="line">            session.writeTransaction(<span class="keyword">new</span> <span class="type">TransactionWork</span>[<span class="type">Unit</span>] ()&#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(tx: <span class="type">Transaction</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">                <span class="keyword">try</span>&#123;</span><br><span class="line">                  tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">                  tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">                  tx.run(<span class="string">"match(a:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;),(b:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;) merge (a) -[:follow]-&gt; (b)"</span>)</span><br><span class="line">                  <span class="comment">//提交事务</span></span><br><span class="line">                  tx.commit()</span><br><span class="line">                &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">                  <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; tx.rollback()</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">//取消关注</span></span><br><span class="line">            session.run(<span class="string">"match(:User &#123;uid: '"</span>+followeruid+<span class="string">"'&#125;) -[r:follow]-&gt; (:User &#123;uid: '"</span>+followuid+<span class="string">"'&#125;) delete r"</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//关闭会话</span></span><br><span class="line">        session.close()</span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        driver.close()</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="RealTimeFollowScala-scala"><a href="#RealTimeFollowScala-scala" class="headerlink" title="RealTimeFollowScala.scala"></a>RealTimeFollowScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那下面我们就开始处理试卷。那你这边注意，你首先将这个卡不卡呀，转一下。转换为RDD。然后呢，就可以。就要用RDD中的这个for each part。好，第二和一起，阿D先把它转换成阿里D。这样这里面传过来其实就是阿里列。转成R之后，我们就可以调用它里面这个for each了，那这样的话就可以一次处理一批的数据。比如说一次处理一个分区的数。为什么要这样做呢？因为我们在这里面需要去连neo4j。我们要把这个具体用户实时的这个关注，以及取消关注这些关系呢，给他维护到neo4j里面，所以说在这呢，相当于我们需要获取用户这些链接，然后呢去操作它。</span><br><span class="line"></span><br><span class="line">那这样的话，你最好是一批获取一次链接，这样效率比较高对吧。我每一条数据过来都获取链接，这样效率比较低。因为本身你这个实时其实就是一小批一小批的数据，那针对这一小批里面的数据，你可以再按分区对吧，每个分区获取一次链接。啊，for。这是一个AI，嗯。好，那在这里面。我们就可以调用那个it点，不好意思就可以迭代它里面的每一条数据了，对吧。这是一个了吧。那现在注意，我们需要先获取u或这个链接。怎么获取呢？因为我们提前已经把那个六后力杠Java杠driver那个依赖已经引进去了，所以说你在这儿可以直接用。哇。第二。我们使用这个。这个前面呢，是那个具体的URLURL其实就是那个开头的那个。这个零四冒号7687。后面的话我们需要指定下这个用户名和密码，就这个奥。basic。嗯，指定一下用户名。你徒弟还有密码。嗯。好，这样的话，下面我们就获取到这个用户的一个链接了。那下面的话，我们需要使用这个链接呢，开启一个会话。开启绘画之后才可以去用啊。这个。那这样的话，在里面就可以使用这个session。那你在最后啊，用完之后啊，需要把它们两个给它关闭掉，先把这个写一下，关闭会话。session。close。嗯。关闭连接。there are the clothes。好，那下面呢，我们来把里面这个代码给它完善一下，这里面是核心的代码。那在这里面，我们那就可以获取这个粉丝关注相关的数据。它其实呢，就是那个阶层串出了一块阶层数据。在这了。men。这样的话就可以获取到这行数据了。把这行计算数据获取到，获取到之后注意下面呢，我们来解析数据。用杰森。yeah。us objects。嗯，把它传过去。这个呢，其实就是user。follow。好，那接下来我们就从它里面去获取一些数据。获取数据类型。到底是？关注哦，取消关注。你又使用那个user点，咱们刚才分析了，它里面有一个DC这个词对吧。follow types。那接下来我们还需要获取这个follow u ID。嗯。thatw。嗯。W。what ID？接下来获取了一个follow u ID。follow u ID。嗯嗯。好，这三个核心的这个字段呢，我们都获取到。那获取到之后，下面的话，我们其实就可以根据这个photo这个类型来执行具体关注或者是取消关注这个操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">那我们接着来判断一下。如果呀，这个。follow the equal。这个full time。如果说你这个值等于follow，那相当于就是关注。else。就是取消关注了。添加关注。这是取消关注。注意你说这种写法。和这样写，你说我把它放到前面。这里面呢，写那个follow有什么区别吗？注意你这种写法，如果说。他呢，是空是闹，你这样调用会报了一个空，人异常啊。但我把它放到前面，那肯定不会报空人了啊，这肯定是有值的。你就算你那个pro等于no，这个顶多是不相等，它也不会出现这个控制异常啊，这个需要注意一下。下面呢，我们来执行添加关注操作。这需要注意。因为呢，它这里面啊，会涉及多条命令。所以说呢，我们需要使用事物。类似于我们先需要添加两个节点，然后呢，再给这两个基点绑定一个关注关系。这样的话就是三条命令。这三条命令在执行的时候，要么都成功，要么都失败。好，那所以说在这我们需要这样来做，通过session。第二。right transaction。开启一个事物。你要传一个，你有一个栓死action。walk。注意这块这个泛型啊，表示你这里面这个代码啊，最终的返回的数据类型在这我们不需要返回东西，所以说呢。把要空就行了。实现里面微信的方法。就这个。好，那这里面就需要实现我们具体的逻辑了。注意。它这里面相当有一个数啊transaction，所以说我们直接调TS点。你看它里面传的是一个query string，其实就是具体的一个执行的一个语句。墨。user。ID。这是一个字符串，所以说要加个单引号啊。那我们在这获取这个UID。那下面还有一个。这是follow u ID对吧。先把这两个节点给它创建了。那下面我们要给它们两个绑定关系。注意你在这绑定关系的话，这个相当于它是分开执行的啊，只不过说他们在一个事物里面，所以你在这啊，先添加这两个下面再来查。match。把你刚才添加这两个节点查出来，再给它们两个绑定关系。user。ad。这是这个UID，我们先把这个查出来，然后呢，再把第二个。给它起个名字叫B。user。这是UID。ID对吧，这样的话，你看其实我们就把这两个节点个查出来了。你前面是添加在这，把他们两个都查出来，这查第一个，这是查第二个。并且呢，给每一个都起了一个别名，它叫a，它叫B，对吧。那后面呢，就可以使用这个。墨。a。好。这样就可以了。在这x.com提交15。那注意，如果说他在执行的时候失败了，这样的话，我们需要让这个事物回滚。catch。直接捕获这个最大的异常。只要抛一场，我们就调用它的一个back。这样就可以了。所以这里面啊，执行的还是咱们前面讲那个step语句对吧。OK，这是添加关注。那取消关注就简单了呀。取消关注，其实呢，一条命令就可以了。相当于呢，我使用match把它查出来，然后后边的话使用一个DD的，那这样的话，其实呢，你就不需要开启这种事物了，因为你相当于取消关注，只有一条命令，就不需要开启这个事故了啊，然后自动提交就行。所以说我们直接使用这个session点。软。match。user。UID。说了UID。然后面他呢。follow。这个哥们在。UID。follow u ID。注意这时候呢，给这个关系呢，起一个名字叫啊。所以说在最后面直接把这个关系给它删掉就行。这个意思啊。这个呢，就是取消关注，取消他们两个之间的佛的关系。这样的话其实就可以了啊。我们这里面的一个核心代码就搞定了。最后是一个启动任务。start。应该在这里面。这个是等待任务停止。嗯。嗯。好，这样就可以了。这样的话，我们这个代码啊，其实就开发好了。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">那接下来我们需要确保卡夫卡服务cub的采集程序、数据分发程序以及服务端接口服务正常运行。那这样的话，我们就可以在本地去运行这个Spark人命程序了。好在这样我们把它启动起来。先在本地去执行一下。等这个程序启动之后呢，我们再去调那个接口，模拟产生这个用户实时的关注和取消关注数据。你先确认你这个人命程序是正常的</span><br><span class="line"></span><br><span class="line">看到没有，它这个程序不停就说明了它是OK的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">好，那接下来呢，我们就来模拟产生一些数据。找到这个generate date这个项目对吧。我们调里面这个generate real time执行这个，它每执行一次都会产生一条这个实时的一个粉丝关注或者取消关注的数据。执行一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261635084.png" alt="image-20230426163505984"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是一个follow。你看这个2009FOLLOW了1005，那我们来看一下。在这刷新一下，或者你重新点那个follow，或者你点那个刷新都可以。看到没有2009FOLLOW了1005没问题吧。我们之前是没有这个的啊，之前它是没有这个follow关系的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261636657.png" alt="image-20230426163612378"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">OK，所以说呢，我们这个代码呢，正常执行了。</span><br><span class="line"></span><br><span class="line">但是注意了，目前这个程序啊，其实是存在一些问题的，因为数据通过filebeat的采集，再到kafka，最终我们消费的数据顺序和数据产生的顺序可能就不一致了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261641417.png" alt="image-20230426164158476"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">举个例子：</span><br><span class="line">用户A先关注了用户B</span><br><span class="line">用户A很快又取关了用户B</span><br><span class="line">这样就会产生两条日志数据，这两条数据经过采集分发之后，最后我们消费过来的数据顺序很有可能是这样的</span><br><span class="line">用户A取关用户B</span><br><span class="line">用户A关注了用户B</span><br><span class="line">这种最终的结果就是用户A关注了用户B，那这样就不准确了，虽然这种情况是小概率事件，但是也是存在的，在SparkStreaming中如何解决呢？</span><br><span class="line">由于SparkStreaming是一小批一小批处理的，所以我们可以针对每次获取的这一小批数据根据数据产生的时间戳进行排序，从小到大，然后按照这个顺序去操作这些数据，这样其实就能在很大程度上避免我们刚才分析的这种问题了，但是这样并没有完全解决掉这个问题，如果两条数据分到了两批数据里面，还是会存在这个问题的，不过这种情况出现的概率就很低了，我们暂时就忽略不计了。</span><br><span class="line">在这我把这个思路分析好了，给大家留一个作业，大家下去之后自己尝试动手实现一下这个功能</span><br><span class="line"></span><br><span class="line">使用sparkstreaming解决数据乱序的问题。因为我们的原始数据里面，里面有个FUID，还有UID，还有一个时间戳，对吧，那现在在sparkstreaming里面，你获取到这一批数据之后，你对这一批数据按照time stamp进行排序，从小到大。排完序之后呢，再去执行这些操作。这样就可以了。大家呢，先自己动手做一下啊。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那接下来呢，我们来对这个代码啊，再给它完善一下。可以让它呢，同时支持在本地运行和集群运行。把这个给它停掉。</span><br><span class="line"></span><br><span class="line">这里面啊，可能会发生变化的这些信息呢，全部都给它提取出来。让它支持动态传递啊，这样的话，我们这个代码呢，更会更加通用。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>, <span class="type">Transaction</span>, <span class="type">TransactionWork</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务2：</span></span><br><span class="line"><span class="comment"> * 实时维护粉丝关注数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTimeFollowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> masterUrl = <span class="string">"local[2]"</span></span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"RealTimeFollowScala"</span></span><br><span class="line">    <span class="keyword">var</span> seconds = <span class="number">5</span></span><br><span class="line">    <span class="keyword">var</span> kafkaBrokers = <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span></span><br><span class="line">    <span class="keyword">var</span> groupId = <span class="string">"con_1"</span></span><br><span class="line">    <span class="keyword">var</span> topic = <span class="string">"user_follow"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> username = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> password = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      masterUrl = args(<span class="number">0</span>)</span><br><span class="line">      appName = args(<span class="number">1</span>)</span><br><span class="line">      seconds = args(<span class="number">2</span>).toInt</span><br><span class="line">      kafkaBrokers = args(<span class="number">3</span>)</span><br><span class="line">      groupId = args(<span class="number">4</span>)</span><br><span class="line">      topic = args(<span class="number">5</span>)</span><br><span class="line">      boltUrl = args(<span class="number">6</span>)</span><br><span class="line">      username = args(<span class="number">7</span>)</span><br><span class="line">      password = args(<span class="number">8</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(masterUrl).setAppName(appName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(seconds))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka的配置信息</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Object</span>](</span><br><span class="line">      <span class="comment">//kafka的broker地址信息</span></span><br><span class="line">      <span class="string">"bootstrap.servers"</span>-&gt;kafkaBrokers,</span><br><span class="line">      <span class="comment">//key的序列化类型</span></span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//value的序列化类型</span></span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//消费者组id</span></span><br><span class="line">      <span class="string">"group.id"</span> -&gt; groupId,</span><br><span class="line">      <span class="comment">//消费策略</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">//自动提交offset</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定要读取的topic的名称</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(topic)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">    <span class="keyword">val</span> kafkaDstream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    <span class="comment">//首先将kafkaDstream转换为rdd，然后就可以调用rdd中的foreachPartition了</span></span><br><span class="line">    kafkaDstream.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">      <span class="comment">//一次处理一个分区的数据</span></span><br><span class="line">      rdd.foreachPartition(it=&gt;&#123;</span><br><span class="line">        <span class="comment">//获取neo4j的连接</span></span><br><span class="line">        <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(username, password))</span><br><span class="line">        <span class="comment">//开启一个会话</span></span><br><span class="line">        <span class="keyword">val</span> session = driver.session()</span><br><span class="line">        it.foreach(record=&gt;&#123;</span><br><span class="line">          <span class="comment">//获取粉丝关注相关数据</span></span><br><span class="line">          <span class="keyword">val</span> line = record.value()</span><br><span class="line">          <span class="comment">//解析数据</span></span><br><span class="line">          <span class="keyword">val</span> userFollowObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">          <span class="comment">//获取数据类型，关注 or  取消关注</span></span><br><span class="line">          <span class="keyword">val</span> followType = userFollowObj.getString(<span class="string">"desc"</span>)</span><br><span class="line">          <span class="comment">//获取followeruid</span></span><br><span class="line">          <span class="keyword">val</span> followeruid = userFollowObj.getString(<span class="string">"followeruid"</span>)</span><br><span class="line">          <span class="comment">//获取followuid</span></span><br><span class="line">          <span class="keyword">val</span> followuid = userFollowObj.getString(<span class="string">"followuid"</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span>(<span class="string">"follow"</span>.equals(followType))&#123; <span class="comment">// 顺序前后不要改</span></span><br><span class="line">            <span class="comment">//添加关注：因为涉及多条命令，所以需要使用事务</span></span><br><span class="line">            session.writeTransaction(<span class="keyword">new</span> <span class="type">TransactionWork</span>[<span class="type">Unit</span>] ()&#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(tx: <span class="type">Transaction</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">                <span class="keyword">try</span>&#123;</span><br><span class="line">                  tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">                  tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">                  tx.run(<span class="string">"match(a:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;),(b:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;) merge (a) -[:follow]-&gt; (b)"</span>)</span><br><span class="line">                  <span class="comment">//提交事务</span></span><br><span class="line">                  tx.commit()</span><br><span class="line">                &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">                  <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; tx.rollback()</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">//取消关注</span></span><br><span class="line">            session.run(<span class="string">"match (:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;) -[r:follow]-&gt; (:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;) delete r"</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//关闭会话</span></span><br><span class="line">        session.close()</span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        driver.close()</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：建议这个项目中的所有依赖包全部在spark-submit脚本后面的–jars中指定(neo4j-java-driver、fastjson、spark-streaming-kafka这三个需要手动指定，其它的spark安装包已有)，这样最终生成的任_务jar就比较小了，提交任务的时候速度会比较快。所以这里面的jar-with-dependencies插件就可以不使用了，因为我们打jar包的时候不需要把依赖打进去，这个时候也不需要在依赖中添加provided参数了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261654714.png" alt="image-20230426165401187"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261704743.png" alt="image-20230426170407568"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">为了方便的提交任务，我们再开发一个任务提交脚本，</span><br><span class="line">在项目中创建一个bin目录，把脚本放到这个bin目录一样，这样便于管理维护</span><br><span class="line">startRealTimeFollow.sh</span><br></pre></td></tr></table></figure><h3 id="startRealTimeFollow-sh"><a href="#startRealTimeFollow-sh" class="headerlink" title="startRealTimeFollow.sh"></a>startRealTimeFollow.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">masterUrl&#x3D;&quot;yarn-cluster&quot;</span><br><span class="line">master&#x3D;&#96;echo $&#123;masterUrl&#125; | awk -F&#39;-&#39; &#39;&#123;print $1&#125;&#39;&#96;</span><br><span class="line">deployMode&#x3D;&#96;echo $&#123;masterUrl&#125; | awk -F&#39;-&#39; &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line"></span><br><span class="line">appName&#x3D;&quot;RealTimeFollowScala&quot;</span><br><span class="line">seconds&#x3D;5</span><br><span class="line">kafkaBrokers&#x3D;&quot;bigdata01:9092,bigdata02:9092,bigdata03:9092&quot;</span><br><span class="line">groupId&#x3D;&quot;con_1&quot;</span><br><span class="line">topic&#x3D;&quot;user_follow&quot;</span><br><span class="line">boltUrl&#x3D;&quot;bolt:&#x2F;&#x2F;bigdata04:7687&quot;</span><br><span class="line">username&#x3D;&quot;neo4j&quot;</span><br><span class="line">password&#x3D;&quot;admin&quot;</span><br><span class="line"></span><br><span class="line">yarnCommonLib&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;yarnCommonLib&quot;</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.RealTimeFollowScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;&#x2F;fastjson-1.2.68.jar,$&#123;yarnCommonLib&#125;&#x2F;spark-streaming-kafka-0-10_2.11-2.4.3.jar,$&#123;yarnCommonLib&#125;&#x2F;neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;&#x2F;kafka-clients-2.4.1.jar,$&#123;yarnCommonLib&#125;&#x2F;reactive-streams-1.0.3.jar \</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;jobs&#x2F;real_time_follow-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;seconds&#125; $&#123;kafkaBrokers&#125; $&#123;groupId&#125; $&#123;topic&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">jar包路径可以是本地，也可以是hdfs上，建议hdfs</span><br><span class="line"></span><br><span class="line">在本地如何找这些jar包?在.m2(是maven本地目录)目录下去找，然后再上传</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">针对这个参数：yarnCommonLib&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;yarnCommonLib&quot;</span><br><span class="line">使用公共的依赖包目录，使用起来方便，管理维护起来也方便，并且还可以提高任务执行效率，因为当我们向集群提交的时候，任务需要的依赖jar包是会自动上传到hdfs的一个临时目录的，如果我们提前把jar包上传到hdfs上面，就不会再重新上传了</span><br><span class="line"></span><br><span class="line">针对–jars后面指定的依赖jar包，需要额外再指定kafka-clients-2.4.1.jar和reactive-streams-1.0.3.jar，一个是kafka的，一个是neo4j需要依赖的，否则提交任务到集群执行是会报错的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261823893.png" alt="image-20230426182327467"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261841795.png" alt="image-20230426184121107"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261842193.png" alt="image-20230426184238164"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261843621.png" alt="image-20230426184330270"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">重新生成一条实时关注数据，查看结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261844934.png" alt="image-20230426184425485"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">停止sparkStreaming任务</span><br><span class="line"></span><br><span class="line">这是正常的一帆风顺的流程。</span><br><span class="line"></span><br><span class="line">那我们如果是第一次这样做，肯定会遇到各种各样的问题，在这里来给大家复现一下。</span><br><span class="line">按照正常的思路，这个项目依赖的jar包最开始我们肯定只会使用这三个</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">masterUrl&#x3D;&quot;yarn-cluster&quot;</span><br><span class="line">master&#x3D;&#96;echo $&#123;masterUrl&#125; | awk -F&#39;-&#39; &#39;&#123;print $1&#125;&#39;&#96;</span><br><span class="line">deployMode&#x3D;&#96;echo $&#123;masterUrl&#125; | awk -F&#39;-&#39; &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line"></span><br><span class="line">appName&#x3D;&quot;RealTimeFollowScala&quot;</span><br><span class="line">seconds&#x3D;5</span><br><span class="line">kafkaBrokers&#x3D;&quot;bigdata01:9092,bigdata02:9092,bigdata03:9092&quot;</span><br><span class="line">groupId&#x3D;&quot;con_1&quot;</span><br><span class="line">topic&#x3D;&quot;user_follow&quot;</span><br><span class="line">boltUrl&#x3D;&quot;bolt:&#x2F;&#x2F;bigdata04:7687&quot;</span><br><span class="line">username&#x3D;&quot;neo4j&quot;</span><br><span class="line">password&#x3D;&quot;admin&quot;</span><br><span class="line"></span><br><span class="line">yarnCommonLib&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;yarnCommonLib&quot;</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.RealTimeFollowScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;&#x2F;fastjson-1.2.68.jar,$&#123;yarnCommonLib&#125;&#x2F;spark-streaming-kafka-0-10_2.11-2.4.3.jar,$&#123;yarnCommonLib&#125;&#x2F;neo4j-java-driver-4.1.1.jar \</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;jobs&#x2F;real_time_follow-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;seconds&#125; $&#123;kafkaBrokers&#125; $&#123;groupId&#125; $&#123;topic&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startRealTimeFollow.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时你会发现任务执行失败了</span><br><span class="line">报错信息如下，在控制台就可以看到具体的报错信息：</span><br><span class="line">这个报错信息表示是缺少kafka的一些依赖，org&#x2F;apache&#x2F;kafka&#x2F;common&#x2F;serialization&#x2F;StringDeserializer，通过这里面的包名可以看出来，这个是kafka-clients的依赖，如果直接看不出来，那就到网上搜一下这个报错信息看看有没有收获java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;kafka&#x2F;common&#x2F;serialization&#x2F;StringDeserializer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">所以我们在脚本中再添加这个kafka-client的依赖</span><br><span class="line"></span><br><span class="line">再提交</span><br><span class="line"></span><br><span class="line">发现任务还是执行失败，在控制台可以看到报错信息如下</span><br><span class="line"></span><br><span class="line">这里提示neo4j中的org.neo4j.driver.Config初始化失败，但是neo4j的依赖我们也添加进去了，为什么还会报这个错呢？</span><br><span class="line"></span><br><span class="line">如果这里看不到有用的错误信息，我们可以尝试到YARN中查看</span><br><span class="line">在YARN的8088界面中我们进入到spark的任务界面(注意：需要确保hadoop的日志聚合功能开启，以及Spark的historyServer进程也是开启的。)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262122548.png" alt="image-20230426212208228"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262122638.png" alt="image-20230426212224386"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262122756.png" alt="image-20230426212235821"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">其实根源是在这，主要是缺少这个类org.reactivestreams.Publisher，最终导致的org.neo4j.driver.Config无法初始化。</span><br><span class="line"></span><br><span class="line">那这个类org.reactivestreams.Publisher是从哪来的呢？</span><br><span class="line">它是neo4j需要使用的一个依赖里面的类</span><br><span class="line">到哪找呢？</span><br><span class="line">查看neo4j-java-driver(子pom里ctrl+点击neo4j-java-driver)这个依赖的依赖，会发现它里面确实有一个依赖的名称是org.reactivestreams，所以说我们还需要把这个依赖的jar包找到引进去。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">到此为止，把这个jar包再加进去就可以正常执行了，这就是我们排查问题的一个思路和流程，这个思路大家一定要学以致用。</span><br><span class="line"></span><br><span class="line">这个任务开发到这就结束了</span><br></pre></td></tr></table></figure><h2 id="数据计算之每天定时更新主播等级-第三个任务"><a href="#数据计算之每天定时更新主播等级-第三个任务" class="headerlink" title="数据计算之每天定时更新主播等级(第三个任务)"></a>数据计算之每天定时更新主播等级(第三个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">主播等级数据来源于服务端数据库(定时增量导入到HDFS中)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262130828.png" alt="image-20230426213050640"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：表中有两个等级字段，一个是用户等级，一个是主播等级</span><br><span class="line"></span><br><span class="line">在这我们需要使用主播等级</span><br><span class="line">针对这份数据，最核心的两个字段是第2列和第4列</span><br><span class="line">第2列是用户uid</span><br><span class="line">第4列是主播等级anchor_level</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个任务需要做的是把每天主播等级发生了变化的数据更新到neo4j中，在neo4j中也维护一份主播的等级</span><br><span class="line">创建一个子module：update_user_level</span><br><span class="line">创建scala目录，添加scala2.11的sdk</span><br><span class="line">引入依赖</span><br></pre></td></tr></table></figure><h3 id="所需依赖"><a href="#所需依赖" class="headerlink" title="所需依赖"></a>所需依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateUserLevelScala-scala"><a href="#UpdateUserLevelScala-scala" class="headerlink" title="UpdateUserLevelScala.scala"></a>UpdateUserLevelScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在update_user_level下面的scala里面创建包：com.imooc.spark</span><br><span class="line">创建类：UpdateUserLevelScala</span><br><span class="line">代码如下</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务3：</span></span><br><span class="line"><span class="comment"> * 每天定时更新主播等级</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateUserLevelScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> masterUrl = <span class="string">"local"</span></span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"UpdateUserLevelScala"</span></span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/cl_level_user/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> username = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> password = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      masterUrl = args(<span class="number">0</span>)</span><br><span class="line">      appName = args(<span class="number">1</span>)</span><br><span class="line">      filePath = args(<span class="number">2</span>)</span><br><span class="line">      boltUrl = args(<span class="number">3</span>)</span><br><span class="line">      username = args(<span class="number">4</span>)</span><br><span class="line">      password = args(<span class="number">5</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(masterUrl)</span><br><span class="line">      .setAppName(appName)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取用户等级数据</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD = sc.textFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//校验数据准确性</span></span><br><span class="line">    <span class="keyword">val</span> filterRDD = linesRDD.filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="comment">//判断每一行的列数是否正确，以及这一行是不是表头</span></span><br><span class="line">      <span class="keyword">if</span> (fields.length == <span class="number">8</span> &amp;&amp; !fields(<span class="number">0</span>).equals(<span class="string">"id"</span>)) &#123;</span><br><span class="line">        <span class="literal">true</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">   </span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    filterRDD.foreachPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(username, password))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//添加等级</span></span><br><span class="line">        session.run(<span class="string">"merge(u:User &#123;uid:'"</span>+fields(<span class="number">1</span>).trim+<span class="string">"'&#125;) set u.level = "</span>+fields(<span class="number">3</span>).trim) <span class="comment">//.trim防止空格被引入</span></span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在本地执行代码</span><br><span class="line">效果如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262214894.png" alt="image-20230426221422858"></p><h3 id="startUpdateUserLevel-sh"><a href="#startUpdateUserLevel-sh" class="headerlink" title="startUpdateUserLevel.sh"></a>startUpdateUserLevel.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发任务执行脚本</span><br><span class="line">startUpdateUserLevel.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/cl_level_user/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">master=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $1&#125;'`</span><br><span class="line">deployMode=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $2&#125;'`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">appName="UpdateUserLevelScala"`date +%s` // 加的这个是为了后面过滤任务列表，查看是否执行成功</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">username="neo4j"</span><br><span class="line">password="admin"</span><br><span class="line"></span><br><span class="line">yarnCommonLib="hdfs://bigdata01:9000/yarnCommonLib"</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.UpdateUserLevelScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;/neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;/reactive-streams-1.0.3.jar \</span><br><span class="line">/data/soft/video_recommend/jobs/update_user_level-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $7&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262237405.png" alt="image-20230426223703754"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262238354.png" alt="image-20230426223841949"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262239377.png" alt="image-20230426223922101"></p><h3 id="打包配置"><a href="#打包配置" class="headerlink" title="打包配置"></a>打包配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure><h3 id="子项目完整pom"><a href="#子项目完整pom" class="headerlink" title="子项目完整pom"></a>子项目完整pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_level&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262246028.png" alt="image-20230426224642629"></p><h3 id="集群执行"><a href="#集群执行" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startUpdateUserLevel.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262254705.png" alt="image-20230426225404311"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262254651.png" alt="image-20230426225455351"></p><h2 id="数据计算之每天定时更新用户活跃时间-第四个任务"><a href="#数据计算之每天定时更新用户活跃时间-第四个任务" class="headerlink" title="数据计算之每天定时更新用户活跃时间(第四个任务)"></a>数据计算之每天定时更新用户活跃时间(第四个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据来源于客户端上报，每天只要打开过APP就会上报数据</span><br><span class="line">数据格式：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262258359.png" alt="image-20230426225850610"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">之前我们通过埋点模拟上报数据，通过flume落盘到hdfs上面，这样在hdfs上面产生的目录会使用当天日期，为了保证我这里使用的目录和大家都保持一致，所以在这我就生成一个固定的日期目录</span><br><span class="line">使用代码GenerateUserActiveDataV2，在代码中指定日期2026-02-01，这样会把模拟生成的用户活跃数据直接上传到hdfs上面，因为之前的数据采集流程我们已经详细分析过了，所以在这就直接把数据上传到hdfs上面了。</span><br></pre></td></tr></table></figure><h3 id="生成数据"><a href="#生成数据" class="headerlink" title="生成数据"></a>生成数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行代码：GenerateUserActiveDataV2，将会把数据上传到hdfs的这个目录下</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;user_active&#x2F;20260201&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262316695.png" alt="image-20230426231654458"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262323730.png" alt="image-20230426232332517"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这个任务需要做的是把每天主动活跃的用户更新到neo4j中，在neo4j中维护一份用户的最新活跃时间</span><br><span class="line">创建子module项目：update_user_active</span><br><span class="line">创建scala目录，引入scala2.11版本的sdk</span><br><span class="line">在scala目录中创建包：com.imooc.spark</span><br><span class="line">引入依赖</span><br></pre></td></tr></table></figure><h3 id="所需依赖-1"><a href="#所需依赖-1" class="headerlink" title="所需依赖"></a>所需依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateUserActiveScala-scala"><a href="#UpdateUserActiveScala-scala" class="headerlink" title="UpdateUserActiveScala.scala"></a>UpdateUserActiveScala.scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务4：</span></span><br><span class="line"><span class="comment"> * 每天定时更新用户活跃时间</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateUserActiveScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> masterUrl = <span class="string">"local"</span></span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"UpdateUserActiveScala"</span></span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/user_active/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> username = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> password = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      masterUrl = args(<span class="number">0</span>)</span><br><span class="line">      appName = args(<span class="number">1</span>)</span><br><span class="line">      filePath = args(<span class="number">2</span>)</span><br><span class="line">      boltUrl = args(<span class="number">3</span>)</span><br><span class="line">      username = args(<span class="number">4</span>)</span><br><span class="line">      password = args(<span class="number">5</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(masterUrl)</span><br><span class="line">      .setAppName(appName)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取用户活跃数据</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD = sc.textFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据 </span></span><br><span class="line">    linesRDD.foreachPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(username, password))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">        <span class="keyword">val</span> uid = jsonObj.getString(<span class="string">"uid"</span>)</span><br><span class="line">        <span class="keyword">val</span> timeStamp = jsonObj.getString(<span class="string">"UnixtimeStamp"</span>)</span><br><span class="line">        <span class="comment">//添加用户活跃时间</span></span><br><span class="line">        session.run(<span class="string">"merge(u:User &#123;uid:'"</span>+uid+<span class="string">"'&#125;) set u.timestamp = "</span>+timeStamp)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-1"><a href="#本地执行-1" class="headerlink" title="本地执行"></a>本地执行</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262336732.png" alt="image-20230426233615284"></p><h3 id="startUpdateUserActive-sh"><a href="#startUpdateUserActive-sh" class="headerlink" title="startUpdateUserActive.sh"></a>startUpdateUserActive.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/user_active/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">master=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $1&#125;'`</span><br><span class="line">deployMode=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $2&#125;'`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">appName="UpdateUserActiveScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">username="neo4j"</span><br><span class="line">password="admin"</span><br><span class="line"></span><br><span class="line">yarnCommonLib="hdfs://bigdata01:9000/yarnCommonLib"</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.UpdateUserActiveScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;/fastjson-1.2.68.jar,,$&#123;yarnCommonLib&#125;/neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;/reactive-streams-1.0.3.jar \</span><br><span class="line">/data/soft/video_recommend/jobs/update_user_active-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $7&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="打包配置-1"><a href="#打包配置-1" class="headerlink" title="打包配置"></a>打包配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure><h3 id="子项目完整pom-1"><a href="#子项目完整pom-1" class="headerlink" title="子项目完整pom"></a>子项目完整pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_active&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="集群执行-1"><a href="#集群执行-1" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startUpdateUserActive.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262344942.png" alt="image-20230426234450357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262345661.png" alt="image-20230426234521405"></p><h2 id="数据计算之每周一计算最近一个月主播视频评级-1-第五个任务"><a href="#数据计算之每周一计算最近一个月主播视频评级-1-第五个任务" class="headerlink" title="数据计算之每周一计算最近一个月主播视频评级-1(第五个任务)"></a>数据计算之每周一计算最近一个月主播视频评级-1(第五个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">视频数据来源于服务端，当主播开播结束后会产生一条视频数据</span><br><span class="line">数据格式：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262349598.png" alt="image-20230426234917792"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">之前我们通过埋点模拟上报数据，通过flume落盘到hdfs上面，这样在hdfs上面产生的目录会使用当天日期，为了保证我这里使用的目录和大家都保持一致，所以在这我就生成一个固定的日期目录</span><br><span class="line">使用代码GenerateVideoInfoDataV2，在代码中指定日期2026-02-01，这样会把模拟生成的用户活跃数据直接上传到hdfs上面，因为之前的数据采集流程我们已经详细分析过了，所以在这就直接把数据上传到hdfs上面了。</span><br><span class="line"></span><br><span class="line">执行代码：GenerateVideoInfoDataV2，将会把数据上传到hdfs的这个目录下</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;video_info&#x2F;20260201&#x2F;</span><br></pre></td></tr></table></figure><h3 id="生成数据-1"><a href="#生成数据-1" class="headerlink" title="生成数据"></a>生成数据</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262359274.png" alt="image-20230426235956140"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这个任务需要做的就是统计最近一个月内主播的视频评级信息</span><br><span class="line">在这我们先初始化一天的数据即可，计算一天和计算一个月的数据，计算逻辑是一样的，只有spark任务的输入路径不一样</span><br><span class="line">如果是一个月的数据，假设这一个月有30天，则需要把这30天对应的30个目录使用逗号分隔，拼接成一个字符串，作为Spark任务的输入即可。</span><br><span class="line"></span><br><span class="line">为什么这个任务要每周计算一次，而不是每天计算一次呢？</span><br><span class="line">因为很多主播不会每天都开播，所以我们每天都计算意义不大，均衡考虑之后按照每周计算一次这个频率。</span><br><span class="line"></span><br><span class="line">创建子module项目：update_video_info</span><br><span class="line">创建scala目录，引入scala2.11版本的sdk</span><br><span class="line">在scala目录中创建包：com.imooc.spark</span><br><span class="line">引入依赖</span><br></pre></td></tr></table></figure><h3 id="所需依赖-2"><a href="#所需依赖-2" class="headerlink" title="所需依赖"></a>所需依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateVideoInfoScala-scala"><a href="#UpdateVideoInfoScala-scala" class="headerlink" title="UpdateVideoInfoScala.scala"></a>UpdateVideoInfoScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.spark</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.neo4j.driver.&#123;AuthTokens, GraphDatabase&#125;</span><br><span class="line">import org.slf4j.LoggerFactory</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 任务5：</span><br><span class="line"> * 每周一计算最近一个月主播视频评级</span><br><span class="line"> * 把最近几次视频评级在3B+或2A+的主播，在neo4j中设置flag&#x3D;1</span><br><span class="line"> *</span><br><span class="line"> * 注意：在执行程序之前需要先把flag&#x3D;1的重置为0</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object UpdateVideoInfoScala &#123;</span><br><span class="line">  val logger &#x3D; LoggerFactory.getLogger(&quot;UpdateVideoInfoScala&quot;)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    var masterUrl &#x3D; &quot;local&quot;</span><br><span class="line">    var appName &#x3D; &quot;UpdateVideoInfoScala&quot;</span><br><span class="line">    var filePath &#x3D; &quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;video_info&#x2F;20260201&quot;</span><br><span class="line">    var boltUrl &#x3D; &quot;bolt:&#x2F;&#x2F;bigdata04:7687&quot;</span><br><span class="line">    var username &#x3D; &quot;neo4j&quot;</span><br><span class="line">    var password &#x3D; &quot;admin&quot;</span><br><span class="line">    if(args.length &gt; 0)&#123;</span><br><span class="line">      masterUrl &#x3D; args(0)</span><br><span class="line">      appName &#x3D; args(1)</span><br><span class="line">      filePath &#x3D; args(2)</span><br><span class="line">      boltUrl &#x3D; args(3)</span><br><span class="line">      username &#x3D; args(4)</span><br><span class="line">      password &#x3D; args(5)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;在Driver端执行此代码，将flag&#x3D;1的重置为0</span><br><span class="line">    &#x2F;&#x2F;获取neo4j的连接</span><br><span class="line">    val driver &#x3D; GraphDatabase.driver(boltUrl, AuthTokens.basic(username, password))</span><br><span class="line">    &#x2F;&#x2F;开启一个会话</span><br><span class="line">    val session &#x3D; driver.session()</span><br><span class="line">    session.run(&quot;match(a:User) where a.flag &#x3D;1 set a.flag &#x3D; 0&quot;)</span><br><span class="line">    &#x2F;&#x2F;关闭会话</span><br><span class="line">    session.close()</span><br><span class="line">    &#x2F;&#x2F;关闭连接</span><br><span class="line">    driver.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;获取SparkContext</span><br><span class="line">    val conf &#x3D; new SparkConf()</span><br><span class="line">      .setMaster(masterUrl)</span><br><span class="line">      .setAppName(appName)</span><br><span class="line">    val sc &#x3D; new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;读取视频评级数据</span><br><span class="line">    val linesRDD &#x3D; sc.textFile(filePath)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;解析数据中的uid，rating，timestamp</span><br><span class="line">    val tup3RDD &#x3D; linesRDD.map(line &#x3D;&gt; &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        val jsonObj &#x3D; JSON.parseObject(line)</span><br><span class="line">        val uid &#x3D; jsonObj.getString(&quot;uid&quot;)</span><br><span class="line">        val rating &#x3D; jsonObj.getString(&quot;rating&quot;)</span><br><span class="line">        val timestamp: Long &#x3D; jsonObj.getLong(&quot;timestamp&quot;)</span><br><span class="line">        (uid, rating, timestamp)</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        case ex: Exception &#x3D;&gt; logger.error(&quot;json数据解析失败：&quot; + line)</span><br><span class="line">          (&quot;0&quot;, &quot;0&quot;, 0L)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;过滤异常数据</span><br><span class="line">    val filterRDD &#x3D; tup3RDD.filter(_._2 !&#x3D; &quot;0&quot;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;获取用户最近3场直播(视频)的评级信息</span><br><span class="line">    val top3RDD &#x3D; filterRDD.groupBy(_._1).map(group &#x3D;&gt; &#123;</span><br><span class="line">      &#x2F;&#x2F;获取最近3次开播的数据，使用制表符拼接成一个字符串</span><br><span class="line">      &#x2F;&#x2F;uid,rating,timestamp \t uid,rating,timestamp \t uid,rating,timestamp </span><br><span class="line">      val top3 &#x3D; group._2.toList.sortBy(_._3).reverse.take(3).mkString(&quot;\t&quot;)</span><br><span class="line">      (group._1, top3)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;过滤出来满足3场B+的数据</span><br><span class="line">    val top3BRDD &#x3D; top3RDD.filter(tup &#x3D;&gt; &#123;</span><br><span class="line">      var flag &#x3D; false</span><br><span class="line">      val fields &#x3D; tup._2.split(&quot;\t&quot;)</span><br><span class="line">      if (fields.length &#x3D;&#x3D; 3) &#123;</span><br><span class="line">        &#x2F;&#x2F;3场B+，表示里面没有出现C和D</span><br><span class="line">        val tmp_str &#x3D; fields(0).split(&quot;,&quot;)(1) + &quot;,&quot; + fields(1).split(&quot;,&quot;)(1) + &quot;,&quot; + fields(2).split(&quot;,&quot;)(1)</span><br><span class="line">        if (!tmp_str.contains(&quot;C&quot;) &amp;&amp; !tmp_str.contains(&quot;D&quot;)) &#123;</span><br><span class="line">          flag &#x3D; true</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;把满足3场B+的数据更新到neo4j中，增加一个字段flag，flag&#x3D;1表示是视频评级满足条件的主播，允许推荐给用户</span><br><span class="line">    &#x2F;&#x2F;注意：针对3场B+的数据还需要额外再限制一下主播等级，主播等级需要&gt;&#x3D;15，这样可以保证筛选出来的主播尽可能是一些优质主播</span><br><span class="line">    top3BRDD.foreachPartition(it&#x3D;&gt;&#123;</span><br><span class="line">      &#x2F;&#x2F;获取neo4j的连接</span><br><span class="line">      val driver &#x3D; GraphDatabase.driver(boltUrl, AuthTokens.basic(username, password))</span><br><span class="line">      &#x2F;&#x2F;开启一个会话</span><br><span class="line">      val session &#x3D; driver.session()</span><br><span class="line">      it.foreach(tup&#x3D;&gt;&#123;</span><br><span class="line">        session.run(&quot;match (a:User &#123;uid:&#39;&quot;+tup._1+&quot;&#39;&#125;) where a.level &gt;&#x3D;15 set a.flag &#x3D; 1&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line">      &#x2F;&#x2F;关闭会话</span><br><span class="line">      session.close()</span><br><span class="line">      &#x2F;&#x2F;关闭连接</span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;过滤出来满足2场A+的数据</span><br><span class="line">    val top2ARDD &#x3D; top3RDD.filter(tup &#x3D;&gt; &#123;</span><br><span class="line">      var flag &#x3D; false</span><br><span class="line">      val fields &#x3D; tup._2.split(&quot;\t&quot;)</span><br><span class="line">      if (fields.length &gt;&#x3D; 2) &#123;</span><br><span class="line">        &#x2F;&#x2F;2场A+，获取最近两场直播评级，里面不能出现B、C、D</span><br><span class="line">        val tmp_str &#x3D; fields(0).split(&quot;,&quot;)(1) + &quot;,&quot; + fields(1).split(&quot;,&quot;)(1)</span><br><span class="line">        if (!tmp_str.contains(&quot;B&quot;) &amp;&amp; !tmp_str.contains(&quot;C&quot;) &amp;&amp; !tmp_str.contains(&quot;D&quot;)) &#123;</span><br><span class="line">          flag &#x3D; true</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;把满足2场A+的数据更新到neo4j中，设置flag&#x3D;1</span><br><span class="line">    &#x2F;&#x2F;注意：针对2场A+的数据还需要额外限制一下主播等级，主播等级需要&gt;&#x3D;4 这样可以保证筛选出来的主播尽可能是一些优质主播</span><br><span class="line">    top2ARDD.foreachPartition(it&#x3D;&gt;&#123;</span><br><span class="line">      &#x2F;&#x2F;获取neo4j的连接</span><br><span class="line">      val driver &#x3D; GraphDatabase.driver(boltUrl, AuthTokens.basic(username, password))</span><br><span class="line">      &#x2F;&#x2F;开启一个会话</span><br><span class="line">      val session &#x3D; driver.session()</span><br><span class="line">      it.foreach(tup&#x3D;&gt;&#123;</span><br><span class="line">        session.run(&quot;match (a:User &#123;uid:&#39;&quot;+tup._1+&quot;&#39;&#125;) where a.level &gt;&#x3D;4 set a.flag &#x3D; 1&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line">      &#x2F;&#x2F;关闭会话</span><br><span class="line">      session.close()</span><br><span class="line">      &#x2F;&#x2F;关闭连接</span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-2"><a href="#本地执行-2" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在本地执行代码</span><br><span class="line">然后到neo4j的web界面查看结果，发现只有uid为1005的数据对应的flag不等于1(没有flag属性)</span><br><span class="line">这样是正确的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304271046115.png" alt="image-20230427104641625"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304271048485.png" alt="image-20230427104824499"></p><h3 id="startUpdateVideoInfo-sh"><a href="#startUpdateVideoInfo-sh" class="headerlink" title="startUpdateVideoInfo.sh"></a>startUpdateVideoInfo.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面开发任务执行脚本</span><br><span class="line">注意：这个脚本中需要实现获取最近一个月的数据目录</span><br><span class="line">startUpdateVideoInfo.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取最近一个月的文件目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash">filepath=<span class="string">""</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">for</span>((i=1;i&lt;=30;i++))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">do</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    filepath+=<span class="string">"hdfs://bigdata01:9000/data/video_info/"</span>`date -d <span class="string">"<span class="variable">$i</span> days ago"</span> +<span class="string">"%Y%m%d"</span>`, // 这里的，号很经典</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">done</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/video_info/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">master=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $1&#125;'`</span><br><span class="line">deployMode=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $2&#125;'`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">appName="UpdateVideoInfoScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">username="neo4j"</span><br><span class="line">password="admin"</span><br><span class="line"></span><br><span class="line">yarnCommonLib="hdfs://bigdata01:9000/yarnCommonLib"</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.UpdateVideoInfoScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;/fastjson-1.2.68.jar,$&#123;yarnCommonLib&#125;/neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;/reactive-streams-1.0.3.jar \</span><br><span class="line">/data/soft/video_recommend/jobs/update_video_info-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $7&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="子项目完整依赖"><a href="#子项目完整依赖" class="headerlink" title="子项目完整依赖"></a>子项目完整依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_active&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="集群执行-2"><a href="#集群执行-2" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startUpdateVideoInfo.sh 20260201</span><br></pre></td></tr></table></figure><h2 id="数据计算之每周一计算三度关系推荐列数据-第六个任务"><a href="#数据计算之每周一计算三度关系推荐列数据-第六个任务" class="headerlink" title="数据计算之每周一计算三度关系推荐列数据(第六个任务)"></a>数据计算之每周一计算三度关系推荐列数据(第六个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">前面我们在neo4j中维护了粉丝和主播的一些信息，在这里我们就需要基于neo4j中的数据统计主播的三度关系推荐列表了</span><br><span class="line"></span><br><span class="line">这个任务在这也是每周计算一次，我们测试过，每天都计算的话，最终的结果变化是不大的，所以就没必要每天计算了。</span><br><span class="line"></span><br><span class="line">创建子module项目：get_recommend_list</span><br><span class="line">创建scala目录，引入scala2.11版本的sdk</span><br><span class="line">在scala目录中创建包：com.imooc.spark</span><br><span class="line">引入依赖，这里面需要额外用到spark-sql和neo4j-spark-connector这两个依赖</span><br></pre></td></tr></table></figure><h3 id="生成数据-2"><a href="#生成数据-2" class="headerlink" title="生成数据"></a>生成数据</h3><h3 id="所需依赖-3"><a href="#所需依赖-3" class="headerlink" title="所需依赖"></a>所需依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;neo4j-contrib&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-spark-connector&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在使用spark读取neo4j中数据的时候，可以使用一个插件，在官网中可以找到</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281349757.png" alt="image-20230428134919089"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281349353.png" alt="image-20230428134931154"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281350275.png" alt="image-20230428135056752"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个版本是基于neo4j 4.0，我们现在使用的neo4j是3.5的，这种一般是向下兼容的，所以操作neo4j 3.5也是可以的，后面写的spark是2.4.5，这个也是可以的，我们使用的spark是2.4.3的，最后一位版本号不一致没问题。</span><br><span class="line">目前最新版本是基于scala2.12版本编译的，我们在spark项目中使用的scala版本是2.11，所以使用2.4.5-M1这个版本。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在使用这个依赖的时候，还需要配置它对应的repository，因为这个依赖没有在maven仓库中，把这些配置添加到父项目的pom.xml文件中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281352725.png" alt="image-20230428135228605"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281352639.png" alt="image-20230428135241954"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">咱们前面使用的neo4j-java-driver相当于是使用原生代码操作neo4j，而现在使用neo4j-spark-connector相当于把neo4j封装到spark中了，使用起来比较方便。</span><br><span class="line"></span><br><span class="line">在使用neo4j-spark-connector的时候，选择哪个版本呢？</span><br><span class="line">点这里进去看一下</span><br></pre></td></tr></table></figure><h3 id="GetRecommendListScala-scala"><a href="#GetRecommendListScala-scala" class="headerlink" title="GetRecommendListScala.scala"></a>GetRecommendListScala.scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.spark.<span class="type">Neo4j</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务6：</span></span><br><span class="line"><span class="comment"> * 每周一计算最近一周内主活主播的三度关系列表</span></span><br><span class="line"><span class="comment"> * 注意：</span></span><br><span class="line"><span class="comment"> * 1：待推荐主播最近一周内活跃过</span></span><br><span class="line"><span class="comment"> * 2：待推荐主播等级&gt;4</span></span><br><span class="line"><span class="comment"> * 3：待推荐主播最近1个月视频评级满足3B+或2A+(flag=1)</span></span><br><span class="line"><span class="comment"> * 4：待推荐主播的粉丝列表关注重合度&gt;2</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GetRecommendListScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> masterUrl = <span class="string">"local"</span></span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"GetRecommendListScala"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> username = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> password = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">var</span> timestamp = <span class="number">0</span>L <span class="comment">//过滤最近一周内是否活跃过</span></span><br><span class="line">    <span class="keyword">var</span> duplicateNum = <span class="number">2</span> <span class="comment">//粉丝列表关注重合度</span></span><br><span class="line">    <span class="keyword">var</span> level = <span class="number">4</span> <span class="comment">//主播等级</span></span><br><span class="line">    <span class="keyword">var</span> outputPath = <span class="string">"hdfs://bigdata01:9000/data/recommend_data/20260201"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      masterUrl = args(<span class="number">0</span>)</span><br><span class="line">      appName = args(<span class="number">1</span>)</span><br><span class="line">      boltUrl = args(<span class="number">2</span>)</span><br><span class="line">      username = args(<span class="number">3</span>)</span><br><span class="line">      password = args(<span class="number">4</span>)</span><br><span class="line">      timestamp = args(<span class="number">5</span>).toLong</span><br><span class="line">      duplicateNum = args(<span class="number">6</span>).toInt</span><br><span class="line">      level = args(<span class="number">7</span>).toInt</span><br><span class="line">      outputPath = args(<span class="number">8</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(appName)</span><br><span class="line">      .setMaster(masterUrl)</span><br><span class="line">      .set(<span class="string">"spark.driver.allowMultipleContexts"</span>,<span class="string">"true"</span>)<span class="comment">//允许创建多个context</span></span><br><span class="line">      .set(<span class="string">"spark.neo4j.url"</span>,boltUrl)<span class="comment">//bolt的地址</span></span><br><span class="line">      .set(<span class="string">"spark.neo4j.user"</span>,username)<span class="comment">//neo4j用户名</span></span><br><span class="line">      .set(<span class="string">"spark.neo4j.password"</span>,password)<span class="comment">//neo4j密码</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取一周内主活的主播 并且主播等级大于4的数据</span></span><br><span class="line">    <span class="keyword">var</span> params = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Long</span>]()</span><br><span class="line">    params += (<span class="string">"timestamp"</span>-&gt;timestamp)</span><br><span class="line">    params += (<span class="string">"level"</span>-&gt;level)</span><br><span class="line">    <span class="keyword">val</span> neo4j: <span class="type">Neo4j</span> = <span class="type">Neo4j</span>(sc).cypher(<span class="string">"match (a:User) where a.timestamp &gt;= &#123;timestamp&#125; and a.level &gt;= &#123;level&#125; return a.uid"</span>).params(params)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将从neo4j中查询出来的数据转换为rowRDD</span></span><br><span class="line">    <span class="comment">//val rowRDD = neo4j.loadRowRdd</span></span><br><span class="line">    <span class="comment">//repartition 这里的repartition是为了把数据分为7份，这样下面的mapPartitions在执行的时候就有7个线程</span></span><br><span class="line">    <span class="comment">//这7个线程并行查询neo4j数据库</span></span><br><span class="line">    <span class="keyword">val</span> rowRDD = neo4j.loadRowRdd.repartition(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一次处理一批</span></span><br><span class="line">    <span class="comment">//过滤出粉丝关注重合度&gt;2的数据，并且对关注重合度倒序排列</span></span><br><span class="line">    <span class="comment">//最终的数据格式是：主播id,待推荐的主播id</span></span><br><span class="line">    <span class="keyword">val</span> mapRDD = rowRDD.mapPartitions(it =&gt; &#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(username, password))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      <span class="comment">//保存计算出来的结果</span></span><br><span class="line">      <span class="keyword">val</span> resultArr = <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">      it.foreach(row =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> uid = row.getString(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//计算一个用户的三度关系(主播的二度关系)</span></span><br><span class="line">        <span class="comment">//注意：数据量大了之后，这个计算操作是非常耗时</span></span><br><span class="line">        <span class="comment">//val result = session.run("match (a:User &#123;uid:'" + uid + "'&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt; (c:User) return a.uid as auid,c.uid as cuid,count(c.uid) as sum order by sum desc limit 30")</span></span><br><span class="line">        <span class="comment">//对b、c的主活时间进行过滤，以及对c的level和flag值进行过滤</span></span><br><span class="line">        <span class="keyword">val</span> result = session.run(<span class="string">"match (a:User &#123;uid:'"</span> + uid + <span class="string">"'&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt; (c:User) "</span> +</span><br><span class="line">          <span class="string">"where b.timestamp &gt;= "</span> + timestamp + <span class="string">" and c.timestamp &gt;= "</span> + timestamp + <span class="string">" and c.level &gt;= "</span> + level + <span class="string">" and c.flag = 1 "</span> +</span><br><span class="line">          <span class="string">"return a.uid as auid,c.uid as cuid,count(c.uid) as sum order by sum desc limit 30"</span>)</span><br><span class="line">        <span class="keyword">while</span> (result.hasNext) &#123;</span><br><span class="line">          <span class="keyword">val</span> record = result.next()</span><br><span class="line">          <span class="keyword">val</span> sum = record.get(<span class="string">"sum"</span>).asInt()</span><br><span class="line">          <span class="keyword">if</span> (sum &gt; duplicateNum) &#123;</span><br><span class="line">            resultArr += record.get(<span class="string">"auid"</span>).asString() + <span class="string">"\t"</span> + record.get(<span class="string">"cuid"</span>).asString()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      resultArr.iterator</span><br><span class="line">    &#125;).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>) <span class="comment">//把RDD数据缓存起来</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//把数据转成tuple2的形式</span></span><br><span class="line">    <span class="keyword">val</span> tup2RDD = mapRDD.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> splits = line.split(<span class="string">"\t"</span>)</span><br><span class="line">      (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//根据主播id进行分组，可以获取到这个主播的待推荐列表</span></span><br><span class="line">    <span class="keyword">val</span> reduceRDD = tup2RDD.reduceByKey((v1, v2) =&gt; &#123;</span><br><span class="line">      v1 + <span class="string">","</span> + v2</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//最终把结果组装成这种形式</span></span><br><span class="line">    <span class="comment">//1001 1002,1003,1004</span></span><br><span class="line">    reduceRDD.map(tup=&gt;&#123;</span><br><span class="line">      tup._1+<span class="string">"\t"</span>+tup._2</span><br><span class="line">    &#125;).repartition(<span class="number">1</span>).saveAsTextFile(outputPath)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-3"><a href="#本地执行-3" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">先使用这一行代码，在计算三度关系数据的时候暂时先不进行条件过滤。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281537558.png" alt="image-20230428153711075"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281537439.png" alt="image-20230428153735751"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接着使用这一行代码，在计算三度关系数据的对数据进行条件过滤</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281539054.png" alt="image-20230428153929765"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281540381.png" alt="image-20230428154023306"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到neo4j中验证一下，确实是正确的，因为1005的flag不为1，被过滤掉了，所以我在关注1000这个主播的时候平台只需要给我推荐主1004这个主播即可。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这个代码在执行mapPartitions的时候，最好把rowRDD的分区重新设置一下，如果让程序自动分配的话可能不太合理，分多了分少了都不太好</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">由于我们在mapPartitions中需要操作neo4j，所以这个时候rowRDD分区的数量就可以等于（neo4j服务器的CPU数量-1），要给neo4j预留出来一个cpu来处理其它任务请求。</span><br><span class="line">我们当时的服务器是8个CPU，给neo4j预留出来一个，剩下还有7个，所以说，neo4j此时可以对外提供的最大并发处理能力是7，那我们就把rowRDD设置为7个分区，就会有7个线程并行处理数据，它们可以并行操作neo4j，这样效率最高。</span><br><span class="line">如果给rowRDD设置的分区太多，对应的就会有多个线程并行操作neo4j，会给neo4j造成很大的压力，相当于neo4j在满负荷的运行，这个时候我们另外一个实时维护neo4j中粉丝关注数据的程序执行起来就很慢了，以及其他人如果这个时候想查询neo4j，也会非常慢，所以这样就不太好了。</span><br><span class="line">如果给rowRDD设置的分区太少，对应产生的执行线程就比较少，此时neo4j会比较空闲，没有多大压力，但是我们这个三度关系的任务执行就非常慢了。</span><br><span class="line">综上所述，建议把rowRDD的分区数量设置为7，这样可以充分利用neo4j服务器的性能，也不至于把neo4j服务器拖垮。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281547995.png" alt="image-20230428154738077" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">还有一点可以优化的，增加RDD持久化，把RDD数据缓存起来，这样可以避免个别task失败导致的数据重算，因为这个计算还是比较消耗时间的，所以说尽可能保证计算出来的数据不丢失。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281553783.png" alt="image-20230428155333860"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">问题：大家有没有想过，我们是否可以直接在Neo4j(sc).cypher(…)中指定一条查询语句，直接把所有的三度关系全部查询出来？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这样理论上是可以的，但是在实际中，当neo4j中存储的节点数和关系数量达到千万级别之后，同时查询所有满足条件主播的三度关系推荐列表的时候会很慢，有时候会导致等了几十分钟也查询不出来数据，所以在这我们就把这个功能进行了拆解，先查看满足条件的主播uid列表，然后再一个一个计算这些主播的三度关系推荐列表，这样可以提高计算效率，并且不会出现查询不出来结果的情况。</span><br></pre></td></tr></table></figure><h3 id="startGetRecommendList-sh"><a href="#startGetRecommendList-sh" class="headerlink" title="startGetRecommendList.sh"></a>startGetRecommendList.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取上周一的时间</span></span><br><span class="line">dt=`date -d "7 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">    dt=`date -d "7 days ago $1" +"%Y%m%d"`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">master=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $1&#125;'`</span><br><span class="line">deployMode=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $2&#125;'`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">appName="GetRecommendListScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">username="neo4j"</span><br><span class="line">password="admin"</span><br><span class="line"><span class="meta">#</span><span class="bash">获取上周一的时间戳(单位：毫秒)</span></span><br><span class="line">timestamp=`date --date="$&#123;dt&#125;" +%s`000</span><br><span class="line"><span class="meta">#</span><span class="bash">粉丝列表关注重合度</span></span><br><span class="line">duplicateNum=2</span><br><span class="line"><span class="meta">#</span><span class="bash">主播等级</span></span><br><span class="line">level=4</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">输出结果数据路径</span></span><br><span class="line">outputPath="hdfs://bigdata01:9000/data/recommend_data/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yarnCommonLib="hdfs://bigdata01:9000/yarnCommonLib"</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.GetRecommendListScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;/neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;/reactive-streams-1.0.3.jar,$&#123;yarnCommonLib&#125;/neo4j-spark-connector-2.4.5-M1.jar \</span><br><span class="line">/data/soft/video_recommend/jobs/get_recommend_list-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125; $&#123;timestamp&#125; $&#123;duplicateNum&#125; $&#123;level&#125; $&#123;outputPath&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $7&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="子项目完整依赖-1"><a href="#子项目完整依赖-1" class="headerlink" title="子项目完整依赖"></a>子项目完整依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;get_recommend_list&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;neo4j-contrib&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-spark-connector&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="集群执行-3"><a href="#集群执行-3" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startGetRecommendList.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281605986.png" alt="image-20230428160537691"></p><h2 id="数据计算之三度关系数据导出到Mysql-第七个任务"><a href="#数据计算之三度关系数据导出到Mysql-第七个任务" class="headerlink" title="数据计算之三度关系数据导出到Mysql(第七个任务)"></a>数据计算之三度关系数据导出到Mysql(第七个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">接下来我们需要使用Sqoop将HDFS中计算好的三度关系推荐列表数据导出到MySQL中</span><br><span class="line">需要在mysql中创建一个表recommend_list</span><br><span class="line">这个表有两列：</span><br><span class="line">第一列为主播uid</span><br><span class="line">第二列为待推荐主播uid</span><br><span class="line"></span><br><span class="line">当用户关注某个主播的时候，会根据这个主播的uid到这个表里面进行查询，把待推荐主播uid获取到，在页面中进行展现，推荐给用户，这样就都可以实现三度关系推荐的效果了。</span><br><span class="line"></span><br><span class="line">建表语句如下：</span><br><span class="line">recommend_list.sql</span><br></pre></td></tr></table></figure><h3 id="recommend-list-sql"><a href="#recommend-list-sql" class="headerlink" title="recommend_list.sql"></a>recommend_list.sql</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> recommend_list;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> recommend_list(</span><br><span class="line">uid <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">recommend_uids <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">primary <span class="keyword">key</span> (uid)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="export-recommend-list-sh"><a href="#export-recommend-list-sh" class="headerlink" title="export_recommend_list.sh"></a>export_recommend_list.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来使用sqoop将hdfs中的输出导出到mysql中</span><br><span class="line">在导出的时候实现插入和更新功能，如果uid对应的数据已存在，则更新，如果不存在则插入</span><br><span class="line">开发一个脚本</span><br><span class="line">export_recommend_list.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#默认获取上周一的时间</span><br><span class="line">dt&#x3D;&#96;date -d &quot;7 days ago&quot; +&quot;%Y%m%d&quot;&#96;</span><br><span class="line">if [ &quot;x$1&quot; !&#x3D; &quot;x&quot; ]</span><br><span class="line">then</span><br><span class="line">    dt&#x3D;&#96;date -d &quot;7 days ago $1&quot; +&quot;%Y%m%d&quot;&#96;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;video?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table recommend_list \</span><br><span class="line">--export-dir hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;recommend_data&#x2F;$&#123;dt&#125; \</span><br><span class="line">--input-fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--update-key uid \</span><br><span class="line">--update-mode allowinsert</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x exexport_recommend_list.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281750435.png" alt="image-20230428175018471"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其实在实际工作中我们需要做的到这就可以了，然后把这个数据库的名称、表名、表中的字段含义写一个文档同步给服务端即可，具体的数据交互是由服务端和客户端进行对接的。</span><br></pre></td></tr></table></figure><h2 id="数据展现"><a href="#数据展现" class="headerlink" title="数据展现"></a>数据展现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在后面的v2.0架构中，我们会开发一个接口，对外提供数据，因为直接把数据库暴露给其它用户不太安全，倒不是怕他们删库跑路，是担心他们误操作把某些数据删掉了。</span><br><span class="line">等到我们在v2.0中开发了数据接口之后，我们再通过本地启动项目进行效果演示。</span><br></pre></td></tr></table></figure><h2 id="项目代码双语支持"><a href="#项目代码双语支持" class="headerlink" title="项目代码双语支持"></a>项目代码双语支持</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">咱们前面在开发具体的数据计算代码的时候，使用的都是scala代码，为了兼顾Java开发人员，针对数据处理中的功能代码在这我也提供了Java代码支持</span><br><span class="line">在这里这些java代码我就不再手敲了，到时候直接把对应的java代码一起提交到git上面，大家如果有需要了可以去一下。</span><br><span class="line">其实我是不建议在工作中使用java去开发spark和flink的，在这只是为了给大家多一个选择而已，根据行业经验而言，scala语言是开发spark和flink最好的选择。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">在v1.0中，我们侧重于学习整个项目的业务、架构逻辑和实战代码开发。</span><br><span class="line">针对项目调忧、项目数据规模、集群规模、Neo4j性能指标等内容，在后面的V2.0中给大家安排上。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v1.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v1.0-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-2.html</id>
    <published>2023-04-24T07:37:28.000Z</published>
    <updated>2023-05-20T10:28:05.377Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v1-0-2"><a href="#第十八周-直播平台三度关系推荐v1-0-2" class="headerlink" title="第十八周 直播平台三度关系推荐v1.0-2"></a>第十八周 直播平台三度关系推荐v1.0-2</h1><h2 id="数据采集架构详细设计"><a href="#数据采集架构详细设计" class="headerlink" title="数据采集架构详细设计"></a>数据采集架构详细设计</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242348422.png" alt="image-20230424234832289"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们就从我们整体架构里面的第一个模块数据采集模块开始。注意，在实际过程中，数据采集模块不是只针对某一个项目而言的，而是一个公共的采集平台，所有项目依赖的数据全部都来源于数据采集模块，所以在设计采集模块的时候要考虑通用性。不能仅仅是为了这一个项目而服务。</span><br><span class="line"></span><br><span class="line">咱们前面在分析整体架构的时候说过，filebeat采集的数据到达kafka以后，会通过flume再做一下分发，为什么要有这个分发这个过程呢？这个分发过程实现了什么功能呢？我们来看一下这张图。这个图里面呢，针对数据采集模块做了详细的分析，把数据采集模块呢，又划分了三层，数据采集聚合层，数据分发省数据落盘层。</span><br><span class="line"></span><br><span class="line">在这个数据采集聚合层，我们为了保证采集程序的通用性，不至于每次新增一个业务指标的数据，就去重新增加一个采集进程，或者修改采集程序的配置文件。所以呢，我们定义了一个规则。所有的日志数据全部保存在服务器的一个特定的目录下面。我会让filebeat的监控这个目录下面的所有文件。如果后期有新增业务日志，那么就会在这个目录下新增一种日志文件，filebeat就可以自动识别。但是这个时候会有一个问题。filebeat的输出只有一个。多种类型的日志数据会被filebeat采集到同一个topic中。如果各种类型的日志数据全部混到一块儿，会导致后期处理数据的时候比较麻烦。本来呢，我只想计算一种数据，但是这个时候我就需要读取这个大的topic。它里面呢，包含了很多种数据类型。这里面的一个数据量也很大，那我计算的时候呢，我就需要把它里面所有数据全部都读出来，然后再过滤。这样在计算的时候就会影响计算效率，也间接的浪费了计算资源。所以针对这个问题，我们又定义了一个规则，所有的日志数据全部使用json格式，并且呢，在json中增加一个type字段，标识数据的类型，这样每一条数据都有自己的类型标识，然后汇聚到kafka中的一个大的topic中。为了后面使用方便，我们就需要把这个大的topic中的数据啊，根据业务类型进行拆分，把不同类型的数据啊分发到不同的topic中。那这块的话，其实就是我们这个数据分发层要干的事情，相当于filebeat呢，它呢会把这个数据啊，全部都采集到kafka里面这个大topic里面。所有业务类型的日全部都采集到这一个topic里面。</span><br><span class="line"></span><br><span class="line">然后呢，我们接着就可以使用这个flume，对kafka中这个大topic中的数据进行分发。利用flume中的拦截器解析数据中的那个type字段的值，把type字段的值作为输出topic的一个名称。这样就可以把相同类型的数据分发到同一个topic中了。当然了，这些topic呢，我们需要提前创建。如果想要提高这个数据分发的能力，我们还可以在这儿启动多个flume进程。只需要保证多个flume中指定相同的group.id就可以了。这样就可以并行执行这个数据分发操作。那把这个数据分发到对应的topic里面以后呀，后面的实施计算程序就可以直接消费这个topic进行计算了。不需要再去读取那个大的topic，这样就可以提高计算性能。并且呢，我们还可以把需要备份的topic里面的数据啊，使用flume进行落盘，把它保存到Hdfs里面。这个呢，就是数据采集架构的详细设计。</span><br></pre></td></tr></table></figure><h2 id="数据来源分析"><a href="#数据来源分析" class="headerlink" title="数据来源分析"></a>数据来源分析</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181718754.png" alt="image-20230425104401098"></p><h3 id="服务端日志数据"><a href="#服务端日志数据" class="headerlink" title="服务端日志数据"></a>服务端日志数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们来分析一下，针对这个项目，我们需要采集哪些业务类型的数据，以及这些数据来源于什么地方。首先是服务端日志数据。什么是服务端日志呢？可以这样理解。就是我们在APP中点击一些按钮的时候，例如我们要关注一个主播，这个时候当我们点击这个关注按钮之后。APP呢，它会去请求对应的接口。接口中的代码逻辑就是将我们关注的数据保存到数据库里面。同时，这个接口也会记录一份日志。因为这个接口呢，是为APP提供后台服务的，所以它记录的日志我们称之为服务端日志。接着我们来简单画一个图来看一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181712187.png" alt="image-20230518171257153"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个呢，是隔壁老王。那是我们的一个用户。画的形象一点。这是我们的一个APP。我们这个APP里面呢，它有一个关注功能。这时候呢，你看老王啊，他会点击这个关注功能。要关注某一个主播。当他点击这个关注功能之后，这个APP里面这个关注功能，它对应的它会调用一个接口。那我们这块呢，有一个后台服务器。这个服务器里面呢，部署的有一个接口服务，就是这种HTTP接口。那对应的这个关注功能，其实呢，还会调那个接口。所以你在这呢，点击APP里面这个关注按钮。它底层啊，其实会调这个接口。这个接口呢，其实呢，它会操作一个数据库。相遇老王在这儿呢，关注了一个主播，最终啊调这个接口。接口呢去操作这个数据库，最终把老王关注了，谁把这个数据呢存到数据库里面。就是这个逻辑啊。那这个时候注意我们在这个接入服务里面呢，因为它俩现在也是一个外部项目，所以什么它里面可以记录日志，那这里面记录的日志呢。我们就把它称为是服务端日志。</span><br><span class="line"></span><br><span class="line">那在我们这个项目里面，针对服务端日志。主要包含实时粉丝、观众数据以及视频数据。这个实时粉丝关注数据啊，是因为用户呢，在点击关注以及取消关注的时候，都需要调用服务端接口。所以这个数据呢，会在服务端通过日志记录。还有就是这个视频数据。下面这个视频啊，其实就是直播。当主播关闭直播的时候，会调用服务端接口上报本次直播的相关指标数据。其实服务端记录的还有很多其他类型的数据，只不过说呢，我们这个项目目前呢，只需要这两种数据。</span><br></pre></td></tr></table></figure><h3 id="服务端数据库数据"><a href="#服务端数据库数据" class="headerlink" title="服务端数据库数据"></a>服务端数据库数据</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181716837.png" alt="image-20230518171613701"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下服务端数据库中的数据。注意服务端数据库中的数据啊，其实啊，就是我们刚才图里面这个数据库里面的数据。来看一下。就这块儿。就相当于啊，我们App某一些功能呢，它会调用这个后台的接口，然后调用这个接口之后呢，最终啊，其实操作的是这个数据库。我们所说的这个服务端数据库的数据，其实说的就是这块它里面的数据。那在这个项目里面，我们主要获取历史粉丝关注数据，以及呢主播等级数据。这里面我们需要历史粉丝关注数据，因为我们在做这个项目的时候，我们的直播平台已经运营了两三年了。所以说，我们需要把历史粉丝关注数据初始化到图数据库中。这些历史数据服务端存储在数据库中，所以说我们需要从数据库里面去取。还有就是这个主播的等级数据。其实这个数据呢，在服务端日志中也有，但是我们考虑到服务端数据库中的数据是最准确的。特别是针对用户相关的数据，最好是以服务端数据库中的为准。所以说呢，我们就从那个服务端数据库中，每天凌晨啊，定时把昨天等级发生了变化的这个主播等级数据呢，导入到hdfs，方便我们后面的离线计算时。</span><br></pre></td></tr></table></figure><h3 id="客户端日志数据"><a href="#客户端日志数据" class="headerlink" title="客户端日志数据"></a>客户端日志数据</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181715348.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">最后呢，我们来看一下客户端日志数据。刚才我们分析了服务端日志。</span><br><span class="line"></span><br><span class="line">那什么是客户端日志呢？其实啊，就是用户在APP客户端操作的时候。直接通过埋点上报的日志数据，这种数据称之为客户端日志数据。我们在这再画一下这个图。如果说呀，你在这做了一些操作，你说呢，我不调用这个后台这个接口。我呢通过买点直接上报用户这个行为说你关注了谁对吧，直接通过买点上报。这个时候的话，在这呢，我们会有一个。这个日志接口服务器。他这个呢，就是直接通过这个埋点上报。那最终啊，可以把这个用户的行为数据啊，直接上报到这个日志接口服务器里面。然后就没有，然后。他不会去操作数据库啊。里面啊，一般上报的都是一些用户的行为。咱们刚才你看服务端日志呢，现在这啊，我们是要调这个接口，最终呢，是要操作数据库的，要把我们这个关注行为。就是谁关注，谁要把这个数据给它保存到数据库里面。</span><br><span class="line"></span><br><span class="line">而这种呢，通过埋点上报呢，其实呢，他就不会和那个数据库打交道。所以说这种话一般称之为是客服端日志。是通过客户端直接上报的，不需要和这个服务端这个接口去交互的。这里记录的日志，我们称之为后端日志。</span><br><span class="line"></span><br><span class="line">那这个服务端日志和客端日志有什么区别吗？为什么再分成两种日志呢？以及说为什么有的地方我们使用客户端日志，为什么有的地方我们使用服务端日志呢。针对我们这个APP里面啊，它这个关注功能服务端记录的这个日志啊，会更加准确。因为服务端接口里面，它会涉及到对于数据库的一个操作里面会有事务。只有这条数据真正保存成功的时候，才会记录日志，如果说你在操作数据库保存失败了。现在你会回滚，这时候就不需要去记录这个操作日志了。你顶多记录一条失败的日志。但是客户端日志，只要用户在APP里面点击了一次关注功能，他就会上报一次日志。最终啊，这个日志接收服务器啊，就会接收到这个日志，并且呢，把它记录下来。他可能呢，会由于网络等原因啊，导致最终关注失败，但是呢，你这条日志。你是发过来了，并且呢，这块啊也记录下来。所以相对来说，服务端数据的准确性是比这个客户端日志这个准确性高的。</span><br><span class="line"></span><br><span class="line">如果说一份数据在服务端日志中和客户端日志中同时都有。那我们肯定要优先选择服务端中的日志数据。一般啊，我们在客户端通过埋点上报的数据啊，都是一些用户行为数据，这些数据啊，就算有一些误差也没有多大影响。它不涉及到一些和数据库交互的一些操作。那在我们这个项目中，针对客户端日志，我们只要获取用户活跃数据。活跃呢表示啊，只要用户每天打开APP就认为用户活跃，用户的这些行为数据呢，会在客户端通过埋点上报管。</span><br><span class="line"></span><br><span class="line">那这些呢，就是我们项目中需要的一些基础数据，主要是这五份儿数据。那刚才我们分析到服务端日志，服务端数据库数据，以及客户端日志，那这个这个图里面啊，其实这样这个呢，就是服务端日志数据。这块呢，就是服务端数据库里面的数据。那这块呢，其实就是客户端的一些日志数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181725293.png" alt="image-20230518172542961"></p><h2 id="模拟产生数据"><a href="#模拟产生数据" class="headerlink" title="模拟产生数据"></a>模拟产生数据</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181741539.png" alt="image-20230518174114092"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251045006.png" alt="image-20230425104534521"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">前面我们分析了具体需要什么数据，以及数据从哪里来，下面我们来看一下如何通过代码模拟产生这些数据。先看这个图。我们通过执行generate date这个项目中的这五个入口类，可以模拟产生这五种数据。就是咱们前面分析的那五种数据。那在这里注意一下，因为我们不能直接使用企业中的真实数据啊，所以在这里我会根据企业中真实数据的格式去模拟生成。最终的效果是没有区别的。</span><br><span class="line"></span><br><span class="line">我们来看一下这些对应的代码。生成服务端数据和客户端数据代码如下：</span><br><span class="line"></span><br><span class="line">【服务端日志】实时粉丝关注数据: GenerateRealTimeFollowData</span><br><span class="line">【服务端日志】视频数据：GenerateVideoInfoData</span><br><span class="line">【服务端数据库】历史粉丝关注数据：GenerateHistoryFollowData</span><br><span class="line">【服务端数据库】主播等级数据：GenerateUserLevelData</span><br><span class="line">【客户端日志】用户活跃数据：GenerateUserActiveData</span><br><span class="line">在执行这些代码的时候还是需要使用之前在微信公众号中获取的校验码</span><br><span class="line"></span><br><span class="line">注意：在执行这些代码之前，我们需要先把基础环境搞定了</span><br><span class="line">这些代码在执行的时候会调用服务端接口和客户端日志接收服务，以及还会向MySQL中写入数据</span><br><span class="line">所以需要把这两个服务部署起来，以及在MySQL中初始化数据库和对应的表。</span><br><span class="line"></span><br><span class="line">所以说我们需要提前把这个两个接口服务，以及mysql中数据库和对的表都给它初始化好，都给它部署起来。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">第一步：</span><br><span class="line">对data_collect项目编译打包</span><br><span class="line"></span><br><span class="line">部署data_collect</span><br><span class="line">将生成的jar包上传到bigdata04机器的&#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;data_collect目录下，如果目录不存在则创建</span><br><span class="line"></span><br><span class="line">[root@bigdata04 soft]# mkdir -p &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;data_collect</span><br><span class="line"># 中间省略了上传jar包的过程</span><br><span class="line">[root@bigdata04 soft]# cd video_recommend&#x2F;data_collect&#x2F;</span><br><span class="line">[root@bigdata04 data_collect]# ll</span><br><span class="line">total 16932</span><br><span class="line">-rw-r--r--. 1 root root 17336051 Aug 29  2020 data_collect-1.0-SNAPSHOT.jar</span><br><span class="line">[root@bigdata04 data_collect]# nohup java -jar data_collect-1.0-SNAPSHOT.jar &amp;</span><br><span class="line"></span><br><span class="line">测试服务是否正常</span><br><span class="line">注意：这个服务监听的端口是8080</span><br><span class="line">[root@bigdata04 data_collect]# jps -ml</span><br><span class="line">1598 data_collect-1.0-SNAPSHOT.jar</span><br><span class="line">[root@bigdata04 data_collect]# curl -XGET &#39;http:&#x2F;&#x2F;localhost:8080&#x2F;v1&#x2F;t1?name&#x3D;test&#39;</span><br><span class="line">&#123;&quot;status&quot;:200,&quot;msg&quot;:&quot;success&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">第二步：</span><br><span class="line">对server_inter项目编译打包</span><br><span class="line"></span><br><span class="line">部署server_inter</span><br><span class="line">将生成的jar包上传到bigdata04机器的&#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;server_inter目录下，如果目录不存在则创建</span><br><span class="line"></span><br><span class="line">[root@bigdata04 data_collect]# mkdir -p &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;server_inter</span><br><span class="line"># 中间省略了上传jar包的过程</span><br><span class="line">[root@bigdata04 data_collect]# cd ..&#x2F;server_inter&#x2F;</span><br><span class="line">[root@bigdata04 server_inter]# ll</span><br><span class="line">total 16932</span><br><span class="line">-rw-r--r--. 1 root root 17336083 Aug 29  2020 server_inter-1.0-SNAPSHOT.jar</span><br><span class="line">[root@bigdata04 server_inter]# nohup java -jar server_inter-1.0-SNAPSHOT.jar &amp;</span><br><span class="line"></span><br><span class="line">测试服务是否正常</span><br><span class="line"></span><br><span class="line">注意：这个服务监听的端口是8081</span><br><span class="line">[root@bigdata04 server_inter]# jps -ml</span><br><span class="line">1673 server_inter-1.0-SNAPSHOT.jar</span><br><span class="line">1598 data_collect-1.0-SNAPSHOT.jar</span><br><span class="line">[root@bigdata04 server_inter]# curl -XGET &#39;http:&#x2F;&#x2F;localhost:8081&#x2F;s1&#x2F;t1?name&#x3D;test&#39;</span><br><span class="line">&#123;&quot;status&quot;:200,&quot;msg&quot;:&quot;success&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第三步：</span><br><span class="line">初始化数据库脚本</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181742580.png" alt="image-20230518174258258"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">接下来执行代码，开始模拟产生数据</span><br><span class="line">1：【服务端日志】实时粉丝关注数据: GenerateRealTimeFollowData</span><br><span class="line">注意：修改服务端接口地址，我是在bigdata04机器上部署的</span><br><span class="line">代码执行之后，可以看到服务端记录的日志数据</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;log&#x2F;</span><br><span class="line">[root@bigdata04 log]# more server_inter.log </span><br><span class="line">&#123;&quot;followuid&quot;:&quot;2002&quot;,&quot;followeruid&quot;:&quot;2001&quot;,&quot;type&quot;:&quot;user_follow&quot;,&quot;times</span><br><span class="line">tamp&quot;:1598684303487,&quot;desc&quot;:&quot;unfollow&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2：【服务端日志】视频数据：GenerateVideoInfoData</span><br><span class="line">注意：修改服务端接口地址，我是在bigdata04机器上部署的</span><br><span class="line">代码执行之后，可以看到服务端记录的日志数据</span><br><span class="line">[root@bigdata04 log]# tail -1 server_inter.log         </span><br><span class="line">&#123;&quot;area&quot;:&quot;A_US&quot;,&quot;watchnumpv&quot;:364,&quot;follower&quot;:364,&quot;hosts&quot;:364,&quot;watchnumuv&quot;:364,&quot;gifter&quot;:364,&quot;nofollower&quot;:364,&quot;length&quot;:464,&quot;rating&quot;:&quot;A&quot;,&quot;smlook&quot;:364,&quot;type&quot;:&quot;video_info&quot;,&quot;gold&quot;:364,&quot;uid&quot;:&quot;2009&quot;,&quot;nickname&quot;:&quot;jack38&quot;,&quot;looktime&quot;:364,&quot;id&quot;:&quot;1769913940296&quot;,&quot;exp&quot;:364,&quot;timestamp&quot;:1769913940000&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3：【服务端数据库】历史粉丝关注数据：GenerateHistoryFollowData</span><br><span class="line">注意：执行这个代码的时候需要注意修改项目中的db.properties中数据库的地址信息</span><br><span class="line">代码执行之后，可以到数据库中看到数据</span><br><span class="line">查看数据库中的follower_00这个表，发现里面有数据了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4：【服务端数据库】主播等级数据：GenerateUserLevelData</span><br><span class="line">注意：执行这个代码的时候需要注意修改项目中的db.properties中数据库的地址信息</span><br><span class="line">代码执行之后，可以到数据库中看到数据</span><br><span class="line">查看数据库中的cl_level_user这个表，发现里面有数据了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">5：【客户端日志】用户活跃数据：GenerateUserActiveData</span><br><span class="line">注意：修改客户端日志接收服务器地址，我是在bigdata04机器上部署的</span><br><span class="line">代码执行之后，可以看到客户端埋点上报的日志数据</span><br><span class="line">[root@bigdata04 log]# head -1 data_collect.log </span><br><span class="line">&#123;&quot;uid&quot;:&quot;1000&quot;,&quot;ver&quot;:&quot;3.6.41&quot;,&quot;countryCode&quot;:&quot;VN&quot;,&quot;ip&quot;:&quot;171.247.0.154&quot;,&quot;UnixtimeStamp&quot;:&quot;1598587868773&quot;,&quot;mcc&quot;:&quot;452&quot;,&quot;type&quot;:&quot;user_active&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">至此，项目中需要的数据都可以正常产生了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来第一步我们先对这个data collect这个项目给他呢编译打包。嗯。CDDB。video。recommend。C的这个data collect。肯定package。港地skip test。好，编译成功。那把它上传到我们这个服务器上面，注意在这呢，我统一都上传到我们这个BD的零四这台机器上面。那接着呢，我统一再建一个目录。每个点啊。V6。recommend。所以说呢，针对这个项目相关的一些架包啊，那些东西全部都放到这个目录里面，这样方便管理维护啊。那我们进到这个目录下面，注意现在里面呢，我在创建一个目录。叫data collect。那这里面呢，就放我们这个data这个接口，这个夹包。打开这个图形化界面。把我们刚才生成那个架包给他传上去。it soft。在这。把这个家包传上来。好，那下面呢，我们把它启动起来。Java杠架。后面呢，直接指定那个另一个加包就行了，注意我们需要把它放到后台运行，所以说呢，前面加个no ho。后面呢，加个and符，这样就可以了。GPS一下。看到没有，这个架已经启动了，你可以这样GPS杠ML。这样看起来更加清晰，对吧，就这个data collect。那下面呢，我们来测试一下这个服务啊是否正常，因为它是一个接口服务。这里面呢，我们其实开放的有一个测试的一个接口。看到没有？你直接调用这个就行，它是一个测试结果，当然你在这需要传一个参数啊，它需要提出一个参数name。他会把它打印出来，这样可以确认一下你这个服务是否正常啊。这个怎么验证呢？它是一个HTTP请求，你可以选择在我们的浏览器里面去操作，或者说呢，我们直接在这个控制台里面也是可以的，通过curl这个命令杠X。盖着操作啊。HTTP冒号双斜线logo。冒号，注意它这个端口呢，是8080。我在这指定了。看到没有8080啊。VT对吧。name。等于test吧。你看状态是200是OK的啊，说明我那个服务呢是正常启动了。那接下来第二步，我们把这个server这个项目呢，给它编译打包。有这个项目。他是负责写收我们那个服务日志的。嗯。clean港。开始。好，编译成功。把它传上来。注意，那我们在这需要创建一个目录啊。你可以在这直接对吧，右键这样创建也是可以的啊都可以。serve。因此。嗯。好，上传成功。那这个呢，我们要把它启动一下。no Hu Java刚加。安福。嗯。先确认这个服务是不是在啊在吗。好，这个呢，我们也来验证一下。大家注意它这个接口名称呢，就不是这个，你可以到这儿来确认一下。这是S1T1对吧，以及呢，它的端口注意。因为我们是在同一台服务器上启动的，所以说呢，这两个项目啊，它监听的端口肯定是不能重复的啊，这个我给它改成80812。四上这个是8081。后面的是S1T1对吧。嗯。好，OK啊，还是这个size。好，那这样的话，这两个接口服务呢，就搞定了，这个以这个都部署好了。那接着第三步，我们需要把这个MY面这个数据库啊，还有表啊，给它触发好，这个呢，给大家提供了有这个触发脚本也比较方便啊。现在这右键。进行一个so文件。直接把这个脚本。指另外就行了。就是in my circle tables啊。这个脚本。好，执行成功。在这刷新一下。好，它呢，会产生这么些表啊。OK，这个后面我们具体用的时候再来具体查看。那接下来呢，我们就需要去执行这些代码了。这样的话呢，就可以模拟产生数据了。首先呢，我们先产生这个实时粉丝关注数据。就这个服务端呢。啊，服务端日志。模拟生成实时粉丝关注和取消关注数据。注意了，大家在下面呀，在执行我这个代码的时候，需要注意，你们需要改点东西。注意在上面，你看这个是调用我线上这个接口来获取这个模拟数据，获取到之后呢，注意。下面呢，会调用我们本地刚才部署的那个接口服务。因为这个呢是服务端日志，所以说它会调那个服务端接口。我呢是在BD的零四这台机箱部署的。对吧，这个服装接口，它这个对应的端口号是8081。对吧。后面这些东西呢，都不用改，你们下去改的话，主要改一下这个对吧，你在哪一台机器上去部署的这个什么server这个接入服务，那你在这就把那个IP或者主机名写到这就可以了。所以接着的话，就可以模拟产生这个服务端的这个调用请求，最终呢，去调用这个服务端这个接口，让这个接口呢记录日志。那你说这个接口，它具体把日志记录到哪个目录下面呢？注意，在这也能看懂啊。在resources这个目录下面有一个log back点查明。看到没有，我把这个日呢记到这个Di这个母下面了。然后他这块呢，具体的一个日志文件名称是server下换in加log。那对应的这个对collect看了也有。它日志呢，也是放到这个电log这个目录下面。然后它的日文名称呢，叫data collect。所以说这个到时候可以通过这个文件名就知道到底是哪个接口服务记录的日志啊，也好分析。那我们这个代码呢，都是OK的，以及这些接口这些东西呢，我们也不需要改，对吧，也都是OK的，下面呢，我们来执行一下。注意这个代码呢，它是实时产生数据，现在呢，你在这执行一次，它会产生一条数据，就是模拟那个实时产生啊。看到没有接口调用成功，注意这块是我在这记录的日志啊。借口立项成功，它最终产生的日是这个。好，那我们来看一下，我们具体去调这个服务端接口，这个服务端接口有没有把这个日给记录下来了，我们来确认一下。在这重新克隆一个绘画。对，log。到这个木下面。看到没有？这个里面其实已经有值了，对吧，我们可以在这摸看一下啊。没问题吧，他已经记录下来了，说明我们这个流程目前是通的啊。这个呢，就是服务端的实时粉丝关注数据。那接下来往下面看。这个generate video for。这个也是服务端日志对吧，是模拟生成视频相关的数据。就是你这个主播开播结束之后呢，其实它就会调用服务端接口。把当前这个直播的相关的一些信息上报过去啊。那注意你这个在用的时候也是一样的流程，对吧，这个上面还是就我那个接口。那下面的话呢，你需要在这对应改一下对吧。好，那我在这来执行一下。注意这个呢，一次性我可以采用多条数据啊。你看接口交换成功是OK的。那下面注意我们来确认一下数据。因为这时候它里面产生数据比较多，我就使用T杠一吧，查看里面最新的一条数据。对吧。也怪了。OK，所以说这个呢，是服务端的这两种类型的数据都采集过来了。往下面看。那接着呢，我们来看一下这个服务端数据库里面的数据。首先呢，是这个。就是这个历史，粉丝关注数据。注意。这个电板执行之后呢，它会把这个数据啊，初始化到我们那个数据库里面啊。注意，你在执行这个代码之前，你需要先执行我这个脚本。进行这个数据库和表的一个初始化。初始化之后的话就可以直接执行了，这里面别的不需要改动了，注意你要改点东西。看到没有，要改一下数据库啊。你的一个数据库在哪，把那个IP地址啊，以数据库啊，一个名称啊，对吧，用户名啊，密码啊对吧，这些东西都改一下就可以了。来，我们来执行一下。从那个里面你可以看出来，他最终把数据啊，写到这个FOLLOW00这个表里面。好，我们到这来看一下。好，数据呢，都写进来了。这些测数据啊。这个呢，其实就是历史的一个粉丝关注出去了。那接下来注意我们来看这个主播等级的。也是这个服务端数据库里面数据模拟生成主播等级数据啊。它也是操作数据库。那这里面的话，他会把数据写到这个level user这个表里面来执行一下。OK，我们来确认一下。好，数据都写进了。那这样的话，相当于服务端数据库里面这两份数据呢，我们也都给他触发好了。那其实还剩一份数据，就是这个客端数据，就是那个用户活跃的。gena use active。对吧，客端是模拟生成用户活跃数据。这个呢，还是通过调用我这个接口获取数据。那下面注意这时候呢，它会调用我们那个客户端的那个日志收集服务器，对吧，其实就是一个client。他们最终会调用他，把数据上报给他。所以你的用处呢，也要改成这个，看一下你这个data client，你部署到哪个机器，把这个改一下就可以了。来执行一下。好，这个data可爱呢，他接收到用户上报的这些数据之后呢，他呢也会把它记录日志记录到本地。嗯。嗯。在这他又起到这个did collect点里面。我们取头一条还杠一。看到没有，就这个这个类型是user active，看到没有，都是接格式的啊。那到此为止呢，项目中需要的数据呢，我们都可以正常产生了。</span><br></pre></td></tr></table></figure><h3 id="客服端日志接收服务"><a href="#客服端日志接收服务" class="headerlink" title="客服端日志接收服务"></a>客服端日志接收服务</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251102183.png" alt="image-20230425110236188"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251055893.png" alt="image-20230425105526920"></p><h3 id="服务端http接口"><a href="#服务端http接口" class="headerlink" title="服务端http接口"></a>服务端http接口</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251101505.png" alt="image-20230425110135498"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251101273.png" alt="image-20230425110105056"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251104599.png" alt="image-20230425110444470"></p><h3 id="实时粉丝关注"><a href="#实时粉丝关注" class="headerlink" title="实时粉丝关注"></a>实时粉丝关注</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251136630.png" alt="image-20230425111454589"></p><h3 id="视频数据"><a href="#视频数据" class="headerlink" title="视频数据"></a>视频数据</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251136684.png" alt="image-20230425111654573"></p><h3 id="历史粉丝关注"><a href="#历史粉丝关注" class="headerlink" title="历史粉丝关注"></a>历史粉丝关注</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251136744.png" alt="image-20230425112531665"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251125314.png" alt="image-20230425112548087"></p><h3 id="历史主播等级"><a href="#历史主播等级" class="headerlink" title="历史主播等级"></a>历史主播等级</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251130414.png" alt="image-20230425113019519"></p><h3 id="客户端用户活跃日志"><a href="#客户端用户活跃日志" class="headerlink" title="客户端用户活跃日志"></a>客户端用户活跃日志</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251133562.png" alt="image-20230425113331494" style="zoom:67%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251133869.png" alt="image-20230425113349659"></p><h2 id="数据采集聚合"><a href="#数据采集聚合" class="headerlink" title="数据采集聚合"></a>数据采集聚合</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251159150.png" alt="image-20230425115912855"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">咱们前面把数据产生好了，下面就可以进行数据采集了，首先呢是使用这个filebeat呢，将所有日志数据采集到kafka的一个topic中中，把这个日志数据采集到kafka里面。日志数据有三份，服务端有两份，客户端日志有一份。注意，在这具体执行之前，我们需要先把这个zookeeper以及kafka这两个服务给它启动起来。</span><br><span class="line"></span><br><span class="line">启动zk</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[root@bigdata02 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[root@bigdata03 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">启动kafka</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">好，那接下来呢，我们需要在kafka中呢，去创建一些topic。我们主要创建哪些topic呢？就根据我们前面啊，在那个架构图里面分析的，首先第一个是一个大的topic，它里面呢，包含所有的这种日数据。all_type_data_r2p40，因为它是一个大topic里面数据量比较大，所以说呢，我们给它多分一些分区，后期的话呢，就可以提高你一个消费能力了。这个里面呢，存储所有采集过来的日志数据。</span><br><span class="line">我们后面其实还有一个数据分发层，分发层的话，相当于把这里面的数据啊，这个它分发出来。我们服务端有两种数据，一个呢是这个实时的一个粉丝的关注和取消关注数据。</span><br><span class="line">这个它具体那个类型的名称呢，叫user_follow。这个topic这个名称后面为什么不加这种后缀了呀。这是因为啊，这个数据它相当于是我们的原始json数据里面的某一个自动的值，就那个type自动的值，这个日志啊，是之前记录下来的，所以后期我们再用的话，就把它直接拿过来用就行了，就不要再去改它这个值了，改起来就比较麻烦了。并且呢，你这个对应起来也不太好对应，所以说呢，我们就使用那个原始那个数据里面那个type值作为这个topic贝这个名称。</span><br><span class="line"></span><br><span class="line">那下面还有一个video_info。这个里面存储视频信息</span><br><span class="line"></span><br><span class="line">还有一个客户端的。user_active就是用户的活跃数据。存储客户端上报的用户活跃数据。</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 40 --replication-factor 2 --topic all_type_data_r2p40</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic user_follow</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic video_info</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic user_active</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic default_r2p5</span><br><span class="line"></span><br><span class="line">注意后面还有一个default_r2p5，所以这个topic是干什么呢？注意现在啊，我们要把这个里面的数据给它分发到这几个topic里面。那在分发的时候有可能有一些数据啊，它里面没有type字段呀，或者说那个type字段的值不是这三个里面的。也就是说那个数据异常，这样的话，你需要把那个异常数据放到这个topic。这里面的存储无法具体分配到对应topic的异常数据。</span><br><span class="line"></span><br><span class="line">所以说呢，我们下面就需要去创建这些topic。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251200726.png" alt="image-20230425120032836"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们呢，就可以使用filebeat的去采集数据了，注意现在filebeat呢，我们还没有安装呢。需要安装到哪些机器上面呢？注意你要看你之前那个客户端的那个接口，以及服务端的接口，你都部署到哪些服务器上面，我在这呢，都给它部署到这个bigdata04上面了。因为filebeat呢，要采集data_collect这个接口记录的日志，以及server_inter这个接口记录的日志，所以说呢，你后期这两个接口你部署到哪台机器上面，那么你在对应的机器上就需要部署这个filebeat去采集数据。</span><br><span class="line"></span><br><span class="line">解压一下</span><br><span class="line">注意这个filebeat就是一个采集工具。所以说呢，使用起来很简单，我们只需要在里面指定输入和输出就可以。我们先修改它的配置文件，在这个目录下面有一个这个filebeat.yml这个配置文件</span><br><span class="line"></span><br><span class="line">注意它是一个YML格式的，不是那种XML这种格式的啊，这是种新型的那种配置文件格式。</span><br><span class="line"></span><br><span class="line">你可以在这配置它的输入。你看它这有一个什么呀，这个类型是日志的，但是目前这个呢没有开启。我们在这呢，想先测试一下。在测试的时候呢，我就在这个配置文件里面，把这个输入啊，给它指定成标准的一个输入，就是键盘输入，输出的话呢，就把它指定成控制台。这样分析起来好分析，也比较直观。那怎么加呢？注意其实我们在这里面需要加一个输入，就是配置一个输入。对吧，你就按照它这个格式去写就行了，冒号注意后面一个空格，注意这个空格是不能少的。找那个输出。没有output。输出注意你看它默认呢，现在开启了一个什么呀，这个输出呢，是把数据输出的这个elasticserch，那现在我们先不用这个，你把它注掉。就指定那个console，注意下面呢，我们来加一些参数，注意下面这个缩进，我能不能使用那个tab，不行，必须使用空格。你可以使用两个或者四个。pretty。注意这个表示什么意思啊，它表示啊，会把你这个输出的结果啊，给你格式化一下，要不然是非常乱的啊。大家后期啊，在改这个配置文件的时候一定要注意啊，首先是这个冒号后面这个空格不能少，还有呢，就是说你这个在做缩进的时候不能使用制表符，要使用空格，官方建议使用四个，你用两个啊三个啊都可以啊。那下面呢，我们需要把这个配置好的这个配置文件呢，给它上传上去行吗？保存一下。如果说你对这个文件熟悉了之后，你可以直接在这里面去改都是可以的。不熟悉的话，建议呢，你还是把这个配送件拿到本地去，改完之后呢再传上来。直接覆盖就行啊。那这样的话就可以啊，下面我们就可以启动filebeat</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251209159.png" alt="image-20230425120937212"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251211306.png" alt="image-20230425121132023"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251212200.png" alt="image-20230425121239236"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 filebeat-7.4.2-linux-x86_64]# .&#x2F;filebeat -c filebeat.yml</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251218213.png" alt="image-20230425121824126"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输出了一堆东西。虽然是一堆啊，还是有格式的，是一个阶层格式的，注意这个呢，就是因为我们在这加了这个属性，等于true，它呢会对你返回的数据呢，你做一个格式化。反馈的数据，你看就这个message里面内容，其他内容呢，都是它里面默认加的啊。接着你想停掉的话，直接CTRLC对吧。</span><br><span class="line"></span><br><span class="line">接下来我们需要继续修改配置文件，因为我们是希望filebeat的可以采集日志文件中的数据，将数据呢采集到kafka中。把刚才我们那个配置啊都给它删掉。然后呢，我们把这个基于日志的这个类型啊，给它开启了。这样的话它就可以读取文件了，你看path里面指定一些路径，注意你可以指定一个或者多个都可以。你指定一个的话，你可以这样，你指定一个目录下面可以使用那种通配符也是OK的啊。所以说的话，它可以监控多个目录里面的多个文件。这是输入就配置好了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251222076.png" alt="image-20230425122235895"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">接下来我们需要配置输出。这是一个输出组件，注意在这我们需要指定那个kafka的一个输出组件，那在这都需要指定什么配置呢？这个信息呢，到他那个官网上面是可以找到的啊，所以在边我就直接拿过来，你看output.kafka，把数据呢，输入到卡夫卡里面，直行卡卡的一个博物块地址对吧。123。还有这个topic，我们要把这个数据呢写到这里面。下面这几个配置你不用改就行了，这个呢表示啊，它往这个topic里面这些分区写数据的一个规则。这个表示卡里面那个艾机制是吧。这个呢是就是往里面写数据的时候，对这个数据15度压缩，这块呢，使用那个机会压缩可以提高性能。这个呢，表示呢，每一条数据最大的一个字节数量。好这样就可以了，你们下用的时候呢，有可能要改这以及这对吧，如果说你都和我的一样的话，其实都不用改。OK，这样的话就可以了，下面呢，把这个配置文件重新再传一下。</span><br><span class="line"></span><br><span class="line">这样就可以实现将服务端日志和客户端日志全部都采集到kafka中的all_type_data_r2p40这个topic中了。</span><br><span class="line"></span><br><span class="line">注意：filebeat需要部署在所有服务端接口机器和客户端日志收集机器上，</span><br><span class="line"></span><br><span class="line">在实际工作中，服务端接口机器会有多个，我们当时的服务端接口机器有100多台，客户端日志收集机器是有6台，在部署的时候是通过运维同学开发的部署工具批量部署的，要不然一个一个部署会疯的。</span><br><span class="line">在这由于服务端接口和客户单日志收集服务都在bigdata04上面，所以我就只需要在bigdata04上部署一套即可。</span><br><span class="line"></span><br><span class="line">在这里我们暂时先不启动filebeat进程，等我们把采集模块中所有的配置全部修改好了以后，从后面挨个开始启动进程，这样不会漏数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251225756.png" alt="image-20230425122506901"></p><h2 id="数据分发"><a href="#数据分发" class="headerlink" title="数据分发"></a>数据分发</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下数据分发层，就是使用实现对采集到kafka指定topic里面的数据啊进行分发。咱们前面呢，把这个日数据采集过来啊，全部都放到kafka一个大的topic里面了，所以说下面呢，我们需要使用flume对这个大的topic里面数据啊进行分发，分发到一些小的topic里面，这样可以方便使用。注意，那这样的话，我们就需要在这个flume里面增加一配置文件，就是从那个大的topic里面消费数据，通过拦截器获取数据中的一个type自段的值作为输出kafka的一个topic的名称。这个配置文件呢，在这儿我已经写好了，我们来直接看一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#source+channle+sink的名字</span><br><span class="line">agent.sources &#x3D; kafkaSource</span><br><span class="line">agent.channels &#x3D; fileChannl</span><br><span class="line">agent.sinks &#x3D; kafkaSink</span><br><span class="line"></span><br><span class="line"># 指定source使用的channel</span><br><span class="line">agent.sources.kafkaSource.channels &#x3D; fileChannl</span><br><span class="line"># 指定sink需要使用的channel的</span><br><span class="line">agent.sinks.kafkaSink.channel &#x3D; fileChannl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-------- kafkaSource相关配置-----------------</span><br><span class="line">agent.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agent.sources.kafkaSource.batchSize &#x3D; 1000</span><br><span class="line">agent.sources.kafkaSource.batchDurationMillis &#x3D; 1000</span><br><span class="line">agent.sources.kafkaSource.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line">agent.sources.kafkaSource.kafka.topics &#x3D; all_type_data_r2p40</span><br><span class="line">agent.sources.kafkaSource.kafka.consumer.group.id &#x3D; flume_con_id_1</span><br><span class="line"></span><br><span class="line">#----------------- 拦截器 -------------------</span><br><span class="line"># 定义拦截器</span><br><span class="line">agent.sources.kafkaSource.interceptors &#x3D; i2 i1</span><br><span class="line"># 设置拦截器类型</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.type &#x3D; regex_extractor</span><br><span class="line"># 设置正则表达式，匹配指定的数据，这样设置会在数据的header中增加topic&#x3D;aaa</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.regex &#x3D; &quot;type&quot;:&quot;(\\w+)&quot;</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.serializers &#x3D; s1</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.serializers.s1.name &#x3D; topic</span><br><span class="line"># 避免遇到数据中没有type字段的，给这些数据赋一个默认topic【注意：这个拦截器必须设置】</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.type &#x3D; static</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.key &#x3D; topic</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.preserveExisting &#x3D; false</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.value &#x3D; default_r2p5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#------- fileChannel相关配置-------------------------</span><br><span class="line"># channel类型</span><br><span class="line">agent.channels.fileChannl.type &#x3D; file</span><br><span class="line">agent.channels.fileChannl.checkpointDir &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;all_type_data&#x2F;checkpoint</span><br><span class="line">agent.channels.kafka2HdfsShow.dataDirs &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;all_type_data&#x2F;data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#---------kafkaSink 相关配置------------------</span><br><span class="line">agent.sinks.kafkaSink.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent.sinks.kafkaSink.kafka.topic &#x3D; default</span><br><span class="line">agent.sinks.kafkaSink.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line">agent.sinks.kafkaSink.kafka.flumeBatchSize &#x3D; 1</span><br><span class="line">agent.sinks.kafkaSink.kafka.producer.acks &#x3D; 1</span><br><span class="line">agent.sinks.kafkaSink.kafka.producer.linger.ms &#x3D; 1</span><br><span class="line">agent.sinks.kafkaSink.kafka.producer.compression.type &#x3D; snappy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来就把它呢传上去就可以了，注意如果说我们需要在一台机器中启动多个flume进程的时候，最好在里面复制多个conf目录，因为如果在一个conf目录中启动多个agent的进程的话，多个agent的进程的日志信息会混到一块，后期排查问题会很麻烦啊，这个我们之前呢讲过了对吧啊，所以说呢，在这。进到这个。都门面。我们来复制一个。打靠谱。告不告？卡不卡？高不高相以我们从卡夫卡里面读出去，再把数据写到卡夫卡里面。进到里面。改一下他这个log的这个配置文件。把那个改一下，搞搞不搞搞卡不卡。好，这样的话就可以了。嗯。OK，那接下来我们在这里面来创建这个配置文件，就是刚才我们分析的这个。敢不敢杠后杠？敢不敢点。com？嗯。我们就组织一下。好，这样就可以了，注意这块搞定之后呢，我们就需要往下面走了，在这啊，我们也先不启动啊，那我们把这个数据落盘这块也搞定之后呢，从后往前启动。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252144970.png" alt="image-20230425214444152"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252145763.png" alt="image-20230425214458087"></p><h2 id="数据落盘"><a href="#数据落盘" class="headerlink" title="数据落盘"></a>数据落盘</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252156385.png" alt="image-20230425215638800"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下这个数据落盘层。我们需要使用采集指定topic各种数据进行落盘，便于离线计算。你看咱们前面呢，把这个数据呢，全部都采集到kafka这个大的topic里面，接下来做了一个分发，分发之后后面我们就要做这个落盘了。把我们需要落盘的数据呢，给它呢，落到这个HDFS上面，注意如果这里面这个大topic里面的所有类型的数据呢，我们都需要落盘的话，我们就可以直接读取这个大topic里面的所有数据，然后呢，使用这个拦截器。根据数据类型把它们呢保存到hdfs的多个目录中，这样呢比较方便，不过在我们这个项目中，这个大的topic里面一共有三种类型的数据，其中呢，有两种数据我们是需要进行落盘后期进行离线计算的。有一种数据呢是不需要的，只有实时计算上讲会用的，所以说我们可以使用两个flume agent来对我们需要的数据执行落盘操作。当然了，我们也可以啊，只使用一个flume agent的，那你这里面啊，也可以使用拦截器，只获取我们需要落盘的那两种数据，这样也是可以的，这个呢，给大家留一个作业，大家下去自己研究一下。使用flume拦截器如何呢？获取我们需要的两种数据，然后呢，分别把它呢落盘到hdfs的不同目录下面。</span><br><span class="line"></span><br><span class="line">那我在这呢，就使用两个agent来实现了，我们需要落盘的这两个topic呀，分别是这个user_active和这个video_info。所以说呢，在这我还需要复制两个目录。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252211650.png" alt="image-20230425221141206"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那我们进去去改一下那个logo的配置。那接下来我们还需要在这两个目录里面创建对应的一个配置文件，这两个配置文件啊，我也提前写好了，我们来看一下。</span><br><span class="line"></span><br><span class="line">因为这个东西啊，它就是一个体力活。咱们前面用过很多次了啊，没有什么技术含量。首先看这个user active，这都没什么好处了。S呢，是一个卡夫卡S，你读取这个user active。那个group使用这个be China。注意，下面是这个H里边的think。按T分目录存储就行了，放到对目录下面一个user。这就可以。那这个呢，是一个video。对吧，它读取的是一个video in这个topic。然后呢，就是说放到这个data木下面有个video。对吧，这是那个年月日。看天分，母乳好。那我在这呢，把这个复制一下。这是一个user active。脸卡不卡？到as。到。user。active。com。好，这个搞定。下一个先进到这个目录里面这个音符，把这个呢复制一下。VI、卡夫卡杠杠、video info。好。好，那这两个呢，我们都配置好了。所以接下来我们需要先确认一下这个含毒不集群啊，是否启动。对吧，我们之前已经起来了啊。好，那到此为止啊，这个采集相关的这个配置呢，我们都修改好了，下面呢，我们就要来启动一下。我们要从后往前启动，所以说呢，我们先启动这个数据落盘的这个辅助。我们来启动一下。诺哈普。b my name。agent康复指定配置文件跟目录。这个是com user active先求这个。进行配置文件刚刚。靠谱。在康复，然后呢。有点active下面有一个。啊，不搞搞HS user active com这个文件。agent。好，接下来呢，是第二个，我把前面这些能敷的呢复制过来。这个呢是video。刚刚看。六。下面有一个卡杠，杠HS杠video杠infor.com。最后是这个大纲内。agents。嗯。确认一下。这是一个，这是一个对吧，这两个呢都起来，嗯。这就是这两个数据落盘的一个a的进程啊，那接下来再往回推，我们需要把那个数据分发那个给它起下来。说。NG到康复。到卡夫卡，到卡夫卡。刚刚康复杠。这个木下面有一个。卡夫卡，卡夫卡点。com。我们之前少写一个这个F的这个强迫症，我给他改一下啊。来确认一下，确实少一个F，这个也不影响啊，只不过看起来有点别扭。好。这就可以了。怎么呢，重新来启动的啊，刚才也没启动成功啊no。b agent。等到。卡夫卡，卡夫卡，然后呢，刚刚。哎。卡夫卡，卡夫卡下面有一个卡夫卡，卡夫卡加。祷告，name isn&#39;。好。可以确认一下啊。对吧，这个也启动好。那最后呢，我们就要启动那个Bob的采集程了。接着来启动。牛逼了，杠C。比点两秒。注意啊，针对这个feel进程啊，正式启动的时候也需要使用这个no ho，把它放到后台运行在这里，我们为了一会儿使用方便，所以说我先在前台启动了，对吧。我们按一个回车把它启动起来，那接下来呢，我们就可以启动生成数据的程序了。注意在这呢，我们先执行那个generate real time for这个实时生成那个粉丝关注和取消关注的数据啊。来执行一下。</span><br></pre></td></tr></table></figure><h3 id="kafka-hdfs-user-active-conf"><a href="#kafka-hdfs-user-active-conf" class="headerlink" title="kafka-hdfs-user-active.conf"></a>kafka-hdfs-user-active.conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#source+channel+sink的名字</span><br><span class="line">agent.sources &#x3D; kafkaSource</span><br><span class="line">agent.channels &#x3D; kafka2Hdfs</span><br><span class="line">agent.sinks &#x3D; hdfsSink</span><br><span class="line"></span><br><span class="line"># 指定source使用的channel名字</span><br><span class="line">agent.sources.kafkaSource.channels &#x3D; kafka2Hdfs</span><br><span class="line"># 指定sink需要使用的channel的名字</span><br><span class="line">agent.sinks.hdfsSink.channel &#x3D; kafka2Hdfs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-------- kafkaSource相关配置-----------------</span><br><span class="line"># 定义消息源类型</span><br><span class="line">agent.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 定义kafka所在zk的地址</span><br><span class="line">agent.sources.kafkaSource.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line"># 配置消费的kafka topic</span><br><span class="line">agent.sources.kafkaSource.kafka.topics &#x3D; user_active</span><br><span class="line"># 配置消费者组的id</span><br><span class="line">agent.sources.kafkaSource.kafka.consumer.group.id &#x3D; user_active_con_1</span><br><span class="line"></span><br><span class="line">#------- fileChannel-1相关配置-------------------------</span><br><span class="line"># channel类型</span><br><span class="line">agent.channels.kafka2Hdfs.type &#x3D; file</span><br><span class="line">agent.channels.kafka2Hdfs.checkpointDir &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;user_active&#x2F;checkpoint</span><br><span class="line">agent.channels.kafka2Hdfs.dataDirs &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;user_active&#x2F;data</span><br><span class="line"></span><br><span class="line">#---------hdfsSink 相关配置------------------</span><br><span class="line">agent.sinks.hdfsSink.type &#x3D; hdfs</span><br><span class="line"># 注意, 我们输出到下面一个子文件夹datax中</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;user_active&#x2F;%Y%m%d</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat &#x3D; Text</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType &#x3D; DataStream</span><br><span class="line">agent.sinks.hdfsSink.hdfs.callTimeout &#x3D; 3600000</span><br><span class="line"></span><br><span class="line">#当文件大小为104857600字节时，将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize &#x3D; 104857600</span><br><span class="line">#events数据达到该数量的时候，将临时文件滚动成目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount &#x3D; 0</span><br><span class="line">#每隔N s将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval &#x3D; 3600</span><br><span class="line"></span><br><span class="line">#配置前缀和后缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix&#x3D;run</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileSuffix&#x3D;.data</span><br></pre></td></tr></table></figure><h3 id="kafka-hdfs-video-info-conf"><a href="#kafka-hdfs-video-info-conf" class="headerlink" title="kafka-hdfs-video-info.conf"></a>kafka-hdfs-video-info.conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#source+channel+sink的名字</span><br><span class="line">agent.sources &#x3D; kafkaSource</span><br><span class="line">agent.channels &#x3D; kafka2Hdfs</span><br><span class="line">agent.sinks &#x3D; hdfsSink</span><br><span class="line"></span><br><span class="line"># 指定source使用的channel名字</span><br><span class="line">agent.sources.kafkaSource.channels &#x3D; kafka2Hdfs</span><br><span class="line"># 指定sink需要使用的channel的名字</span><br><span class="line">agent.sinks.hdfsSink.channel &#x3D; kafka2Hdfs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-------- kafkaSource相关配置-----------------</span><br><span class="line"># 定义消息源类型</span><br><span class="line">agent.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 定义kafka所在zk的地址</span><br><span class="line">agent.sources.kafkaSource.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line"># 配置消费的kafka topic</span><br><span class="line">agent.sources.kafkaSource.kafka.topics &#x3D; video_info</span><br><span class="line"># 配置消费者组的id</span><br><span class="line">agent.sources.kafkaSource.kafka.consumer.group.id &#x3D; video_info_con_1</span><br><span class="line"></span><br><span class="line">#------- fileChannel-1相关配置-------------------------</span><br><span class="line"># channel类型</span><br><span class="line">agent.channels.kafka2Hdfs.type &#x3D; file</span><br><span class="line">agent.channels.kafka2Hdfs.checkpointDir &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;video_info&#x2F;checkpoint</span><br><span class="line">agent.channels.kafka2Hdfs.dataDirs &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;video_info&#x2F;data</span><br><span class="line"></span><br><span class="line">#---------hdfsSink 相关配置------------------</span><br><span class="line">agent.sinks.hdfsSink.type &#x3D; hdfs</span><br><span class="line"># 注意, 我们输出到下面一个子文件夹datax中</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;video_info&#x2F;%Y%m%d</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat &#x3D; Text</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType &#x3D; DataStream</span><br><span class="line">agent.sinks.hdfsSink.hdfs.callTimeout &#x3D; 3600000</span><br><span class="line"></span><br><span class="line">#当文件大小为104857600字节时，将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize &#x3D; 104857600</span><br><span class="line">#events数据达到该数量的时候，将临时文件滚动成目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount &#x3D; 0</span><br><span class="line">#每隔N s将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval &#x3D; 3600</span><br><span class="line"></span><br><span class="line">#配置前缀和后缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix&#x3D;run</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileSuffix&#x3D;.data</span><br></pre></td></tr></table></figure><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后将hadoop集群启动起来，因为数据落盘会使用到HDFS</span><br></pre></td></tr></table></figure><h3 id="数据落盘-1"><a href="#数据落盘-1" class="headerlink" title="数据落盘"></a>数据落盘</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">好 到这为止，采集需要的配置都修改好了</span><br><span class="line">下面我们就来启动一下</span><br><span class="line">1：先启动数据落盘的flume</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252219426.png" alt="image-20230425221947341"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252220669.png" alt="image-20230425222021013"></p><h3 id="数据分发-1"><a href="#数据分发-1" class="headerlink" title="数据分发"></a>数据分发</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2：再启动数据分发的flume</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252225085.png" alt="image-20230425222459145"></p><h3 id="数据采集聚合-1"><a href="#数据采集聚合-1" class="headerlink" title="数据采集聚合"></a>数据采集聚合</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3：最后启动filebeat采集进程</span><br><span class="line"></span><br><span class="line">注意：针对filebeat进程，正式启动的时候也需要使用nohup 放在后台运行，在这里我们为了一会使用方便，所以先在前台启动。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252227824.png" alt="image-20230425222700964"></p><h3 id="执行数据生成程序"><a href="#执行数据生成程序" class="headerlink" title="执行数据生成程序"></a>执行数据生成程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">4：接下来启动生成数据的程序</span><br><span class="line">先执行GenerateRealTimeFollowData</span><br><span class="line">验证效果</span><br><span class="line">查看all_type_data_r2p40这个topic中的数据</span><br><span class="line"></span><br><span class="line">验证</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252230600.png" alt="image-20230425223043250"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252231054.png" alt="image-20230425223117407"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以执行成功了，那我们来验证一下结果。我们到那个卡夫卡里面来消费一下。基于console的。注意这个内容还是比较多的，但是我们可以大致看一下啊，你看。这是一条数据，是不是很乱呀？我们的日志数据没有这么乱吧，并且我们的日志数据里面也没有这个东西。对吧，我们的日志数据里面其实是这些东西在这，你看它其实啊，其实在这面又封装了一层，他把我这个具体的业务数据啊，给封装到这个message字段里面。注意这些字段的话，相对来讲是filebeat默认生成的，那这些字段呢，它不是我们需要的，我们希望啊，只记录我们的原始日志即可。那怎么解决这个问题呢？可以解决啊，我们需要在那个filebeat那个配置文件里面加一个配置。需要在filebeat中的output.kafka里面增加codec.format配置。在这相当于啊，你要对它输出这个数据啊，做一个格式化。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252235554.png" alt="image-20230425223504841"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CD format。做一个格式化。四六类形的。对。我们呢，只取它里面的这个东西，哎，问号首先呢，括括号中括号我们只取里面那个message。所以这个配置的意思呢，就是说我们从这个数据里面。你看它这个采集过来数据里面有很多字段，我们只需要message字段里面内容，其实这里面就是我们的原始的日志数据。然后把这个filebeat再停一下。你再来启动，我们再来执行这个生成数据的这个。看到没有？这个就正常了，这样就可以</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252237920.png" alt="image-20230425223717162"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们再来查看一下这个user_follow这个topic，这个看一看它里面能不能消费到数据，如果能消费到数据，就说明那个flume的数据分发过程是没有问题的啊</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252240353.png" alt="image-20230425224013711"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">收到了吧。注意你在这能收到就说明啊，我们中间那个flume的分发程序是OK的，他从那个大的topic里面把这个数据读出来，然后呢写到这里，这样就可以好。那接下来我们再来执行两个程序。一个是这个active。其实呢，我们之前已经执行过了，在这我们再重新执行一下啊。然后重新呢，再生成一批数据，就是往那个日志文件里面再写一批啊，因为你调用接口，这个接口就会记录了，以及这个video info。好，这个执行成功之后呀，注意我们就可以到那个HDPS里面去确认一下结果数据了。因为你这个执行之后啊，他呢，这个接口就会把那个日志记录到本地。这个接口呢，就会把那个日志数据啊，记录到这个本地的日文件里面，然后filebeat呢，发现你里面啊有新增数据，所以说他就把这个数据给读出来，读出来之后呢，把它呢，采集到那个大的topic，就是all type data那个topic里面。然后呢，这里面有数据之后，后面那个的数据分发程序，它就会读取这个数据，对吧，对这个数据做分发，然后呢，把这个数据分发到不同的那个子topic里面，那我们最后呢，还有一个flume数据落盘层。他呢，就会从那个子topic里面把那些数据呢读出来，最终呢落到hdfs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以下面我们来验证一下。看到没有，这个user active，对这个都是有的。好，这个目录来我们直接查看一下它里面的数据啊。使用管道来取，前一条取一条就行，好里面有数据。那说明是OK的，那其实对应的我可以直接把这个改一下，改成video。看一下他们的数据。也是有了。没有问题，好。那在这我们都可以获取到数据，那就说明了是没有问题的，这就意味着我们前面的整个数据采集流程是通的。那大家在下面做实验的时候呀，我估计啊，可能会遇到各种各样的问题啊，就是大家在操作这一块的时候，如果发现最终啊，看不到我们这个希望的结果。那你就需要一步一步去排查，你要确认一个数据到了哪一步。你先看一下那个大套贝里面有没有数据，然后呢，再看一下那个子套贝壳里面有没有数，如果子套贝格里面也有数据，那最终hps里面没数据，那肯定是你那个数据落盘程序有问题。所以说你需要一点点去分析啊。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252308990.png" alt="image-20230425230808125"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252309870.png" alt="image-20230425230907753"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252309790.png" alt="image-20230425230920558"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对这个filebeat呀，我再多说一点，这个filebeat它在采集日文件中的数据的时候呢，它会将日文件数据的采集的一个偏移量啊，记录到本地文件里。所以说它在这会记录一下，这样话你filebeat给重启之后，它呢会读取这个文件，然后呢，根据你上一次记录的off继续往下面消费。它可以保证，就算你那个filebeat要停了，那在它停的中间，你往那个日里面记录数据，它后期启动之后还可以把它呢采集过来是这样。注意了，如果说呢，我们想要让这个filebeat的重启之后啊，继续重新开始消费。这样怎么办呢？暴力一点的话，我们可以直接把这个。这个data目录啊，给它删掉，你删掉之后这些信息是不是就没了，没了之后它就认为它是一个新文件，就从头开始读了啊，这个需要注意一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252316813.png" alt="image-20230425231605565"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">还有一点就是我们在使用filebeat的时候啊，如果发现filebeat没有正常工作，这个时候呢，我们需要去查看filebeat的日志文件，来排查具体是什么问题，因为有时候有一些错误信息啊，他不会暴露到这个工作台上面，它会记录到日志文件里面。像few b的下面有一个move。看到没有，它下面有这个这文件，注意你看的话就看这个，它后面这个相当于是一些备份的啊。你直接看这个就行。这是最新的一些日志。这些呢，是之前的一些老的日志。如果说他有一些错误信息在这里面就可以看。你在这儿可能看不到，它不会暴露到这个地方啊，你说呢，我把这个飞票启动了，也没见他报错呀，结果呢，他也没有把数据采集到我的卡夫卡里面，那所以说你就需要来这儿来看啊，看那个日文面。好，那针对服务端日志和这个客端日志的一个数据采集啊，我们先讲到这儿，在这呢，大家主要掌握数据采集的整理思路，重点是里面那个数据聚合，数据分发这两个步骤，那个数据落盘呢，倒没什么特殊的，对吧，咱们之前已经用过很多次。</span><br></pre></td></tr></table></figure><h2 id="采集服务端数据库数据"><a href="#采集服务端数据库数据" class="headerlink" title="采集服务端数据库数据"></a>采集服务端数据库数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面呢，我们把这个服务端日志以及客户端日志呢采集过来，下面呢，我们还需要将服务端数据库的数据也采集过来。咱们前面分析了啊，由于历史粉丝关注数据呢，只需要导出一次，所以说呢，没必要使用sqoop，那还有一份数据呢。是那个每日的主播等级数据对吧？所以这份数据呢，我们在最开始的时候会将数据库里面的全量数据导出来一份，后期只需要根据表中的更新字段获取发生了变化的数据即可，这样每天需要导出的数据量就很小了，属于增量采集。这时候呢，可以选择使用sqoop，其实呢也可以选择使用shell脚本，sqoop的使用在上一个项目中我们已经用过了，所以接着呢，我们来讲点不一样的，我来使用shell脚本，将mysql中的数据导出来，所以说么，这两份数据我全部呢使用mysql脚本。把它倒出来。那首先呢，我们使用这个mysql里面那个-e这个命令，将这个具体的查询命令啊准备好。</span><br><span class="line"></span><br><span class="line">注意我其实在这呢，可以直接操作我Windows本地的那个MYSQL，这个里面它默上是有那个MYSQL客户端的，你如果没有的话，你就装一下啊，我们使用mysql -e后面的话就可以写具体的sql了。因为你现在也不是连本地的马，你是连其他机器的马，对吧。我们在这个list里面连接我们Windows里边那个马。那我们Windows的那个机器这个IP是也有2.168.182.1。这样就可以，这也可以编了哈。那这里面写了一个参考，我们先写那个历史粉丝关注数据啊。再来呢，它里面有两列，一个UID，一个呢是UID。from我们这个库的名称啊，是一个点。follow。零零，我们先查这一张表。看到没有，它是可以执行的。那以及呢，我们还要查询这个每日主播等级数据啊，就每天发生了变化的那些主播数据。注意它里面的话字段比较多。注意它里面有一个update time，就是更新字段啊。就这个，如果说这里面这个数据发生了变化。某一个字段被改了，那么这个就会变化，所以说我们每天呢，就根据它去抽取数据。up time。大于等于。所以我们这里面这个日期啊，目前的话看一下。那是2月1号对吧。26年2月1号，所以说我在里面这样来，我就把这些数据给它查出来，二六杠零二杠零幺。000000对吧。只要凌晨开始按着。update time。小于等于。2026杠零二杠零一。23:59:59，这样的话就可以把这些数据全都查出来。没问题吧，也是可以的啊。好，那这两条命令啊</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">历史粉丝关注数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252328190.png" alt="image-20230425232805742"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252328755.png" alt="image-20230425232851743"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252350277.png" alt="image-20230425235033202"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每日主播等级数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252351285.png" alt="image-20230425235117301"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252351462.png" alt="image-20230425235132504"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">验证成功之后呢，我们就需要把这个命令啊公布到脚本里面，首先是这个用户历史关注数据，注意这份数据啊。它其实呢是分表存取。你看我在这其实只是创建了一部分表，它有很多张。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252352125.png" alt="image-20230425235203139"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个分表逻辑是什么样子的，你看它里面存储的是什么呀，是一个用户的一个UID啊。所以说这样的，它会根据这个用户的UID来计算一个MD5值。你这个MD5值是什么样子的呢？给他搜一下。对吧，你看它的MD5值其实就是这样。就是类似于这么一串。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252354258.png" alt="image-20230425235426902"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我们呢，会根据这个用户的UID去计算一个MD5值，然后呢获取最后两位字符，然后呢拼接到这个follow后面。组装一个新的表名。这样话呢，他会算一下你那个UID最后两位是什么，然后呢，存到对应的表里面。这是它一个分表逻辑啊，那你后期查的话呢，也会根据这个UID计算MD5获取最后两位，然后呢来前面呢拼那个follow下划线，后面的话拼上了两位，这样就找到它对应的一个表。</span><br><span class="line"></span><br><span class="line">注意这种组合呀。这两位你看它有可能是零到九以及a到z中的任意一个，所以说呢，这位它呢有36种情况，这一位呢也是36种情况，你26个英文字母加上零到九有十个，一共是36，每一位都三十六三十六乘以36是吧。1296张表。所以说呢，我们其实在实际过程中，我们需要将这1296张表中的数据全部都导出来，所以我在这呢，也没有建那么多张表，太多了啊，所以说在这建了一部分，在这大家知道它是一个分表的就行啊。然后呢，你要知道它这个分表规则。OK。这样的话，一会我们采集数据，我就先采集一张就行，但是呢，我会写脚本，让他可以支持把这个1296张表中的数据全部都采集出来。我们把那个脚本写好，但是具体采的时候，我们就只采这一张表就行啊，因为就它里面有数据。</span><br></pre></td></tr></table></figure><h3 id="extractFollower-sh"><a href="#extractFollower-sh" class="headerlink" title="extractFollower.sh"></a>extractFollower.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">好，下面我们来开发第一个脚本。我现在来写。简单号。它保存一下啊，给它起个名字叫attracted。要抽取follow。是脚本。只需要执行一次即可。你只需要抽取自己的是历史数据，后期的话我们还有一份实时数据，就可以实时维护了。针对1296张表。需要使用这个双层或循环。动态生成表明。I。12345678。所以后面还有呢a。FGH。I z KL mn pqr。ST。UVWXYZ，好吧，这题又是个直接画啊度。这是一层，你里面还要套一层呢。因为我们要批那个表明的最后两个字符嘛，对吧。G。这个我就不是文桥了。50块。好。那现在里面了，我来艾打印一下。就把那个表明啊给他批出来。哎。当然，这行吧。其实你在这儿只要能把这1000多张表的那个表明全都拼出来，那继续把它导出去，那不就很简单了吗？对吧？我先在这呢，先写一个导师的一个脚本。马杠u杠P杠H。18291。藏意后面是circle。这个UIDUID。from。video。零零。注意这样的话就可以获取那些数据了，然后我把这个数据呢，给它重定向。到这个里面这个soft。video recommend。就使用这个表名作为文件里面的前缀log，这样的话其实就可以把这张表数据给它导出来了。你如果把这个东西放到这儿。好把重要啊，我们一会执行不执行，你把里面改一下是不是就行了，你这个东西。还有那个。这东西，然后那个是不是可以了呀。你只要说这个循环执行完，其实就可以把所有数据全都倒出来。我在这通过IO把这个表名导一下就行，行吗。最后的话呢，我们只是把这一个标点数据给它打开就行。好，那接下来呢。先试一下啊，重新在这儿来建一个脚本。video&#39;。到了。OK，这样就可以了，那接下来我们来执行SH。看到没有，前面都打印出来了，没问题啊。那我们来确认一下，这个木下面有没有产生那个零零.log。产生了吧。看一下里面的数据。没问题吧，没问题啊好，</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 此脚本只需要执行一次即可</span><br><span class="line"></span><br><span class="line"># 针对1296张表，需要使用双层for循环动态生成表名</span><br><span class="line">for i in 0 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o p q r s t u v w x y z</span><br><span class="line">do</span><br><span class="line"> for j in 0 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o p q r s t u v w x y z</span><br><span class="line"> do</span><br><span class="line">echo follower_$&#123;i&#125;$&#123;j&#125;</span><br><span class="line">#mysql -uroot -padmin -h 192.168.182.1 -e &quot;select fuid,uid from video.follower_$&#123;i&#125;$&#123;j&#125;&quot; &gt;&gt; &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;follower_$&#123;i&#125;$&#123;j&#125;.log</span><br><span class="line"> done</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">mysql -uroot -padmin -h 192.168.182.1 -e &quot;select fuid,uid from video.follower_00&quot; &gt;&gt; &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;follower_00.log</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260002090.png" alt="image-20230426000249498"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260003322.png" alt="image-20230426000303163"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260003085.png" alt="image-20230426000343050"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260004849.png" alt="image-20230426000403681"></p><h3 id="extractUserLevel-sh"><a href="#extractUserLevel-sh" class="headerlink" title="extractUserLevel.sh"></a>extractUserLevel.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line"># 此脚本每天执行一次，添加到crontab或者Azkaban调度器中[每天0:30分开始执行]</span><br><span class="line"></span><br><span class="line"># 正常情况下获取昨天的数据，如果需要补数据，可以直接指定日期</span><br><span class="line">if [ &quot;X$1&quot; &#x3D;&#x3D; &quot;X&quot; ]</span><br><span class="line">then</span><br><span class="line">yesterday&#x3D;&#96;date --date&#x3D;&quot;1 days ago&quot; +%Y-%m-%d&#96;</span><br><span class="line">else</span><br><span class="line">yesterday&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">mysql -uroot -padmin -h 192.168.182.1 -e &quot;select * from video.cl_level_user where update_time &gt;&#x3D;&#39;$&#123;yesterday&#125; 00:00:00&#39; and update_time &lt;&#x3D; &#39;$&#123;yesterday&#125; 23:59:59&#39;&quot; &gt;&gt; &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;cl_level_user_$&#123;yesterday&#125;.log</span><br><span class="line"></span><br><span class="line"># 将数据上传到hdfs上，每天一个目录</span><br><span class="line"></span><br><span class="line"># 先在hdfs上创建日期目录</span><br><span class="line">hdfs dfs -mkdir -p &#x2F;data&#x2F;cl_level_user&#x2F;$&#123;yesterday&#x2F;&#x2F;-&#x2F;&#125;</span><br><span class="line"></span><br><span class="line"># 上传</span><br><span class="line">hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;cl_level_user_$&#123;yesterday&#125;.log &#x2F;data&#x2F;cl_level_user&#x2F;$&#123;yesterday&#x2F;&#x2F;-&#x2F;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来开发第二个脚本。这个也是抽取数据了。他呢是抽取了一个user level，有这个主播等级的啊。注意在这个脚本中啊，我们需要将数据上传到hdfs上面，并且呢，这个脚本啊，也需要添加到Crontab或者azakaban那个调度器中，每天凌晨00:30了，然后执行一次。就是每天呢抽取一次啊。</span><br><span class="line"></span><br><span class="line">那下面我们写一下，就是正常情况下。每天凌晨获取昨天的数据。你如果呢，你需要补数据的话。可以直接指定日期。所以这个的话，我来获取一个日期。呃，一。等等a。那说明道一为空。then。还有这个yesterday。等于。加个反引号。刚刚date。days ago，一天以前就是昨天嘛，对吧，加号百分号Y杠百分号M，杠百分号D。这就过去昨天日期了。else，如果说呢不相等，那说明了多少？有值，有值的话呢？二。这样就快。那下面的话其实就可以把我们那个circle语句给它拿回来，那个搜有点长，我在这呢。到这儿拿一下。我们在前面是不是写过呀。对，这里面写这个星号就可以啊，没问题。这边需要改一下。把那个日期拼上。注意你说我在这成这个单引号，它不是不解析吗？大家注意外面还有一层双引号，咱们之前讲过对吧，双引号里面这个单引号这个会解析这个变量啊。我反过来又不行。那最后呢，我们把这个日志数据呢，给它重进现到这个目下面，在这我可以把前面这个复制一下是吧过来。后面的话呢，其实就是这个。表明了。使它来拼接一个热面。注意其在后面呢，你最好拼一个日期对吧，因为他每天都有一次。填了一个日期。点到这就可以。注意，那你到这还没完，我们还需要把这个日上传到HDS上面。杨淑玉上传。的X元。每天一个目录。所以你在这呢，你先在hps上创建日期目录，因为每天一个目录嘛，对吧。CS杠那个地可以加个杠P对吧。it。后面呢，我们就使用这个level user。然后后面把这个复制过来。大家注意这个日期格式呢，中间是带这个横杠的年月日，我们其实想获取这个不带横杠的。但是由于这他确实需要带横杠的，那我们这又不想要怎么办呢？那你就改一下呗，是吧。把横杠给替换掉就行，对吧，全部横杠替换成空，这样的话就是这个年月日中间不带任何分隔符了。这个用法咱们前面也讲过吧。还没上班。高foot。那就是这。把它呢，上传到这个目录下。这一块。</span><br><span class="line"></span><br><span class="line">好，注意，我这个脚本其实就是一个增量脚本，咱们前面说了，你在最开始的时候，其实啊，你还需要将这个表里面的之前的全量数据也给它导一次。那个脚本我这里先不写。我直接写了一个每日的增量脚本啊，大家要知道这个事情。因为我们现在这里面这个数据其实都是认为是一些增量数据啊，我直接。使用这个日期过滤，就可以把里面所有东西全都过滤出来。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260033394.png" alt="image-20230426003329063"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来确认一下。杠，我们来查一下这个date。小level user。有吧？再来查一下这个。没问题吧，有数据啊，那我们来看一下里面内容呗。对吧，这就是我们采集过来的内容。</span><br><span class="line"></span><br><span class="line">但是你在这需要注意点，它是有一个表头的。所以说呢，我们后期啊，在处理这个数据的时候，需要把这些数据给它归掉就行。那到此为止，服务端数据库中的数据我们就采集完毕了，那在这里面啊，大家需要熟悉我们的数据来源，以及数据最终的存储位置。后面我们的计算程序呢，都需要依赖于这些数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260034352.png" alt="image-20230426003427969"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260034803.png" alt="image-20230426003455794"></p><h2 id="数据计算核心指标详细分析"><a href="#数据计算核心指标详细分析" class="headerlink" title="数据计算核心指标详细分析"></a>数据计算核心指标详细分析</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261043614.png" alt="image-20230426104322881"></p><h3 id="第一个任务"><a href="#第一个任务" class="headerlink" title="第一个任务"></a>第一个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大家好，前面我们把项目需要的数据都采集过来了，下面我们开始基于这些数据开发数据计算模块。由于我们需要计算的指标比较多，所以呢，我们先对需要计算的这些核心指标呢进行详细分析。来看一下这张图。我们从左往右分析，首先看左边第一列，这里面表示的是计算程序的数据源，其实就是咱们前面分析的那五种数据。服务端日，客端日加上服务端数据库数据。</span><br><span class="line"></span><br><span class="line">那既往右边看。注意，右边这里面列出来的都是具体的计算任务。那我们首先呢，来看一下第一个任务。它表示的是历史粉丝关注数据的初始化，这个任务负责把服务端数据库中的历史粉丝关注数据批量初始化到neo4j。这个任务呀，只需要运行一次即可，后面就不需要了，它属于一个临时的任务，因为后续的粉丝关注数据就通过第二个任务来实施维护了。</span><br></pre></td></tr></table></figure><h3 id="第二个任务"><a href="#第二个任务" class="headerlink" title="第二个任务"></a>第二个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们来看一下第二个任务。这个任务呢，是实时维护follow和UN follow关系，follow呢就是表示关注，UN follow表示取消关注。这个任务是需要一直运行的，他会实时维护粉丝和主播的关注情况。它的数据来源是kafka中的实时粉丝关注数据，这个是通过我们的日志采集程序采集过来的。那通过前面这两个任务呀，就可以将我们全平台的粉丝关注数据进行全量维护了。第一个任务负责历史粉丝关注数据的迁移，第二个任务负责实时维护，这样在neo4j中就维护了全量的粉丝关注数据。</span><br></pre></td></tr></table></figure><h3 id="第三个任务"><a href="#第三个任务" class="headerlink" title="第三个任务"></a>第三个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下第三个任务。他需要每天定时更新用户最新活跃时间。这个任务的主要作用呢，是为了维护所有用户的活跃时间。因为最终我们在统计三度关系的时候，如果是针对全平台所有的用户都计算，这样是没有意义的。为什么呢？我们来画一个图，再来分析一下这个三个关系的一个流程。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305201757733.png" alt="image-20230520175733543"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这个呢，是一个主播，主播一吧。好，这个主播呢，他有一些粉丝。这些粉丝是不是还可能会关注了其他主播呀？所以说在这呢，我们再画几个主播。你看这时候如果说有一个用户。当这个用户呢，你看他去关注这个主播一的时候。其实呢，我们需要把它对应的一个三度关系推荐给这个用户，对吧，其实就是这块这些主播对吧。它们两个之间的一个关系，它到它是两个关系，再到它是三种关系好。注意这里面这个主播以及粉丝呢，其实都属于用户，只是说呢，他的角色不一样吧。对于我们平台而言，所有的主播以及粉丝都属于用户。</span><br><span class="line"></span><br><span class="line">那我们在计算三的关系的时候，针对这里面这个主播和粉丝最好呀，都是最近活跃过的。如果说这些粉丝很长时间都不回来了。那么我们认为他的这些关注数据啊，这个参考价值就没那么大了。如果这些主播呀，也是很长时间都不活跃了，那么更不应该进行计算。因为最终假设把它计算出来的话，就会把它推荐给用户。那用户关注他了之后呢，就想去看一下他最近的开播视频。结果发现这个哥们呢，很久都没开播了，那你这种推荐就没有任何意义了，其实就浪费了一次机会对吧。</span><br><span class="line"></span><br><span class="line">所以后期啊，我们在计算三度关系的时候，会对这里面所有的用户，不管你是粉丝还是主播，都会对他的一个活跃时间进行过滤。当时我们在实际工作中使用的那个时间，是根据最近一周活跃过的用户来计算三种关系。就说你这里面这些主播以及粉丝都要是最近一周活跃过来，如果没有活跃过，我在计算三种关系的时候，就不把你们计算在内，直接把你刨除掉。OK，那这个就是第三个任务。这个任务呢，它是每天执行，因此每天凌晨根据昨天的主活数据来维护每个用户的活跃时间。当我们后期计算三个关系的时候，会根据这个条件过滤出来满足条件的用户。</span><br></pre></td></tr></table></figure><h3 id="第四个任务"><a href="#第四个任务" class="headerlink" title="第四个任务"></a>第四个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">好，接下来我们来看一下第四个任务。这个任务啊，主要是在neo4j中维护主播的最新等级信息，因为我们在计算三度关系的时候，还需要对最终计算出来的主播等级进行过滤。如果主播等级太低，那就没必要推荐了。这个任务呢，也是一个离线任务，每天执行一次即可，它的数据呢来源于hdfs，这个数据是我们通过脚本每天定时从服务端数据库中找出来的。</span><br></pre></td></tr></table></figure><h3 id="第五个任务"><a href="#第五个任务" class="headerlink" title="第五个任务"></a>第五个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下第五个任务，这个任务呢是每周一计算一次，每周一会到HDPS上面获取最近一个月的视频数据。计算最近一个月内视频评级满足3B加或者是2a加的主播。注意视频等级呢，主要有sabcd这五个等级，S是评级最高的，D是最差的。这里面的3B＋表示最近一个月至少要有三次开播，并且最近三次开播视频的评级需要是B级以上，包含B级。那这里面的2a＋表示最近一个月至少要有两次开播，并且最近两次开播视频的评级需要是a级以上，包含a级。这两个指标，一个是考虑主播的开播频次比较高，但是呢，视频的评级不是特别高，这样也是可以推荐的，相当于它是一种勤劳的主播。还有一种呢，是主播开播频次比较低，但是呢视频评级特别高，比较优秀，这种呢也可以推荐啊。所以说你只需要满足3B加或者2a加都是可以的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">那最终啊，我们把满足条件的主播计算出来，然后呢到neo4j中进行维护。给主播增加一个flag属性，如果主播最近的开播数据满足3B加或者是2A加，则把这个flag属性置为一。后面在计算三度关系的时候呢，会对主播的这个属性进行过滤，不满足条件呢就不推荐了。那到这为止，前面这五个任务啊，都是对这个neo4j它里面这个数据进行维护的。其中第一个任务呢是初始化数据的是执行一次。</span><br><span class="line"></span><br><span class="line">因此。第二个任务呢，是一个实时任务，需要一直执行，其实这个任务呢，也可以改为离线的，一天执行一次，我们当时主要考虑的是这样的neo4j中的那个粉丝关注数据啊，我们会有其他业务部门也在使用。所以说呢，我们就让这个任务实施维护了。那第三个和第四个任务呢，是每天只一次。第五个任务，本来啊，也考虑每天执行一次，但是呢，每天都计算最近一个月的数据，这样太频繁了，浪费计算资源，所以呢，我们就改为了每周一计算一次。因为我们最终的三个关系数据也是一周计算一次。所以这里面啊，这些离线任务，它的一个计算周期，其实最迟都是一周。所以说第三个和第四个任务其实也可以每周一计算一次。不过当时呢，是因为这些指标，其他业务部门在使用用户率的时候呢，也会用到，所以说呢，我们就每天执行一次。</span><br><span class="line"></span><br><span class="line">那我在这呢，说这么多呀，主要是为了让大家明白，为什么有的任务需要实时，为什么有的任务需要离线，以及离线的任务为什么还要分为一天进行一次和一周之间一次。这些都是由一些具体的业务环境而导致的。</span><br><span class="line"></span><br><span class="line">那到现在为止，你neo4j中的数据是这样。假设我们这里面创建的是一个user这个节点，对吧，这个节点的话，它俩具备以下这些属性。UID。以及呢，它里面有一个时间桌，代表它的一个活跃时间。就是最近的一个活跃时间。以及呢，有一个等级叫level对吧，主播等级。还有一个flag。表示啊，你这个主播最近这一个月，你这个视频平移是否满足3B加或者2a加，如果满足它的值就是一，不满足就是零了。</span><br><span class="line">好，所以说呢，经过前面这五个任务在neo4j里面，我维护的数据其实就是这样。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261105667.png" alt="image-20230426110524589"></p><h3 id="第六个任务"><a href="#第六个任务" class="headerlink" title="第六个任务"></a>第六个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下第六个任务。这个任务呢，是计算满足条件的主播的三度关系推荐列表。在计算的时候会根据前面几个任务的指标作为过滤条件。首先第一点在这里。他俩需要过滤出来最近一周里活跃过的。第二点呢，需要过滤出来主播等级大于四级的。第三点再过滤出来，最近一个月开播视频，满足这个3B加或者2a加这个条件。第四点，再过滤出来推荐列表中粉丝列表关注重合度大于二的。那根据这四点过滤之后呀，最终计算的三个关系列表数据才是我们需要的。</span><br><span class="line"></span><br><span class="line">第六个任务，计算好的数据会先存储到hdfs上面，这个任务是每周一计算一次。因为这个三种关系出现，数据的更新力度没有必要细化到每天。如果每天都计算一次，最终的结果数据变化也不大，并且每天都计算一次，对计算资源的要求也比较高，所以当时我们定的是每周计算一次。更新一次最新的三度关系列表数据。</span><br></pre></td></tr></table></figure><h3 id="第七个任务"><a href="#第七个任务" class="headerlink" title="第七个任务"></a>第七个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据计算出来以后，会通过第七个任务把这个数据导出到MYSQ里面。这样就可以通过MYSQL对外提供数据了。这就是数据计算模块中的详细计算流程。下面呢，我把这个数据计算步骤啊做了一个汇总，我们来看一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261109573.png" alt="image-20230426110912322"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261109688.png" alt="image-20230426110948070"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261111274.png" alt="image-20230426111106202"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一步，历史粉丝关注，数据初始化。第二步，实时维护粉丝关注数据。第三步，每天定时更新主播等级。第四步，每天定时更新用户活跃时间。第五步，每周一计算最近一个月主播视频评级信息。第六步，每周一计算最近一周内主活主播的三组关系列表。第七步，三种关系列表数据导出到马，这个就是具体的我们的计算步骤。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261111021.png" alt="image-20230426111116924"></p><h2 id="数据计算之历史粉丝关注数据初始化-第一个任务"><a href="#数据计算之历史粉丝关注数据初始化-第一个任务" class="headerlink" title="数据计算之历史粉丝关注数据初始化(第一个任务)"></a>数据计算之历史粉丝关注数据初始化(第一个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们首先来看一下数据计算中的第一步。历史粉丝关注出于初始化。咱们前面分析过啊，历史粉丝关的数据呢，来源于服务端数据库，并且呢，这些数据在存储的时候还是分表存储，一共1296张。这个表里的数据格式呢是一样的。fuid、uid，还有一个time stamp。这个fuid呢，表示关注者，其实呢就是粉丝。这个UID表示被关注者的UID，其实就是主播。这份数据呢，在前面开发数据采集模块的时候，我们已经生成过。那接下来我们就要把这个数据啊，初始化到neo4j中。那我们需要把之前生成的这个文件呢，上传到neo4j的一个import下面才可以使用。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261117713.png" alt="image-20230426111700471"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们之前导出的这个粉丝关注数据在这就这个文件，那我们需要把这个文件给它复制到import目下面。嗯了，好，这样就可以了。那下面呢，我们在初始化这个数据之前啊，我们把那个neo4j重新初始化一下，就把里面数据全给它删掉。因为之前里面有一些特殊数据。把那个data目录删掉。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261118389.png" alt="image-20230426111836337"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261118328.png" alt="image-20230426111852239"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">停一下。嗯。在其中。好，这样的话就可以了，注意因为你在这把那个data目录删了之后啊，你再重新启动之后呢，我们需要去修改一下密码。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261119828.png" alt="image-20230426111933953"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们需要到这儿来初始化数据了。我们在这先连到这个bin&#x2F;cypher-shell这里面。bin&#x2F;cypher-shell -a bolt:&#x2F;&#x2F;bigdata04:7687 -u neo4j -p admin。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261122789.png" alt="image-20230426112214786"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">好，那这里面呢，我们首先要针对这个关键字段建立索引，对吧。咱们前面是不是已经做过这种操作呀。所以下面呢，我们还需要执行一个这个。好，执行成功，那我们到界面上来确认一下数据。好，是可以的。</span><br><span class="line">neo4j&gt; CREATE CONSTRAINT ON (user:User) ASSERT user.uid IS UNIQUE;</span><br><span class="line">0 rows available after 281 ms, consumed after another 0 ms</span><br><span class="line">Added 1 constraints</span><br><span class="line">然后批量导入数据</span><br><span class="line">neo4j&gt; USING PERIODIC COMMIT 1000</span><br><span class="line">       LOAD CSV WITH HEADERS FROM &#39;file:&#x2F;&#x2F;&#x2F;follower_00.log&#39; AS line FIELDTERMINATOR &#39;\t&#39;</span><br><span class="line">       MERGE (viewer:User &#123; uid: toString(line.fuid)&#125;)</span><br><span class="line">       MERGE (anchor:User &#123; uid: toString(line.uid)&#125;)</span><br><span class="line">       MERGE (viewer)-[:follow]-&gt;(anchor);</span><br><span class="line">0 rows available after 791 ms, consumed after another 0 ms</span><br><span class="line">Added 11 nodes, Created 17 relationships, Set 11 properties, Added 11 labels</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261127580.png" alt="image-20230426112713190"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261133414.png" alt="image-20230426113352894"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意，我们这儿呢，只初始化一个数据文件。那你说真的，我们实际工作中啊，我们有1000多个这种数据文件，你需要让我在这执行这个命令，执行1000多次吗？这肯定是不合理的。那如何一次性批量导入呢？可以这样来做啊。我们可以把这多个文件合并成一个大文件，对吧，你在Linux命行里面，你看这个文件，然后呢，通过这个右箭头&gt;&gt;重定向。把这个1000多个文件里面那个数据啊，整合到一个大文件里，这就可以了。好，那这样的话呢，针对数据计算的第一步初始化数据，这个我们就搞定了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v1.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v1.0-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1.html</id>
    <published>2023-04-24T07:37:23.000Z</published>
    <updated>2023-05-18T09:02:45.974Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v1-0-1"><a href="#第十八周-直播平台三度关系推荐v1-0-1" class="headerlink" title="第十八周 直播平台三度关系推荐v1.0-1"></a>第十八周 直播平台三度关系推荐v1.0-1</h1><h2 id="项目"><a href="#项目" class="headerlink" title="项目"></a>项目</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面我们开始正式学习直播平台三度关系推荐系统这个项目，这个项目分为1.0和2.0这两个版本。本周我们先学习1.0这个版本。首先我们来看一下项目效果。大家呢，可以在这里面扫码体验。这个就是我们直播平台的首页，当我们点击某一个主播，会进入到主播的详情页，我们在点击这个follow关注按钮的时候。这里面呢，会插入一个模块，它里面显示的是关注了此主播的人，也关注了哪些主播。这就是三度关系推进的效果。这页面上看起来只是把数据展现出来，很简单，但是具体这些数据是怎么来的，如何保证推荐的主播也是用户感兴趣的，这才是我们这个项目的核心内容。下面我们来看一下针对这个项目官方一点的介绍。</span><br><span class="line"></span><br><span class="line">在直播平台中，用户在主播页面关注该主播时。粉丝状态栏下方插入三度关系推荐模块，显示该主播的粉丝同时又关注了哪些主播。按照推荐重合度且满足一定的筛选条件进行择优展示，这样推荐的主播才是用户最可能喜欢的，可以帮助用户发现更多他喜欢的主播，促进用户活跃，进而挖掘用户消费潜力。这就是我们这个项目最终想要达到的效果。</span><br><span class="line"></span><br><span class="line">想要实现我们前面所说的三种关系推荐是需要由数据来支撑的，那么这些数据从哪里来呢？</span><br><span class="line">这就涉及到我们的第一块内容，数据采集。我们需要将项目中需要用的所有数据全部采集过来，包括离线数据和实时数据。</span><br><span class="line"></span><br><span class="line">这些数据采集过来以后，就需要涉及第二块内容了，数据存储，离线数据一般存储到分布式文件系统中，实时数据一般存储到消息队列中，数据存储起来以后，就需要涉及到数据计算了。</span><br><span class="line"></span><br><span class="line">数据计算模块，对前面存储起来的数据进行计算，分为离线计算和实时计算，计算之后的结果数据还会进行存储。</span><br><span class="line"></span><br><span class="line">那计算出来结果之后呢，就会涉及到数据展示了。将数据在页面中展示，查看最终的一个推荐效果。所以这个项目中的四大模块，它们之间的关系就是这样。在这里我们先从整体上对这个项目进行一个划分，后面还会有更详细的划分。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241546289.png" alt="image-20230424154642727"></p><h2 id="技术选型之数据采集"><a href="#技术选型之数据采集" class="headerlink" title="技术选型之数据采集"></a>技术选型之数据采集</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241549395.png" alt="image-20230424154922323"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">练好。接下来我们先针对这四个模块分析一下他们里面是需要用到的具体的技术框架，俗称技术选型。我们首先来看一下数据采集工具的选择。针对日志数据，目前业内常用的采集工具有下面这些。</span><br><span class="line"></span><br><span class="line">首先是这个apache开源的这个顶级项目flume。还有这个elastic公司开源logstash以及filebeat。那在这呢，我们把这个kafka也列出来了，但是kafka并不算是日志采集工具。只是说呢，它一般会和采集工具一块使用，所以呢，在这就一块列出。以及呢，这个sqoop组件。</span><br><span class="line"></span><br><span class="line">下面呢，我们来详细分析一下这个flume、logstash，还有filebeat这几个日志采集工具。</span><br><span class="line">首先呢，是flume。如果它是基于Java语言实现的。flume主要由source、channel、sink这三个组件组成。针对这三个组件中提供了很多实现。针对source，有基于文件的，基于socket的，基于kafka的等等，还有很多我们常用的数据源，flume几乎都提供了支持。这个channel提供了有常见的基于文件的，基于内存的。这个sink有基于hdfs的，基于kafka的等等，还有很多我们常用的存储系统几乎都提供了支持。就算是部分特殊的source和sink，flume没有提供支持，那么也没有关系，flume允许我们自定义这些组件。由于它也是基于Java的，所以开发这些自定义的组件也没有多大问题，我们都是Java程序员对吧？所以目前啊，在企业中，针对日志数据采集这一块，flume占据了主要地位。</span><br><span class="line"></span><br><span class="line">那接下来我们来看一下logstash。那是基于Jruby实现的。Jruby是ruby语言的Java实现。logstash的架构有点类似于flume，主要由输入、输出和过滤组成。这里的输入和输出类似于中的S和。那也提供了很多输入和输出的组件支持。常见的数据源和存储系统也都是支持的。并且带中还提供了强大的过滤功能，可以将采集到的数据进行一些处理之后再写出去。flume中的拦截器也可以实现类似的功能。logstash可以和elasticsearch、kibana轻松的实现一个日志收集检索、展现平台，非常方便。俗称ELK全家桶。</span><br><span class="line"></span><br><span class="line">那其实分析到这儿，我们会发现flume和那logstash还是非常相似的。但是呢，他们两个的典型应用场景是有一些区别的。logstash常用的场景是帮助运维人员采集服务器自身的运行日志，方便运维人员排查服务器的问题。这种场景下，对于数据的完整性和安全性要求不是特别高。因为logstash内部没有一个持久化的队列，所以在异常情况下是可能出现数据丢失的问题。而flume内部呢，是有自己的ack机制来去确保这个问题。所以说呢，它可以用于一些更重要的业务日志台词。</span><br><span class="line"></span><br><span class="line">那接下来我们再来看一下这个filebeat采集工具。是采用这个go语言开发的。它只支持文件数据采集。它可以将文件中的数据采集到，kafka，ES等等这些常见的存储系统。它会记录文件采集到的offset信息。就算filebeat的采集进程挂掉，也不会导致数据丢失，它下一次重新启动之后，还会延续之前的offset，继续往下面采。并且呢，这个filebeat呢，它还是一个轻量级的采集工具。咱们前面分析的这个flume，还有这个logstash，它们都是一些重量级的产品。在某些特定场景下，这个轻量级的组件可能会更加合适。filebeat和logstash同属于elastic这个公司，这些公司呢，提供了很多的这种Beat组件。filebeat的只是其中一个。因为我们的数据采集呢，主要是基于文件的，所以在这呢，我就只分析了这个filebeat。</span><br><span class="line"></span><br><span class="line">那到目前为止，这三个采集工具啊，我们都分析了一遍。logstash、flume属于重量级的组件。它们都是基于JVM虚拟机运行的。filebeat的呢是一个轻量级的组件，它是基于go语言。从语言层面来分析。go语言开发的程序，性能消耗是比那个基于JVM虚拟机运行的程序要小的。并且我们也在实际的服务器上进行了测试。相同数据规模下，filebeat的内存和CPU消耗是flume和logstash低的。</span><br><span class="line"></span><br><span class="line">那我们就直接选择filebeat了吗？并不是因为它的优点是性能消耗低，但是它的功能是有点弱的。所以我们在实际过程中会这样做。在前端业务机器上部署Filebeat的，将日志数据采集到消息队列里面。因为这个时候的要求是尽可能少的占用服务器资源，保证服务器上面的其他业务正常运行。数据到消息队列以后，后面我们可能还需要对数据进行一些简单的预处理，之后再存储到不同的地方。那所以在这个地方就可以使用flume了，因为提供了丰富的source和sink，并且也可以使用拦截器对数据进行一些简单的处理。这个时候就不需要太纠结性能消耗了，因为flume是部署在单独的服务器上面，不会对其他应用程序造成影响。在这呢，我们先简单画图看一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171558502.png" alt="image-20230517155827983"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后这个呢，是前端业务机器啊。那如果说我们想采集这个机器上面的一个日志数据的话，最好呢，是部署一个性能消耗低的一个采集工具。因为这上面除此还有其他进程。所以说我们在这里面部署于一个filebeat的性能消耗比较低。还是有那么两台吧。好，那我们在每一台上面都部署一个Bo的，让他呢去采集当前机器里面的人数数据。采集到之后呢，把这个日志数据啊，放到我们的消息队列里面。这个数据进到这个消息页之后呢，后面我们其实就可以使用去对这些数据做一些简单的预处理，预处理之后呢，再把数据写到其他地方。所以接着呢就可以接一个flume，这个时候就不需要去考虑这个性能消耗。这个有可能，它可以直接去读它里面的数据，然后呢，再把数据再写进去，都是有可能的啊。类似于如果从第一个topic上面就是消费数据。把数据拿出来之后呢，对数据做一些处理，处理之后呢，再放到第二个topic都是OK的。以及呢，flume可以直接消费这个消息队列里面这个数据，然后呢，直接把数据呢，写到我们的hdfs分布式文件系统里面也是可以的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">所以在这里我们就需要选择Filebeat和flume了，</span><br><span class="line"></span><br><span class="line">这里的消息队列我们就直接使用kafka了，因为kafka是大数据领域中最常用的消息队列了。</span><br><span class="line"></span><br><span class="line">所以 我们最终的选择就是FileBeat+Kafka+Flume，这就是针对日志数据采集工具的选择。</span><br><span class="line"></span><br><span class="line">后面我们也会涉及到数据库数据的采集，在我们这个项目中，需要从数据库中采集的数据量比较小，可以选择使用sqoop，或者我们使用mysql -e命令直接导出数据也是可以的，上一个项目我们已经使用过sqoop采集mysql中的数据了，在这我们就使用一个不一样的，自己开发脚本使用mysql的命令导出数据。</span><br><span class="line">不过我们在最后是需要把HDFS中的结果数据导出到MySQL中，这个时候还是需要用到Sqoop的。</span><br></pre></td></tr></table></figure><h2 id="技术选型之数据存储"><a href="#技术选型之数据存储" class="headerlink" title="技术选型之数据存储"></a>技术选型之数据存储</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241618486.png" alt="image-20230424161849176"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面我们把数据采集工具分析完了，下面我们来分析一下数据存储系统的选择。我们采集到的数据最终会存储到分布式文件系统中，这个分布式文件系统一般就直接选择hadoop中的hdfs，这个就不需要额外的对比啊，因为我们在搭建大数据平台的时候，hdfs已经安装好了。并且它也可以和很多采集工具以及计算框架无缝衔接，所以大数据领域的分布式存储系统一般都直接使用hdfs。目前一些大的厂商也有分布式存储的一些服务，例如亚马逊的s3。我们也可以选择使用这些服务，但是这样的话，针对数据计算就不太友好了。分布式计算框架无法实现本地计算，因为数据和计算节点不在一块儿。所以在这呢，离线数据我们就使用HDFS来求我们的计算框架，计算的结果数据有一些是需要和前端交互的。这些数据呢，前期可以选择存储到MYSQL里。</span><br><span class="line"></span><br><span class="line">那我们在维护这个用户三度关系数据的时候呢，如果使用普通的关系型数据库进行存储的话，会造成很多数据冗余，并且查询起来也非常麻烦，所以一般啊会使用一些图数据库。这里面这个Graphx或者Gelly。Graphx是spark中的图计算。gelly属于flink中的图计算，它们只能实现分布式图计算，不能保存图数据，所以说呢，并不满足我们的需求。</span><br><span class="line"></span><br><span class="line">下面这几个neo4j、orientDB、JanusGraph这几个都是图数据库，它们几个又有什么区别呢？我们来看一下。在这里面，我通过这些层面。对这三个图数据库做了一些对比分析啊。其中这个neo4j啊，它是目前人气最高的图数据库，它可以支持高度扩展，完全支持acid acid是数据库里面的一个特性啊，neo4j啊，它提供的Cypher查询语言是比较人性化的，非常容易上手使用，并且 它支持社区版和商业版，社区版是开源的。社区版呢，不支持分布式，商业版支持分布式。neo4j入门，相当简单，学习成本比较低，并且比较稳定。还有就是neo4j，它对各种语言的支持也比较好，Java呀，python啊，这些语言它都支持，并且官方提供的还有一个connector插件，可以实现Spark直接操作neo4j非常方便，那在这呢，我们主要考虑到应用性以及快速上线这些特性，所以说neo4j是我们目前最优的选择。所以说呢，针对这些存储系统啊，我们最后的选择就是hdfs加上MYSQL加上neo4j。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241621070.png" alt="image-20230424162131164"></p><h2 id="技术选型之数据计算-数据展现"><a href="#技术选型之数据计算-数据展现" class="headerlink" title="技术选型之数据计算+数据展现"></a>技术选型之数据计算+数据展现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下数据计算框架的选择。目前啊，大数据领域最常用的几种计算框架包括mapreduce、storm、spark、flink。其中mapreduce是第一代计算引擎，它主要针对离线数据进行计算。由于mapreduce计算框架的模型啊是固定的，针对复杂的计算，需要开发多个mapreduce任务，代码量比较多，也比较麻烦，并且它的计算是基于磁盘的，计算效率也比较低，所以现在已经很少使用。</span><br><span class="line"></span><br><span class="line">接下来看一下storm这个计算框架，是一个比较早的实时计算框架，可以实现真正意义上的实时处理。在前期的数据计算领域立下了汗马功劳，但是由于此框架太过于独立，没有自己的生态圈，所以最近这几年呢，日渐没落。</span><br><span class="line"></span><br><span class="line">那接下来看一下spark的这个计算框架，它是一个分布式的内存计算框架，支持离线和实时数据计算，由于它是基于内存的，所以说呢，它的计算性能非常高。但是在这需要注意一下，虽然spark支持实时计算，但是它的实时计算并不是真正意义上的实时。这是由于它底层的计算模型决定。spark最快只能支持到秒级别的实时计算，相当于一秒执行一个小型的批处任务。</span><br><span class="line"></span><br><span class="line">最后我们来看一下flink这个计算框架，flink属于最近新兴起的一个流式计算框架，它侧重于的是实时计算。flink在支持实时计算的基础上，也可以实现离线数据计算，所以说flink也是支持离线和实时数据计算的。在我们这个项目中，既需要离线计算，也需要实时计算，所以单纯的使用mapreduce或者storm都不合适，并且呢，他们两个现在几乎呢已经快被淘汰了。用的非常少啊，所以说我们需要在Spark和flink中进行选择，当时我们在开发这个项目的时候，flink才刚出来。还不是很稳定，并且我们团队内部也是刚开始接触flink，之前我们一直是使用Spark，所以说为了保证项目快速稳定上线，我们当时决定先使用spark，等后期对项目进行迭代优化的时候再考虑使用flink。所以在这针对数据计算，我们选择Spark</span><br><span class="line"></span><br><span class="line">最后是这个数据展现模块，数据展现模块不需要我们实现。这块是由安卓开发组还有iOS开发组负责的，我们只需要把结果数据计算好，存储起来就可以。好，最后我们做一个总结啊，就我们前面啊，针对各个模块进行技术选型的时候，大家不要盲目的追星，我们要根据具体的业务场景和不同框架的特点进行选择，同时还要考虑已经在使用的成熟的框架，不要盲目追求一些所谓的好的新的框架，因为技术成本也要考虑。所以说，技术选型不单单是选技术，是要在结合业务场景的前提下进行选择。</span><br></pre></td></tr></table></figure><h2 id="项目整体架构"><a href="#项目整体架构" class="headerlink" title="项目整体架构"></a>项目整体架构</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241727480.png" alt="image-20230424172738182"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">大家好，前面我们把技术选型搞定了，下面我们来看一下项目的整体架构设计。在这里，我把这个项目分为了三个模块，数据采集、数据计算、存储以及数据展现。因为这个计算以后啊，就涉及到存储了，所以说呢，我把这两个划分到一块儿，那接下来我们来详细分析一下这个项目的整体流程。</span><br><span class="line"></span><br><span class="line">首先呢，看这个数据采集模块。数据采集模块的数据啊，包含两大类，一个是服端数据，还有一个是客户端数据。其中服务端数据啊，它里面包含服务器中接口调用时记录的日志数据和数据库中的数据。在这里需要注意一下，针对服务端日志数据的采集，是在提供接口服务的机器上部署filebeat来采集。这样机器会有上百台，在这里我们先用一台server01来表示。这里面DB呢，表示的就是MYSQL数据库。</span><br><span class="line"></span><br><span class="line">接下来呢，是客户端数据，就是用户使用APP的时候上报的一些用户行为日志。例如打开关闭APP以及呢在APP中的滑动点击等行为，其实呢都会记录日志。这些数据呢，客户端会通过接口定时上报。那接口收到这个请求之后呀，会把请求中包含的日志信息呢，记录到本地文件中，然后使用filebeat进行采集。也就是说呢，我们会在server02上去部署一个接口服务，接收客户端上报的日志数据。那这样其实就可以统一流程了。针对服务端日志和客户端日志，最终啊，都是通过这个filebeat的来进行采集。那filebeat呢，最终把这个数据啊，都采集到这个卡夫卡里面。那针对服务端数据库里面的一个数据啊，我们会通过脚本直接呢，把它导入到hdfs里面。</span><br><span class="line"></span><br><span class="line">那filebeat的采集的实时数据啊，导入到卡夫卡里面之后呢，还会通过flume。对这个数据进行一些分发处理，以及落盘到hdfs的操作。落盘就是存储的意思。那针对这里面我们刚才所说这个数据分发的一个详细内容，我们在后面开发数据采集模块的时候，会详细分析它的架构。这就是数据采集模块的主要内容。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下数据计算，还有存储这个模块。计算模块主要呢是利用spark。针对卡夫卡中的数据呢，进行实时计算，针对hdfs中的数据啊，进行离线计算。那在计算的时候呢，它还会和这个noe4j这个图数据库进行交互。既会向里面写数据，也会从里面读数据。最终呢，会把这个spark计算的结果呀，使用sqoop导出到MYSQL里面。针对数据计算这一块，一共有六七种计算指标，具体直接计算指标我们在开发数据计算模块的时候会详细分析。这就是这块的一个流程。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后一个呢，是这个数据展现。那在这个模块里面，我们可以看到最终的一个项目效果，这里面其实就是一个手机端的一个项目。这个不是我们的重点。这就是我们这个项目的一个整体架构设计。注意，在这个架构里面其实存在三个主要的问题，第一个针对实时计算，Spark其实不是最优的选择，最好是使用flink。针对这个结果，数据的存储mysql也不是最优的选择。最好是使用redis。针对数据展现这一块，直接查mysql中的数据也不是最优的选择。最好是开发接口。对外提供接口查询数据。不过我们为了快速迭代上线，所以前期呢会使用相对来说比较简洁的架构，先把功能快速上线，后面再迭代优化。</span><br></pre></td></tr></table></figure><h2 id="Neo4j介绍及安装部署"><a href="#Neo4j介绍及安装部署" class="headerlink" title="Neo4j介绍及安装部署"></a>Neo4j介绍及安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">大家好，针对我们前面分析的这些技术组件，只有filebeat和neo4j我们没有使用过。不过非呢比较简单，它类似于在使用的时候主要是写配置文件，所以在后面用的时候我们再具体分析。下面我们就来学习一下neo4j的使用，让大家快速了解它，并掌握它的常见用。</span><br><span class="line"></span><br><span class="line">neo4j，它是一个高性能的图数据库。它和普通的关系型数据库是不一样的，它里面侧重于存储关系数据。针对各种明星之间错综复杂的关系，如果我们使用mysql这种数据库存储，在查询所有人之间的关系的时候是非常复杂的。但是使用neo4j这种数据库只需要一条命令就可以了。neo4j，它是一个嵌入式的基于磁盘的、具备完全的事物特性的持久化引擎。它将结构化数据存储在网络上，而不是表中。注意这块，这个网络，从数学角度我们可以把它称之为是图。这个并不是我们所说的4G网络，5G网络，不是这个意思。</span><br><span class="line"></span><br><span class="line">目前这个neo4j有两种发行版。一个呢是商业版，它是支持集群的，另一个是社区版。这个只支持单机。目前我们这个平台用户量啊，在三四千万这个规模，这个时候呢，我们使用单机也是足够用的。等后期单机无法支撑之后呢，再考虑使用商业版。那接下来我们来看一下neo4j的一个安装部署。用它支持在Windows以及Linux中进行安装，由于在实际过程中肯定是要在Linux中进行安装，所以说在这呢，我们就直接使用Linux环境。</span><br><span class="line"></span><br><span class="line">那下面呢，我们首先来下载一下。在这我已经起先打开了，因为它这个打开比较慢啊，你在这搜new，就这个new.com，这是它官网。接下之后把鼠标放在这个product上面，然后到这看到没有下载new，点那个。进入这个界面之后，注意。点这个。大陆的用户g serve。记下之后，注意，这呢是商业版。我们要用那个社区版的，就是这个。这是免费的。往下面走，你看他现在最新的版本呢，是4.1的，建议的话呢，我们可以往上面走一走，使用它之前比较稳定的是3.5的那个版本。往下边你看。3.5.21建议使用这个版本。看到没有，这是针对linknux或者麦克对吧？下这个是一个table包，如果想在Windows里面运行，你选那个Z。那我们在实际工作中，开发环境肯定是要用这个基于Linux的，所以说我就直接下载这个啊，你点这个就可以了，但是注意。你直接使用这个链接下载啊，很大概率可能会由于网络原因导致你下载失败。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241747028.png" alt="image-20230424174708992"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">呃，建议的话不要用这个链接下怎么办。现在呢，还有一个链接。这个。点，建议大家使用这个链接下载。你直接把这个复制到你的那个浏览器里面，打开就可以，它就会自动开始下载。这个现在呢是比较快。sentence。我在这呢已经卸载过了，到时候呢也会把去的安装包发给大家，所以说你想下的话就去下一下，不想下就算了。</span><br><span class="line"></span><br><span class="line">那这个呢，我们就需要把这个安装包啊，上传到我们的bigdata04机器上面。我呢，提前已经上传过了。到这看一下。我放到对soft这个下面。有这个文件对吧，已经有了。那下面呢，我们呢，先解压。解压之后我们来吸到里面。接下来我们需要修改配置。注意它下面有一个conf目录。嗯。在看下面有一个neo4j.conf这个文件，我们就要改这个配置文件。这里面呢，其实也比较简单，我们只需要改两个地方。dbms.connectors.default_listen_address。这个默认的一个监听地址在这呢，把它打开，你看它这个默认是注释掉的，把它打开。默认0.0.0.0，你使用这个也行，或者呢，我们建议把它改一下，直接改成我们那个bigdata04。除了这之外，还有一个。dbms.connectors.default_advertised_address。把注释呢给它去掉，在这呢，指定到bigdata这样就可以了啊。都是一个末接听地。</span><br><span class="line"></span><br><span class="line">那接下来我们就可以去启动了。退到上一级。现在bin面有一个neo4j这个脚本，后面传一个start，这边启动new。这样就起来了。起来之后呢，你可以通过JPS验证一下。看到没有，它确实有一个进程，对吧，对。这个时候呢，我们还可以访问一下neo4j的一个web界面。bigdata04:7474。看到没有，这样就可以了。你默认进来之后啊，你看它默认呢，会连那个bigdata04，这时候这个端口是7687，注意这个是真正连接neo4j这个服务的。这个端口我们注意的呢，是new负G，它的一个web界面的一个端口。这个需要注意下面需要输入用户名和密码，注意它这里面啊，默认用户名和密码都是neo4j。密码也是一样的。就这。注意你第一次使用的时候，它呢会让你重新去修改密码，因为那个默认的密码不安全。那我们接着把它的密码改成medin。好，这样也可以。注意，只有第一次你在访问你复制的时候，他才会强制让你修改密码，以后呢就不需要了。啊，能看到这个界面呢，也可以说明我们这个逆复瑞呢，是正常启动了。那如果说我们想把它停止掉，怎么停呢？很简单，你启动穿一个start，那停止了就穿一个stop。嗯。这样就可以了。这个进程又没了。这个就是用户队的一个安装部署以及启动行驶。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;neo4j stop</span><br></pre></td></tr></table></figure><h2 id="Neo4j之添加数据"><a href="#Neo4j之添加数据" class="headerlink" title="Neo4j之添加数据"></a>Neo4j之添加数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用neo4j可以很方便的展示一些人物或者事物之间的错综复杂的关系。下面我们来看一张图。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242111885.png" alt="image-20230424211116004"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这张图里面展示了这些人物之间的关系。使用这种展示形式看起来是很清晰的。也方便理解，后期如果说我们想查询某一个人的一个关系链，也是很方便的。这些数据如果让你去使用MY数据存储是很繁琐的，并且呢，查询起来也很烦。那这在这里面呢，有几个概念我们需要明确一下。因为neo4j，它是一个图数据库。我们可以认为它里面存储的呢，都是图数据。这个图到底是一个什么东西呢？注意图呢，它是由点边和属性组成的。我们这个图里面这个圆圈呢，它就是一个点。这里面这个线呢，它就是一个边圆圈中的这个姓名呢，就是属性。以及这个边里面这个值呢，也是属性。就是点和边上面都可以设置属性。对，这个点呢，你还可以把它称为是节点。这个边的话，可以把它称为是关系。就类似于这两个人之间的一个关系。每个节点和关系，它都可以有一个或者是多个属性。比如这上面呢，可以保存多属性。</span><br><span class="line"></span><br><span class="line">那在这里面大家啊，先对这个neo4j有一个整体的认识，下面呢，我们开始具体学习neo4j中的具体使用。在这儿我们主要学习neo4j的以下操作。添加数据、查询数据、更新数据、建立索引、批量导入数据。主要是这五种。那下面呢，我们把这个neo4j给它起来</span><br></pre></td></tr></table></figure><h3 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h3><h4 id="create"><a href="#create" class="headerlink" title="create"></a>create</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">创建一个点</span><br><span class="line">create (p1:Person &#123;name:&quot;zs&quot;&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create (p2:Person &#123;name:&quot;ls&quot;&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create (p1:Person &#123;name:&quot;zs&quot;&#125;) -[:like]-&gt; (p2:Person &#123;name:&quot;ls&quot;&#125;)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242125348.png" alt="image-20230424212534229"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242125430.png" alt="image-20230424212513916"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242129575.png" alt="image-20230424212950366"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p1,p2并不会实际存储，用create创建关系，不会检查之前是否存在待创建关系同名的节点</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">表示每次都创建新的点或者边。第二个表示每次创建点或者编之前呢，会先查询一下，如果存在则不创建。那下面我们就来演示一下。由于在这儿我们需要挑一些密径，所以说呢，在这儿我简单记录一下，这样看起来会更加清晰。a。每次都创建新的点。左边。那我们来在这先创建一个点。嗯。我先把meeting在这写一下，create。注意后面怎么写呢？注意先写一个小括号。英文的啊。冒号后面呢，表示你要创建这个点，你要给它起一个类型，它是什么类型的，表示一个person。还是那些。那你这个person的话，你可以给它设置一些属性，就相当于你在这出现点的时候呢，可以里面给它加些属性，属性怎么加呢？括号括号。我们给它加一个name属性。冒号。后面是这个属性的值，叫张三。那我们在这呢，还可以给它起一个别名P，就类似呢，我们在这创建了一个判对象。判断对象里面有一个内部属性，它的值呢是张三，最终呢，给它起了一个变量的名称叫P。我们来执行一下。放到这里面，点了一个play去执行就可以。添加成功看到没有，创建一个note，创建一个节点，然后设置一个属性。OK，那接下来我们再来创建一个节点。还使用这个。改一下。P2吧。第。把这个拿过来执行一下。那现在的话呢，我们就有两个点了。但是这两个点之间呢，还没有什么关系。那我们想他们两个之间有关系的话，就需要给它们设置一个边了。这个边的话呢，同样可以使用这个create命令来创业。注意，看我怎么实现啊。VISA。对前面的话呢，就类似是这个对吧，把它拿过来。注意，我们想给他设置一个边。就是一个关系。我们想让这个PE啊，这个张三，让他去喜欢李四。表示它们间的关系吗？后面一个横杠中括号冒号，这是固定格式。后面给他写个like，表示呢，张三喜欢李四。一个横杠，一个右键轴。对，这个表示呢，是张三喜欢李四，所以说李四是在后面。对，它这个箭头是往右边指啊，比如他喜欢它，所以呢，在这两个点之间呢，给它设置一个关系叫like。比如张三喜欢的，我们来执行一下。成功了对吧，两个基点。是这两个属性。以及呢，创建了一个关系，或者说呢，是一个边都可以啊。注意，这条命令执行之后，我们可以到这个界面上点这个device。来看一下，你看这是节点，这是关系，这个呢是属性。这是我们创建的person，以及这个like的关系，以及你person里面name的一个属性，在这都可以查看。我们接着可以点那个like。看到没有？张三like李四。这样的话，就可以很清晰的看到他们两个之间的一个关系了。但是呢，这时候呢，你回到这儿来看一下。你看点了一个person。你发现啊，它有四个person。你看两个张三，两个李四。然后你往这个位置看，你看这是内幕李四对吧，所以我们现在啊，选中这个李四和ID，你看是一，那这个呢。李四，它ID是21，看到没有，这个ID是它自动生成的，是唯一的。你看他们两个还不一样，那就意味着这是两个节点，对吧，不是同一个，虽然说他们两个名字一样，但是他们不是同一个极点。那这是什么原因呢？注意，因为这个create呀，它每次呢都会创建新的激烈或者关系。所以说呢，最开始啊，我们使用这个create，你看创建了两个节点，P1还有P2，对这个P1还有P2这个东西它不会扯到就里面。这个以及这个name对应的值是会存储到里面的，这个相当于你给它起了一个别名而已，这个东西不会存进去。那我们之前你看在这创建了一个P1，又创建一个P2，现在我们出现了两个P对象。这是新建的。那接着呢，我们使用这条命令，看到没有。它相当于又创建了两个，这个杠三和离子。所以说呢，你这时候在你的纽扣针里面就有四个。那其实啊，我们在这是想给最开始创建的这两个机制增加一个这个like关系的。因为在实际过程中也会有这种需求，就是节点已经存在了，需要我们后期给他们指定关系。你这种写法，它相当会重新生成。那肯定是不满足我们需求的。这个时候该怎么做呢？咱们前面说了，谬杯里面除了有这个create meaning，它还有一个me meaning。这个命令呢，表示啊，在创建几点之前都会先查询一下，如果存在则不创建。我。这个命令。在创建节点之前都会。先查询一下。如果存在。则不创建。所以说这个默认命令啊，你就算是重复执行，他也不会产生重复的结果。注意你这个奎的命令。你重复执行，你执行一次，他就给你创建一个这个person这个节点。这个需要注意一下啊。那我们看一下me的话，后面的写法是一样的。后面这种写法是一样的。现在我要看起来清晰一些，我给它起个别名叫P3吧。其实都无所谓啊。注意那下面呢，我再写一个，这是P4对吧。这个名字给他改一下吧，这个叫Jack。同时呢，我们想要这个Jack呢，去like to。这样写much。第三。like。kiss。这样写就可以了。现在呢，我先复制这一行，拿过来来执行一下。拖延成功了对吧，创建一个节点，设置一个属性，那你说进行明我再执行一下，你可以直接点那个。再执行一下。看到没有，没有改变。点着来确认一下。看到没有，还是一个，所以说这个默认命令啊，你重复执行，它是不会重新创建的，因为在这的话，它呢，会根据你在这使用的这个名称去查一下，看看有没有重复的，如果有的话，他就不再创建了。所以说呢，在工作中啊，建议使用这个墨。可以避免重复。注意。这条命令你说我单独执行行不行啊不行，你必须要保证这条命令和前面两个一块来执行，因为在这这个变量，我前面说变量它是不会存到u里面的。他呢，只在当前绘画有效，所以说呢，这三条命令需要一块儿来执行。把它们放到一块儿，这样来执行就可以。来确认一下。看到没有，Tom Jack。你点那个也是OK的啊，一样的。这个。like to。对吧，我们记住那个person的话，等于之前我们是四个，现在又加了两个，一共是六个，没有问题。在这里其实还有另外一种写法，如果节点已经存在了，我们只需要创建关系，我们还可以使用那个match来实现。再让我们先简单用一下。那。那么可以查询之前。已有的。节点或者关系。那其实就是电或者是编了嘛。一样的意思啊。那接着呢，我们就想要这个Tom和这个也产生一个like关系。对，你看之前的话呢，是这个。Jack like Tom，那现在的话，我们想让Tom也去like Jack一下，互相习惯，这样的话就不是单相思了嘛，对吧。单相思最难受。所以说我们在这呢这样来做，使用ma先查询那个节点，因为那个节点之前已经创建过了，对吧。可以这样，那我们要给他起个名字叫a吧，还一个person。他的name呢？the Tom。对吧，我们要查两个人啊，要把这个汤还有这个都得查出来，对吧，要找到这两个人，重新再给他们加一个关系。B。name。Jack。把他们两个都查出来，注意，查出来之后注意。a。然后a就是Tom，然后Tom呢去like。B注意这里面这个a还有B，只是为了在这去使用，没有其他含义，你给它起个什么XY也是可以的。来我们来执行一下，注意他们两个也需要一起来执行，要不然那你这个a他是找不到他的。好，执行成功，你看创建了一个关系。这个还是六对吧，没有变。只不过这时候你看没有，汤姆也喜欢Jack克，Jack克也喜欢汤姆，他们两个就互相喜欢了。所以说呢，我们就可以通过match呢，去查询之前已有的机械信息，然后再通过墨创建关系就行。这样也不会额外产生重复的节点。所以说呢，这两种方式啊都可以，你使用这种方式也行，使用这种方式也行。按需选择即可。效果是一样的。</span><br></pre></td></tr></table></figure><h4 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个命令在创建节点前都会先查询一下，如果存在则不创建</span><br><span class="line"></span><br><span class="line">merge (p3:Person &#123;name:&quot;jack&quot;&#125;)</span><br><span class="line">merge (p4:Person &#123;name:&quot;tom&quot;&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">merge (p3) -[:like]-&gt; (p4)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242135053.png" alt="image-20230424213515502"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242135568.png" alt="image-20230424213535288"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">把上述三条命令一起执行</span><br></pre></td></tr></table></figure><h4 id="match"><a href="#match" class="headerlink" title="match"></a>match</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">可以查询之前已有的节点(点)或者关系(边)</span><br><span class="line"></span><br><span class="line">match(a:Person &#123;name:&quot;tom&quot;&#125;),(b:Person &#123;name:&quot;jack&quot;&#125;)</span><br><span class="line">merge (a) -[:like]-&gt; (b)</span><br><span class="line"></span><br><span class="line">这种a,b只是别名，只在当前会话有效；这两个要一起执行，不然第二个命令找不到a,b</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242143938.png" alt="image-20230424214333671"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242144207.png" alt="image-20230424214401568"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">之前jack-&gt;tom</span><br><span class="line">现在jack&lt;-&gt;tom</span><br><span class="line"></span><br><span class="line">等同于之前的三条merge命令</span><br></pre></td></tr></table></figure><h2 id="Neo4j之查询数据"><a href="#Neo4j之查询数据" class="headerlink" title="Neo4j之查询数据"></a>Neo4j之查询数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下用户类中如何查询数据。针对这块我们主要学习以下内容，首先呢，学习一下这个match和return的用法，它们呢可以实现查看满足条件的数据，并且返回。以及最后我们会讲两个案例，如何查询二度关系和三度关系。</span><br></pre></td></tr></table></figure><h3 id="match-return"><a href="#match-return" class="headerlink" title="match+return"></a>match+return</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match(p:Person &#123;name:&quot;tom&quot;&#125;) return p</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">咱们前面呢说过这个match啊，它可以进行一个查询，下面咱们就来继续使用一下。这个呢，其实有点类似于mysql中的select。在这需要注意一下，match不能单独存在。咱们前面在使用的时候，那后面跟着也是有一个merge命令的。如果我们只想查询一些数据，并且把这个数据返回过去，呃，如何实现呢？</span><br><span class="line">就可以使那个match加return。就是查看满足条件数据，并且返回，那下面呢，我们就来查询一条数据。现在我们想查询一下这个Tom这条数据。nice。PAR。指定属性name。对吧，我们就想查他，那查出来之后呢，想要把这个结果啊给返回，怎么返回呢，后面加个return。这样这样就可来我们来执行一下。没问题吧，查出来了对吧，把那个汤姆查出来。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242150553.png" alt="image-20230424215018223"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们来查询一些复杂一点的内容。首先呢，在这我们来初始化一些数据。好，这些数据呢，刚才我已经把它复制过来了，就这些数据，注意这里面创建点的这些操作和创建边的操作需要在一个会话里面一起执行啊。否则它是无法识别这些变量的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242152676.png" alt="image-20230424215221280"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在这你看其实相当于我们初始化ABCXYZ是吧，这几个用户。在他们之间呢，给他加了一些关注关系，其实就类似于直播平台里面用户和主播之间的一个关注关系。好在这把这批数据给他做一下初始化，直接复制过来。好创建成功对吧，六个节点六个属性六个关系。那首先呢，现在我们要做一个查询。假设呢，这些都是主播，你看a follow了B对吧，a关注了B，那B的话，我们可以认为它是一个主播对吧？那所以说呢，这样我们要查询某个主播的粉丝信息。我们就查询这哥们儿，你看有这个a和C都关注了他对吧，他的名字呢叫B。B。好，那这时候怎么实现注意。查询嘛，使用。name。我们要查询这个用户，他的一些粉丝。注意看我下面怎么写，相当于啊，是别人关注了他，按照我们之前教的这种写法，就类似于这种，你说把这个拿过来，注意这个箭头是往右指的，也就是说呢，是他操作了别人，他关注了别人。我们现在要查的是谁关注了它，所以说了这个方向不是往右的往左。对吧？我们要查询哪些人关注到这个user b。所以后边的话呢，零。有的。这时候它后面呢，就不需要加这个括，括号里面也不需要指定什么属性了。相当于我们就要查询到底是谁关注了这个user b。这个人具体是谁，我们现在还不知道呢，所以说呢，后面也不需要加一些具体的限定。OK。那个范围就可以。来，我们来执行一下。看到没有a和C。你回过头来看一下。AC对吧，你看。a的话，它这个名字就是大a嘛，对吧，这是大C没问题。所以说呢，我们是可以查出来的。那其实这种写法呀，它还有一种写法。还有一种写法，矢量。mass。把他们反过来。把它放到前面。这个不是注意改一下。对吧。就是谁去follow了这个user。这样写也是可以的。如果说你感觉这种写法比较别扭，你可以用这种写，对吧，谁关着B。这样把它返回过来就可以，效果是一样。11下午啊。如果说我们只想返回满足条件的那个粉丝的一个name值，你看这个相当于它整个把这个几点都给返回来，如果说我们只想返回它里面这个内的属性的值怎么办呢？也简单。值返为零。没问题吧，也是可以的。好，这个其实啊，就是我们要查询的那个主播的二度关系。为什么这样说呢？我们来分析一下啊。你看这个时候呢，是这样的我。这个呢，是这个主播B。后面呢，是主播B的一个粉丝。那这个时候我和这个主播B的粉丝，我们之间是不是就属于一个二度关系呢？因为我和主播B我们之间呢，是一度关系。主币和粉丝之间呢，也是一度关系，但是我和这些粉丝之间就属于二度关系，我们是通过这个主币来认识。好，那我们在这个项目中呢，是想实现三个关系推荐。也就是说呀，当我要关注某个主播的时候，你呢，要给我推荐这个主播的粉丝，又关注了哪些主播，你把那些主播推荐给我。因为我和那些主播之间才属于三种关系。二的关系。三度关系。看到没有，这个时候我和主播N之间就属于三种关系。看到没有，我和他是一组，和它是二度，和它呢也是三组。那这个三的关系该如何查询呢？其实咱们刚才这个呀，查询的就是二楼关系，其实你只需要把这个主播币的粉丝查出来就行了，后期谁去关注主播币，那这个粉丝和那个人，他们之间是不是就是二度关系。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242153262.png" alt="image-20230424215318019"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查询某个人的粉丝</span><br><span class="line">match (:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (n:User) return n</span><br><span class="line"></span><br><span class="line">另一种写法</span><br><span class="line">match (n:User) -[:follow]-&gt; (:User &#123;name:&quot;B&quot;&#125;) return n</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242156274.png" alt="image-20230424215640635"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查询某个人的粉丝只返回name(属性)值</span><br><span class="line">match (:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (n:User) return n.name</span><br><span class="line"></span><br><span class="line">另一种写法</span><br><span class="line">match (n:User) -[:follow]-&gt; (:User &#123;name:&quot;B&quot;&#125;) return n.name</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1%5Cimage-20230424215940138.png" alt="image-20230424215940138">)<img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242159113.png" alt="image-20230424215940184"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这些属于二度关系(查询我关注的人的粉丝的信息)</span><br><span class="line">我-&gt;主播-&gt;粉丝</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">三度关系(给我推荐我关注的主播的粉丝关注的人)</span><br><span class="line">我-&gt;主播-&gt;粉丝-&gt;主播N</span><br><span class="line"></span><br><span class="line">match (a:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt;(c:User) return a.name as aname,b.name as bname,c.name as cname</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242213902.png" alt="image-20230424221314630"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那我们现在想查三度关系，那其实就是查出来主播B的粉丝又关注了哪些主播。只要把这些数据查出来就可以了。我们返回一下。看到没有？这是主播B，有这么几个粉丝啊，A和C看到没有？他有a和C这两个粉丝，分别关注了XYZ这三个主播，其中呢，A和C这两个粉丝呢，都关注了Y这个主播。那这个时候你再给我推荐深度关系的时候呢，就应该从这里面去挑了，那这个时候是不是应该把这个B的粉丝关注比较多的主播推荐给我呢？你看a和C都关注了Y，你是不是应该把它推荐给我呢？在这里，理论上来说，Y这个主播最有可能是我喜欢。这个Z和X呢，它那个可能性啊，就没那么大，所以说呢，在这里面啊，我们在获取这个三度关系的时候呢，针对这个c.name里面这个结果呀，最好呢是做一下过滤。我们统计一下c.name里面相同主播出现的次数。然后呢，按照倒序排序。最终再取一个topN是不是就可以了？重合度越多的说明了越有可能是我喜欢的。因为相当于我去关注这个主播B了，相当于我喜欢主播B。你看a和C这两个粉丝也关注他了，说明他们两个也喜欢他，那有可能我和这个a还有C这两个粉丝这个口味是一样的。那他们两个同时呢，又都关注了Y这个主播，所以说呢，Y这个主播也是最有可能是我喜欢的。所以说呢，这个其实就是三的关系，最终想要达到一个效果。</span><br><span class="line"></span><br><span class="line">好，那根据我们刚才分析，你在这儿还想对它做一个什么聚合，对吧，再做个排序，这东西怎么实现呢？我们来看一下。其实这个麦呀，后面。也支持什么抗就是抗函数。奥特曼。排序的以及呢，这个厘米上对吧，取多少条这些命令啊，都是支持的。那所以说呢，在这个基础之上，我们可以做一些调整。所以这时候你要把这个去掉。只保留这个a name，还有c name就行。然后后面注意你后面返回多个列的话，中间有多少个开啊。抗的星。也是做一个求和啊，后面order by。some。DSC倒序排序。那我们再做一个厘米。倒序排序之后呢，我们取前几条，这样不就是top n了吗，对吧。来，我们在这儿实现一下。看到没有？他最终统计的这个数量，你看对不对。这个Y西里Y嘛，是吧，两次。Z是一次，X是一次对吧，没问题吧。没问题，那所以说这个时候你其实可以把这个稍微改一下，你改成厘米则点一吧，因为这两个值都是一样的，我们就厘米的一就取了一条。没问题吧，是可以的。注意你这里面啊，你用康兴也行，或者说呢，你用那个什么呀，这种写法。对吧，这样也可以啊，都是一样的效果。要消毒一下。可以使用。它或者它效果是一样的。OK。那其实这里面啊，你看这里面就相当于我们根据这个a，还有这个c name去做一些分组。然后呢，使用抗的。去做了一个求和统计，每组的一个数据行数。好把这个加个a是吧，少了一个a啊。这是一样，这只是一个变量名称啊，无所谓。改过来之后呢，看起来顺眼一些对吧，有强迫症的话，感觉这里面少一个字母，感觉很难受对吧。好，这就是二度关系，还有三度关系的一个查询了。对，这里面呢，其实啊，我们还可以使用where去加一些过滤条件。就实现一下过滤。注意。你想使用where也可以啊。这个where需要放在。return前面因为你这个return啊，就直接返回了呀，你这个where啊，肯定是要放到return之前的，先过滤再返回嘛，对吧。把这个复制过来。注意现在前面加一个外过滤。我们过一下，where name。对，你在这还不能用那个C里，C里姆是在后面定义对吧，我们前面的话还只能用C点内。不等于X吗？把X这个过滤掉行吗？你先回到这儿。对吧，还是0.3。你看现在这个C里是不是有一个X呀。好，我们在它基础上加列过滤。对吧，C点内就是不等于X对吧。看到没有，只有Y和Z啊。OK，所以说呢，这里面也是可以用这个where或者条件的。OK，这个其实就是我们这个没忽略里面的一个查询操作啊。注意这里面这个查询语法呀，其实啊，就是用户这里面的S法语法，这个塞法语法呀，在查询的时候，你看其实有些地方它和circle那个查询还是有点类似啊。所以说呢，这种写法还是比较简单易用的，最起码看起来是比较清晰的。很直啊，所以说呢，我们学起来上手也很快。这就是我们当时为什么选择这个newd啊，这个查询语言确实用起来比较方便。</span><br></pre></td></tr></table></figure><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实match后面也支持count()、order by、limit等命令</span><br><span class="line">match (a:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt;(c:User) return a.name as aname,c.name as cname,count(*) as sum order by sum desc limit 3</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242224375.png" alt="image-20230424222424329"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：这里count(*)等同于count(cname)</span><br><span class="line"></span><br><span class="line">相当于对aname,和cname做了分组，在操作</span><br></pre></td></tr></table></figure><h4 id="where"><a href="#where" class="headerlink" title="where"></a>where</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：where放到return之前</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实match后面也支持count()、order by、limit等命令</span><br><span class="line">match (a:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt;(c:User) where c.name &lt;&gt; &quot;X&quot; return a.name as aname,c.name as cname,count(*) as sum order by sum desc limit 3</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242231619.png" alt="image-20230424223142375"></p><h2 id="Neo4j之更新数据"><a href="#Neo4j之更新数据" class="headerlink" title="Neo4j之更新数据"></a>Neo4j之更新数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下如何在应用中去更新数据。更新数据这块啊，其实总结一下有两种情况。第一种呢，就是更新节点的属性，使用match和set实现，你把它先查出来，然后呢使用set命令去修改。第二种啊，就是更新节点之间的关系，也就是边。这个其实就是删除边。我们使用那个ma和D的视线把它查出来，把它删掉。其实呢，你使用这个match和吉delete也可以实现删除节点。对吧，我们把这个节点查出来，然后把它删掉。那下面呢，我们就来演示一下。首先我们看一下就是如何修改节点中的属性。那。有了。name。我使用那个X这个用户吧，行吧。把它查出来之后呢，后面sa.H。等于八，注意如果说它里面没有这个属性，那就把这个属性给加上去，如果有这个属性了，那把这个属性值改成18。看到没有添加一个属性。现在我们可以查一下它。你看a。看到没有，它里面一个name是XH是什么？具有刚才我们给他加了一个属性啊。那接下来我们看一下如何删除关系。match。然后呢？name。a，对，你前面这个变量呢，你能用到了，那你就给它起个变量，如果你用不到，那你就不用起。我们在这呢用不到，所以说就不给它起变量，变成清不洗都无所谓啊。follow。我们看一下之前那个数据这个a。你看它其实呢，关注了BXY对吧，那我们随便找一个吧。name。X吧，对吧，它对它呢也有一个分关系。注意，我们最终啊，想把他们两个之间那个follow关系给它删掉。那怎么办，这时候啊，你要给这个合作关系啊，也起一个别名。起个名称，这样的话在后面呢，使用第一层。这样就可以把它给删掉。看到没有，删除了一个关系。点包。这时候这个a是不是就没有关注那个X了呀。你可以到这儿来查一下。看到没有对吧，X它现在就没有人去follow。就变成一个孤家寡人了，对吧，又没有连到这里面。OK，这就是用户内容针对更新数据的相关操作。如果说你想去删除一条数据啊，就删除一个节点，那你把它查出来对吧，把它查出来后面呢，直接给它就类似这种。前面呢写这个，后面呢加上一个a，就可以把这个name等于X的这个user给它删掉。这个呢，给大家留一个作业，下一周我们自己操作一下行吧，我这个呢就不再演示这个。</span><br></pre></td></tr></table></figure><h3 id="更新节点属性"><a href="#更新节点属性" class="headerlink" title="更新节点属性"></a>更新节点属性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">match (a:User &#123;name:&quot;X&quot;&#125;) set a.age &#x3D; 18</span><br><span class="line"></span><br><span class="line">有更新，无则添加</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242247278.png" alt="image-20230424224750421"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242248429.png" alt="image-20230424224819117"></p><h3 id="更新节点之间的关系-边"><a href="#更新节点之间的关系-边" class="headerlink" title="更新节点之间的关系(边)"></a>更新节点之间的关系(边)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match (:User &#123;name:&quot;A&quot;&#125;) -[r:follow]-&gt; (:User &#123;name:&quot;X&quot;&#125;) delete r</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242250704.png" alt="image-20230424225000475"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242250214.png" alt="image-20230424225026098"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意:删除节点</span><br><span class="line">match (a:User &#123;name:&quot;A&quot;&#125;) delete a</span><br></pre></td></tr></table></figure><h2 id="Neo4j之建立索引-批量导入数据"><a href="#Neo4j之建立索引-批量导入数据" class="headerlink" title="Neo4j之建立索引+批量导入数据"></a>Neo4j之建立索引+批量导入数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下neo4j中的索引</span><br><span class="line"></span><br><span class="line">neo4j中的索引可以细分为两种</span><br></pre></td></tr></table></figure><h3 id="普通索引"><a href="#普通索引" class="headerlink" title="普通索引"></a>普通索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">普通索引 CREATE INDEX ON :User(name)</span><br><span class="line"></span><br><span class="line">第一种是普通索引，使用create index 可以实现，指定给节点中的某个属性建立索引，</span><br><span class="line">具体建立索引的依据是后期我们在查询的时候是否需要在where中根据这个属性进行过滤，如果需要则建立索引，如果不需要则不建立索引。</span><br></pre></td></tr></table></figure><h3 id="唯一索引"><a href="#唯一索引" class="headerlink" title="唯一索引"></a>唯一索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">唯一约束 CREATE CONSTRAINT ON (u:User) ASSERT u.id IS UNIQUE</span><br><span class="line"></span><br><span class="line">第二种索引称之为唯一约束，类似于mysql数据库中主键的唯一约束。</span><br><span class="line">使用CREATE CONSTRAINT可以实现</span><br><span class="line">CREATE CONSTRAINT ON (u:User) ASSERT u.id IS UNIQUE</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那这两种在使用的时候具体该如何选择呢？</span><br><span class="line"></span><br><span class="line">如果某个字段的值是唯一的，并且后期也需要根据这个字段进行过滤操作，那么就可以建立唯一约束，唯一约束的查询性能比索引更快</span><br></pre></td></tr></table></figure><h3 id="批量导入数据"><a href="#批量导入数据" class="headerlink" title="批量导入数据"></a>批量导入数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们将学习一下neo4j如何批量导入数据</span><br><span class="line">针对项目一开始的时候有一批海量数据需要导入，我们就不能使用前面讲的那种命令一条一条导入了，性能太差，我们需要有一个批量导入的方式来快速导入这一批数据。</span><br><span class="line">neo4j批量导入数据有两种方式，一种是这个batch import的，还有一种呢是load csv</span><br><span class="line"></span><br><span class="line">第一种这个batch import，它呢需要组装三个文件，导入性能呢比较快，但是呢比较麻烦。</span><br><span class="line"></span><br><span class="line">第二种这个load csv呢，它呢只需要把数据组装到一个CSV文件即可。导入性能没有batch import快，但是也没有我们想象中的那么慢，还是可以接受的啊，它的优点呢，就说使用起来很方便，直接把所有需要的数据直接都组装到一个CSV文件即可。</span><br><span class="line"></span><br><span class="line">那在这里啊，我们考虑到一个易用性。由于我们的原始数据都在MYSQL中，我们可以通过MYSQL命令直接把数据导出为一个文件。所以接着呢，我们直接使用load CSV会更加的方便。</span><br><span class="line"></span><br><span class="line">但是在这有一点需要注意。在load csv中啊，我们如果使用到了merge或者match这些命令的时候，我们需要确认关键字段是否有索引，否则呢，性能会很差，怎么理解呢？来看一下。就针对这种match，或者咱们前面讲的那种merge。你这个merge，它在执行的时候，其实呢，它会根据这个name看看有没有这条数据，对吧，如果没有的话，它才会新增，如果有的话，他就不会再新增了，所以说呢，它需要根据name这个字段去查询数据，那所以说呢，你就需要根据name了。去建立索引，如果你没有建立索引，后期用户内容数据量大之后，这块平行效率会很差。以及这个match也是一样的，match里面你看没有，你这个其实是根据那个name去查的。所以这时候的话，这个name字段对吧，它也是需要有索引。以及这个where后面这些过滤条件，对吧，也是需要有索引。如果这些关键字段没有建立索引的话，那其实这些操作它就相当于是一个全表扫描了，所以说呢，你数据量越多，它的查询性能会越差。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如：merge(a:User &#123;name:“A”&#125;)，此时就需要提前对User中的name字段建立索引，否则在进行初始化的时候，数据量大了之后，初始化的性能会很差，因为merge在执行的时候会查询name等于A的数据在不在neo4j中，如果name字段没有建立索引，则会执行全表扫描。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接着啊，我们先把这个neo4j的数据啊给它清空了，那如何清空数据呢。接着呢，就给大家一种暴力的方式啊。你把它的data目录给删了。因为他的所有数据啊，都放到那个date目录里面。然后呢，重启一下neo4j。stop一下。再启动一下，那接着呢，我们来看一下，我准备了一个测试的一个数据文件。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181652001.png" alt="image-20230518165227850"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">fuiduid</span><br><span class="line">10011000</span><br><span class="line">10011004</span><br><span class="line">10011005</span><br><span class="line">10012001</span><br><span class="line">10021000</span><br><span class="line">10021004</span><br><span class="line">10022001</span><br><span class="line">10031000</span><br><span class="line">10031004</span><br><span class="line">10061000</span><br><span class="line">10061005</span><br><span class="line">20021004</span><br><span class="line">20021005</span><br><span class="line">20022004</span><br><span class="line">20031000</span><br><span class="line">20031005</span><br><span class="line">20032004</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个文件里面一共有两列。fuid是关注者。uid是被关注者。你可以把它认为是主播，这是观众对吧。那我们需要把这个数据啊，给他做一下初始化。那怎么初始化呢，注意。你想要对这个数据做初始化的话，你首先啊，需要把这个文件上传到NEO4J_HOME的import目录下才可以使用。注意你必须要放在这个目录下面才能使用啊，否则neo4j会找不到。这个需要注意一下，那下面呢，我们就把这个批量导入命令啊，先给他写一下。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 neo4j-community-3.5.21]# bin&#x2F;cypher-shell -a bolt:&#x2F;&#x2F;bigdata04:7687 -u neo4j -p admin</span><br><span class="line">Connected to Neo4j 3.5.21 at bolt:&#x2F;&#x2F;bigdata04:7687 as user neo4j.</span><br><span class="line">Type :help for a list of available commands or :exit to exit the shell.</span><br><span class="line">Note that Cypher queries must end with a semicolon.</span><br><span class="line">neo4j&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">首先针对关键字段建立索引</span><br><span class="line">neo4j&gt; CREATE CONSTRAINT ON (user:User) ASSERT user.uid IS UNIQUE;</span><br><span class="line">0 rows available after 281 ms, consumed after another 0 ms</span><br><span class="line">Added 1 constraints</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">然后批量导入数据</span><br><span class="line">neo4j&gt; USING PERIODIC COMMIT 1000</span><br><span class="line">       LOAD CSV WITH HEADERS FROM &#39;file:&#x2F;&#x2F;&#x2F;follower_demo.log&#39; AS line FIELDTERMINATOR &#39;\t&#39;</span><br><span class="line">       MERGE (viewer:User &#123; uid: toString(line.fuid)&#125;)</span><br><span class="line">       MERGE (anchor:User &#123; uid: toString(line.uid)&#125;)</span><br><span class="line">       MERGE (viewer)-[:follow]-&gt;(anchor);</span><br><span class="line">0 rows available after 791 ms, consumed after another 0 ms</span><br><span class="line">Added 11 nodes, Created 17 relationships, Set 11 properties, Added 11 labels</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在neo4j的web界面上也可以执行，需要添加:auto命令。</span><br><span class="line">解释：</span><br><span class="line"></span><br><span class="line">PERIODIC COMMIT 1000：每1000条提交一次，这个参数非常关键，如果在数据量很大的情况下内存无法同时加载很多数据，所以需要批量提交事务，这样可以减小任务失败的风险，并且也可以提高数据导入的速度，当然这需要设置一个合适的数量。</span><br><span class="line">WITH HEADERS：是否使用列名，如果文件中有列名，则可以加这个参数，这样在读取数据的时候就会忽略第一行</span><br><span class="line">FIELDTERMINATOR ‘\t’：指定文件中的字段分隔符</span><br><span class="line">然后我们到页面上看一下导入的数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181659391.png" alt="image-20230518165932413"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：此时我们会发现在页面中的圆圈中没有显示数据的具体内容，之前我在用3.2版本的时候是没有这个问题的，现在使用新的3.5版本之后会发现页面显示的时候会出现这种问题，这个问题倒没什么影响，就是在页面中看起来不太方便而已。</span><br><span class="line"></span><br><span class="line">通过我的测试发现</span><br><span class="line">如果我们在添加节点数据的时候，给节点指定一个name属性，那么name属性的值默认会显示在这个圆圈里面，如果不是name字段，则不显示。这个应该是新版本的一些特性。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">批量初始化数据</span><br><span class="line"></span><br><span class="line">针对关键字段建立索引</span><br><span class="line">create constraint on (user:User) assert user.uid is unique;</span><br><span class="line"></span><br><span class="line">批量导入语句</span><br><span class="line">using periodic commit 1000</span><br><span class="line">这什么意思呢？注意这个表示呀，可以是每1000条起交一次，这个表示设置一个事物提交的一个大小啊。这个参数呢非常关键，如果说你这个数据量非常大，那你把这个全量的数据全部都读出来，全部都放到内存里面，这样的话内存可能扛不住，所以说呢，建议呢批量去提交事务。这样可以减小任务失败的风险，并且呢，也可以提高数据导入的速度。当然，这需要设置一个合适的数量，这个数量太大或者太小其实都不合适啊，就类似于我们平时往mysql里面批量提交数据一样，提交数据也是一批一批的。</span><br><span class="line">load csv with headers from &#39;file:&#x2F;&#x2F;&#x2F;follower_demo.log&#39; as line fieldterminator &#39;\t&#39;</span><br><span class="line">注意这个其实呢，就相当于从本地这个根目录下面读取了。我们之前把这个文件放到那个neo4j的import目录下了，注意你只要放到了import目录下面，那其实呢就是相当于是从根目录读取，这个是neo4j来设定的。他就会读取这个文件里面内容，一次读一行，一行数据这个字段之间是分隔符这指定一下。那这样的话，其实前面这两行呢，基本就把这个功能属性设置好了，</span><br><span class="line"></span><br><span class="line">merge (viewer:User &#123;uid: toString(line.fuid)&#125;)</span><br><span class="line">把那个第一列取出来，注意外面这个呢，我们使用的是一个tostring，它是一个函数啊，你本来一读出来之后呢，这个是一个数字，我们要把它转成一个字符串啊，因为本身我们这个UID就是一个字符串。</span><br><span class="line"></span><br><span class="line">merge (anchor:User &#123;uid: toString(line.uid)&#125;)</span><br><span class="line">下面这个呢，是一个主播anchor。</span><br><span class="line"></span><br><span class="line">merge (viewer) -[:follow]-&gt; (anchor);</span><br><span class="line">那下面把他们之间的关系给watch。V。WS。安。这样就可，那我们来执行一下这个命令，注意这个命令呢，你可以在这个外部界面去执行。在这执行也可以啊，但是在这执行的时候呢。你执行这条命令可以直接执行，但是你在执行这个时候，它会提示让你在前面加一个什么auto，有一个自动提交事务。这是一种方式，这个给大家留个作业，下一周呢自己实验一下，我呢先不用这种方式。我用哪种方式呢？我就直接在我们的雷命令行里面去做。注意咱们之前不是把这个六库率给它重新删了相，那重新启动了吗？现在于是一个新的六扣率了，那所以说你在这啊。还需要去修改一下密码。相当于我们把那个对的目录删了之后，它就是一套新的东西。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;cypher-shell -a bolt:&#x2F;&#x2F;bigdata04:7687 -u neo4j -p admin</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242334611.png" alt="image-20230424233404885"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242334532.png" alt="image-20230424233420853"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242335175.png" alt="image-20230424233527600"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242337963.png" alt="image-20230424233725604"></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那这个大家有没有感觉到有一些看起来不太一样的地方？咱们前面在创建这个节点的时候，你看节点上面是不是显示的那个内蒙的一个值啊。那你说我现在其实也有一个UID的值啊，你为什么这边没显示出来呢。注意了啊，之前啊，我在用这个纽破利的3.2那个版本的时候呢，是没有这个问题的，那现在呢，使用新的这个3.5这个版本之后啊，发现它这个页面显示的时候呢。会出现这种现象。这种现象啊，倒也没什么影响啊，就说了我们在页面中看起来啊，不太方便而已。那通过我的测试发现呀。我们在添加节点数据的时候呢，如果说你给这个节点啊，指定的一个内部属性，那么内部属性的值默认会显示在这个圆圈里面。如果不是内部的一个字段。就不显示你现在有这个UID字段，不显示这个呢，相当于是这个新版本的一些特性啊。不过这倒不影响啊，只不过说在这看起来啊，有点不太习惯，它这个数据呢，你看它其实存点对吧，UID这个值都是有的啊。好。下面呢，我们来验证一下，我们把UID这个属性的一个名称啊，给它改一下，把它改成name，看看这个值啊，会不会显示到这个圆圈里面。那个只是一个显示形式而已啊，我们来验证一下。验证呢很简单，把这个复制出来一份。对吧，然后在这呢改一下。看了什么？这个测试的。这块呢，这个属性名称改成name。这个也是name对吧。好，这个时候呢，我就在这里面来执行，注意我直接拿过来执行啊，它其实呢会报错。先看一下。看到没有？我搞错了。它下面有个提示。就说啊，你需要加一个什么呀，冒号凹凸。这样才行。所以说呢，也就意味着在它前面啊，加一个冒号。而是自行提交。这就可以了。该成功了，你看这又有了。这样的一个test。看到没有，这样就显示了。是什么？遇到这个问题啊，也不要太感到惊讶，这个只是在新版本上做一些改动，之前那个老版本是没有问题的，就是我们之前线上那个版本是OK的啊。后来呢，给大家在这讲的时候，我们用了一个新的版本，稍微新一点就有一些变化啊。其实两个效果是完全一样的啊，这个只是在这显示而已啊。就是看起来清晰一些，这样可能看起来不太清晰啊，有点B。那这样的话，我们现在就实现了一个批量数据的一个初始化，其实就很简单了，现在呢，我们后期啊，可以把我们想要初始化那些数据啊呃，提前导成这种文件。然后在这写一个这个P，触发一个脚本就OK。所以说呢，那CSV这种方式还是比较方便的，你直接把你需要的数据全部都组装到这一个文件里。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v1.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-Spark Streaming-6</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-6.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-6.html</id>
    <published>2023-04-23T14:22:49.000Z</published>
    <updated>2023-04-26T10:44:42.994Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-Spark-Streaming-6"><a href="#第十一周-Spark性能优化的道与术-Spark-Streaming-6" class="headerlink" title="第十一周 Spark性能优化的道与术-Spark Streaming-6"></a>第十一周 Spark性能优化的道与术-Spark Streaming-6</h1><h2 id="SparkStreaming-wordcount程序开发"><a href="#SparkStreaming-wordcount程序开发" class="headerlink" title="SparkStreaming wordcount程序开发"></a>SparkStreaming wordcount程序开发</h2><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面我们来学习一下Spark中的Spark streaming。针对Spark Streaming，我们主要讲一些基本的用法，因为目前在实时计算领域，flink的应用场景会更多。Spark streaming啊，它是Spark Core API的一种扩展。它可以用于进行大规模、高吞吐量、容错的实时数据流的处理。大家注意这个实时啊，属于近实时。最小可以支持秒级别的实时处理。</span><br><span class="line"></span><br><span class="line">那spark streaming的工作原理呢？是这样的。它呢会接收实时输入的数据流，然后呢，将数据啊拆分成多个Batch。比如呢，每收集一秒数据给它封装为一个batch，然后将每个batch呢交给这个spark计算引擎进行处理。最后呢，会产生出一个结果数据流。这个结果数据流里面数据呢，也是由一个一个的batch所组成的，所以说呢，spark streaming的实时处理，其实呢，就是一小批一小批的处理。那下面呢，我们就来开发一个spark streaming的实时wordcount程序来感受一下。</span><br><span class="line"></span><br><span class="line">现在我们来创建一个项目。it。点击这个auto。把它那个基本环境再配置一下，因为我们在这呢，也要写了一个SC，所以说在这点右键。到这儿。在这下面创建一个GALA。对吧，然后注意右键把它视为S对吧。接下来到这个depends里面，注意添加那个scholar的SDK，注意针对这个我们需要添加02:11的。因为我们使用的那个卡夫卡集群，它那个是02:11编译的啊。那个SC的版本，所以说呢，在这儿使用02:11。好，这样就行了。那下面呢，我们需要找一下它对应的一个依赖，SPA，人命的依赖。现在我们来说一下。就那个。用02:11的，注意我们之前用的是2.4.3这个版本。S。把这个除掉是吧。好。那这样基本环境就OK了。下面呢，我们来开发这个word的程序，在这呢，先建一个package。I1克点。八。the stream count。需求呢，是这样的。通过socket。模拟产生数据。实时计算数据中。单词出现的次数。这个没方法。好，那在这注意，我们需要先创建一个streaming。context。然后呢，指定数据处理。间隔。所以五秒吧。因为我们前面说了，你SPA死命，他这个实时处理，其实还是一小批一小批的处理，所以说你需要指定它这个一小批这个间隔是多少秒。那现在我们直接利用一个streaming。这边呢，首先传一个。配一样康复。注意第二个呢，才是这个距离的时间叫。五秒。SSC吧。注意，那我们在上面来创建这个。mark。康复配置对象。嗯。嗯。了，我们现在本意来执行。注意咱们之前啊，开发这个发个离线代码的时候，我们呢，穿的都是logo对吧，注意这时候呢。你需要这样来写LOGO2。什么意思呢？所以。止住了。LOCAL2。表示启动两个进程。一个进程。否则读取。数据源的数据一个进程。负责处理数据。ABB name。好，这样就行了。嗯。好，接下来我们来通过socket。获取实时产生的数据。B04端口9001。这个可以叫SRD。是吧，这里面也是RDD啊。下面我们就对接收到的数据使用。空格进行切割。转换成单个单词。改个e lines RD。第二，find map。加你的SP。按空格切就行啊。这样返回的就是wasd是里面呢包含了每个单词。这样把每个单词。转换成。淘宝兔的形式啊。what map对吧？这个RD。下面来执行reduce。BYK操作啊，所以基于K进行求和。嗯。it is by k。嗯。有我。啊。下面呢，将这个结果数据打印到控制台。嗯。认识。对吧，你看这个代码是不是也和咱们前面写那个link代码很像呀。启动任务。嗯。注意它下面这种写法不太一样，和那个SPA离线写法都不一样啊。等待任务形式啊。嗯。啊。好，这样的话你就可以开启一个发达人命实时流处理程序了。那我们在这呢，把这个socket给它打开。来执行。这个没事啊，还是那个when you choose啊，这个不用管了。把这个日志清一下，好，下面注意在这我来输入点数据。右。the me。回来看到没有， hello2161。只要说你输入的那两项数据在它的一个时间段之内，对吧，在五秒之内，它其实就把它切到一块儿了。这就可以啊。所以说呢，你可以这样理解，它相当于是每隔五秒把前五秒的数据给你封装成一个batch。然后后面呢，其实执行的就类似于Spark核心的那个代码Spark I的对吧。其实就是离线的那一套。它前面的话是按照时间去切这个小批，后面的话把你一小批一小批去处理，这样的话可以达到一个进食时的一个效果啊，所以说这个就是方向十命它的一个执行的原理。好，这是实现，接下来呢，我们使用加来实现一下。键package。SPA。world can&#39;t。加。把这个需求拿过来。那下面呢，是一个密方法，好，那接下注意首先还是要获取这套什么使命contact这些东西。先获取。创建。sten。啊。所以这里面你去Java这面你要获取这个。Java。streaming。context。嗯。后面呢，传的还是一个。时间。R。u减。second。嗯嗯。SIC。那上面还是要创建这个SPA配对项啊。嗯。but。嗯。master。LOCAL2。name。好，这样也可以。注意这块报错。对，报错了，一般是你那个包引错了。你可以看一下，把鼠标放到这个上面，你看。说什么这是什么Java FX里面什么？这是有问题的。对吧，我们用的话肯定是用Spark里面。对，其实啊，你这后面是少了一个S啊。嗯。这样也可以。你看这个时候用的是发使命里面的。好。下面是通过。获取实时产生的数据。soirit。这个点零四。等零一。来阿。对接收到的数据使用空格进行切割。转换成。三个单词。嗯。哪一点find map？对，那这里面的话，我们就需要写一个函数了啊。你有一个map function。然后要返回一个swim。我们就不把那个map的代码也写进去啊。这样的话，它是一个。我可以这样来直接来写ari。there as list，它里面呢，直接line there。后面做。直接。这样就可以啊，因为它最终返回一个联系啊。这种写法啊，它返回的速度，这样把这个速度转成list，再把它转成这个就可以。我咋？那接下来是把每个。单词转换。喂。double two的形式。我1MAP。所以呢，这里面也是需要写一个函数的啊。你有一个T。嗯。这个呢是in。嗯。因为你最终要法是一个P2列，这就淘宝里面第一列，淘宝里面第二列。new。double two。这呢，其实就是一个over了，把名字改一下啊，看起来清晰点。war。一。这个呢，叫派。安你。接下来实行。YK。嗯。reduce。function。I1 I2。这个呢，就叫word count。最后将结果数据啊引到。台。我们直接使用那个不好1D啊。这里面呢，我们给它传一个你一个VID方。这里面其实也好办，这个呢，就是一个。higher。不好意思。然后这里面的话，再给它传一个VD方式。对，这个就是具体那个他。嗯。嗯。相量二。这样的话就可以把里面这些数据啊，给它迭代出来。然后呢，就剩下最后这个。行任务。start。还有一个，等待任务停止。好一场，好一起。嗯。嗯。好，这就可以了。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket模拟产生数据，实时计算数据中单词出现的次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCountScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      <span class="comment">//注意：此处的local[2]表示启动2个进程，一个进程负责读取数据源的数据，一个进程负责处理数据</span></span><br><span class="line">      .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      .setAppName(<span class="string">"StreamWordCountScala"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建StreamingContext，指定数据处理间隔为5秒</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//通过socket获取实时产生的数据</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD = ssc.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对接收到的数据使用空格进行切割，转换成单个单词</span></span><br><span class="line">    <span class="keyword">val</span> wordsRDD = linesRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把每个单词转换成tuple2的形式</span></span><br><span class="line">    <span class="keyword">val</span> tupRDD = wordsRDD.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行reduceByKey操作</span></span><br><span class="line">    <span class="keyword">val</span> wordcountRDD = tupRDD.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将结果数据打印到控制台</span></span><br><span class="line">    wordcountRDD.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232245799.png" alt="image-20230423224530561"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232245196.png" alt="image-20230423224547229"></p><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket模拟产生数据，实时计算数据中单词出现的次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamWordCountJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//创建SparkConf配置对象</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">                .setAppName(<span class="string">"StreamWordCountJava"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建StreamingContext</span></span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过socket获取实时产生的数据</span></span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; linesRDD = ssc.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//对接收到的数据使用空格进行切割，转换成单个单词</span></span><br><span class="line">        JavaDStream&lt;String&gt; wordsRDD = linesRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//把每个单词转换为tuple2的形式</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairRDD = wordsRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行reduceByKey操作</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; wordCountRDD = pairRDD.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> i1 + i2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//将结果数据打印到控制台</span></span><br><span class="line">        wordCountRDD.foreachRDD(<span class="keyword">new</span> VoidFunction&lt;JavaPairRDD&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(JavaPairRDD&lt;String, Integer&gt; pair)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                pair.foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        System.out.println(tup._1+<span class="string">"---"</span>+tup._2);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//启动任务</span></span><br><span class="line">        ssc.start();</span><br><span class="line">        <span class="comment">//等待任务停止</span></span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SparkStreaming整合Kafka"><a href="#SparkStreaming整合Kafka" class="headerlink" title="SparkStreaming整合Kafka"></a>SparkStreaming整合Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这个sparkstreaming和kafka的整合。我们的需求是这样的，使用sparkstreaming实时消费kafka中的数据。这种场景也是比较常见的。注意，在这你想使用kafka，我们需要引入对应的一个依赖。它那个依赖是什么呢？到官网来看一下，进到官网点那个。文档SPA人命。你这里面来搜一下啊。就梳里的卡夫卡往下边走。到这块。看到没有，把这个点开右键啊，打开一个新页面。看到没有？你这个SPA命想要和卡夫卡进行交互，想要从卡夫卡里面去消费数据，你需要添加对应的依赖。这个呢是针对0.8.2.1级以上的，这个呢是针对卡夫卡零点十几以上的，那我们用的话肯定要用这个了，要用新一点的啊。那就这个东西。你可以。把它复制过来，到这儿来搜一下。就这个。2.4.3。格拉，02:11。把这个拿回来就可以了。嗯。这样就可以了。那下面呢，我们就来写一下具体的一个代码。swim。卡不卡？bug消费。卡夫卡中的数据。嗯。首先呢，还是希望见。streaming。嗯。context。康复。然后呢，还有一个second。嗯。有没有？我们还使用这个五秒。具体这个时间间隔啊，需要根据你们的业务而定啊。等于你有一个Spark。said master。LOCAL2。set name。嗯。获取消费卡夫卡的数据流。那怎么获取呢？现在我们需要用的这个卡夫卡u。csa。direct。这里面需要传两个泛型参数，three。SH。这里面呢，首先把这个SSC传给他。嗯，所以接下来需要第二个参数。什么那个location。street。这个。它这边会有提示啊。对吧，这几个参数。好使，用它呢，来调一个P开头的这个。就第一个就行。接下来只听下面那个。street。用这个。这个发型呢，还是string？对，这里面你要指定一下topic，对他接触的是一个topic s啊是一个。然后后面呢是一个map map里面传的是卡夫卡的一些参数。这里面这些都是固定写法啊。那下面我们就要呈现这两个。指定。指定卡夫卡的配置信息。好不好？等于一个map。object。我们直接在里面给它初始化就行啊。显得不报错。还有这个topic，在这一个它指定性。嗯。只要topics。注意，在这我们需要传一个。里面呢，可以同时使用多套贝。也就是说，它可以同时从多个topic里面去读取数据，都是可以的。那我们在这一个，我们就写一个就行。那既然把这个参数给它完善一下啊，嗯，首先需要指定卡不卡的。broke地址信息。would strive。B01。9092。零二。9092。039092。嗯。接下来我们需要指定那个K的序列化类型。K点。s Li。展开一个反序变化啊D。Siri。a。对啊。你如果怕拼数的话，可以先把后面这个写。后面的话，我们使用这个class of。spring。size对吧。可以把这个给它复制过去，把这个D改成小写就行了啊。这样也可以。嗯。还有这个value的。序列化类型。嗯。那就把这个改一下就行，改成V。都是死顿类型啊对，这就是咱们前面指定的那个对吧，配合没有一个泛型啊。嗯。那下面来指定那个。消费者ID。就那个YD。胳膊的ID。好，下面呢，再指定一下消费策略。there there。这块呢，只能一个。最后我们来指定一个自动提交。在设置啊。enable。there also？好。这样就可以了。这是一些核心的参数。OK，这样的话就可以从它里面去读取数据啊，这样就可以获取到一个类似于卡夫卡。我在这里面，我们可以把它称为。嗯，这是他们的一个概念啊。这个数据流。那下面我们就可以处理数据了。后面你可以调map啊，map啊这些算子去处理就可以。嗯。这样每次获取到一条数据啊，每次几的一条数据。然后把里面数据迭代出来之后呢，把它封装成一个他报，因为它本身呢是一个record一行记录，那我们在这呢。对的点。先获取的，你们K。然后再获取value。嗯。在这我们就把这个数据打印出来。将数据。印到后来。因为其实你在这只要能获取到数据，你后期你想做map map reduce go是不是都可以呀，对吧，那个就没什么区别了。启动任务。start。等待任务停止。嗯。嗯，好。那下面呢，我们把这个运行起来。但是呢，你发现这块他报错了。看到没有？31行。遇到问题不要怕啊，这个问题我们要排查一下。他说这个类型啊，有点问题。他呢，发现了是一个布尔类型，结果他需要了是一个OB。所以这块的话，你需要在这这样来指定一下。强制执行类型Java点拉点布尔。嗯。这样就可以了啊来执行。好，这样就可以了。那接下来呢，我们来开启一个生产者，往里面写点儿数据。其中一个卡不卡，剩下的我里面写着数据啊。其控制台的生产者。嗯。hello Spark。嗯。看到没有打一回啊。我把它停一下啊。注意。它这个呢，we know，为什么呀。因为现在我们卡卡里面数据啊，其实只有value是没有那个K的啊，所以说你K答出来是no，我们G的那些数据啊，一般都放到value里面啊。就是放到外。这个K的话是为了判定你这个数据到底是放哪个分区里面啊，一般会传一个K。所以说我们那种说法一般是不传的，然后随机分啊。这是没有问题的。那这样的话，我们就可以把那个卡字卡里面数据给他消费出来。是吧，那后面就可以实现你的业务逻辑。OK。那接下来呢，我们使用这个加代码来实现一下。卡夫卡。加了。嗯。把这个注释拿过来。好，首先呢，获取这个streaming context。在指定读取数据的。时间间隔为五秒。嗯。有一个Java。streaming context。好点。这个大家看五秒。嗯。new。said master。logo。嗯。下载APP name。把这个拿过来，嗯。而且这个变量Co。嗯。那接下来我们来获取消费卡夫卡的数据流。还是那个卡不卡。great。direct，那首先SC。后面还是一样的。嗯。嗯。嗯。嗯。嗯。好，接下来是这个。consumer。这个。there。首先是一个topics，还有一个。搞不搞？注意这块啊。你需要指定泛型，你这个泛型写到哪了。我们在SKY面里面是放在这个位置，但是在这里面你写这还是不对的啊，你在写前面。three。W。好。接下来创建这两个啊，把这个topic，还有这个卡夫卡。在线行。指令要读取的。名称。先写这个。three。topics。嗯。挨着。七。那接下来是这个。有一个map。object。好，不搞。下面就往里面添加参数了啊。RI。service。我们俩复制一下吧。这个又是提花。嗯。嗯嗯。第二个呢，是这个K的这个虚化类型。所以这个你别导错包了啊，你要导这个。巴阿巴奇，看到没有，卡布卡点common这个body。name。嗯。嗯嗯。说不爱你。嗯。also。offset。there reet。at。ne。点点commit。好，这样就可以了。of Australia。好，那接下来数数去。嗯。嗯嗯。选一个map，因为一个function。我们最终返回是一个double two。里面是一个string。LW。好，这个就是一个record。我们可以在这直接。你了一个。two，嗯。你告你。six。嗯。我看点160。这样转换成淘宝之后，后期用起来也方便。SP对吧。将数据打印的。启动任务，嗯。start。等待任务停止。嗯。好一场。嗯。嗯。好，这样就可以了，来。把它执行一下。好看没有，这是之前那条数据啊。我们可以再往里面加一条。哈哈哈。可以吧，也是可以的啊。好，这就是Java代码的一个实现。好，那针对Spark命这一块呢，我们暂时就讲到这儿，因为后期大部分的实施计算需求，我们需要使用link去实现了。在这呢，我们是把这个SPA命最常见那个消费卡不卡数据这种案例呢给大家讲一下。</span><br></pre></td></tr></table></figure><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spark 消费Kafka中的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"StreamKafkaScala"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定Kafka的配置信息</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Object</span>](</span><br><span class="line">      <span class="comment">//kafka的broker地址信息</span></span><br><span class="line">      <span class="string">"bootstrap.servers"</span>-&gt;<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>,</span><br><span class="line">      <span class="comment">//key的序列化类型</span></span><br><span class="line">      <span class="string">"key.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//value的序列化类型</span></span><br><span class="line">      <span class="string">"value.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//消费者组id</span></span><br><span class="line">      <span class="string">"group.id"</span>-&gt;<span class="string">"con_2"</span>,</span><br><span class="line">      <span class="comment">//消费策略</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span>-&gt;<span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">//自动提交offset</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span>-&gt;(<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//指定要读取的topic的名称</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"t1"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">    <span class="keyword">val</span> kafkaDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    kafkaDStream.map(record=&gt;(record.key(),record.value()))</span><br><span class="line">      <span class="comment">//将数据打印到控制台</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232301727.png" alt="image-20230423225935601"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232301407.png" alt="image-20230423225949397"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spark 消费Kafka中的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//创建StreamingContext，指定读取数据的时间间隔为5秒</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">                .setAppName(<span class="string">"StreamKafkaJava"</span>);</span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafka的配置信息</span></span><br><span class="line">        HashMap&lt;String, Object&gt; kafkaParams = <span class="keyword">new</span> HashMap&lt;String, Object&gt;();</span><br><span class="line">        kafkaParams.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"key.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"value.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"group.id"</span>,<span class="string">"con_2"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"auto.offset.reset"</span>,<span class="string">"latest"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"enable.auto.commit"</span>,<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定要读取的topic名称</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        topics.add(<span class="string">"t1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; kafkaStream = KafkaUtils.createDirectStream(</span><br><span class="line">                ssc,</span><br><span class="line">                LocationStrategies.PreferConsistent(),</span><br><span class="line">                ConsumerStrategies.&lt;String, String&gt;Subscribe(topics, kafkaParams)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">//处理数据</span></span><br><span class="line">        kafkaStream.map(<span class="keyword">new</span> Function&lt;ConsumerRecord&lt;String, String&gt;, Tuple2&lt;String,String&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(ConsumerRecord&lt;String, String&gt; record)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(record.key(),record.value());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();<span class="comment">//将数据打印到控制台</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//启动任务</span></span><br><span class="line">        ssc.start();</span><br><span class="line">        <span class="comment">//等待任务停止</span></span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-3.html</id>
    <published>2023-04-20T08:46:48.000Z</published>
    <updated>2023-04-24T15:42:18.541Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-2.html</id>
    <published>2023-04-20T08:46:43.000Z</published>
    <updated>2023-04-23T14:21:05.429Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十七周-Flink极速上手篇-Flink高级进阶之路-2"><a href="#第十七周-Flink极速上手篇-Flink高级进阶之路-2" class="headerlink" title="第十七周 Flink极速上手篇-Flink高级进阶之路-2"></a>第十七周 Flink极速上手篇-Flink高级进阶之路-2</h1><h2 id="Kafka-Consumer的使用"><a href="#Kafka-Consumer的使用" class="headerlink" title="Kafka Consumer的使用"></a>Kafka Consumer的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们来看一下flink中针对kafka connect的专题，提供了很多的connect组件，其中应用比较广泛的就是kafka这个connect。我们就针对kafka在flink的应用做详细的分析。针对flink流处里啊，最常用的组件就是kafka。原始日志数据产生后，会被日志采集工具采集到kafka中，让flink去处理。处理之后的数据可能也会继续写入到kafka中。kafka可以作为flink的datasource和datasink来使用。并且kafka中的partition机制和flink的并行度机制可以深度结合，提高数据的读取效率和写入效率。那我们想要在flink中使用kafka，需要添加对应的依赖。(先在flink官网中找到依赖的名字，再到maven中去找符合的版本)</span><br><span class="line"></span><br><span class="line">那在具体执行这个代码之前啊，我们先需要把那个zookeeper集群，还有kafka集群给他起来。我这些相关的服务呢，已经起来了。这是入K班了，这是卡不卡都已经起来啊。好，那下面注意，我们还需要做一件事情。因为我们在这里面呢，用到了一个T1这个topic，所以说在这我们需要去创建这个topic。找一下之前的命令。嗯。其一。这个分区设置为五，后面因子是为二。发现一个T。好，创建成功。那下面呢，我们就可以去启动代码。启动电板之后，那我们需要往那个卡夫卡里面模拟产生数据。这个时候呢，我们可以启动一个基于控制台的一个生产者来模拟产生数据。嗯。使用这个卡不卡console producer。把这个复制一下。好，这个套背上就是T1。那这个时候呢，我们接着就来模拟产生数据。hello。看到没有消费到。再加一个。hello。没问题吧，是可以的，这样的话我们就可以消费卡夫卡中的数据了。好，这个是代码实践，接下来我们使用这个Java代码来实现一下。先创建一个package。嗯。stream。搞不搞？SS。把这个复制过来。嗯。嗯嗯。嗯。好，首先呢，还是获取一个连环应。execution。因为第2GET。因为。下面的env.S。嗯。在这儿，我们需要去利用这个。Li。卡不卡？three。那这里面啊，传一个topic。嗯。第一，嗯。第二个，你有一个simple。视频，game。第三个pop。嗯嗯。有一个薄。嗯。嗯嗯嗯。首先呢，我们在里面set property。我可以把这个呢直接拿过来。嗯。好，下面呢，said。格布利。卡不卡星本。把它拿过来。嗯。感注释，这个就是指定卡夫卡作为S。嗯。这是指令。普林格卡夫卡consumer的相关配置。接下来呢，我们将读取到的数据啊，一到控制台。嗯。嗯。嗯嗯。嗯。嗯。包的异常。嗯。来把这个启动起来。好，那我们在这边呢，再模拟产生的数据。哈哈哈。没问题吧，是可以的。好，这就是Java代码的一个实现。</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">      env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>() prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304231050885.png" alt="image-20230423105004127"></p><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.kafkaconnector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaSourceJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">        String topic = <span class="string">"t1"</span>;</span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">        prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), prop);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafka作为source</span></span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.addSource(kafkaConsumer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将读取到的数据打印到控制台</span></span><br><span class="line">        text.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"StreamKafkaSourceJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KafkaConsumer消费策略设置"><a href="#KafkaConsumer消费策略设置" class="headerlink" title="KafkaConsumer消费策略设置"></a>KafkaConsumer消费策略设置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对卡夫卡康消费数据的时候会有一些策略，我们来看一下。首先这个是默认的消费策略。下面还有一个ear，从最早的开始消费，latest，从最新的开始消费。已经呢，现在要出来一个C呢。按照指定的时间戳往后面开始消费。下面呢，我们来演示一下。我直接在这里面来设置一下。嗯。卡夫卡consumer。的消费策略设置。这个其实我们在讲卡不卡的时候也详细分解过啊，其实是类似的。首先我们看下这个默认策略。嗯。直接使用它来设置点带。start from group of，注意这个呢，其实就默认你不设置它默就类。它是什么意思呢？它会读取。group ID。对应。保存的outside。开始消费数据。那读取不到的话呢。则根据卡夫卡中。这个参数auto点。reite。参数的值开始。消费数据。因为如果说你是第一次使用这个消费者，那么他之前肯定是没有保存这个对应的office的信息，那这样的话呢，他就会根据这个参数的值来开始进行消费。那这个值的话，它那要么是early latest对吧，要么是从最新的，要么是从最近的。那下一次的话呢，他就会根据你之前指定的这个global ID对应的保存的那个开始往下面继续消费数据。那既然下面这个呢，是从那个最早的记录开始，消费主义啊，不搞consumer there that。from earliest。从最早的记录。开始消费。独具。忽略。你提交。信息。这样的话，他就不管你有没有提交，都会每次都从那个最早的数据开始消费。那对了，还有一个从。最新的记录开始消费。也是忽略这个已提交的奥赛的信息。嗯。start。home latest。嗯。那还有一个是从指定的时间戳开始消费数据。对于每个分区。其时间戳大于或等于指定时间戳的记录。江北。作为70位。嗯。that。大的from。这里面你给他传一个时间戳就行了啊，我这边随便写一个行吗？这就是这几种测量啊。其实我们在讲卡不卡的时候，也详细分析过这几种词，那在那就把这个默认的给它打开吧。就你这个呢，你在这儿设置不设置，它其实都是一个默认的策略。这个呢，就是针对这里面这个卡夫卡抗性板，它这个消费策略的一个设置。其实咱们在工作中啊，一般最常见的，那其实就是一种默认的技术。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304231057297.png" alt="image-20230423105719815"></p><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KafkaConsumer的容错"><a href="#KafkaConsumer的容错" class="headerlink" title="KafkaConsumer的容错"></a>KafkaConsumer的容错</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下kafka consumer的容错。flink中呢也有这个checkpoint机制，checkpoint呢是flink实现容错机制的核心功能，它能够根据配置周期性的基于流中各个算子任务的state来生成快照。从而将这些state的数据定期持久化存储下来。当flink的程序一旦意外崩溃时，重新运行程序时，可以有选择的从这些快照进行恢复。从而修正因为故障带来的程序数据异常。</span><br><span class="line"></span><br><span class="line">当这个checkpoint机制开启的时候。consumer呢？它会定期把kafka的offset的信息，还有其他算子任务的信息一块保存起来。当job失败重启的时候，弗会从最近一次的checkpoint中进行恢复数据。重新消费kafka中的数据。那为了能够使用支持容错的kafka，我们需要开启checkpoint，那如何开启呢？很简单。直接呢，就env.enableCheckpointing，指定一个周期就行。这个5000呢，表示五秒，就是每隔五秒执行一次checkpoint。他会周期性的执行。</span><br><span class="line">来，我们来看一下。我呢，就直接在这儿。来配置啊。en enable check。这毫秒啊。每隔5000毫秒。执行一次checkpoint。这个呢，其实就是设置。这个point的周期啊。那针对这个呢，它还有一些相关的配置。那我接着呢，把这个配置拿过来。把这个复制一下。搞一下包。这个是。针对checkpoint的相关配置。下面这个参数的意思呢？表示设置一下checkpoint的一个语义，它可以提供这种锦一词的语义。下面这个呢，表示两次切之间它的一个时间间隔。这个呢，表示呢，必须要在指定时间之内完成one。其实就是给这个check呢，设置一个超时时间，超过这个时间了就被丢弃了。下面这个呢，表示呢，同一时间只允许执行一个checkpoint。下面这个三注意。他呢表示呀，当我们对这个link程序执行一个cancel之后，就是把这个link程序停掉之后，我们呢，会保留这个这个波段数据，这样的话，我们可以根据实际需要，后期呢来恢复这些数据。这是它相关的一些配置啊，</span><br><span class="line"></span><br><span class="line">那其实呢，在这块还有一个配置。设置这个state数据存储的位置，默认情况下的数据会保存在task manager内存中。当我们执行checkpoint的时候呢，会将这个实际的数据存储到jobmanager内存中。这个具体的存储位置呢，取决于StateBackend的配置。FLink呢，一共提供了三种存储方式，第一种是MemoryStateBackend，第二种呢是FsStateBackend，第三种是RocksSBStateBackend。</span><br><span class="line"></span><br><span class="line">我们先分析一下第一种基于内存的。这个时候呢，state数据保存在这个Java堆内存中，当我们执行checkpoint的时候，它呢会把state快照数据啊保存到job manager的内存中。基于内存的呢，在生产环境下面不建议使用。对吧，因为你重启之后，它内存里面数据就没了，所以说是没有意义的。</span><br><span class="line"></span><br><span class="line">第二种呢，FsStateBackend。数据呢？保存在task measure内存中，当我们执行check point的时候，会把的快照数据保存到配置的文件系统中。我们可以使用hdfs等分布式文件系统。这个呢是可以用的。</span><br><span class="line"></span><br><span class="line">当然还有一种叫RocksSBStateBackend啊，它跟上面的都略有不同，它会在本地文件系统中维护这个state。state的会直接写入本地的RocksDB中。同时它需要配置一个远端的文件系统，一般呢是Hdfs。那我们在做checkpoint的时候。会把本地的数据直接复制到远端的文件系统中。故障切换的时候，直接从远端的文件系统中恢复数据到本地。RocksDB克服了state的受内存限制的缺点，同时又能够持久化到远端文件系统中。推荐在生产环境中使用。</span><br><span class="line"></span><br><span class="line">所以在这里我们使用第三种RocksSBStateBackend</span><br><span class="line">maven添加依赖flink-statebackend-rocksdb</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">      env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106563.png" alt="image-20230423210612299"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106374.png" alt="image-20230423210628262"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106556.png" alt="image-20230423210654942"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232107250.png" alt="image-20230423210749485"></p><h3 id="Kafka-Consumers-Offset自动提交"><a href="#Kafka-Consumers-Offset自动提交" class="headerlink" title="Kafka Consumers Offset自动提交"></a>Kafka Consumers Offset自动提交</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink没有开启checkpoint时，offset的提交由之前的enable.auto.commit和auto.commit.interval.ms决定</span><br><span class="line"></span><br><span class="line">当开启了，由checkpoint每次执行时提交</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232113418.png" alt="image-20230423211307520"></p><h2 id="KafkaProducer的使用"><a href="#KafkaProducer的使用" class="headerlink" title="KafkaProducer的使用"></a>KafkaProducer的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下在flink中如何向kafka中写数据，此时需要用到kafka producer。</span><br><span class="line"></span><br><span class="line">所有的数据都写入指定topic的一个分区里面。注意，他会把所有数据写到这个topic的一个分区。那这样的话，其实呢，在我们实习当中，这样是不合适的啊。我们使用操作的肯定是要使用多个分区，你要把数据分别写到不同的分区里面，这样的话后期我们去消费也可以并行消费，提高消费能力，对吧？那你如果都搞一个分区里面，那其实相当于我这个topic卡就一个分区。这样后期我这个处理能力是有限制的，所以说呢，如果不想自定义分具体。也不想使用默认的可以直接。使用一个null即可。</span><br></pre></td></tr></table></figure><h3 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaProducer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.internals.<span class="type">KafkaSerializationSchemaWrapper</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.partitioner.<span class="type">FlinkFixedPartitioner</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink向Kafka中生产数据 </span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSinkScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启checkpoint</span></span><br><span class="line">    <span class="comment">//env.enableCheckpointing(5000)</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaProducer的相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t2"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为sink</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     KafkaSerializationSchemaWrapper的几个参数</span></span><br><span class="line"><span class="comment">     1：topic：指定需要写入的topic名称即可</span></span><br><span class="line"><span class="comment">     2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中</span></span><br><span class="line"><span class="comment">     默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面</span></span><br><span class="line"><span class="comment">     如果不想自定义分区器，也不想使用默认的，可以直接使用null即可</span></span><br><span class="line"><span class="comment">     3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳</span></span><br><span class="line"><span class="comment">     如果写入了，那么在watermark的案例中，使用extractTimestamp()提起时间戳的时候</span></span><br><span class="line"><span class="comment">     就可以直接使用recordTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> kafkaProducer = <span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">KafkaSerializationSchemaWrapper</span>[<span class="type">String</span>](topic, <span class="literal">null</span>, <span class="literal">false</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()), prop, <span class="type">FlinkKafkaProducer</span>.<span class="type">Semantic</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    text.addSink(kafkaProducer)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSinkScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232135912.png" alt="image-20230423213547295"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232137221.png" alt="image-20230423213734278"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232138202.png" alt="image-20230423213834818"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmak里查看</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232144712.png" alt="image-20230423214449177"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.kafkaconnector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink向Kafka中生产数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaSinkJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定FlinkKafkaProducer相关配置</span></span><br><span class="line">        String topic = <span class="string">"t2"</span>;</span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafak作为sink</span></span><br><span class="line">        FlinkKafkaProducer&lt;String&gt; kafkaProducer = <span class="keyword">new</span> FlinkKafkaProducer&lt;&gt;(topic, <span class="keyword">new</span> KafkaSerializationSchemaWrapper&lt;String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">new</span> SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE);</span><br><span class="line">        text.addSink(kafkaProducer);</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"StreamKafkaSinkJava"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KafkaProducer的容错"><a href="#KafkaProducer的容错" class="headerlink" title="KafkaProducer的容错"></a>KafkaProducer的容错</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下kafkaProducer的容错。如果flink开启了checkpoint，那针对flink kafka producer可以提供仅一次予以保证。我们可以通过这个参数来指定三种不同的语义。</span><br><span class="line">点就是不支持任何语音，这个呢是至少一次，这个呢仅一次，那more呢是至少一次。好，那在代码里面怎么体现呢？来看一下。注意看到没有，我们刚才指的就是一个锦一四啊。但是这个时候注意你还要开启这个table。开启这个了。下面那些参数我暂时就先不指定了，行吗？好，那下面呢，我们来执行一下。来开启这个稍的。好注意这时候呢，给大家看一个比较神奇的现象。你看刚才我们把这个搜下打开啊，结果它停了，你再把它打开。他还会请。看没有？什么原因呢？那时候我这块也没有报错呀。注意这个呢，是因为这个原因。我们之前啊，在这加了一个logo的配置文件，对吧。注意这个日级别，我之前给它改成error。我们把这个调一下。调成那个警告级别。因为这个时候有一些日他没有打出来警告信息看不到啊。来再启动把这个打开。对，他这个其实应该是error级别的，但是他写的什么写的不太好，他把这个日志写成那种warning级别，警告级别的，所以说呢，我们之前使用那个error级别的，监控不到这些日志信息啊。啊，停一下吧。</span><br><span class="line"></span><br><span class="line">来分析一下啊。不要往后面看这。还有什么呀，这个事物时间比这个博客里面配置的这个时间还要大。就是说，生产者中设置的事物超时时间大于卡夫卡博客中设置的事物超时时间。因为卡夫卡服务中默认事物的超时时间是15分钟，但是呢，弗林格卡夫卡保留它里面设置的事物超时间默认是一小时，这个仅一次语义啊，它需要依赖这个事物。如果从Li应用程序崩溃到完全重启的时间超过了卡夫卡的事物超时时间，那么将会有数据丢失，所以我们需要合理的配置事物超时时间。因此，在使用这个仅一次语义之前，建议增加卡夫卡博克中这个transaction.max.timeout.ms的值。把这个值啊给它调大。那下面呢，我们就来修改一下卡夫卡里面这个配置，这个配置在哪啊，其实就那个server.properties里面啊。买那个可以试一下。它里面是没有这个参数的，你直接在这把它拿过来。给它做个值。我们也给它改成一小时吧。这个你转换成毫秒是3600000。那么是五个零啊，这样的话就一小时。把这个复制一下。对，这个集群里面所有机器都要改啊。嗯。好，可以了，注意改完之后我们需要重写。那你先把这个卡夫卡集群停掉。好停掉之后再去启动，启动的话，我们使用它这个命令啊。前面加了一个GMX，这样的话我们可以使那个CMA来减轻它里面一些信息啊。嗯。好，这个起来了。嗯。嗯。这个呢也可以啊。好，这个也可以了。那接下来我们重新再执行这个样本，对吧，把这个再看一下。嗯。看到没有，此时他就不报错了啊，我们可以在这来验证一下，先确一下里面的数据对吧。是这样。5211。这个停了，因为刚才我们把那个卡夫卡停掉之后啊，这个c map哎，就停掉了。把它起来。所以这个没不对。嗯。嗯。第三。对吧，这里面是这了来。我们输点作业。好变了吧，对吧。说明这个数据写进来了，并且这块呢也没报错啊。OK，这样就可以了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232152824.png" alt="image-20230423215255115"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line">import java.util.Properties</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Flink向Kafka中生产数据 </span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object StreamKafkaSinkScala &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;开启checkpoint</span><br><span class="line">    env.enableCheckpointing(5000)</span><br><span class="line">      </span><br><span class="line">    val text &#x3D; env.socketTextStream(&quot;bigdata04&quot;, 9001)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;指定FlinkKafkaProducer的相关配置</span><br><span class="line">    val topic &#x3D; &quot;t2&quot;</span><br><span class="line">    val prop &#x3D; new Properties()</span><br><span class="line">    prop.setProperty(&quot;bootstrap.servers&quot;,&quot;bigdata01:9092,bigdata02:9092,bigdata03:9092&quot;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;指定kafka作为sink</span><br><span class="line">    &#x2F;*</span><br><span class="line">     KafkaSerializationSchemaWrapper的几个参数</span><br><span class="line">     1：topic：指定需要写入的topic名称即可</span><br><span class="line">     2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中</span><br><span class="line">     默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面</span><br><span class="line">     如果不想自定义分区器，也不想使用默认的，可以直接使用null即可</span><br><span class="line">     3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳</span><br><span class="line">     如果写入了，那么在watermark的案例中，使用extractTimestamp()提起时间戳的时候</span><br><span class="line">     就可以直接使用recordTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp</span><br><span class="line">     *&#x2F;</span><br><span class="line">    val kafkaProducer &#x3D; new FlinkKafkaProducer[String](topic, new KafkaSerializationSchemaWrapper[String](topic, null, false, new SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE)</span><br><span class="line">    text.addSink(kafkaProducer)</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;StreamKafkaSinkScala&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-1.html</id>
    <published>2023-04-20T08:44:51.000Z</published>
    <updated>2023-04-24T07:32:38.535Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十七周-Flink极速上手篇-Flink高级进阶之路-1"><a href="#第十七周-Flink极速上手篇-Flink高级进阶之路-1" class="headerlink" title="第十七周 Flink极速上手篇-Flink高级进阶之路-1"></a>第十七周 Flink极速上手篇-Flink高级进阶之路-1</h1><h2 id="Window的概念和类型"><a href="#Window的概念和类型" class="headerlink" title="Window的概念和类型"></a>Window的概念和类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">大家好，前面我们学习了flink中的基本概念，集群部署以及核心API的使用，下面我们来学习一下flink中的高级特性的使用。首先，我们需要掌握中的window、time以及whatermark使用。然后我们需要掌握kafka-connector使用，这个是针对kafka一个专题。最后我们会学习一下Spark中的流式计算sparkStreaming，之前在学习spark的时候我们没有涉及这块，在这儿我们和flink一块来学习，可以加深理解，因为它们都是流式计算引擎。</span><br><span class="line"></span><br><span class="line">下面呢，我们首先进入第一块flink中的window和time。flink认为批处理是流处理的一个特例，所以flink底层引擎是一个流式引擎，这上面呢实现了流处理和批处理。而window呢，就是从流处理到批处理的一个桥梁。通常来讲啊，这个window啊，是一种可以把无界数据切割为有界数据块的手段，例如对流动的所有元素进行计数是不可能的，因为通常流是无限的。或者呢，可以称之为是无界了。所以说流上的聚合需要由window来划分范围，比如计算过去五分钟或者最后100个元素的和。</span><br><span class="line"></span><br><span class="line">window可以是以时间驱动的time window，例如每30秒，或者是以数据驱动的count window，例如每100个元素。DataStream API提供了基于time和count的window。同时，由于某些特殊的需要，dataStreamAPI也提供了定制化的window操作，供用户自定义window。</span><br><span class="line"></span><br><span class="line">这个window呀，根据类型可以分为这两种。第一种是滚动窗口，它呢表示窗口内的数据没有重叠，第二种呢是滑动窗口，它呢表示窗口内的数据有重叠。</span><br><span class="line"></span><br><span class="line">那下面我们来看个图分析一下，首先看这个滚动窗口，这个S轴呢是一个时间轴，你看这个是一个窗口的大小，这是WINDOW1 window2 window3，注意每个窗口内的数据是没有重叠的，这个就是滚动窗口。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201704808.png" alt="image-20230420170417289"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面来看这个滑动窗口，这个S轴呢，还是一个时间轴，你看这个是一个window的大小。这个表示是一个window的滑动间隔，这是WINDOW1这个红色的，它这个窗口从这到这儿，下面这个呢，WINDOW2，注意这个窗口它是从这儿到这儿，这个蓝色的看到没有，它里面呢，包含了WINDOW1里面的一部分数据。那你看WINDOW3 window3里面它包含了WINDOW2里面的一部分数据，所以说这个滑动窗口，它们每个窗口之间呀，会有数据重叠，这个就这两种窗口它的一个区别</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201705935.png" alt="image-20230420170515739"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我针对这个窗口的类型做了一个汇总。你看这是window window下面有time window有count window还有自定义window，那这些window再往下面你看它呢，可以实现滚动窗口或者滑动窗口，对吧？不管你是基于time的，还是基于count的，还是自定义的，你们都可以实现滚动窗口或者是滑动窗口。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201707926.png" alt="image-20230420170745169"></p><h3 id="TimeWindow的使用"><a href="#TimeWindow的使用" class="headerlink" title="TimeWindow的使用"></a>TimeWindow的使用</h3><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这些window的具体应用，首先来看第一个time window。time window呢是根据时间对数据流切分窗口，time window可以支持滚动窗口和滑动窗口。</span><br><span class="line"></span><br><span class="line">其中它有这么两种用法，来看一下time window。</span><br><span class="line">timeWindow(Time.seconds(10))</span><br><span class="line">注意，首先这个。他呢是表示。滚动窗口的窗口大小为十秒。对每十秒内的数据,进行聚合计算。这个呢，其实就是设置一个滚动窗口。</span><br><span class="line"></span><br><span class="line">timeWindow(Time.seconds(10),Time.seconds(5))</span><br><span class="line">那下面这个呢，对应的它设置的就是一个滑动窗口，因为它除了有一个窗口大小，它还滑动一个间隔。表示滑动窗口的窗口大小为十秒,滑动间隔为五秒,就是每隔五秒计算前十秒内的数据，所以说是两种用法，一种是滚动，一种是滑动。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TimeWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TimeWindowOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TimeWindow之滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    <span class="comment">/*text.flatMap(_.split(" "))</span></span><br><span class="line"><span class="comment">      .map((_,1))</span></span><br><span class="line"><span class="comment">      .keyBy(0)</span></span><br><span class="line"><span class="comment">      //窗口大小</span></span><br><span class="line"><span class="comment">      .timeWindow(Time.seconds(10))</span></span><br><span class="line"><span class="comment">      .sum(1)</span></span><br><span class="line"><span class="comment">      .print()*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TimeWindow之滑动窗口：每隔5秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">        .timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>),<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print()</span><br><span class="line">    env.execute(<span class="string">"TimeWindowOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeWindow滚动窗口</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201729458.png" alt="image-20230420172901278"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201729476.png" alt="image-20230420172914398"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeWindow滑动窗口，黑色第一次输入，蓝色第二次输入</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201732506.png" alt="image-20230420173241489"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第三次打印，蓝色</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201736543.png" alt="image-20230420173625064"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TimeWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeWindowOpJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//TimeWindow之滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        <span class="comment">/*text.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">            @Override</span></span><br><span class="line"><span class="comment">            public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span><br><span class="line"><span class="comment">                    throws Exception &#123;</span></span><br><span class="line"><span class="comment">                String[] words = line.split(" ");</span></span><br><span class="line"><span class="comment">                for (String word: words) &#123;</span></span><br><span class="line"><span class="comment">                    out.collect(new Tuple2&lt;String, Integer&gt;(word,1));</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;).keyBy(0)</span></span><br><span class="line"><span class="comment">                //窗口大小</span></span><br><span class="line"><span class="comment">                .timeWindow(Time.seconds(10))</span></span><br><span class="line"><span class="comment">                .sum(1)</span></span><br><span class="line"><span class="comment">                .print();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//TimeWindow之滑动窗口：每隔5秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word: words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">10</span>),Time.seconds(<span class="number">5</span>))</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"TimeWindowOpJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CountWindow的使用"><a href="#CountWindow的使用" class="headerlink" title="CountWindow的使用"></a>CountWindow的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下count的Window的使用，count Window是根据元素个数对数据流切分窗口。count window也可以支持滚动窗口和滑动窗口。</span><br><span class="line">countWindow(5)表示滚动窗口的大小,是五个元素。也就是当窗口中填满五个元素的时候，就会对窗口进行计算</span><br><span class="line">countWindow(5,1)</span><br><span class="line">表示滑动窗口的窗口大小是五个元素，滑动的间隔为一个元素，也就是说每新增一个元素就会对前面五个元素计算一次</span><br><span class="line"></span><br><span class="line">那我们再验证一下。有有有。写四个啊。看到没有，这个又要直行了。对吧，你后面再加哈，注意。此时呢，它就不会再执行了，因为你是一个滚动窗口啊，最终呢，你再满足有五个元素之后，它才会重新执行，这是一个滚动窗口。把这个听一下。把这个注意事项啊，我们给它加进来，这是一个解释啊。由于我们在这里使用了可以。会相对数据分组。如果某个分组对应的数据窗口，数据窗口内达到了五个元素，这个窗口才会被主发执行，如果你不使用KPI的话，他就不会在这儿做区分了，所以他接收到所有的数据，在这儿会统一计算。不过那个时候你就需要使用这个count window or这个咱们后面再分析啊，接着我们先使用这个K方式，后面呢直接使用这个count window，好，这是一个滚动窗口，下面呢，我们来实现一个滑动窗口。它的豌豆之滑动窗口。每隔一个元素计算一次前五个元素。map。空格切一下。小点一。零零，它的window，注意第一个参数是窗口大小，第二个是滑动间隔。嗯。窗口大小。第二个参数。滑动间隔。一。BA。好，那接着要把上面这个的读调，嗯。把这个socket呢，再给它打开。嗯。好，那我们到这儿来数数句，hello you。注意它直行了，为什么呀，因为它的滑动间隔是一，只要间隔一个元素，它就会执行，它呢会往前推找五个元素，但是它前面并没有五个元素，就只有这一个，所以说最终的结果呢，就是这样好。那下面呢，我继续往里面添加元素。hello，你。看那个效果，看到没有，hello就两次了，me是一次对吧，hello已经变成两次了，那下面我们还按照刚才这个逻辑。hello，加三次，你看加三次，它其实最终呢，输出了三条如玉，这次是三，这次是四，这次是五。没问题吧，因为你新增一条数据，它就会往前推五条数据去统计。嗯。看到没有2345。这也是可以的啊，然后再加个什么，hello。还是50。you。为什么一直是五次呢？因为它只会往前面统计五个元素啊。好，这就滑动窗口，下面我们来使用Java代码来实现一下。放着window。op加。嗯。嗯嗯。先获取一个环境。get。嗯。嗯。嗯。看window。直滚动方口。每隔五个元素计算一次。前五个元素。加个小碟red map，你有一个red map。注意我们在这呢，还把这个map和map它这个逻辑整合一块，说输入是词频输出是。in。嗯。来。慢点，split。不了。我。在这个。嗯。WORD1。对吧，这样看一下后面一个a。零。嗯。放了window。五。嗯嗯。some。嗯。这个是窗口大小。好，接下来讲第二个把这个注释呢，从这复制一下吧。所以这是每格啊。嗯。好，这个前面啊，其实都一样啊，只有一个地方不一样，对吧。就是把这个放到温度这块，给它改一下就行。和两个参数，嗯。第一个参数窗口大小，第二个参数。滑动间隔。嗯嗯。因为一点。嗯。顺便抛个异常。好，这就可以了，在这我们可以助调一个。验证一下这个滑动窗口。赶紧回来把这个打开。OK。好。没问题吧，没问题啊。这就是Java代码，实现这个count window。</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CountWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CountWindowOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意：由于我们在这里使用keyBy，会先对数据分组</span></span><br><span class="line"><span class="comment">     * 如果某个分组对应的数据窗口内达到了5个元素，这个窗口才会被触发执行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//CountWindow之滚动窗口：每隔5个元素计算一次前5个元素</span></span><br><span class="line">    <span class="comment">/*text.flatMap(_.split(" "))</span></span><br><span class="line"><span class="comment">      .map((_,1))</span></span><br><span class="line"><span class="comment">      .keyBy(0)</span></span><br><span class="line"><span class="comment">      //指定窗口大小</span></span><br><span class="line"><span class="comment">      .countWindow(5)</span></span><br><span class="line"><span class="comment">      .sum(1)</span></span><br><span class="line"><span class="comment">      .print()*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//CountWindow之滑动窗口：每隔1个元素计算一次前5个元素</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">        .countWindow(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print()</span><br><span class="line">    env.execute(<span class="string">"CountWindowOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201756278.png" alt="image-20230420175629114"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201756533.png" alt="image-20230420175643559"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以我在这再输三个，看到没有，到这儿才刚开始执行了一次，他把这个hello打印出来五个。那这个you和me为什么没有打印呢？注意了，所以啊，我们在这啊执行了keyby会对这个数据进行分组，如果某个分组对应的数据窗口内达到了五个元素，这个窗口才会被处罚执行，所以说这个时候相当于是hello对应的那个窗口，它里面够五个元素了，它才会执行。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201758363.png" alt="image-20230420175819357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201758141.png" alt="image-20230420175806865"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">看一下count滑动窗口执行结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801677.png" alt="image-20230420180125565"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801117.png" alt="image-20230420180113979"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801863.png" alt="image-20230420180150478"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201802271.png" alt="image-20230420180220305"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201803247.png" alt="image-20230420180355733"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201805077.png" alt="image-20230420180527334"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201806314.png" alt="image-20230420180609855"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201806384.png" alt="image-20230420180630486"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CountWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowOpJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//CountWindow之滚动窗口：每隔5个元素计算一次前5个元素</span></span><br><span class="line">        <span class="comment">/*text.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">            @Override</span></span><br><span class="line"><span class="comment">            public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span><br><span class="line"><span class="comment">                    throws Exception &#123;</span></span><br><span class="line"><span class="comment">                String[] words = line.split(" ");</span></span><br><span class="line"><span class="comment">                for (String word : words) &#123;</span></span><br><span class="line"><span class="comment">                    out.collect(new Tuple2&lt;String, Integer&gt;(word,1));</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;).keyBy(0)</span></span><br><span class="line"><span class="comment">                //窗口大小</span></span><br><span class="line"><span class="comment">                .countWindow(5)</span></span><br><span class="line"><span class="comment">                .sum(1)</span></span><br><span class="line"><span class="comment">                .print();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//CountWindow之滑动窗口：每隔1个元素计算一次前5个元素</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">                .countWindow(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"CountWindowOpJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自定义Window的使用"><a href="#自定义Window的使用" class="headerlink" title="自定义Window的使用"></a>自定义Window的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下自定义window。其实呢，window还可以再细分一下。可以把它分为呢，一种是基于Key的window。一种是不基于Key的window。其实就是说咱们在使用window之前是否执行了key操作啊，咱们前面演示的都是这种基于Key的window。你看我们在做window之前，前面呢都做了Keyby对吧，那如果呢，需求中不需要根据Key进行分组，你在使用window的时候啊，我们需要对应的去使用那个timeWindowAll和countWindowAll。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201829109.png" alt="image-20230420182825536"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">你使用KeyBy之后的话，它就只能调那个timeWindow,countWindow，这个需要注意一下啊，那如果说是我们自定义的window。如何使用呢？对吧，针对这两种情况。来看一下。针对这个基于Key的window呀，我们需要使用这个window函数</span><br><span class="line"></span><br><span class="line">那针对下面这种不基于Key的window呢，我们可以直接使用这个windowAll就可以了。其实呀，我们前面所说的那个timewindow和timewindowall底层用的就是这个window和windowall，你可以这样理解timewindow是官方封装好的window。所以说呢，timewindow和countwindow呢，都是官方封装好了。</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingProcessingTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：自定义MyTimeWindow</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyTimeWindowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//自定义MyTimeWindow滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_,<span class="number">1</span>))</span><br><span class="line">      .keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//窗口大小</span></span><br><span class="line">  .window(<span class="type">TumblingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)))<span class="comment">//注意这里和后面的基于eventtime计算有点不一样</span></span><br><span class="line">      .sum(<span class="number">1</span>)</span><br><span class="line">      .print()</span><br><span class="line">    env.execute(<span class="string">"MyTimeWindowScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个呢，和咱们之前啊使用的什么timewindow那个效果是一样的。这样的话更加灵活一些，我们想怎么定义都可以啊。如果你不使用这个KeyBy的话，那下面你就可以使用windowAll是一样的效果</span><br></pre></td></tr></table></figure><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：自定义MyTimeWindow</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTimeWindowJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自定义MyTimeWindow滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//窗口大小</span></span><br><span class="line">                .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"MyTimeWindowJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Window中的增量聚合和全量聚合"><a href="#Window中的增量聚合和全量聚合" class="headerlink" title="Window中的增量聚合和全量聚合"></a>Window中的增量聚合和全量聚合</h3><h4 id="增量聚合"><a href="#增量聚合" class="headerlink" title="增量聚合"></a>增量聚合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下Window聚合。就是在进行Window聚合操作的时候呢，可以分为两种情况。一种呢是增量聚合，还有一种是全量聚合。</span><br><span class="line"></span><br><span class="line">那下面我们首先来看一下这个增量聚合。增量聚合呢，它表示呀，窗口中每进入一条数据就进行一次计算，常见的一些增量聚合函数如下:</span><br><span class="line">reduce() aggregate() sum() min() max()</span><br><span class="line"></span><br><span class="line">那下面呢，我们来看一个增量聚合的案例啊，就是累加求和</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202150610.png" alt="image-20230420215009349"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">它的具体执行过程是这样的。第一次进来一条数据，则立刻进行累加，求和结果为八，第二次进来一条数据12，则立刻进行累加，求和结果为20。第三次进来一条数据七，则立刻进行累加求和，结果为27。第四次进来一条数据，则立刻进行累加求和，结果为37。这就是这个增量聚合它的一个执行流程。</span><br><span class="line"></span><br><span class="line">那下面呢，我们来看一下reduce函数的一个使用，从这里面我们可以看出来，reduce是每次获取一条数据和上一次的执行结果求和。也就是来一条数据，立刻计算一次，这个就是增量聚合。</span><br></pre></td></tr></table></figure><h4 id="全量聚合"><a href="#全量聚合" class="headerlink" title="全量聚合"></a>全量聚合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们来看一下全量集合。全量集合呀，它就是等属于窗口的数据都到齐了，才开始进行聚合计算，可以实现对窗口内的数据进行排序等需求。常见的一些全量聚合函数为：</span><br><span class="line">apply(windowFunction)，还有这个process(processWindowFunction)</span><br><span class="line">apply呢，它里面接触的是windowfunction,process里面接触是process windowfunction</span><br><span class="line">注意这个processwindowfunction比windowfunction提供了更多的上下文信息啊。那下面呢，我们来看一个全量聚合的一个案例，求最大值</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202157070.png" alt="image-20230420215701906"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第四次进来调数据10，此时窗口触发，这时候才会对窗口内的数据进行排序，然后获取最大值。</span><br></pre></td></tr></table></figure><h5 id="全量聚合apply"><a href="#全量聚合apply" class="headerlink" title="全量聚合apply"></a>全量聚合apply</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202158547.png" alt="image-20230420215834620"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这个apply函数的一个使用。从这你可以看出来，他接触的是一个iterable，可以认为是一个集合。他可以把这个窗口的数据啊，一次性全都传过来，当这个窗口触发的时候，才会真正执行这个代码。</span><br></pre></td></tr></table></figure><h5 id="全量聚合process"><a href="#全量聚合process" class="headerlink" title="全量聚合process"></a>全量聚合process</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202200468.png" alt="image-20230420220026246"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢是一个process。你看他接触的也是一个iterable，所以说呢，你在这里面就可以获取到这个窗口里面的所有数据了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个呢就是Windows中的全量聚合和增量聚合，后面呢我们就会用到这个apply，还有process它的一个使用，因为有时候我们需要对这个窗口内的所有数据去做一些全量的操作，这样的话就不能用这种增量聚合，而要用这种全量聚合。</span><br></pre></td></tr></table></figure><h3 id="Flink中的Time"><a href="#Flink中的Time" class="headerlink" title="Flink中的Time"></a>Flink中的Time</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对流数据的time可以分为以下三种。第一个Event Time表示事件产生的时间，它通常由事件中的时间戳来描述。第二个ingestion time表示事件进入flink的时间。第三个processing time，它表示事件被处理时当前系统的时间，那这几种时间呀，我们通过这个图可以很清晰的看出来它们之间的关系。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202206362.png" alt="image-20230420220641443"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">首先是even time，这个就是数据产生的时间。第二个是ingestion time表示呢，他进入flink时间，其实就是被那个source把它读取过来那个时间。第三个呢，是这个processing time，它其实呢，就是flink里面具体的算子，在处理的时候它的一个时间，那接下来我们来看一个案例。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202209327.png" alt="image-20230420220902227"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意你看数据呢，是在十点的时候产生的。结果呢，在晚上八点的时候才被flink读取走。那flink真正在处理的时候呢？是8.02秒。</span><br><span class="line">注意，如果说呀，我们想要统计每分钟内接口调用失败的错误日志个数。那这个时候使用哪个时间才有意义呢？因为数据有可能会出现延迟。如果使用那个数据进入flink的时间或者window处理的时间，其实是没有意义的。这个时候我们需要使用原始日中的时间才是有意义的，这个才是数据产生的时间，我们基于这个时间去统计才有意义。</span><br><span class="line">那我们在flink流水中默认使用的是哪个时间呢？某种情况下，flink在流处理中使用的时间是这个processingtime。那如果说我们想要修改的话，怎么改呢？可以使用这个env去改env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)可以设置这个time或者是这个IngestionTime。好，这就是flink中的三种time。</span><br></pre></td></tr></table></figure><h3 id="Watermark的分析"><a href="#Watermark的分析" class="headerlink" title="Watermark的分析"></a>Watermark的分析</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211000567.png" alt="image-20230421100013693"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实时计算中，数据时间比较敏感。有eventTime和processTime区分，一般来说eventTime是从原始的消息中提取过来的，processTime是Flink自己提供的，Flink中一个亮点就是可以基于eventTime计算，这个功能很有用，因为实时数据可能会经过比较长的链路，多少会有延时，并且有很大的不确定性，对于一些需要精确体现事件变化趋势的场景中，单纯使用processTime显然是不合理的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面提到了Time的概念，如果我们使用Processing Time，那么在Flink消费数据的时候，它完全不需要关心数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为Processing Time只是代表数据在Flink被处理时的时间，这个时间是顺序的。</span><br><span class="line">但是如果你使用的是Event Time的话，那么你就不得不面临着这么个问题：事件乱序&amp;事件延迟。</span><br><span class="line"></span><br><span class="line">所以…</span><br><span class="line">为了解决这个问题，Flink中引入了WaterMark机制，即水印的概念。、</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211008547.png" alt="image-20230421100850143"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">然而在有些场景下，尤其是特别依赖于事件时间而不是处理时间，比如：</span><br><span class="line">错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</span><br><span class="line">设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</span><br><span class="line">比如我做过的充电桩实时报文分析，就必须依赖报文产生的时间，即事件时间</span><br><span class="line">…</span><br><span class="line">针对上面的问题（事件乱序 &amp; 事件延迟），Flink引入了Watermark 机制来解决。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">统计8:00 ~ 9:00这个时间段打开淘宝App的用户数量，Flink这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在8:00 ~ 9:00中用户打开 App的事件数据，但又不能无限期的等下去？</span><br><span class="line"></span><br><span class="line">当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是Watermark的思想。</span><br><span class="line"></span><br><span class="line">Watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的Watermark。Watermark本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有Watermark大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink也有相应的机制（下文会讲）去处理。</span><br></pre></td></tr></table></figure><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个timestamp.watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现。</span><br><span class="line"></span><br><span class="line">流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（out-of-order或者说late element）。</span><br><span class="line"></span><br><span class="line">但是对于late element，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">比如：</span><br><span class="line">08:00任务开启，设置1分钟的滚动窗口，在08:00:00-08:01:00为第一个窗口，08:01:00-08:02:00为第二个窗口；</span><br><span class="line">现在有一条数据的事件时间是08:00:50，但是这条数据却在08:01:10到达，按照正常的处理，窗口会在结束时间（08:01:00）的时候就触发计算，那么这条数据就会被丢弃；</span><br><span class="line">但是开启WaterMark后，窗口在08:01:00时不会触发；</span><br><span class="line">因为采用的是EventTime，而数据本身时间是08:00:50，所以该条数据肯定会落到第一个窗口；</span><br><span class="line">假设在08:01:10时的WaterMark为08:01:00（WaterMark可以理解为一个时间戳），发现这个WaterMark和第一个窗口的结束时间相等，此时触发第一个窗口的计算操作，此时这条延迟数据正好参与到计算中；</span><br><span class="line">此时只有水印大于或等于窗口结束时间才会触发窗口的关闭和计算；</span><br><span class="line">此时就不会丢数据。</span><br></pre></td></tr></table></figure><h4 id="WaterMark的传递"><a href="#WaterMark的传递" class="headerlink" title="WaterMark的传递"></a>WaterMark的传递</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Watermark在向下游传递时，是广播到下游所有的子任务中，如果多并行度下有多个watermark传递到下游时，取最小的watermark。</span><br></pre></td></tr></table></figure><h4 id="WaterMark设置"><a href="#WaterMark设置" class="headerlink" title="WaterMark设置"></a>WaterMark设置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注：如果你采用的是事件时间，即你设置了 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">那么你就必须设置获取事件时间的方法，否则会报错（如果是从kafka消费数据，不设置水印的话，默认采用kafka消息自带的时间戳作为事件时间）</span><br><span class="line"></span><br><span class="line">数据处理中需要通过调用DataStream中的 assignTimestampsAndWatermarks方法来分配时间和水印，该方法可以传入两种参数，一个是AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。</span><br><span class="line">所以设置Watermark是有如下两种方式：</span><br><span class="line"></span><br><span class="line">AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime都会产生一个Watermark。</span><br><span class="line">AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实际生产中用第二种的比较多，它会周期性产生Watermark的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时。</span><br></pre></td></tr></table></figure><h3 id="开发Watermark代码"><a href="#开发Watermark代码" class="headerlink" title="开发Watermark代码"></a>开发Watermark代码</h3><h4 id="乱序数据处理"><a href="#乱序数据处理" class="headerlink" title="乱序数据处理"></a>乱序数据处理</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了what mark的一些基本原理，可能大家对它还不够了解，下面我们来通过这个案例加深大家对what mark的理解。我们来分析一下这个案例。乱序数据处理</span><br><span class="line"></span><br><span class="line">通过socket模拟数据。数据的格式是这样的。前面的话代表的是具体的业务数据，后边的话是一个时间戳，这是一个毫秒的时间戳。中间用逗号分隔。</span><br><span class="line"></span><br><span class="line">其中，时间戳是数据产生的时间。也就是even time。那产生这个数据之后呢？然后使用map函数，把数据转换为tuple2的形式。接着再调用这个函数assignTimestampsAndWatermarks。使用这个方法来抽取timestamp并生成watermark。</span><br><span class="line">接着，再调用window打印信息，来验证window被触发的时机。最后验证乱序数据的处理方式，这是我们一个大致的一个处理流程。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">      env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳(EventTime)和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>) <span class="comment">// currentMaxTimstamp它的第一个参数值应该是传错了</span></span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样(这里和前面自定义window时，传的参数有点不一样，这里是event)</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>],<span class="type">String</span>,<span class="type">Tuple</span>,<span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup=&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr+<span class="string">","</span>+arr.length+<span class="string">","</span>+sdf.format(arr.head)+<span class="string">","</span>+sdf.format(arr.last)+<span class="string">","</span>+sdf.format(window.getStart)+<span class="string">","</span>+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="根据数据跟踪观察Watermark"><a href="#根据数据跟踪观察Watermark" class="headerlink" title="根据数据跟踪观察Watermark"></a>根据数据跟踪观察Watermark</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211425983.png" alt="image-20230421142543318"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211428308.png" alt="image-20230421142830892"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211435405.png" alt="image-20230421143511791"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211438988.png" alt="image-20230421143817886"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211440736.png" alt="image-20230421144017493"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211441254.png" alt="image-20230421144059007"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211441192.png" alt="image-20230421144132566"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211442566.png" alt="image-20230421144203353"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到这里，window仍然没有被触发，此时watermark的时间已经等于第一条数据的eventtime了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211445272.png" alt="image-20230421144516628"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211445660.png" alt="image-20230421144534532"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">window仍然没有被触发，此时，我们数据已经发送到2026-10-01 10:11:33了，根据eventtime来算，最早的数据已经过去了11s了，window还没开始计算，那到底什么时候会触发window呢？</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211450426.png" alt="image-20230421145025475"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211451804.png" alt="image-20230421145103798"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211452915.png" alt="image-20230421145208979"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到这里，我们做了一个说明。</span><br><span class="line">window的触发机制，是先按照自然时间将window划分，如果window大小是3s，那么1min内会把window划分成如下的形式(左闭右开的区间)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211459275.png" alt="image-20230421145940525"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211500839.png" alt="image-20230421150008692"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">window的设定无关数据本身，而是系统定义好了的。</span><br><span class="line">输入的数据，根据自身的eventtime，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;&#x3D;eventtime时，就符合了window触发的条件了，最终决定window触发，还是由eventtime所属window中的window_end_time决定。</span><br><span class="line"></span><br><span class="line">上面的测试中，最后一条数据到达后，其水位线(watermark)已经上升至10:11:24，正好是最早的一条记录所在window的window_end_time，所以window就被触发了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211511116.png" alt="image-20230421151156542"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211513051.png" alt="image-20230421151309893"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时，watermark时间虽然已经等于第二条数据的时间，但是由于其没有达到第二条数据所在window，但是由于其没有达到第二条数据所在window的结束时间，所以window并没有被触发。那么，第二条数据所在的window时间区间如下。 </span><br><span class="line">[00:00:24,00:00:27)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">也就是说，我们必须输入一个10:11:37的数据，第二条数据所在的window才会被触发，我们继续输入。</span><br><span class="line">0001,1790820697000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211520479.png" alt="image-20230421152050886"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时，我们已经看到，window的触发条件要符合以下几个条件：</span><br><span class="line">1.watermark时间&gt;&#x3D;wind_end_time</span><br><span class="line">2.在[window_start_time,window_end_time)区间中有数据存在(注意是左闭右开的区间)</span><br></pre></td></tr></table></figure><h3 id="Watermark-EventTime处理乱序数据"><a href="#Watermark-EventTime处理乱序数据" class="headerlink" title="Watermark+EventTime处理乱序数据"></a>Watermark+EventTime处理乱序数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我们前面那个测试啊，数据呢，都是按照这个时间顺序递增的，都是有序的，那现在呢，我们来输入一些的数据，来看看这个whatmark，结合这个一的eventtime机制是如何处理这些乱写数据的。那我们在上面那个基础之上啊，再输入两行数据。</span><br><span class="line">注意这个呢，没有触发对吧</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211904910.png" alt="image-20230421190455535"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211905517.png" alt="image-20230421190548292"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们再输入一条43秒的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211916109.png" alt="image-20230421191608587"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211919959.png" alt="image-20230421191923374"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211920175.png" alt="image-20230421192030363"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">大家注意他没有这个33的。我这个窗口呢，是[30,33)，你看我这个一的代表里面数据是有这个33的，为什么这个33没有输出呢。因为这个窗口啊，它是一个左闭右开的。那这个33的话，它其实啊，属于下一个窗口，就是33到36的那个窗口。</span><br><span class="line">好。所以上面这个结果其实已经表明对迟到的数据了，flink可以通过这个watermark来实现处理一定范围内的乱序数据。因为现在我们允许的最大乱序时间是十秒。就是十秒之内乱序是OK的，那如果超过了这个十秒怎么办？也就是说呢，对于这个迟到(late element)太久的数据，flink是怎么处理的呢？</span><br></pre></td></tr></table></figure><h3 id="延时数据的三种处理方式"><a href="#延时数据的三种处理方式" class="headerlink" title="延时数据的三种处理方式"></a>延时数据的三种处理方式</h3><h4 id="丢弃"><a href="#丢弃" class="headerlink" title="丢弃"></a>丢弃</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们就来看一下针对迟到太久的数据，它的一些处理方案，现在呢一共有三种。</span><br><span class="line">第一种是丢弃默认的啊，那我们再来演示一下。那我们首先呢，来输入一个乱序很多的数据来测试一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221025938.png" alt="image-20230422102536854"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意，下面呢，我们再来输入几个一定的eventtime小于whatmark的时间</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221027933.png" alt="image-20230422102748626"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">看到没有，这三条他都没有触发这个窗口的执行啊，因为你现在你输入的数据所在的窗口已经执行过了。flink默认对这些迟到的数据的处理方案就是丢弃。这几条数据，30对应的那个窗口数据是不是已经执行过了呀，那这样过来它直接丢弃，这是默认的一个处理方案。</span><br></pre></td></tr></table></figure><h4 id="allowedLateness"><a href="#allowedLateness" class="headerlink" title="allowedLateness"></a>allowedLateness</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那接下来看第二种，你可以通过这个allowedLateness来指定一个允许数据延迟的时间本身啊，我们之前通过那个watermark已经设置了一个数据的延迟时间是十秒，对吧。你可以通过这个参数啊，再给他指定一个延迟时间，就类似于我们上班打卡官方延迟对吧，类似于公司统一层面允许大家呢弹性半小时。但是你们这个部门呢，可以再多谈十分钟，有这种效果。</span><br><span class="line"></span><br><span class="line">在某些情况下，我们希望对迟到的数据再提供一个宽容时间。那flink提供了这个方法，可以实现对迟到的数据啊，再给它设置一个延迟时间，在指定延迟时间内到达数据还是可以触发window执行的。所以这时候我们需要去改一下代码了。主要呢，就增加这一行就行。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpForAllowedLatenessScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//允许数据迟到2秒</span></span><br><span class="line">      .allowedLateness(<span class="type">Time</span>.seconds(<span class="number">2</span>))</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>],<span class="type">String</span>,<span class="type">Tuple</span>,<span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup=&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr+<span class="string">","</span>+arr.length+<span class="string">","</span>+sdf.format(arr.head)+<span class="string">","</span>+sdf.format(arr.last)+<span class="string">","</span>+sdf.format(window.getStart)+<span class="string">","</span>+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221044535.png" alt="image-20230422104446863"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221047820.png" alt="image-20230422104738555"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们再输入几条二苯酮 time小于watermark的一个数据来验证一下效果。好，来看一下。注意你会发现，你看。这三条数据过来的时候，窗口同样被触发了，因为之前的话，我们是这个30到33这个窗口对吧。我在这输的这三条数据，一个是30秒了，31、32，它们都属于那个窗口。岁数，你看窗口都被吃光。你看这时候打印的窗口数据是两条，这是三条，这是四条对吧。所以说呢，每条数据都触发了window的执行啊。这三条数据。那下面我们再输一条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221050530.png" alt="image-20230422105039949"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221058268.png" alt="image-20230422105851955"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221100903.png" alt="image-20230422110036594"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这时候啊，我们把这个whatmark呀，给它调到34。往上面调一下。看到没有，这次呢，它是没有触发的啊是34。数据呢是44，这样的话，whatmark变成了34。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221103748.png" alt="image-20230422110328208"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221105374.png" alt="image-20230422110533985"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时呢，把whatmark上升到了34。此时呢，我们再输入几条这种迟到的数据来验证一下效果。因为刚才的话，我们验证了它是可以执行的啊。嗯。结果你会发现，看到没有，这三条又执行了。我们发现数的这三条数据呢，它都触发了window执行。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221107423.png" alt="image-20230422110715967"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221108095.png" alt="image-20230422110824496"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们再输入一行数据，把这个我的妈再调一下，调到35。嗯。对吧，这是刚才调到35。给你输入一下45的数据，我慢了变成35，我把这个清一下。这时候它就上升到了35，</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221112669.png" alt="image-20230422111217143"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意下面我们再输入几条十到的数据，还是那个三十三十一三十二啊。嗯。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221114742.png" alt="image-20230422111430693"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221116286.png" alt="image-20230422111654208"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意此时这个窗口就不会触发，相当于啊，你这个时候你这个迟到的数据啊，我就不管了。来分析一下啊。你看我们又发现这几条数据啊，它都没有触发window啊。那分析一下，当这个呀等于这个33的时候，它正好呢，是属于这个window n time对吧，正好相等，所以说呢，它会触发这个30到33这个窗口执行。当这个窗口执行过后啊，我们再输入30到33这个窗口内的数据的时候呢，会发现这个窗口是可以被触发的。当我们把这个mark提升到34秒的时候，我们再输入这个窗口内的数据，发现温度还是可以被触发的。那当我们把这个mark牺升到35的时候，再输入这个窗口数据，发现window不会被触发。这是为什么呢？这是因为我们在前面设置了这个参数。又给它多加了两秒延迟，因此呢，可以允许延迟在两秒内的数据继续触发温度执行。所以说当你这个我的ma等于34的时候，是可以触发温度的，但是35就不行了，这个需要注意一下。这块有个总结啊，对于这个窗口而言啊，它允许两秒的迟到数据，也就是说呢，你第一次触发是在这个o ma呢，它大于等于这个window and time的时候，对吧，那第二次或者啊以后多次触发条件是这样的。小于window n time加上这个。就是允许的迟到时间加这个二。对吧，并且呢，这个窗口有迟到数据的时候，它就会被触发。那所以说你看，当我们这个omark等于34的时候，我们输入什么三十三十一三十二秒的数据，它是可以出发的，因为这些数据它们的window n time呢，都是这个33。那又是说了，你这个你看是三十四三十四的话，你小于33加二对吧，它是小于的这个是不是出了啊，但是呢，当这个等于35的时候呢，我们再去输入这个三十三十一三十二，这个其实就迟到太久太久太久了。这些数据呢，我n time呢，还是这个33，此时注意我mark是三十五三十五小于这个33加二嘛，你不要不要去较这个真啊。这个是秒，你说一个秒加个二，这啥意思呢？对吧，是尾电门啊，就是33秒加上两秒就是三十五三  十五是不小于35。所以说就是暴走，那所以说最终这些数据呢，迟到时间太久了。本来呢，公司打卡弹性半小时，你们部门啊，又谈了十分钟，结果呢，你还是没有满足这个要求啊。这样就没办法对吧，那这时候呢，就不会再触发温度的执行了，就把你扔掉不管你了。这就是第二种处理方案。</span><br></pre></td></tr></table></figure><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221121053.png" alt="image-20230422112107568"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221121425.png" alt="image-20230422112129357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221603475.png" alt="image-20230422160343061"></p><h4 id="sideOutputLateData"><a href="#sideOutputLateData" class="headerlink" title="sideOutputLateData"></a>sideOutputLateData</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面呢，还有一种处理方案。这个呢，就是收集迟到数据。通过这个函数呢，可以把迟到的数据啊，给它统一收集，统一存储，方便后期排查，问题就是你为什么迟到这么久对吧，这个呢也需要去调整代码，你呢先创建这个out，咱们前面是不是已经用过呀</span><br><span class="line"></span><br><span class="line">注意咱们刚才讲那个第二种方案，其实可以和第三种结合到一块儿来使用，都是可以的啊，你再给他延迟两秒，如果说他还是没有到达，对吧，那你就把它保存起来，丢了保存起来。当然也可以单独使用，都是可以的啊，这个需要具体根据你们的业务需求来定。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">OutputTag</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpForSideOutputLateDataScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间 10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//保存被丢弃的数据</span></span><br><span class="line">    <span class="keyword">val</span> outputTag = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>]](<span class="string">"late-data"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resStream = waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//保存被丢弃的数据</span></span><br><span class="line">      .sideOutputLateData(outputTag)</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>], <span class="type">String</span>, <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup =&gt; &#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr + <span class="string">","</span> + arr.length + <span class="string">","</span> + sdf.format(arr.head) + <span class="string">","</span> + sdf.format(arr.last) + <span class="string">","</span> + sdf.format(window.getStart) + <span class="string">","</span> + sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;) </span><br><span class="line"></span><br><span class="line">    <span class="comment">//把迟到的数据取出来，暂时打印到控制台，实际工作中可以选择存储到其它存储介质中</span></span><br><span class="line">    <span class="comment">//例如：redis，kafka</span></span><br><span class="line">    <span class="keyword">val</span> sideOutput = resStream.getSideOutput(outputTag)</span><br><span class="line">    sideOutput.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将流中的结果数据也打印到控制台</span></span><br><span class="line">    resStream.print()</span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们来验证一下，先输入这两行数据。第一条。所以你看第一次发了一个30，这是43对吧。此时，这头的mark是33。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221620417.png" alt="image-20230422162002780"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221620154.png" alt="image-20230422162014105"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们再输入几条event time小于watermark的一个时间来测试一下啊。现在你这个窗口已经执行过了，我们再往里添加数据来看一下效果。还这三条数据啊。来看一下，注意这个呢，是正常他都会打印的，你看这时候你在输入这三条的时候，他注意它那个窗口就没有执行了，下面这个数据不是那个窗口打印出来，窗口打印出来数据是这种格式啊。这个是谁打印的呀？所以说呢，针对这个迟到的数据，我们就把它放到这里边儿了，这样后期呢，你就可以把这数据可以存到其他地方，方便你们去排查问题，为什么这个数据来这么迟啊，对吧，可以分析一下问题，看是网络原因啊或者是其他原因。是这个啊。那这时候呢，你看针对这个迟到的数据，我们就可以通过这个set out来保存到这个out中。后期你想在保存的其他存储介质中也是没有任何问题的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221622695.png" alt="image-20230422162202360"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221623442.png" alt="image-20230422162306215"></p><h3 id="在多并行度下的watermark应用"><a href="#在多并行度下的watermark应用" class="headerlink" title="在多并行度下的watermark应用"></a>在多并行度下的watermark应用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面呢，我们演示了在单并行度下whatmark的使用，下面呢我们来看一下在多并行度下面watermark的一个使用。咱们前面的话在这设置为一。如果说你把这行代码给它做掉的话，你不设置的话。那我们在IDE中去执行的时候，默认呢，它会读取我本地的CPU的数量来设置默认命度。那所以说我在这把这个给它直接做掉就行了。做了之后啊，你可以在这啊加一个系统ID，这样的话我们就知道了是哪条数据被哪个线程所处理。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.window</span><br><span class="line"></span><br><span class="line">import java.text.SimpleDateFormat</span><br><span class="line">import java.time.Duration</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.eventtime.&#123;SerializableTimestampAssigner, WatermarkStrategy&#125;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.api.scala.function.WindowFunction</span><br><span class="line">import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line">import org.apache.flink.streaming.api.windowing.windows.TimeWindow</span><br><span class="line">import org.apache.flink.util.Collector</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line">import scala.util.Sorting</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Watermark+EventTime解决数据乱序问题</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object WatermarkOpMoreParallelismScala &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    &#x2F;&#x2F;设置使用数据产生的时间：EventTime</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">    &#x2F;&#x2F;设置全局并行度为1</span><br><span class="line">    env.setParallelism(2)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;设置自动周期性的产生watermark，默认值为200毫秒</span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(200)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val text &#x3D; env.socketTextStream(&quot;bigdata04&quot;, 9001)</span><br><span class="line">    import org.apache.flink.api.scala._</span><br><span class="line">    &#x2F;&#x2F;将数据转换为tuple2的形式</span><br><span class="line">    &#x2F;&#x2F;第一列表示具体的数据，第二列表示是数据产生的时间戳</span><br><span class="line">    val tupStream &#x3D; text.map(line &#x3D;&gt; &#123;</span><br><span class="line">      val arr &#x3D; line.split(&quot;,&quot;)</span><br><span class="line">      (arr(0), arr(1).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;分配(提取)时间戳和watermark</span><br><span class="line">    val waterMarkStream &#x3D; tupStream.assignTimestampsAndWatermarks(WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)) &#x2F;&#x2F;最大允许的数据乱序时间 10s</span><br><span class="line">      .withTimestampAssigner(new SerializableTimestampAssigner[Tuple2[String, Long]] &#123;</span><br><span class="line">        val sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)</span><br><span class="line">        var currentMaxTimstamp &#x3D; 0L</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;从数据流中抽取时间戳作为EventTime</span><br><span class="line">        override def extractTimestamp(element: (String, Long), recordTimestamp: Long): Long &#x3D; &#123;</span><br><span class="line">          val timestamp &#x3D; element._2</span><br><span class="line">          currentMaxTimstamp &#x3D; Math.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          &#x2F;&#x2F;计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark&#x3D;currentMaxTimstamp-OutOfOrderness</span><br><span class="line">          val currentWatermark &#x3D; currentMaxTimstamp - 10000L</span><br><span class="line">          </span><br><span class="line">          val threadId &#x3D; Thread.currentThread().getId</span><br><span class="line">          &#x2F;&#x2F;此print语句仅仅是为了在学习阶段观察数据的变化</span><br><span class="line">          println(&quot;threadId:&quot;+threadId+&quot;,key:&quot; + element._1 + &quot;,&quot; + &quot;eventtime:[&quot; + element._2 + &quot;|&quot; + sdf.format(element._2) + &quot;],currentMaxTimstamp:[&quot; + currentWatermark + &quot;|&quot; + sdf.format(currentMaxTimstamp) + &quot;],watermark:[&quot; + currentWatermark + &quot;|&quot; + sdf.format(currentWatermark) + &quot;]&quot;)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(0)</span><br><span class="line">      &#x2F;&#x2F;按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span><br><span class="line">      .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class="line">      &#x2F;&#x2F;使用全量聚合的方式处理window中的数据</span><br><span class="line">      .apply(new WindowFunction[Tuple2[String,Long],String,Tuple,TimeWindow] &#123;</span><br><span class="line">        override def apply(key: Tuple, window: TimeWindow, input: Iterable[(String, Long)], out: Collector[String]): Unit &#x3D; &#123;</span><br><span class="line">          val keyStr &#x3D; key.toString</span><br><span class="line">          &#x2F;&#x2F;将window中的数据保存到arrBuff中</span><br><span class="line">          val arrBuff &#x3D; ArrayBuffer[Long]()</span><br><span class="line">          input.foreach(tup&#x3D;&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          &#x2F;&#x2F;将arrBuff转换为arr</span><br><span class="line">          val arr &#x3D; arrBuff.toArray</span><br><span class="line">          &#x2F;&#x2F;对arr中的数据进行排序</span><br><span class="line">          Sorting.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          val sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)</span><br><span class="line">          &#x2F;&#x2F;将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span><br><span class="line">          val result &#x3D; keyStr+&quot;,&quot;+arr.length+&quot;,&quot;+sdf.format(arr.head)+&quot;,&quot;+sdf.format(arr.last)+&quot;,&quot;+sdf.format(window.getStart)+&quot;,&quot;+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;WatermarkOpScala&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这时候窗口并没有触发。比如我们发现这个window没有被触发，因为这个时候呢，这七条数据啊，都是被不同的线程处理的，每个线程呢，都有一个watermark。我们前面分析了，在这种多并行度的情况下呢，whatmark呢，它呢有一个对齐机制，它呢会取所有材中最小的那个wordmark。所以说我们现在有八个并行度，你这七条数据呢，都被不同的线路所处理啊。到现在呢，还没有获取到最小的那个我。所以说呢，这个window是无法被处罚执行的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221654047.png" alt="image-20230422165413793"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221654240.png" alt="image-20230422165427792"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为这个线路太多了，验证起来了，不太好验证，所以说啊这样。把这个稍微再改一下。我们也不用了一个默认的八个了，我们给它改成两个吧。这个也是多并行度了。好。接下来呢，我们往里面输这么三条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221700312.png" alt="image-20230422170004992"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221702331.png" alt="image-20230422170229823"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一条，没有触发。接下来，第二条，其实理论上如果是单线程的话，这个时候这个窗口已经被触发，但是现在呢，还没有触发。这第三条数据。嗯。好。看到没有，这个时候他就出发。看一下这块的一个总结。此时呢，我们会发现，当第三条数据输入完以后，这个窗口呢，它就被触发了。你前两条数据啊，输入之后呢，它获取到的那个具体的wordmark是20。这个时候呢，它对应的window中呢，是没有数据的，所以说呢，什么都没有执行，当你第三条数据输入之后呢，它获取到那个最小的mark呢，就是33了，这个时候呢，它对应的窗口就是它，它里面有数据，所以说呢，这个window就触发了。</span><br></pre></td></tr></table></figure><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来针对这个watermark案例做一个总结。我们在flink中，针对这个watermark，我们该如何设置它的最大乱序时间？注意。最大乱序时间。首先第一点这个要结合我们自己的业务以及呢数据的实际情况去设置，如果说呢，这个最大落地时间设置的太小，而我们那个自身数据啊，发送时由于网络等原因导致乱序或者迟到太多，那么呢，最终的结果就是会有很多数据被丢弃。这样的话，对我们数据的正确性影响太大。那对于这个严重外序的数据呢？我们需要严格统计数据的最大延迟时间。这样才能最大程度保证计算数据的一个准确度。延时时间呢？实时太小会影响数据准确性。延时时间是太大，不仅影响数据的一个实时性。更加的会加重flink作业的一个负担。所以说不是对eventtime要求特别严格的数据，尽量呢不要采用这种eventtime的方式来处理数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221721954.png" alt="image-20230422172108445"></p><h2 id="Flink并行度"><a href="#Flink并行度" class="headerlink" title="Flink并行度"></a>Flink并行度</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们来分析一下Flink的并行度。一个flink程序由多个组件组成，datasource、transformation、datasink。</span><br><span class="line">一个组件呢，由多个并行的实例来执行，或者说呢，是由多个线程来执行。一个组件的并行实际数目呢？就被称之为该组件的并行度。其实就是说你这个组件有多少个线程去执行，那么它的并行度就是多少。</span><br></pre></td></tr></table></figure><h3 id="task-manager和slot之间的关系"><a href="#task-manager和slot之间的关系" class="headerlink" title="task manager和slot之间的关系"></a>task manager和slot之间的关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，在具体分析这个并行度之前，我们先分析一下这个task manager和slot之间的关系。flink的每个task manager为集群提供的slot的数量通常与每个task manager的可用CPU数量成正比。一般情况下的数量就是每个task manager的可用CPU数量。这个task manager节点就是我们集群的一个从节点。那上面这个slot数量就是这个task manager具有的一个并发执行能力。这里面啊，实行的就是具体的一些实例。source、map、keyBy、sink。还有这个图也是一样的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221751734.png" alt="image-20230422175152422"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221755921.png" alt="image-20230422175512133"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们就来看一下这个并行度该如何来设置。任务的并行度可以通过四个层面来设置。首先第一个是算式层面。第二个是执行环境层面。第三个是客户端层面。第四个呢，是系统层面。</span><br><span class="line">那这四个层面，他们执行的优先级是什么样的？注意。这个算式层面了大于执行环境层面的，执行环境层面了大于客户端层面了，客户端层面了大于系统层面。这是他们之间的优先级。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221757173.png" alt="image-20230422175706876"></p><h3 id="Operator-Level"><a href="#Operator-Level" class="headerlink" title="Operator Level"></a>Operator Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面我们来具体分析一下这四种。首先看这个算子层面的。算子层面其实很简单，首先呢，在这去设置就可以了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221758256.png" alt="image-20230422175809024"></p><h3 id="Execution-Environment-Level"><a href="#Execution-Environment-Level" class="headerlink" title="Execution Environment Level"></a>Execution Environment Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个执行环境层面的。主要呢，是在这个ENV后面来设置一个并行度。这设置的是一个全局的并行度。当然，你也可以选择在下面针对某一个算子再去改它的并行度也是可以的。因为你那个算子层面并行度是大于这个执行环境层面这个并行度的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221758507.png" alt="image-20230422175856304"></p><h3 id="Client-Level"><a href="#Client-Level" class="headerlink" title="Client Level"></a>Client Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来是一个客户端层面。这个并行度呢，可以在客户端提交Job的时候来设定。通过那个-P参数来动态指令就可以了。具体呢，是这样的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221802253.png" alt="image-20230422180247995"></p><h3 id="System-Level"><a href="#System-Level" class="headerlink" title="System Level"></a>System Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那最后呢，是这个系统层面了。我们在系统层面可以通过在这个配置文件里面来设定。parallelism.default属性来指定所有执行环境的默认并行度啊，当然了，你是可以在具体的任务里面再去动态的去改这个并行度。因为他们呢，可以覆盖这个系统层面的并行度。</span><br></pre></td></tr></table></figure><h3 id="并行度案例分析"><a href="#并行度案例分析" class="headerlink" title="并行度案例分析"></a>并行度案例分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来通过一些案例来具体分析一下Flink中的并行度。首先看这个图。这个图里面呢，它表示啊，我们这个集群是有三个从节点。M1，M2，M3，注意每个节点上面具有三个slot。这个表示这个从节点，它具有的3个并发处理能力。那如何实现三个呢？在这个flink-conf.yaml里面来配置了taskmanager.numberOfTaskSlots，把它设置为3。这样话相当于我这个节点上面有三个空闲CPU。那这样的话，我这个集群啊，目前具有的一个处理能力就是9 slot</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221811958.png" alt="image-20230422181155611"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一个案例，它的并行度为1，那如何让它的并行度为1呢？很简单，你在提交这个任务的时候，什么参数都不设置就行。并且我们在开发这个word代码的时候，里面啊，也不设置并行度相关的代码，这样就可以了，这样它就会默认呢，读取这个flink-conf.yaml里面的parallelism的值。这个参数的默认值为1。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221817269.png" alt="image-20230422181742166"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第二个案例，如何实现让它的并行度为2呢？你可以通过这几种方式，首先呢，去改了一份文件。把里面这个默认参数值改为二，或者说我们在动态提交的时候通过-P来指定。或者我们通过这个env来设置都是可以的。这样的话呢，我抗里面它的一个冰度都为二。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221823152.png" alt="image-20230422182315555"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221827716.png" alt="image-20230422182700747"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第三个案例，它的并行度为9，那如何实现呢？你要么在这个配置文件里面，把这个参数设置为9，要么呢动态指定。要么呢，通过env来设置都是可以的。这样的话，它就是9份了。这样就占满了，那说我能不能把这个并行度设为10呢？不能，因为你现在最终呢，只有九个slot。这个需要注意啊。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221831842.png" alt="image-20230422183105341"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第四个。我看呀，它这个并行度呢，还是9，但是注意针对这个sink组件的并行度啊，给它设置为1啊。我们在这主要分析一下这个新的组件并行度，全局设置为9，就是根据咱们前面这个案例。这三种你用哪种都可以。但是呢，我们还需要把这个新的组件并行度设置为1，那怎么设置呢？就说你在代码里面啊，通过算式层面来把这个新组件的并行度设置为1，这样的话它就会覆盖那个全局的那个9。当然你其他组件还是按那个九那个并行度去执行，而我这个组件的话，我在这给它覆盖掉，使用一给它覆盖掉。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-5</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-5.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-5.html</id>
    <published>2023-04-17T08:47:07.000Z</published>
    <updated>2023-04-19T08:27:36.561Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第四章-Hbase"><a href="#第四章-Hbase" class="headerlink" title="第四章 Hbase"></a>第四章 Hbase</h1><h2 id="01-Hbase基本原理"><a href="#01-Hbase基本原理" class="headerlink" title="01 Hbase基本原理"></a>01 Hbase基本原理</h2><h3 id="Region定位–region"><a href="#Region定位–region" class="headerlink" title="Region定位–region"></a>Region定位–region</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182307738.png" alt="image-20230418230749193"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在HBase中，表的所有行都是按照RowKey的字典序排列的，表在行的方向上分割为多个分区（Region）。如图1所示。</span><br><span class="line">每张表一开始只有一个Region，但是随着数据的插入，HBase会根据一定的规则将表进行水平拆分，形成两个Region，当表中的行越来越多时，就会产生越来越多的Region，而这些Region无法存储到一台机器上时，需要分布存储到多台机器上。每个Region服务器负责管理一个Region，通常在每个Region服务器上会放置10~1000个Region，HBase中Region的物理存储如图2所示。</span><br><span class="line"></span><br><span class="line">客户端在插入，删除，查询数据时需要知道哪个Region服务器上存有自己所需的数据，这个查找Region的过程称之为Region定位。</span><br><span class="line"></span><br><span class="line">HBase中每个Region由三个主要要素组成，包括Region所属的表、包含的第一行和包含的最后一行。</span><br></pre></td></tr></table></figure><h3 id="Region定位–META表"><a href="#Region定位–META表" class="headerlink" title="Region定位–META表"></a>Region定位–META表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">有了Region标识符，就可以唯一标识每个Region。为了定位每个Region所在的位置，就可以构建一张映射表，映射表的每个条目包含两项内容，一个是Region标识符，另一个是Region服务器标识，这个条目就表示Region和Region服务器之间的对应关系，从而就可以知道某个Region被保存在哪个Region服务器中。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182308191.png" alt="image-20230418230821649"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">映射表包含了关于Region的元数据，因此也被称为“元数据表”，又名“meta表”。可以scan命令查看meta</span><br><span class="line"></span><br><span class="line">表的结构如图3所示。</span><br><span class="line">Meta表中每一行记录了一个Region的信息。</span><br><span class="line">首先RowKey包含表名、起始行键和时间戳信息。</span><br><span class="line">中间用逗号隔开，第一个Region的起始行键为空。</span><br><span class="line">时间戳只有用.隔开的为分区名称的编码字符串，该信息是由前面的表名、起始行键和时间戳进行字符串编码后形成。</span><br><span class="line">Meta表里有一个列族info。info包含了三个列，分别为regionInfo、server和serverstartcode。</span><br><span class="line">Regioninfo中记录了Region的详细信息，包括行键范围StartKey和EndKey、列族列表和属性。</span><br><span class="line">Server记录了管理该Region的Region服务器的地址，如localhost:16201。</span><br><span class="line">Serverstartcode记录了Region服务器开始托管该Region的时间。</span><br></pre></td></tr></table></figure><h3 id="Region定位–Region定位"><a href="#Region定位–Region定位" class="headerlink" title="Region定位–Region定位"></a>Region定位–Region定位</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182309337.png" alt="image-20230418230932193"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在HBase的早期设计中，Region的查找是通过三层架构来进行查询的，即在集群中有一个总入口ROOT表，记录了meta表分区信息及各个入口地址，这个ROOT表存放在某个Region服务器上，但是在Zookeeper中保存有它的地址。这种早期的三层架构通过先找到ROOT表，从中获取分区meta表位置，然后再获取分区meta表信息，得到Region所在的Region服务器。</span><br><span class="line">从0.96版本以后，三层架构被改为二层架构，去掉了ROOT表，同时Zookeeper中的&#x2F;hbase&#x2F;root-region-server也被去掉。meta表所在的RegionServer信息直接存储在Zookeeper中的&#x2F;hbase&#x2F;meta-region-server中。如图所示</span><br><span class="line">当客户端进行数据操作时，根据操作的表名和行键通过一定的顺序寻找对应的分区数据。</span><br><span class="line">客户端通过Zookeeper获取到Meta表分区存储的地址，然后在对应Region服务器上获取meta表的信息，得到所需表和行键所在的Region信息，然后在从Region服务器上找到所需的数据。一般客户端获取到Region信息后会进行缓存，下次再查询不必从Zookeeper开始寻址。</span><br></pre></td></tr></table></figure><h3 id="数据存储与读取"><a href="#数据存储与读取" class="headerlink" title="数据存储与读取"></a>数据存储与读取</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">HBase集群数据的构成如图所示</span><br><span class="line">HBase的核心模块是Region服务器。</span><br><span class="line">Region服务器由多个Region块构成，Region块中存储的一系列连续的数据集。</span><br><span class="line">Region服务器主要构成部分是：HLog和Region块。</span><br><span class="line">HLog记录该Region的操作日志。</span><br><span class="line">Region对象由多个Store组成，每个Store对应当前分区中的一个列族，每个Store管理一块内存，即MemStore。</span><br><span class="line">当MemStore中的数据达到一定条件时会写入到StoreFile文件中，因此每个Store包含若干个StoreFile文件。StoreFile文件对应HDFS中的HFile文件。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182338403.png" alt="image-20230418233830777"></p><h4 id="MemStore"><a href="#MemStore" class="headerlink" title="MemStore"></a>MemStore</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当Region服务器收到写请求的时候，Region服务器会将请求转至相应的Region。数据首先写入到Memstore，然后当到达一定的阀值的时候，Memstore中的数据会被刷到HFile中进行持久性存储。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase将最近接收到的数据缓存在MemStore中，在持久化到HDFS之前完成排序，再顺序写入HDFS，为后续数据的检索做了优化。因为MemStore缓存的是最近增加的数据，所以也提高了对近期数据的操作速度。在持久化写入之前，在内存中对行键或单元格做些优化。</span><br></pre></td></tr></table></figure><h4 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Store是Region服务器的核心，存储的是同一个列族下的数据，每个Store包含有一块MemStore和0个或多个StoreFile。StoreFile是HBase中最小的数据存储单元。</span><br><span class="line"></span><br><span class="line">  Store存储是HBase存储的核心，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。MemStore是Sorted Memory Buffer（内存写缓存），用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile(底层实现是HFile)， 当StoreFile文件数量增长到一定阈值，会触发Compaction合并操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase将最近接收到的数据缓存在MemStore中，在持久化到HDFS之前完成排序，再顺序写入HDFS，为后续数据的检索做了优化。因为MemStore缓存的是最近增加的数据，所以也提高了对近期数据的操作速度。在持久化写入之前，在内存中对行键或单元格做些优化。</span><br></pre></td></tr></table></figure><h4 id="Store的合并分裂"><a href="#Store的合并分裂" class="headerlink" title="Store的合并分裂"></a>Store的合并分裂</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182339676.png" alt="image-20230418233949215"></p><h4 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182340842.png" alt="image-20230418234015451"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MemStore内存中的数据写到StoreFile文件中，StoreFile底层是以HFile的格式保存。</span><br><span class="line">HFile的存储格式如图7所示</span><br><span class="line">HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。</span><br><span class="line">Trailer中有指针指向其他数据块的起始点，FileInfo记录了文件的一些meta信息。</span><br><span class="line">每个Data块的大小可以在创建一个Table的时候通过参数指定（默认块大小64KB）。</span><br><span class="line">每个Data块除了开头的Magic以外就是一个键值对拼接而成，Magic内容就是一些随机数字，目的是防止数据损坏。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HFile里面的每个键值对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182340877.png" alt="image-20230418234045908"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">键值对结构以两个固定长度的数值开始，分别表示Key的长度和Value的长度。</span><br><span class="line">紧接着是Key，Key以RowLength开始，是固定长度的数值，表示RowKey的长度，</span><br><span class="line">紧接着是RowKey，然后是固定长度的数值ColumnFamilyLength，表示Family的长度，</span><br><span class="line">然后是Family列族，接着是Qualifier列标识符，Key最后以两个固定长度的数值Time Stamp和Key Type（Put&#x2F;Delete）结束。</span><br><span class="line">Value部分没有这么复杂的结构，就是纯粹的二进制数据。</span><br></pre></td></tr></table></figure><h3 id="数据存储与读取-1"><a href="#数据存储与读取-1" class="headerlink" title="数据存储与读取"></a>数据存储与读取</h3><h4 id="HBase写文件流程"><a href="#HBase写文件流程" class="headerlink" title="HBase写文件流程"></a>HBase写文件流程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">客户端首先访问zookeeper，从meta表得到写入数据对应的region信息和相应的region服务器。</span><br><span class="line">找到相应的region服务器,把数据分别写到HLog和MemStore上一份</span><br><span class="line">MemStore达到一个阈值后则把数据刷成一个StoreFile文件。（若MemStore中的数据有丢失，则可以总HLog上恢复）</span><br><span class="line">当多个StoreFile文件达到一定的大小后，会触发Compact合并操作，合并为一个StoreFile，（这里同时进行版本的合并和数据删除。）</span><br><span class="line">当Storefile大小超过一定阈值后，会把当前的Region分割为两个（Split分裂），并由Hmaster分配到相应的HRegionServer，实现负载均衡。</span><br></pre></td></tr></table></figure><h4 id="HBase读文件流程"><a href="#HBase读文件流程" class="headerlink" title="HBase读文件流程"></a>HBase读文件流程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">客户端先访问zookeeper，从meta表读取Region的信息对应的服务器。</span><br><span class="line">客户端向对应Region服务器发送读取数据的请求，Region接收请求后，先从MemStore找数据，如果没有，再到StoreFile上读取，然后将数据返回给客户端。</span><br></pre></td></tr></table></figure><h3 id="WAL机制"><a href="#WAL机制" class="headerlink" title="WAL机制"></a>WAL机制</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191129537.png" alt="image-20230419112916756"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">分布式环境下，必须要考虑到系统出错的情形，比如Region服务器发生故障时，MemStore缓存中还没有被写入文件的数据会全部丢失。</span><br><span class="line">因此，HBase采用HLog来保证系统发生故障时能够恢复到正常的状态。</span><br><span class="line">如图所示，每个Region服务器都有一个HLog文件，同一个Region服务器的Region对象共用一个HLog，HLog是一种预写日志（Write Ahead Log）文件，就是说，用户更新数据必须首先被记入日志后才能写入MemStore缓存，当缓存内容对应的日志已经被写入磁盘后，即日志写成功后，缓存的内容才会被写入磁盘。</span><br><span class="line">HBase系统中，每个Region服务器只需要一个HLog文件，所有Region对象共用一个HLog，而不是每个Region使用一个HLog。在这种Region对象共用一个HLog的方式中，多个Region对象的进行更新操作需要修改日志时，只需要不断把日志记录追加到单个日志文件中，而不需要同时打开、写入到多个日志文件中，因此可以减少磁盘寻址次数，提高对表的写操作性能。</span><br></pre></td></tr></table></figure><h2 id="02-Hbase-Region管理"><a href="#02-Hbase-Region管理" class="headerlink" title="02 Hbase Region管理"></a>02 Hbase Region管理</h2><h3 id="HFile合并"><a href="#HFile合并" class="headerlink" title="HFile合并"></a>HFile合并</h3><h4 id="Minor合并"><a href="#Minor合并" class="headerlink" title="Minor合并"></a>Minor合并</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191135924.png" alt="image-20230419113533020"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面讲过</span><br><span class="line">用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile(底层实现是Hfile)， 当StoreFile文件数量增长到一定阈值，会触发Compaction合并操作。</span><br><span class="line">HFile的合并分为两种类型，分别是Minor合并和Major合并，这两种合并都发生在Store内部，不是Region的合并。</span><br><span class="line">Minor合并是把多个小HFile合并生成一个大的Hfile</span><br><span class="line">执行合并时，HBase读出已有的多个HFile的内容，把记录写入到一个新文件中。然后把新文件设置为激活状态，并标记旧文件为删除。在Minor合并中，这些标记为删除的旧文件是没有被移除的，任然会出现在HFile中，只有在进行Major合并时才会移除这些旧文件。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191136210.png" alt="image-20230419113609308"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">对需要进行Minor合并的文件的选择是触发式的，当达到触发条件才会进行Minor合并，而触发条件有很多，比如在将MemStore的数据刷到HFile时会申请对Store下符合条件的HFile进行合并，或者定期对Store内的HFile进行合并。另外对选择合并的HFile也是有条件的，如表1所示。</span><br><span class="line">在执行Minor合并时，会根据上述配置参数选择合适的HFile进行合并。Minor合并对HBase的性能是有轻微影响的，所以合并的HFile数量是有限的，默认最多为10个。</span><br></pre></td></tr></table></figure><h4 id="Major合并"><a href="#Major合并" class="headerlink" title="Major合并"></a>Major合并</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191136792.png" alt="image-20230419113630473"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Major合并针对的是给定Region的一个列族的所有Hfile。</span><br><span class="line">它将Store中的所有HFile合并成一个大文件，有时也会对整个表的同一列族的HFile进行合并，这是一个耗时和耗费资源的操作，会影响集群性能。</span><br><span class="line">一般情况下都是做Minor合并，不少集群是禁止Major合并的，只有在集群负载较小时进行手动Major合并，或者配置Major合并周期，默认为7天。</span><br><span class="line">另外Major合并时会清理Minor合并中被标记删除的HFile。</span><br><span class="line">如上右图所示</span><br></pre></td></tr></table></figure><h3 id="Region拆分"><a href="#Region拆分" class="headerlink" title="Region拆分"></a>Region拆分</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191146636.png" alt="image-20230419114652571"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Region拆分是HBase能够拥有良好扩展性的最重要因素。一旦一个Region的负载过大或者超过阈值时，会被分裂成新的两个Region，如图所示。</span><br><span class="line">这个过程是由RegionServer完成，其拆分流程如下：</span><br><span class="line">（1）将需要拆分的Region下线，阻止所有对该Region的客户端请求，master会检测到Region的状态为SPLITING；</span><br><span class="line">（2）将一个Region拆分成两个子Region，先在父Region下建立两个引用文件，分别指向Region的首行和末行，这时两个引用文件并不会从父Region中拷贝数据；</span><br><span class="line">（3）之后在HDFS上建立两个子Region的目录，分别拷贝上一步建立的引用文件，每个子Region分别占父Region的一半数据。拷贝完成后删除两个引用文件。</span><br><span class="line">（4）完成子Region创建后，向.META.表发送新产生的region的元数据信息；</span><br><span class="line">（5）Region的拆分信息更新到Hmaster，并且每个Region进入可用状态。</span><br></pre></td></tr></table></figure><h4 id="拆分策略"><a href="#拆分策略" class="headerlink" title="拆分策略"></a>拆分策略</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191147462.png" alt="image-20230419114724603"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上表列举的切分策略中，切分点的定义是一致的，即当Region中最大Store的大小大于设置阈值之后才会触发拆分。而不同策略中，阈值的定义是不同的，且对集群中Region的分布有很大的影响。</span><br></pre></td></tr></table></figure><h3 id="Region合并"><a href="#Region合并" class="headerlink" title="Region合并"></a>Region合并</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">当RegionServer中的Region数到达最大阈值时，RegionServer就会发起Region合并。其合并过程如下：</span><br><span class="line">（1）客户端发起Region合并处理并发送Region合并请求给Master；</span><br><span class="line">（2）Master在RegionServer上把Region移到一起并发起一个Region合并操作的请求；</span><br><span class="line">（3）RegionServer将准备合并的Region下线，然后进行合并；</span><br><span class="line">（4）从.META.表删除被合并的Region元数据，新的合并了的Region的元数据被更新写入.META.表中；</span><br><span class="line">（5）合并的Region被设置为上线状态并接受访问，同时更新Region信息到Master。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从Region的拆分可以看到随着表的增大，Region的数量是越来越大的，如果很多Region，它们中Memstore也过多，内存大小会触发RegionServer级别的限制，会频繁出现数据从内存刷到HFile的操作，就会对用户请求产生较大的影响，可能阻塞该RegionServer上的更新操作。过多Region会增加ZooKeeper的负担。因此当RegionServer中的Region数到达最大阈值时，RegionServer就会发起Region合并。</span><br></pre></td></tr></table></figure><h3 id="Region负载均衡"><a href="#Region负载均衡" class="headerlink" title="Region负载均衡"></a>Region负载均衡</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191148622.png" alt="image-20230419114829628"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在分布式系统中，负载均衡是一个非常重要的功能，在HBase中通过Region的数量来实现负载均衡。</span><br><span class="line">每次负载均衡操作分为两步进行，首先生成负载均衡计划表，然后按照计划表执行Region的分配。</span><br><span class="line">Master内部使用一套集群负载评分的算法，来评估HBase某一个表的Region是否需要进行重新分配。</span><br><span class="line">这套算法分别从RegionServer中Region的数目、表的Region数，MenStore大小、StoreFile大小，数据本地性等几个维度来对集群进行评分，评分越低代表集群的负载越合理。</span><br><span class="line">确定需要负载均衡后，在根据不同策略选择Region进行分配，负载均衡策略有三种，如表所示。</span><br><span class="line">根据上述策略选择分配Region后再继续对整个表的所有Region进行评分，如果依然未达到标准，循环执行上述操作直至整个集群达到负载均衡的状态。</span><br></pre></td></tr></table></figure><h2 id="03-Hbase集群管理"><a href="#03-Hbase集群管理" class="headerlink" title="03 Hbase集群管理"></a>03 Hbase集群管理</h2><h3 id="运维管理"><a href="#运维管理" class="headerlink" title="运维管理"></a>运维管理</h3><h4 id="移除RegionServer节点"><a href="#移除RegionServer节点" class="headerlink" title="移除RegionServer节点"></a>移除RegionServer节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当集群由于升级或更换硬件等原因需要在单台机器上停止守护进程时，需要确保集群的其他部分正常工作，并且确保从客户端应用来看停用时间最短。满足此条件必须把这台RegionServer服务的Region主动转移到其他RegionServer上，而不是让HBase被动地对此RegionServer的下线进行反应。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">集群运行时，有些操作任务是必需的，包括增加和移除节点。</span><br><span class="line">用户可以在指定节点的HBase目录下使用hbase-damon.sh stop命令来停止集群中的一个RegionServer。执行此命令后，RegionServer先将所有Region关闭，然后再把自己的进程停止，RegionServer在ZooKeeper中对应的临时节点将会过期。Master检测到RegionServer停止服务后将此RegionServer上的Region重新分配到其他机器上。</span><br><span class="line">HBase也提供了脚本来主动转移Region到其他RegionServer，然后下掉下线的RegionServer这样会让整个过程更加安全。在HBase的bin目录下提供了graceful_stop.sh脚本可以完成这种主动移除节点的功能。此脚本停止一个RegionServer的过程如下：</span><br><span class="line">（1）关闭Region均衡器；</span><br><span class="line">（2）从需要停止的RegionServer上移出Region，并随机把他们分配给集群中其他服务器；</span><br><span class="line">（3）停止RegionServer进程</span><br></pre></td></tr></table></figure><h4 id="增加RegionServer节点"><a href="#增加RegionServer节点" class="headerlink" title="增加RegionServer节点"></a>增加RegionServer节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">随着应用系统需求的增长，整个HBase集群需要进行扩展，这时就需要往HBase集群中增加一个节点。添加一个新的RegionServer是运行集群的常用操作，首先需要修改conf目录下的regionserver文件，然后将此文件复制到集群中所有机器上，这样可以使用启动脚本就能够添加新的服务器。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HBase底层是以HDFS来存储数据的，一般部署HBase集群时，HDFS的DataNode和HBase的RegionServer位于同一台物理机上。</span><br><span class="line">所以往HBase集群增加一个RegionServer之前需要往HDFS里增加DataNode，</span><br><span class="line">等待DataNode进程启动并加入HDFS集群后，再启动HBase的RegionServer进程。</span><br><span class="line">启动新增节点上的RegionServer可以使用命令hbase-damon.sh start，启动成功后可以在Master用户界面看到此节点。</span><br><span class="line">如果需要重新均衡分配每个节点上的Region，则使用HBase的负载均衡功能。</span><br></pre></td></tr></table></figure><h4 id="增加Master备份节点"><a href="#增加Master备份节点" class="headerlink" title="增加Master备份节点"></a>增加Master备份节点</h4><h3 id="数据管理"><a href="#数据管理" class="headerlink" title="数据管理"></a>数据管理</h3><h4 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h4><h4 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h4><h4 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h4><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-4</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-4.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-4.html</id>
    <published>2023-04-17T08:47:02.000Z</published>
    <updated>2023-04-19T08:35:25.572Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第三章-Hbase"><a href="#第三章-Hbase" class="headerlink" title="第三章 Hbase"></a>第三章 Hbase</h1><h2 id="05-Hbase过滤器"><a href="#05-Hbase过滤器" class="headerlink" title="05 Hbase过滤器"></a>05 Hbase过滤器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">可以根据主键、列簇、列、版本等更多的条件来对数据进行过滤。</span><br><span class="line">类似SQL中的WHERE</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; show_filters</span><br><span class="line">DependentColumnFilter                                       KeyOnlyFilter                                               ColumnCountGetFilter                                       SingleColumnValueFilter                                     PrefixFilter                                               SingleColumnValueExcludeFilter                             FirstKeyOnlyFilter                                         ColumnRangeFilter                                           TimestampsFilter                                           FamilyFilter                                               QualifierFilter                                             ColumnPrefixFilter                                         RowFilter                                                   MultipleColumnPrefixFilter                                 InclusiveStopFilter                                         PageFilter                                                 ValueFilter                                                 ColumnPaginationFilter</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">过滤器语法格式：</span><br><span class="line"></span><br><span class="line">scan&#x2F;get  ‘表名’，&#123;Filter &#x3D;&gt; “过滤器 ( 比较运算符，’比较器’)”&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182137386.png" alt="image-20230418213720968"></p><h3 id="RowFilter"><a href="#RowFilter" class="headerlink" title="RowFilter"></a>RowFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RowFilter：针对rowkey进行字符串的比较过滤器。</span><br><span class="line"></span><br><span class="line">举例：</span><br><span class="line">例1：显示行键包含0的键值对；</span><br><span class="line">scan &#39;student&#39;,&#123;FILTER&#x3D;&gt;&quot;RowFilter(&#x3D;,&#39;substring:0’)”&#125;</span><br><span class="line">例2：显示行键字节顺序大于002的键值对；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;RowFilter(&gt;,&#39;binary:002&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="PrefixFilter"><a href="#PrefixFilter" class="headerlink" title="PrefixFilter"></a>PrefixFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PrefixFilter：rowkey前缀比较器，一种更简单的比较行键前缀的命令，等值比较。</span><br><span class="line">举例：</span><br><span class="line"></span><br><span class="line">例1：显示行键前缀为0开头的键值对；</span><br><span class="line">scan &#39;student&#39;,&#123;FILTER&#x3D;&gt;&quot;RowFilter(&#x3D;,&#39;substring:0’)”&#125;</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;PrefixFilter(&#39;003&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="KeyOnlyFilter"><a href="#KeyOnlyFilter" class="headerlink" title="KeyOnlyFilter"></a>KeyOnlyFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KeyOnlyFilter：</span><br><span class="line">只对cell的键进行过滤和显示，但不显示值。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182142565.png" alt="image-20230418214240115"></p><h3 id="FirstKeyOnlyFilter"><a href="#FirstKeyOnlyFilter" class="headerlink" title="FirstKeyOnlyFilter"></a>FirstKeyOnlyFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FirstKeyOnlyFilter：只扫描相同键的第一个cell，其键值对都会显示出来。</span><br><span class="line"></span><br><span class="line">例4：统计表的逻辑行数；</span><br><span class="line">count &#39;student’</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;FirstKeyOnlyFilter()&quot;</span><br><span class="line"></span><br><span class="line">hbase(main):008:0&gt; scan &#39;student&#39;, FILTER&#x3D;&gt;&quot;FirstKeyOnlyFilter()&quot;</span><br><span class="line">ROW                    COLUMN+CELL                           001                   column&#x3D;grades:englisg, timestamp&#x3D;1541485306878, value&#x3D;80        </span><br><span class="line"> 002                   column&#x3D;grades:bigdata, timestamp&#x3D;1541485403649, value&#x3D;88        </span><br><span class="line"> 003                   column&#x3D;grades:bigdata, timestamp&#x3D;1541485412686, value&#x3D;80        </span><br><span class="line">3 row(s) in 0.0400 seconds</span><br></pre></td></tr></table></figure><h3 id="InclusiveStopFilter"><a href="#InclusiveStopFilter" class="headerlink" title="InclusiveStopFilter"></a>InclusiveStopFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">InclusiveStopFilter：替代ENDROW返回终止条件行；</span><br><span class="line">例5：显示起始行键为001，结束行为003的记录；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182149501.png" alt="image-20230418214950938"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182153377.png" alt="image-20230418215313360"></p><h3 id="FamilyFilter"><a href="#FamilyFilter" class="headerlink" title="FamilyFilter"></a>FamilyFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FamilyFilter：针对列族进行比较和过滤。</span><br><span class="line"></span><br><span class="line">例1：显示列族前缀为stu开头的键值对；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,&#39;substring:stu’)”</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,‘binary:stu’)”</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182155971.png" alt="image-20230418215532681"></p><h3 id="QualifierFilter"><a href="#QualifierFilter" class="headerlink" title="QualifierFilter"></a>QualifierFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">QualifierFilter：列标识过滤器。</span><br><span class="line"></span><br><span class="line">例2：显示列名为name的记录；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;substring:name&#39;)&quot;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182200092.png" alt="image-20230418220036827"></p><h3 id="ColumnPrefixFilter"><a href="#ColumnPrefixFilter" class="headerlink" title="ColumnPrefixFilter"></a>ColumnPrefixFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ColumnPrefixFilter：对列名前缀进行过滤。</span><br><span class="line">例2：显示列名为name的记录；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPrefixFilter(&#39;name’)”</span><br><span class="line"></span><br><span class="line">等价于scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;substring:name&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MultipleColumnPrefixFilter：可以指定多个前缀</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">例3：显示列名为name和age的记录；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;MultipleColumnPrefixFilter(&#39;name&#39;,&#39;age&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="ColumnRangeFilter"><a href="#ColumnRangeFilter" class="headerlink" title="ColumnRangeFilter"></a>ColumnRangeFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ColumnRangeFilter ：设置范围按字典序对列名进行过滤；</span><br><span class="line">例4：查询列名在bi和na之间的记录</span><br><span class="line">Student表中有以下列族和列名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182205876.png" alt="image-20230418220523574"></p><h3 id="TimestampsFilter"><a href="#TimestampsFilter" class="headerlink" title="TimestampsFilter"></a>TimestampsFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TimestampsFilter ：时间戳过滤器。支持等值方式比较，但可以设置多个时间戳</span><br><span class="line"></span><br><span class="line">例5：只查询时间戳为1和2的键值对；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;TimestampsFilter(2,4)&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):030:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;TimestampsFilter(2,4)&quot;</span><br><span class="line">ROW                    COLUMN+CELL                                   004                   column&#x3D;stuinfo:age, timestamp&#x3D;2, value&#x3D;19      004                   column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry 004                   column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male   1 row(s) in 0.0150 seconds</span><br></pre></td></tr></table></figure><h3 id="ValueFilter"><a href="#ValueFilter" class="headerlink" title="ValueFilter"></a>ValueFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ValueFilter ：值过滤器。</span><br><span class="line"></span><br><span class="line">例6：查询值等于19的所有键值对</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;binary:19’)”</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;substring:19&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="SingleColumnValueFilter"><a href="#SingleColumnValueFilter" class="headerlink" title="SingleColumnValueFilter"></a>SingleColumnValueFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SingleColumnValueFilter：在指定的列族和列中进行值过滤器。</span><br><span class="line">例7：查询stuinfo列族age列中值等于19的所有键值对</span><br><span class="line"></span><br><span class="line">scan &#39;student&#39;,&#123;COLUMN&#x3D;&gt;&#39;stuinfo:age’,</span><br><span class="line">FILTER&#x3D;&gt;&quot;SingleColumnValueFilter(&#39;stuinfo&#39;,&#39;age&#39;,&#x3D;,&#39;binary:19&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">SingleColumnValueExcludeFilter：在指定的列族和列中进行值过滤器，与SingleColumnValueFilter功能相反。</span><br></pre></td></tr></table></figure><h3 id="ColumnCountGetFilter"><a href="#ColumnCountGetFilter" class="headerlink" title="ColumnCountGetFilter"></a>ColumnCountGetFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ColumnCountGetFilter：限制每个逻辑行返回的键值对数</span><br><span class="line">例7：返回行键为001的前3个键值对</span><br><span class="line"></span><br><span class="line">hbase(main):004:0&gt; get &#39;student&#39;, &#39;001&#39;,FILTER&#x3D;&gt;&quot;ColumnCountGetFilter(3)&quot;</span><br><span class="line">COLUMN                    CELL                                       grades:englisg           timestamp&#x3D;1541485306878, value&#x3D;80           grades:math              timestamp&#x3D;1541485384199, value&#x3D;90           stuinfo:age              timestamp&#x3D;1541485224974, value&#x3D;18           3 row(s) in 0.0950 seconds</span><br></pre></td></tr></table></figure><h3 id="PageFilter"><a href="#PageFilter" class="headerlink" title="PageFilter"></a>PageFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">PageFilter ：基于行的分页过滤器，设置返回行数。</span><br><span class="line"></span><br><span class="line">hbase(main):005:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;PageFilter(1)&quot;</span><br><span class="line">ROW                       COLUMN+CELL                                   001                      column&#x3D;grades:englisg, timestamp&#x3D;1541485306878, value&#x3D;80                 </span><br><span class="line"> 001                      column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90                    </span><br><span class="line"> 001                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485224974, value&#x3D;18                    </span><br><span class="line"> 001                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485170696, value&#x3D;alice                </span><br><span class="line"> 001                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485235855, value&#x3D;female                </span><br><span class="line">1 row(s) in 0.0680 seconds</span><br></pre></td></tr></table></figure><h3 id="ColumnPaginationFilter"><a href="#ColumnPaginationFilter" class="headerlink" title="ColumnPaginationFilter"></a>ColumnPaginationFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ColumnPaginationFilter：基于列的进行分页过滤器，需要设置偏移量与返回数量 。</span><br><span class="line"></span><br><span class="line">例9：显示每行第1列之后的2个键值对。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):006:0&gt; scan &#39;student&#39;</span><br><span class="line">ROW                       COLUMN+CELL                                                     001                      column&#x3D;grades:englisg, timestamp&#x3D;1541485306878, value&#x3D;80         001                      column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90           001                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485224974, value&#x3D;18           001                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485170696, value&#x3D;alice       001                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485235855, value&#x3D;female       002                      column&#x3D;grades:bigdata, timestamp&#x3D;1541485403649, value&#x3D;88         002                      column&#x3D;grades:englisg, timestamp&#x3D;1541485319132, value&#x3D;85         002                      column&#x3D;grades:math, timestamp&#x3D;1541485376414, value&#x3D;78           002                      column&#x3D;stuinfo:class, timestamp&#x3D;1541485278646, value&#x3D;1802       002                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485187403, value&#x3D;nancy       002                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485245291, value&#x3D;male</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):007:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(2,1)&quot;</span><br><span class="line">ROW                       COLUMN+CELL                                                     001                      column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90           001                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485224974, value&#x3D;18           002                      column&#x3D;grades:englisg, timestamp&#x3D;1541485319132, value&#x3D;85         002                      column&#x3D;grades:math, timestamp&#x3D;1541485376414, value&#x3D;78           003                      column&#x3D;grades:englisg, timestamp&#x3D;1541485328260, value&#x3D;90         003                      column&#x3D;grades:math, timestamp&#x3D;1541485368087, value&#x3D;80           004                      column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry                   004                      column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                     </span><br><span class="line">4 row(s) in 0.0840 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">003                      column&#x3D;grades:bigdata, timestamp&#x3D;1541485412686, value&#x3D;80       003                      column&#x3D;grades:englisg, timestamp&#x3D;1541485328260, value&#x3D;90       003                      column&#x3D;grades:math, timestamp&#x3D;1541485368087, value&#x3D;80           003                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485209410, value&#x3D;19           003                      column&#x3D;stuinfo:class, timestamp&#x3D;1541485271479, value&#x3D;1803       003                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485198223, value&#x3D;harry       003                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485253075, value&#x3D;male         004                      column&#x3D;stuinfo:age, timestamp&#x3D;2, value&#x3D;19                       004                      column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry                   004                      column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                              </span><br><span class="line">4 row(s) in 0.0980 seconds</span><br></pre></td></tr></table></figure><h3 id="组合使用过滤器"><a href="#组合使用过滤器" class="headerlink" title="组合使用过滤器"></a>组合使用过滤器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">组合使用过滤器：使用AND或OR等连接符，组合多个过滤器进行组合扫描。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">例10：组合过滤器的使用</span><br><span class="line">hbase(main):008:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(2,1) AND ValueFilter(&#x3D;,&#39;substring:ma&#39;)&quot;</span><br><span class="line">ROW                       COLUMN+CELL                                                              </span><br><span class="line"> 004                      column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                              </span><br><span class="line">1 row(s) in 0.1040 seconds</span><br><span class="line">hbase(main):010:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(1,1) OR ValueFilter(&#x3D;,&#39;substring:ma&#39;)&quot;</span><br><span class="line">ROW                           COLUMN+CELL                                                 001                          column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90       001                          column&#x3D;stuinfo:sex, timestamp&#x3D;1541485235855, value&#x3D;female   002                          column&#x3D;grades:englisg, timestamp&#x3D;1541485319132, value&#x3D;85   002                          column&#x3D;stuinfo:sex, timestamp&#x3D;1541485245291, value&#x3D;male     003                          column&#x3D;grades:englisg, timestamp&#x3D;1541485328260, value&#x3D;90   003                          column&#x3D;stuinfo:sex, timestamp&#x3D;1541485253075, value&#x3D;male     004                          column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry               004                          column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                                        </span><br><span class="line">4 row(s) in 0.0440 seconds</span><br></pre></td></tr></table></figure><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">FamilyFilter：针对列族进行比较和过滤。</span><br><span class="line">QualifierFilter：列标识过滤器。</span><br><span class="line">ColumnPrefixFilter：对列名前缀进行过滤。</span><br><span class="line">MultipleColumnPrefixFilter：可以指定多个前缀</span><br><span class="line">ColumnRangeFilter ：设置范围按字典序对列名进行过滤；</span><br><span class="line">TimestampsFilter ：时间戳过滤器。支持等值方式比较，但可以设置多个时间戳</span><br><span class="line">ValueFilter ：值过滤器。</span><br><span class="line">SingleColumnValueFilter ：在指定的列族和列中进行值过滤器。</span><br><span class="line">SingleColumnValueExcludeFilter：在指定的列族和列中进行值过滤器，与SingleColumnValueFilter功能相反。</span><br><span class="line">ColumnCountGetFilter ：限制每个逻辑行返回的键值对数</span><br><span class="line">PageFilter ：基于行的分页过滤器，设置返回行数。</span><br><span class="line">ColumnPaginationFilter ：基于列的进行分页过滤器，需要设置偏移量与返回数量 。</span><br></pre></td></tr></table></figure><h2 id="06-Hbase-Java编程方法"><a href="#06-Hbase-Java编程方法" class="headerlink" title="06 Hbase Java编程方法"></a>06 Hbase Java编程方法</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182232258.png" alt="image-20230418223225014"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">首先介绍基于JAVA的编程方法，HBase是基于java语言开发，用户可以利用包含java语言在内的多种语言进行调用开发。由于java是原生语言，因此利用java进行应用开发，以及过滤器等内容的开发最为方便。</span><br><span class="line"></span><br><span class="line">HBase对java开发环境并无特殊要求，只要将用到的HBase的库包加入引用路径即可。</span><br><span class="line">本节讲述以eclipse为java的集成开发环境，具体步骤如下：</span><br><span class="line">首先在eclipse中建立标准的java工程，给工程命名为hbase，其他使用默认配置，按步骤完成项目的创建。</span><br><span class="line">然后打开hbase项目的属性（对应图中序号1），选择java构建路径标签页（2），选择库（3），点击添加外部jar按钮（4）。然后弹出框会让你选择hbase安装目录下的lib。将需要的包导入工程，导入成功后会在工程里引用的库中出现你所选择的jar包。（解说完后播放操作视频hbase_java环境配置.mp4）</span><br></pre></td></tr></table></figure><h3 id="包导入"><a href="#包导入" class="headerlink" title="包导入"></a>包导入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">使用Hadoop和HBase的环境配置</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line"></span><br><span class="line">HBase的客户端接口</span><br><span class="line">import org.apache.hadoop.hbase.*;</span><br><span class="line">import org.apache.hadoop.hbase.client.*;</span><br><span class="line"></span><br><span class="line">HBase工具包</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line">HBase过滤器</span><br><span class="line">import org.apache.hadoop.hbase.filter.*;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工程目录src下新建类文件，在java文件中import需要的HBase包，比如HBase的环境配置包需要导入Configuration包（对应显示红色）</span><br><span class="line">HBase客户端接口需要导入hbase.client(绿色两行)</span><br><span class="line">工具包需要导入hbase.util.Bytes（蓝色），而如果使用过滤器需要导入hbase.filter包。如果还需要其他包可以从hbase lib里去寻找，并导入工程即可。</span><br></pre></td></tr></table></figure><h3 id="建立连接"><a href="#建立连接" class="headerlink" title="建立连接"></a>建立连接</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public static Configuration conf;</span><br><span class="line">public static Connection connection;</span><br><span class="line">           public static Admin admin;</span><br><span class="line"></span><br><span class="line">public void getconnect() throws IOException</span><br><span class="line">&#123;</span><br><span class="line">conf&#x3D;HBaseConfiguration.create();</span><br><span class="line">conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;cm-cdh01&quot;);</span><br><span class="line">conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</span><br><span class="line">try&#123;</span><br><span class="line">connection&#x3D;ConnectionFactory.createConnection(conf);</span><br><span class="line">admin&#x3D;connection.getAdmin();</span><br><span class="line">&#125;</span><br><span class="line">catch(IOException e)&#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">环境配置好后，接下来开始连接数据库，在分布式环境下，客户端访问HBase需要通过ZooKeeper的地址和端口来获取当前活跃的Master和所需的RegionServer地址，</span><br><span class="line">首先建立三个全局变量，conf、connection和admin（高亮显示红色字体），conf用来描述zookeeper集群的访问地址，connect用来建立连接，admin是创建的数据库管理员，执行具体的数据表操作。</span><br><span class="line">Conf使用set方法来设置集群地址和端口号，然后使用ConnectionFactory来建立连接，并让admin获取管理员的操作权限，至此已经java客户端已经连接上hbase数据库。</span><br></pre></td></tr></table></figure><h3 id="Admin接口"><a href="#Admin接口" class="headerlink" title="Admin接口"></a>Admin接口</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182235010.png" alt="image-20230418223516389"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来具体看下HBase API中常用的接口 ，首先是Admin接口，Admin用于管理HBase数据库的表信息，如创建表，用到了createtable，删除表用到disabletable和deletetable，与Hbase中的create，delete和disable命令对应。另外admin接口还可以使用listtables方法列出hbase中所有的表，使用getTableDescriptor来获取表的结构信息，分别于hbase shell中的，list 和describe命令。</span><br><span class="line"></span><br><span class="line">注意listtables和getTableDescriptor方法返回的是HTableDescriptor类结构的数据。</span><br></pre></td></tr></table></figure><h3 id="创建和删除表"><a href="#创建和删除表" class="headerlink" title="创建和删除表"></a>创建和删除表</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182236492.png" alt="image-20230418223634346"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接着通过具体代码来看下Admin接口提供的方法的使用。首先是创建表，在创建表之前用tablesexists来判断表是否存在，注意设定tableName对象的语法，使用valueOf方法设置表名。如果表已经存在则使用disableTable和deleteTable方法来删除表，注意这里与hbaseshell中操作一样，先禁用表再删除表（红框）。</span><br><span class="line"></span><br><span class="line">（这一段对应蓝色文字框）HTableDescriptor类用来描述表结构，包括HBase中表格的详细信息，例如表中的列族、该表的类型、是否只读、MemStore的最大空间等，并且提供了一些操作表的方法，比如增加列族addFamliy()、删除列族removeFamily()和设置属性值setValue()等方法。</span><br><span class="line"></span><br><span class="line">（这一段对应绿色文字框）HColumnDescriptor类则用来描述列族，比如列族的版本数，压缩设置等。此类通常在添加列族或者创建表的时候使用，一旦列族建立就不能被修改，只有通过删除列族，再创建新的列族来间接修改。HCloumnDescriptor类提供getName()、getValue()和setValue等方法对列族的数据进行操作。</span><br><span class="line"></span><br><span class="line">（对应绿色虚线框的内容）这里的代码使用了两种方式创建列族（绿色框），建立stuinfo时（黄色三行），先通过对象HColumnDescriptor自定义列族属性，比如列族名和块大小，然后根据列族属性使用addfamily建立列族。而建立grades列族时（绿色行）直接使用addfamily方法根据默认属性建立的。</span><br><span class="line"></span><br><span class="line">描述和建立完列族信息后，通过createTable建立表格。</span><br></pre></td></tr></table></figure><h3 id="Table接口"><a href="#Table接口" class="headerlink" title="Table接口"></a>Table接口</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182236561.png" alt="image-20230418223657412"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再来看下另一个接口Table，如果不需要创建表，直接插入数据，可以不用建立Admin对象，使用Table接口即可。Table接口主要用来进行数据的操作，比如删除指定行使用delete，获取指定行的数据使用get，以及向表中添加数据使用put方法。</span><br></pre></td></tr></table></figure><h3 id="Put数据更新"><a href="#Put数据更新" class="headerlink" title="Put数据更新"></a>Put数据更新</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public static void addData() throws IOException &#123;</span><br><span class="line">Table table &#x3D; connection.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">Put put &#x3D; new Put(Bytes.toBytes(“001&quot;));</span><br><span class="line">put.addColumn(Bytes.toBytes(&quot;StuInfo&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(“harry&quot;));</span><br><span class="line">                               put.addColumn(Bytes.toBytes(&quot;StuInfo&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(“harry&quot;));</span><br><span class="line">table.put(put);</span><br><span class="line">table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Put put1 &#x3D; new Put(Bytes.toBytes(&quot;002&quot;));</span><br><span class="line">put1.addColumn(Bytes.toBytes(&quot;StuInfo&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;jess&quot;));</span><br><span class="line">Put put2 &#x3D; new Put(Bytes.toBytes(&quot;003&quot;));</span><br><span class="line">put2.addColumn(Bytes.toBytes(&quot;Grades&quot;), Bytes.toBytes(&quot;english&quot;), Bytes.toBytes(&quot;98&quot;));</span><br><span class="line">List&lt;Put&gt; putList &#x3D; new ArrayList&lt;Put&gt;();</span><br><span class="line">putList.add(put1);</span><br><span class="line">putList.add(put2);</span><br><span class="line">table.put(putList);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先来看下put方法，put主要用来插入，也可以对已有的记录进行更新，在使用put方法前，需要根据表名建立和它的连接（红色行）</span><br><span class="line"></span><br><span class="line">与HBase shell相同，使用put方法可以逐条的插入数据，put对象中首先指明行键为001，并通过addColumn方法加入键值对，addColumn方法的参数分别为列族stuinfo，列name和值harry。这里加入了2个键值对，说明这一行有2列，table.Put方法将put对象写入内存和日志，此时数据已经可以被查出。</span><br><span class="line"></span><br><span class="line">另外可以采用链表的方式一次性插入多个键值对，如下图，put1插入1键值对，put2插入1个键值对，然后建立链表putList，使用add方法将两个put对象插入到链表中，最后一次性插入到表中。</span><br></pre></td></tr></table></figure><h3 id="查询数据-Get"><a href="#查询数据-Get" class="headerlink" title="查询数据-Get"></a>查询数据-Get</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public static void getRow(String tableName, String rowKey) throws IOException &#123;  </span><br><span class="line">Table table &#x3D; connection.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">Get get &#x3D; new Get(Bytes.toBytes(rowKey));  </span><br><span class="line">Result result &#x3D; table.get(get);  </span><br><span class="line">for (Cell cell : result.rawCells()) &#123;  </span><br><span class="line">System.out.println(  </span><br><span class="line">&quot;行键:&quot; + new String(CellUtil. getCellKeyAsString(cell)) + &quot;\t&quot; +  </span><br><span class="line">&quot;列族:&quot; + new String(CellUtil.cloneFamily(cell)) + &quot;\t&quot; +   </span><br><span class="line">&quot;列名:&quot; + new String(CellUtil.cloneQualifier(cell)) + &quot;\t&quot; +   </span><br><span class="line">&quot;值:&quot; + new String(CellUtil.cloneValue(cell)) + &quot;\t&quot; +  </span><br><span class="line">&quot;时间戳:&quot; + cell.getTimestamp());  </span><br><span class="line">&#125;  </span><br><span class="line">table.close();  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">插入数据后，可以根据行键进行查询，比如get方法，此代码中，先建立连接，通过get对象描述查询条件，再通过table.get方法进行实际查询（紫色两行）。查询结构写入result中，由于get方法一次获取一个逻辑行，即可能包含多个键值对，因此查询结构通过循环的方法将逐个键值对输出显示。</span><br><span class="line"></span><br><span class="line">显示时，getCellKeyAsString用来获取行键，cloneFamily用来获取列族，cloneQualifier用来获取列名，cloneValue获取具体值，getTimestamp获取时间戳</span><br></pre></td></tr></table></figure><h3 id="查询数据-Scan"><a href="#查询数据-Scan" class="headerlink" title="查询数据-Scan"></a>查询数据-Scan</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public static void scanTable(String tableName) throws IOException &#123;  </span><br><span class="line">       Table table &#x3D; connection.getTable(TableName.valueOf(&quot;student&quot;)); </span><br><span class="line">       Scan scan &#x3D; new Scan();  </span><br><span class="line">       ResultScanner results &#x3D; table.getScanner(scan);  </span><br><span class="line">       for (Result result : results) &#123;  </span><br><span class="line">           for (Cell cell : result.rawCells()) &#123;  </span><br><span class="line">               System.out.println(  </span><br><span class="line">                       &quot;行键:&quot; + new String(CellUtil.getCellKeyAsString(cell)) + &quot;\t&quot; +  </span><br><span class="line">                       &quot;列族:&quot; + new String(CellUtil.cloneFamily(cell)) + &quot;\t&quot; +   </span><br><span class="line">                       &quot;列名:&quot; + new String(CellUtil.cloneQualifier(cell)) + &quot;\t&quot; +   </span><br><span class="line">                       &quot;值:&quot; + new String(CellUtil.cloneValue(cell)) + &quot;\t&quot; +  </span><br><span class="line">                       &quot;时间戳:&quot; + cell.getTimestamp());  </span><br><span class="line">           &#125;  </span><br><span class="line">       &#125;   </span><br><span class="line">       table.close();</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Scan的操作语法与get类似，都需要建立连接，描述查询条件，再进行实际查询，核心语句是table.getScanner，后续的显示页和之前代码基本相同，但是，由于scan的结果得到的多个逻辑行，且每个逻辑行包含多个键值对，因此采用二层循环的方式来显示每一个键值对的内容，（红框所示）。</span><br></pre></td></tr></table></figure><h3 id="删除行和列"><a href="#删除行和列" class="headerlink" title="删除行和列"></a>删除行和列</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Table table &#x3D; connection.getTable(TableName.valueOf(tableName));</span><br><span class="line"></span><br><span class="line">1，删除指定行       </span><br><span class="line">Delete delete1 &#x3D; new Delete(Bytes.toBytes(‘001’));</span><br><span class="line"></span><br><span class="line">2，删除指定列族</span><br><span class="line">Delete delete2 &#x3D; new Delete(Bytes.toBytes(‘002’));</span><br><span class="line">delete2.addFamily(Bytes.toBytes(‘stuinfo’));</span><br><span class="line"></span><br><span class="line">3，删除指定列族中的列</span><br><span class="line">Delete delete3 &#x3D; new Delete(Bytes.toBytes(‘003’));</span><br><span class="line">delete3.addColumn(Bytes.toBytes(‘grades’),Bytes.toBytes(‘math’));</span><br><span class="line"></span><br><span class="line">调用 table.delete执行删除  </span><br><span class="line">table.delete(delete);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">完成了数据的新增和查询，Hbase api也提供删除数据，可以指定行键，列族和列进行删除，</span><br><span class="line">如第一个delete对象只有行键属性，因此会删除一个逻辑行，即所有行键为001的键值对都将被删除；</span><br><span class="line">第二个delete对象通过addfamily方法加入了列族参数stuinfo，因此只会删除再stuinfo列族中，行键为002的键值对；</span><br><span class="line">第三个delete对象通过addColumn方法加入列名参数math，同时也指定了列族grades，因此只会删除行键为003，grades列族中math列的键值对。</span><br><span class="line"></span><br><span class="line">最后调用table.delete方法来完成删除。</span><br></pre></td></tr></table></figure><h2 id="07-Hbase-Python编程方法"><a href="#07-Hbase-Python编程方法" class="headerlink" title="07 Hbase Python编程方法"></a>07 Hbase Python编程方法</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-3</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-3.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-3.html</id>
    <published>2023-04-17T08:46:58.000Z</published>
    <updated>2023-04-17T15:26:37.989Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第三章-Hbase"><a href="#第三章-Hbase" class="headerlink" title="第三章 Hbase"></a>第三章 Hbase</h1><h2 id="01-Hbase数据模型"><a href="#01-Hbase数据模型" class="headerlink" title="01 Hbase数据模型"></a>01 Hbase数据模型</h2><h3 id="逻辑模型"><a href="#逻辑模型" class="headerlink" title="逻辑模型"></a>逻辑模型</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172154639.png" alt="image-20230417215406265"></p><h3 id="HBase相关概念"><a href="#HBase相关概念" class="headerlink" title="HBase相关概念"></a>HBase相关概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（1）表（table）：HBase采用表来组织数据；</span><br><span class="line">（2）行（row）：每个表都由行组成，每个行由行键（row key）来标识，行键可以是任意字符串；</span><br><span class="line">（3）列族（column family）：一个table有许多个列族，列族是列的集合，属于表结构，也是表的基本访问控制单元；</span><br><span class="line">（4）列标识（column qualifier）：属于某一个Column Family：Column Qualifier形式标识，每条记录可动态添加</span><br><span class="line">（5）时间戳（timestamp）：时间戳用来区分数据的不同版本；</span><br><span class="line">（6）单元格（cell）：在table中，cell中存储的数据没有数据类型，是字节数组byte[] ，通过&lt;RowKey，Column Family: Column Qualifier，Timestamp&gt;元组来访问单元格</span><br></pre></td></tr></table></figure><h3 id="物理模型"><a href="#物理模型" class="headerlink" title="物理模型"></a>物理模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库特点：</span><br><span class="line">表结构预先定义；</span><br><span class="line">每列的数据类型不同；</span><br><span class="line">空值占用存储空间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HBase特点：</span><br><span class="line">只需定义表名和列族，可以动态添加列族和列；</span><br><span class="line">数据都是字符串类型；</span><br><span class="line">空值不占用存储空间；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172203728.png" alt="image-20230417220304381"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172214589.png" alt="image-20230417221413176"></p><h3 id="实际存储方式"><a href="#实际存储方式" class="headerlink" title="实际存储方式"></a>实际存储方式</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172218013.png" alt="image-20230417221843341"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172220818.png" alt="image-20230417222012709"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172220613.png" alt="image-20230417222052208"></p><h2 id="02-Hbase数据定义"><a href="#02-Hbase数据定义" class="headerlink" title="02 Hbase数据定义"></a>02 Hbase数据定义</h2><h3 id="HBase-Shell"><a href="#HBase-Shell" class="headerlink" title="HBase Shell"></a>HBase Shell</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase Shell：HBase的命令行工具，最简单的接口，适合HBase管理使用；</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost bin]# hbase shell</span><br><span class="line">HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.</span><br><span class="line">Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell</span><br><span class="line">Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt; </span><br><span class="line"></span><br><span class="line">命令：help,status,version,exit,quit</span><br></pre></td></tr></table></figure><h3 id="数据定义"><a href="#数据定义" class="headerlink" title="数据定义"></a>数据定义</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172230877.png" alt="image-20230417223050379"></p><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法：creat‘表名’，‘列族名’</span><br><span class="line">描述：</span><br><span class="line">●  必须指定表名和列族；</span><br><span class="line">●  可以创建多个列族；</span><br><span class="line">●  可以对标和列族指明一些参数；</span><br><span class="line">●  参数大小写敏感；</span><br><span class="line">●  字符串参数需要包含在单引号中；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172233954.png" alt="image-20230417223313646"></p><h4 id="表相关操作"><a href="#表相关操作" class="headerlink" title="表相关操作"></a>表相关操作</h4><h5 id="exsit"><a href="#exsit" class="headerlink" title="exsit"></a>exsit</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exsit：查看某个表是否存在</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172239630.png" alt="image-20230417223907126"></p><h5 id="List"><a href="#List" class="headerlink" title="List"></a>List</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List：查看当前所有的表名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172239913.png" alt="image-20230417223934111"></p><h5 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe：查看选定表的列族及其参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172236116.png" alt="image-20230417223645853"></p><h5 id="Alter"><a href="#Alter" class="headerlink" title="Alter"></a>Alter</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Alter：修改表结构</span><br><span class="line">功能：</span><br><span class="line">修改表中列族的参数信息；</span><br><span class="line">增加列族；</span><br><span class="line">移除或删除已有的列族；</span><br><span class="line"></span><br><span class="line">注意：删除列族时，表中至少有两个列族组成；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172241746.png" alt="image-20230417224143643"></p><h5 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop：删除表</span><br><span class="line">注意：删除表之前需要先禁用表。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172242902.png" alt="image-20230417224227529"></p><h5 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate：删除表中所有数据，想到于对表完成禁用、删除，按原结构重新建立表结构的过程</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172242617.png" alt="image-20230417224251643"></p><h2 id="03-Hbase数据操作"><a href="#03-Hbase数据操作" class="headerlink" title="03 Hbase数据操作"></a>03 Hbase数据操作</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172254865.png" alt="image-20230417225437539"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在HBase中对数据的增删改查命令如表所示，由put命令向表中添加和修改数据，get和scan命令用来查询数据，delete删除列族或列的数据。接下来详细介绍这几个命令的具体用法。</span><br></pre></td></tr></table></figure><h3 id="put"><a href="#put" class="headerlink" title="put"></a>put</h3><h4 id="为单元格插入数据"><a href="#为单元格插入数据" class="headerlink" title="为单元格插入数据"></a>为单元格插入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">语法：put  ‘表名’，‘行键’，‘列族：列限定符’，‘单元格值’，时间戳</span><br><span class="line">描述：必须指定表名、行键、列族、列限定符。</span><br><span class="line">参数区分大小，字符串使用单引号。</span><br><span class="line">只能插入单条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172255143.png" alt="image-20230417225550657"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在HBase中，更新数据时，不管是添加新的数据还是修改数据都使用put命令，它的语法结构如ppt所示，put命令所带的第一个参数为表名，指定某一张表，第二参数为行键的名称，用来指定某一行，第三个参数是为列族和列的名称，中间用冒号隔开，列族名必须是已经创建的，否则HBase会报错；列名是临时定义的，所以列族里的列是可以随意扩展的。第四个参数为单元格的值，在HBase里，所有数据都是字符串的形式。最后一个参数为时间戳，如果不设置时间戳，系统会自动插入当前时间为时间戳。</span><br><span class="line">HBase中所有命令参数是区分大小写的，字符串是需要包含在单引号中的，这一点在介绍后面操作命令不再提示。</span><br><span class="line">从命令形式来看，put只能插入单元格的数据，如果需要将逻辑表中的一行数据插入到HBase中需要执行几条put命令。</span><br><span class="line">比如，需要将此逻辑表的第一行数据（左边图和红色虚线框）插入HBase中，需要执行5条命令（右图）</span><br></pre></td></tr></table></figure><h4 id="更新单元格数据"><a href="#更新单元格数据" class="headerlink" title="更新单元格数据"></a>更新单元格数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">描述：</span><br><span class="line">如果指定的单元格已经存在，则put为更新数据；</span><br><span class="line">单元格会保存指定version&#x3D;&gt;n的多个版本数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172302591.png" alt="image-20230417230227658"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">另外如果由‘表名’，‘行键’，‘列族：列限定符’指定的单元格已经存在表中，则执行put命令为数据更新操作，</span><br><span class="line"></span><br><span class="line">比如，在执行了左边的5条命令后（左图），再执行这条命令（鼠标指向“put ‘Student’, ‘0001’, ‘StuInfo:Name’,‘Tom Green‘,1），学号为1的学生姓名将改成了tom green。</span><br><span class="line"></span><br><span class="line">默认情况下数据更新后，旧版本的数据将不可见，但如果建表时对列族指定了Version属性值，则旧版数据依然存在，用户查询时可以获得最新的多个版本；</span><br></pre></td></tr></table></figure><h3 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">语法：delete  ‘表名’，‘行键’，‘列族&lt;：列限定符&gt;’，&lt;时间戳&gt;</span><br><span class="line">描述：必须指定表名、行键和列族，列限定符和时间戳是可选参数；</span><br><span class="line">Delete最小删除粒度为单元格，且不能跨列族删除。</span><br><span class="line"></span><br><span class="line">(1)delete ‘Student’, ‘0001’, ‘Grades’</span><br><span class="line">(2)delete ‘Student’, ‘0001’, ‘Grades:Math’ </span><br><span class="line">(3)delete ‘Student’, ‘0001’, ‘Grades:Math’,2</span><br><span class="line">(4)Deleteall ‘Student’, ‘0001’</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase中删除数据采用delete命令，其语法与put命令类似，必须指定表名，行键，和列族。而列限定符和时间戳是可选的</span><br><span class="line">下面通过举例说明delete的使用，如第一条命令，只指定了表名行键和列族，表示删除student表中，学号为0001的学生所有的成绩信息。即将表中第一行grades列族的信息全部删除。</span><br><span class="line">第二条命令，指定了列族和列限定符，表示只删除这个学生的数学成绩。</span><br><span class="line">第三条命令，指定了列族和列限定符的同时，还指定了时间戳，表示所有时间戳小于等于2的数据都会被删掉。注意这里不是只删除时间戳等于2的数据。</span><br><span class="line"></span><br><span class="line">从上面语法和命令来看，delete最小的删除粒度为单元格，而且不能跨列族删除，如果想删除表中所有列族在某个行键上的数据，也就是说想删除一个逻辑行，可以使用deleteall命令，例如第四条命令，则删除0001学号学生的所有信息，包括stuinfo列族中的基本信息和grades列族中的所有成绩信息。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete操作并不会马上删除数据，只是将对应的数据打上删除标记（tombstone），只有在数据产生合并时，数据才会被删除。</span><br></pre></td></tr></table></figure><h3 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h3><h4 id="get"><a href="#get" class="headerlink" title="get"></a>get</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">get：根据行键获取一条数据</span><br><span class="line">scan：扫描一个表，可以指定行键范围，或使用过滤器</span><br><span class="line">语法：get  ‘表名’，‘行键’，&lt;‘列族：列限定符’，时间戳&gt;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172313248.png" alt="image-20230417231312108"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第三条命令指定了列族和时间戳范围，</span><br><span class="line">第四条命令则指定列族和显示的版本数，其结果如图所示（蓝色图），在执行此命令之前先向表的stuinfo列族的name列插入了三个版本的数据，注意这里前提是stuinfo列族在创建时已指定VERSION参数可以保存最近的3个版本的数据。在向同一单元格put三条数据后，再执行第四条命令，显示的结果可以看到，只将最近两个更新的数据显示出来了（红色虚线框）</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172315547.png" alt="image-20230417231510110"></p><h4 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">语法：scan  ‘表名’，&#123;&lt; ‘列族：列限定符’，时间戳&gt;&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172321582.png" alt="image-20230417232102333"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">另外一种数据查询方式使用scan命令进行全表扫描，scan命令必须带的参数是表名，其他参数都可选，还可以指定输出行键范围，以及使用过滤器来对全表数据进行过滤显示。</span><br><span class="line">依然通过举例说明scan命令的方法。</span><br><span class="line">第一条命令指定表名查询全表数据；如图所示将表中所有行和所有列族信息都显示出来了。</span><br><span class="line">第二条命令指定列族名称，显示student表中stuinfo列族的所有数据，注意与get不同的是，get只获得某一行的，而scan获取所有行的stuinfo列族数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172324207.png" alt="image-20230417232442038"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第五条命令指定输出行的范围；显示结果输出起始行和结束行但不包括结束行的数据，如图的命令只显示了001行的数据，并没有显示003行。</span><br><span class="line"></span><br><span class="line">另外这些限定条件可以组合使用，中间使用逗号隔开，如第六条命令所示：查询起始行为001，结束行为002的所有行的stuinfo列族的数据信息。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-2</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.html</id>
    <published>2023-04-17T08:46:52.000Z</published>
    <updated>2023-04-19T13:24:55.759Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第二章-Hbase"><a href="#第二章-Hbase" class="headerlink" title="第二章 Hbase"></a>第二章 Hbase</h1><h2 id="01-Hbase简介"><a href="#01-Hbase简介" class="headerlink" title="01 Hbase简介"></a>01 Hbase简介</h2><h3 id="什么是HBase"><a href="#什么是HBase" class="headerlink" title="什么是HBase"></a>什么是HBase</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase是一个开源的NoSQL数据库，参考google的BigTable建模，使用Java语言实现，运行于HDFS文件系统上，为Hadoop提供类似BigTable的服务，可以存储海量稀疏的数据，并具备一定的容错性、高可靠性及伸缩性。</span><br><span class="line"></span><br><span class="line">具备NoSQL数据库的特点：</span><br><span class="line">不支持SQL的跨行事务</span><br><span class="line">不满足完整性约束条件</span><br><span class="line">灵活的数据模型</span><br></pre></td></tr></table></figure><h3 id="HBase的发展历程"><a href="#HBase的发展历程" class="headerlink" title="HBase的发展历程"></a>HBase的发展历程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Apache HBase最初是Powerset公司为了处理自然语言搜索产生的海量数据而开展的项目</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171750807.png" alt="image-20230417175013383"></p><h3 id="HBase特性"><a href="#HBase特性" class="headerlink" title="HBase特性"></a>HBase特性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">容量巨大</span><br><span class="line">列存储</span><br><span class="line">稀疏性</span><br><span class="line">扩展性</span><br><span class="line">高可靠性</span><br></pre></td></tr></table></figure><h4 id="容量巨大"><a href="#容量巨大" class="headerlink" title="容量巨大"></a>容量巨大</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171755056.png" alt="image-20230417175551796"></p><h4 id="列存储"><a href="#列存储" class="headerlink" title="列存储"></a>列存储</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171758370.png" alt="image-20230417175839159"></p><h4 id="稀疏性"><a href="#稀疏性" class="headerlink" title="稀疏性"></a>稀疏性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。</span><br></pre></td></tr></table></figure><h4 id="扩展性"><a href="#扩展性" class="headerlink" title="扩展性"></a>扩展性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">纵向扩展：不断优化主服务器的性能，提高存储空间和性能</span><br><span class="line"></span><br><span class="line">横向扩展：不断向集群添加服务器来提供存储空间和性能</span><br><span class="line"></span><br><span class="line">HBase是横向扩展的，理论上无限横向扩展</span><br></pre></td></tr></table></figure><h4 id="高可靠性"><a href="#高可靠性" class="headerlink" title="高可靠性"></a>高可靠性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于HDFS的多副本机制</span><br><span class="line"></span><br><span class="line">WAL（Write-Ahead-Log）预写机制</span><br><span class="line"></span><br><span class="line">Replication机制</span><br></pre></td></tr></table></figure><h3 id="Hbase安装"><a href="#Hbase安装" class="headerlink" title="Hbase安装"></a>Hbase安装</h3><h4 id="单机模式"><a href="#单机模式" class="headerlink" title="单机模式"></a>单机模式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以本地文件系统作为基础，所有进程运行在一个JVM上，一般用于测试</span><br></pre></td></tr></table></figure><h4 id="伪分布式"><a href="#伪分布式" class="headerlink" title="伪分布式"></a>伪分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">主从模式，以hdfs文件系统为基础，所有进程运行在一个JVM中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改：hbase-env.sh，hbase-site.xml文件</span><br><span class="line">hbase-env.sh:配置java路径，配置zookeeper是否随hbase一起启动，还是先启动zookeeper，再hbase</span><br><span class="line"></span><br><span class="line">hbase-site.xml：主要是zookeeper,hadoop，是否集群部署的一些设置</span><br></pre></td></tr></table></figure><h4 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">主从模式，以hdfs文件系统为基础，守护进程运行在多个jvm上</span><br><span class="line"></span><br><span class="line">除了上述两个文件，还需设置集群有哪些节点文件的配置</span><br></pre></td></tr></table></figure><h2 id="02-HDFS原理"><a href="#02-HDFS原理" class="headerlink" title="02 HDFS原理"></a>02 HDFS原理</h2><h3 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS- 分布式文件系统"></a>HDFS- 分布式文件系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HDFS即Hadoop分布式文件系统（Hadoop Distributed File System）</span><br><span class="line">提供高可靠性和高吞吐量的文件存储服务</span><br><span class="line"></span><br><span class="line">通过软件设计来保证系统的可靠性</span><br><span class="line"></span><br><span class="line">具有容错性，高可靠性，高可扩展性，高吞吐率。</span><br></pre></td></tr></table></figure><h3 id="HDFS基本架构"><a href="#HDFS基本架构" class="headerlink" title="HDFS基本架构"></a>HDFS基本架构</h3><h3 id="HDFS-块"><a href="#HDFS-块" class="headerlink" title="HDFS- 块"></a>HDFS- 块</h3><h3 id="HDFS-NameNode"><a href="#HDFS-NameNode" class="headerlink" title="HDFS-NameNode"></a>HDFS-NameNode</h3><h3 id="HDFS-SecondaryNameNode"><a href="#HDFS-SecondaryNameNode" class="headerlink" title="HDFS-SecondaryNameNode"></a>HDFS-SecondaryNameNode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">定期的合并edits和fsimage文件</span><br><span class="line">Checkpiont：合并的时间点，默认3600秒，或editlog文件达到64M。</span><br></pre></td></tr></table></figure><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2%5C202304171810344.png" alt="image-20230417181053843"></p><h3 id="HDFS-DataNode"><a href="#HDFS-DataNode" class="headerlink" title="HDFS-DataNode"></a>HDFS-DataNode</h3><h3 id="HDFS读文件流程"><a href="#HDFS读文件流程" class="headerlink" title="HDFS读文件流程"></a>HDFS读文件流程</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171814207.png" alt="image-20230417181429835"></p><h4 id="HDFS读写机制-读文件机制"><a href="#HDFS读写机制-读文件机制" class="headerlink" title="HDFS读写机制-读文件机制"></a>HDFS读写机制-读文件机制</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171815977.png" alt="image-20230417181523597"></p><h4 id="HDFS读写机制-写文件机制"><a href="#HDFS读写机制-写文件机制" class="headerlink" title="HDFS读写机制-写文件机制"></a>HDFS读写机制-写文件机制</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171816393.png" alt="image-20230417181609459"></p><h3 id="HDFS副本机制"><a href="#HDFS副本机制" class="headerlink" title="HDFS副本机制"></a>HDFS副本机制</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171819478.png" alt="image-20230417181934890"></p><h3 id="HDFS容错"><a href="#HDFS容错" class="headerlink" title="HDFS容错"></a>HDFS容错</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171821864.png" alt="image-20230417182129386"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、NameNode出错：用Secondary NameNode备份的fsimage恢复</span><br><span class="line">2、DataNode出错：DataNode与NameNode通过“心跳”报告状态，当DataNode失效后，副本数减少，而NameNode会定期检查各节点的副本数量， 检查出问题后会启动数据冗余机制。</span><br><span class="line">3、数据出错：数据写入同时保存总和校验码，读取时校验。</span><br></pre></td></tr></table></figure><h2 id="03-Hbase组件和功能"><a href="#03-Hbase组件和功能" class="headerlink" title="03 Hbase组件和功能"></a>03 Hbase组件和功能</h2><h3 id="HBase-架构"><a href="#HBase-架构" class="headerlink" title="HBase 架构"></a>HBase 架构</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172115566.png" alt="image-20230417211518368"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172116455.png" alt="image-20230417211612969"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Client</span><br><span class="line">包含访问HBase的接口(用户通过client来访问hbase)并维护cache(region的位置)来加快对HBase的访问</span><br><span class="line"></span><br><span class="line">Zookeeper</span><br><span class="line">保证任何时候，集群中只有一个活跃master</span><br><span class="line">存贮所有Region的寻址入口。</span><br><span class="line">实时监控Region server的上线和下线信息。并实时通知Master</span><br><span class="line">存储HBase的schema和table元数据</span><br><span class="line"></span><br><span class="line">Master</span><br><span class="line">为Region server分配region</span><br><span class="line">负责Region server的负载均衡</span><br><span class="line">发现失效的Region server并重新分配其上的region</span><br><span class="line">管理用户对table的增删改操作</span><br><span class="line"></span><br><span class="line">RegionServer</span><br><span class="line">Region server维护region，处理对这些region的IO请求</span><br><span class="line">Region server负责切分在运行过程中变得过大的region</span><br><span class="line"></span><br><span class="line">Region</span><br><span class="line">HBase自动把表水平划分成多个区域(region)，每个region会保存一个表里面某段连续的数据</span><br><span class="line"></span><br><span class="line">每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变）</span><br><span class="line"></span><br><span class="line">当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上。</span><br><span class="line"></span><br><span class="line">Memstore与storefile</span><br><span class="line">一个region由多个store组成，一个store对应一个CF（列族）</span><br><span class="line"></span><br><span class="line">store包括位于内存中的memstore和位于磁盘的storefile写操作先写入memstore，当memstore中的数据达到某个阈值，hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile</span><br><span class="line"></span><br><span class="line">当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile</span><br><span class="line"></span><br><span class="line">当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡</span><br><span class="line"></span><br><span class="line">客户端检索数据，先在memstore找，找不到再找storefile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。</span><br><span class="line">HRegion由一个或者多个Store组成，每个store保存一个columns family。</span><br><span class="line">每个Strore又由一个memStore和0至多个StoreFile组成。如图：StoreFile以HFile格式保存在HDFS上。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172142998.png" alt="image-20230417214228696"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172144442.png" alt="image-20230417214420908"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-1</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-1.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-1.html</id>
    <published>2023-04-17T08:46:47.000Z</published>
    <updated>2023-04-19T10:07:34.371Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><h2 id="数据库基本知识"><a href="#数据库基本知识" class="headerlink" title="数据库基本知识"></a>数据库基本知识</h2><h3 id="什么是数据库？"><a href="#什么是数据库？" class="headerlink" title="什么是数据库？"></a>什么是数据库？</h3><h3 id="什么是数据模型？"><a href="#什么是数据模型？" class="headerlink" title="什么是数据模型？"></a>什么是数据模型？</h3><h4 id="有哪些数据模型？"><a href="#有哪些数据模型？" class="headerlink" title="有哪些数据模型？"></a>有哪些数据模型？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库技术发展至今，传统数据库根据不同的数据模型，主要有以下几种：层次型、网状型和关系型。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171657536.png" alt="image-20230417165719863"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171700245.png" alt="image-20230417170008924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">关系模型要点回顾   </span><br><span class="line">1. 数据结构：</span><br><span class="line">现实世界的实体以及实体之间的各种联系均用关系来表示</span><br><span class="line">数据逻辑结构：二维表</span><br><span class="line">   </span><br><span class="line">   2. 完整性约束条件</span><br><span class="line">域完整性，实体完整性，参照完整性</span><br><span class="line"></span><br><span class="line">    3. 关系操作</span><br><span class="line">选择，投影，连接 等等关系运算；操作对象和结果都是集合</span><br></pre></td></tr></table></figure><h3 id="关系型数据库的优点"><a href="#关系型数据库的优点" class="headerlink" title="关系型数据库的优点"></a>关系型数据库的优点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库的特点</span><br><span class="line">（1）容易理解：用二维表表示</span><br><span class="line">（2）使用方便：通用的SQL语言。</span><br><span class="line">（3）易于维护：丰富的完整性约束大大减低了数据冗余和数据不一致的可能性。</span><br></pre></td></tr></table></figure><h3 id="关系型数据库的不足"><a href="#关系型数据库的不足" class="headerlink" title="关系型数据库的不足"></a>关系型数据库的不足</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对海量数据的读写效率低</span><br><span class="line">    表中有大量数据时，数据的读写速率非常的缓慢</span><br><span class="line">无法适应多变的数据结构</span><br><span class="line">    现代网络中存在大量的半结构化、非结构化数据，针对结构化数据而设计的关系型数据库系统来说，对这些不断变化的数据结构，很难进行高效的处理。</span><br><span class="line">高并发读写的瓶颈</span><br><span class="line">     当数据量达到一定规模时由于关系型数据库的系统逻辑非常复杂，使得在并发处理时非常容易发生死锁，导致其读写速度下滑严重。</span><br><span class="line">可扩展性的限制</span><br><span class="line">由于关系型数据库存在类似的join操作，使得数据库在扩展方面很困难。</span><br></pre></td></tr></table></figure><h2 id="NOSQL数据库理论基础"><a href="#NOSQL数据库理论基础" class="headerlink" title="NOSQL数据库理论基础"></a>NOSQL数据库理论基础</h2><h3 id="什么是NoSQL"><a href="#什么是NoSQL" class="headerlink" title="什么是NoSQL"></a>什么是NoSQL</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171707315.png" alt="image-20230417170659240"></p><h3 id="分布式数据库的特征"><a href="#分布式数据库的特征" class="headerlink" title="分布式数据库的特征"></a>分布式数据库的特征</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">分布式数据库必须具有如下特征，才能应对不断增长的海量数据。</span><br><span class="line">● 高可扩展性：分布式数据库必须具有高可扩展性，能够动态地增添存储节点以实现存储容量的线性扩展</span><br><span class="line">● 高并发性：分布式数据库必须及时响应大规模用户的读&#x2F;写请求，能对海量数据进行随机读写</span><br><span class="line">● 高可用性：分布式数据库必须提供容错机制，能够实现对数据的冗余备份，保证数据和服务的高度可靠性</span><br></pre></td></tr></table></figure><h3 id="NoSQL的特点"><a href="#NoSQL的特点" class="headerlink" title="NoSQL的特点"></a>NoSQL的特点</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171713669.png" alt="image-20230417171305105"></p><h3 id="分布式数据库的数据管理"><a href="#分布式数据库的数据管理" class="headerlink" title="分布式数据库的数据管理"></a>分布式数据库的数据管理</h3><h4 id="什么是数据库系统？"><a href="#什么是数据库系统？" class="headerlink" title="什么是数据库系统？"></a>什么是数据库系统？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库系统  &#x3D;  数据库管理系统     +     数据库</span><br></pre></td></tr></table></figure><h4 id="什么是数据库管理系统？"><a href="#什么是数据库管理系统？" class="headerlink" title="什么是数据库管理系统？"></a>什么是数据库管理系统？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库管理系统(Database Management System)是一种操纵和管理数据库的大型软件，用于建立、使用和维护数据库，简称DBMS。主要任务就是对外提供数据，对内要管理数据。</span><br></pre></td></tr></table></figure><h4 id="数据处理方式：集中式VS分布式"><a href="#数据处理方式：集中式VS分布式" class="headerlink" title="数据处理方式：集中式VS分布式"></a>数据处理方式：集中式VS分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">集中式数据库是指数据库中的数据集中存储在一台计算机上，数据的处理也集中在一台机器上完成。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分布式数据库是指利用高速计算机网络将物理上分散的多个数据存储单元连接起来组成一个逻辑上统一的数据库。</span><br></pre></td></tr></table></figure><h4 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">C:一致性（consistency）（强一致性）</span><br><span class="line">它是指任何一个读操作总是能够读到之前完成的写操作的结果。所有节点在同一时间具有相同的数据。</span><br><span class="line"></span><br><span class="line">A:可用性（Availability）（高可用性）</span><br><span class="line">每个请求都能在确定时间内返回一个响应，无论请求是成功或失败。</span><br><span class="line"></span><br><span class="line">P:分区容忍性（Partition Tolerance）</span><br><span class="line">它是指在一个集群，即系统中的一部分节点无法和其他节点进行通信，系统也能正常运行。也就是说，系统中部分信息的丢失或失败不会影响系统的继续运作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">当处理CAP的问题时，可以有几个明显的选择：</span><br><span class="line">CA：也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最简单的做法是把所有与事务相关的内容都放到同一台机器上。</span><br><span class="line">CP：也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务</span><br><span class="line">AP：也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允许系统返回不一致的数据</span><br></pre></td></tr></table></figure><h5 id="设计原则：在C、A、P之中取舍"><a href="#设计原则：在C、A、P之中取舍" class="headerlink" title="设计原则：在C、A、P之中取舍"></a>设计原则：在C、A、P之中取舍</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171721678.png" alt="image-20230417172155421"></p><h2 id="ACID、BASE与一致性"><a href="#ACID、BASE与一致性" class="headerlink" title="ACID、BASE与一致性"></a>ACID、BASE与一致性</h2><h3 id="ACID与BASE"><a href="#ACID与BASE" class="headerlink" title="ACID与BASE"></a>ACID与BASE</h3><h4 id="为什么会出现ACID、BASE-？"><a href="#为什么会出现ACID、BASE-？" class="headerlink" title="为什么会出现ACID、BASE ？"></a>为什么会出现ACID、BASE ？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CAP理论定义了分布式存储的根本问题，但并没有指出一致性和可用性之间到底应该如何权衡。于是出现了ACID、BASE ，给出了权衡A与C的一种可行方案。</span><br><span class="line">ACID和BASE代表了在一致性-可用性两点之间进行选择的设计哲学</span><br><span class="line">ACID强调一致性被关系数据库使用，BASE强调可用性被大多数Nosql使用</span><br></pre></td></tr></table></figure><h4 id="ACID是什么？"><a href="#ACID是什么？" class="headerlink" title="ACID是什么？"></a>ACID是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">指数据库事务正确执行的四个基本要素的缩写。包含：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">原子性：一个事务的所有系列操作步骤被看成是一个动作，所有的步骤要么全部完成要么都不会完成。</span><br><span class="line"></span><br><span class="line">一致性：事务执行前后，数据库的状态都满足所有的完整性约束。不能发生表与表之间存在外键约束，但是有数据却违背这种约束性。</span><br><span class="line"></span><br><span class="line">隔离性：并发执行的事务是隔离的，保证多个事务互不影响，隔离能够确保并发执行的事务能够顺序一个接一个执行，通过隔离，一个未完成事务不会影响另外一个未完成事务。</span><br><span class="line"></span><br><span class="line">持久性：一个事务一旦提交，它对数据库中数据的改变就应该是永久性的，不会因为和其他操作冲突而取消这个事务。</span><br></pre></td></tr></table></figure><h4 id="BASE原则又是什么？"><a href="#BASE原则又是什么？" class="headerlink" title="BASE原则又是什么？"></a>BASE原则又是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BASE原则 &#x3D; 基本可用性（Basically Available）+软状态（Soft state）+最终一致性（Eventuallyconsistent）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基本可用性：分布式系统在出现故障的时候，允许损失部分可用性，即保证核心功能或者当前最重要功能可用，但是其他功能会被削弱。</span><br><span class="line"></span><br><span class="line">软状态：允许系统数据存在中间状态，但不会影响到系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步时存在延时。</span><br><span class="line"></span><br><span class="line">最终一致性：要求系统数据副本最终能够一致，而不需要实时保证数据副本一致。最终一致性是弱一致性的一种特殊情况。</span><br></pre></td></tr></table></figure><h2 id="NoSQL数据库分类"><a href="#NoSQL数据库分类" class="headerlink" title="NoSQL数据库分类"></a>NoSQL数据库分类</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171731393.png" alt="image-20230417173132275"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-5</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-5.html</id>
    <published>2023-04-15T17:08:43.000Z</published>
    <updated>2023-04-17T07:37:40.431Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-5"><a href="#第十四周-消息队列之Kafka从入门到小牛-5" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-5"></a>第十四周 消息队列之Kafka从入门到小牛-5</h1><h2 id="实战：Flume集成Kafka"><a href="#实战：Flume集成Kafka" class="headerlink" title="实战：Flume集成Kafka"></a>实战：Flume集成Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在实际工作中flume和kafka会深度结合使用</span><br><span class="line">1：flume采集数据，将数据实时写入kafka</span><br><span class="line">2：flume从kafka中消费数据，保存到hdfs，做数据备份</span><br><span class="line"></span><br><span class="line">下面我们就来看一个综合案例</span><br><span class="line">使用flume采集日志文件中产生的实时数据，写入到kafka中，然后再使用flume从kafka中将数据消费出来，保存到hdfs上面</span><br><span class="line">那为什么不直接使用flume将采集到的日志数据保存到hdfs上面呢？</span><br><span class="line">因为中间使用kafka进行缓冲之后，后面既可以实现实时计算，又可以实现离线数据备份，最终实现离线计算，所以这一份数据就可以实现两种需求，使用起来很方便，所以在工作中一般都会这样做。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171406839.png" alt="image-20230417140517700"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">下面我们来实现一下这个功能</span><br><span class="line">其实在Flume中，针对Kafka提供的有KafkaSource和KafkaSink</span><br><span class="line">KafkaSource是从kafka中读取数据</span><br><span class="line">KafkaSink是向kafka中写入数据</span><br><span class="line"></span><br><span class="line">所以针对我们目前这个架构，主要就是配置Flume的Agent。</span><br><span class="line">需要配置两个Agent：</span><br><span class="line">第一个Agent负责实时采集日志文件，将采集到的数据写入Kafka中</span><br><span class="line">第二个Agent负责从Kafka中读取数据，将数据写入HDFS中进行备份(落盘)</span><br><span class="line">针对第一个Agent：</span><br><span class="line">source：ExecSource，使用tail -F监控日志文件即可</span><br><span class="line">channel：MemoryChannel</span><br><span class="line">sink：KafkaSink</span><br><span class="line"></span><br><span class="line">针对第二个Agent</span><br><span class="line">Source：KafkaSource</span><br><span class="line">channel：MemoryChannel</span><br><span class="line">sink：HdfsSink</span><br><span class="line"></span><br><span class="line">这里面这些组件其实只有KafkaSource和KafkaSink我们没有使用过，其它的组件都已经用过了。</span><br></pre></td></tr></table></figure><h3 id="配置Agent"><a href="#配置Agent" class="headerlink" title="配置Agent"></a>配置Agent</h3><h4 id="file-to-kafka-conf"><a href="#file-to-kafka-conf" class="headerlink" title="file-to-kafka.conf"></a>file-to-kafka.conf</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来配置第一个Agent：</span><br><span class="line">文件名为： file-to-kafka.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;test.log</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"></span><br><span class="line"># 指定topic名称</span><br><span class="line">a1.sinks.k1.kafka.topic &#x3D; test_r2p5</span><br><span class="line"># 指定kafka地址，多个节点地址使用逗号分割</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03</span><br><span class="line"># 一次向kafka中写多少条数据，默认值为100，在这里为了演示方便，改为1</span><br><span class="line"># 在实际工作中这个值具体设置多少需要在传输效率和数据延迟上进行取舍</span><br><span class="line"># 如果kafka后面的实时计算程序对数据的要求是低延迟，那么这个值小一点比较好</span><br><span class="line"># 如果kafka后面的实时计算程序对数据延迟没什么要求，那么就考虑传输性能，一次多传输一些</span><br><span class="line"># 建议这个值的大小和ExecSource每秒钟采集的数据量大致相等，这样不会频繁向kafka中写数</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize &#x3D; 1</span><br><span class="line">a1.sinks.k1.kafka.producer.acks &#x3D; 1</span><br><span class="line"># 一个Batch被创建之后，最多过多久，不管这个Batch有没有写满，都必须发送出去</span><br><span class="line"># linger.ms和flumeBatchSize(不积到设置的条数，则一直不写入到topic)，哪个先满足先按哪个规则执行，这个值默认是0，在这设置为1</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms &#x3D; 1</span><br><span class="line"># 指定数据传输时的压缩格式，对数据进行压缩，提高传输效率</span><br><span class="line">a1.sinks.k1.kafka.producer.compression.type &#x3D; snappy</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka的producer的相关参数，可以直接在这里设置：a1.sinks.k1.kafka.producer.+。。。</span><br></pre></td></tr></table></figure><h4 id="kafka-to-hdfs-conf"><a href="#kafka-to-hdfs-conf" class="headerlink" title="kafka-to-hdfs.conf"></a>kafka-to-hdfs.conf</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来配置第二个Agent：</span><br><span class="line">文件名为： kafka-to-hdfs.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 一次性向channel中写入的最大数据量，在这为了演示方便，设置为1</span><br><span class="line"># 这个参数的值不要大于MemoryChannel中transactionCapacity的值</span><br><span class="line">a1.sources.r1.batchSize &#x3D; 1</span><br><span class="line"># 最大多长时间向channel写一次数据</span><br><span class="line">a1.sources.r1.batchDurationMillis &#x3D; 2000</span><br><span class="line"># kafka地址</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata</span><br><span class="line"># topic名称，可以指定一个或者多个，多个topic之间使用逗号隔开</span><br><span class="line"># 也可以使用正则表达式指定一个topic名称规则</span><br><span class="line">a1.sources.r1.kafka.topics &#x3D; test_r2p5</span><br><span class="line"># 指定消费者组id</span><br><span class="line">a1.sources.r1.kafka.consumer.group.id &#x3D; flume-con1</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;kafkaout</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data-</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04机器的flume目录下复制两个目录</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cp -r conf conf-file-to-kafka</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cp -r conf conf-kafka-to-hdfs</span><br><span class="line"></span><br><span class="line">修改 conf_file_to_kafka和conf_kafka_to_hdfs中log4j的配置</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-file-to-kafka</span><br><span class="line">[root@bigdata04 conf_file_to_kafka]# vi log4j.properties </span><br><span class="line">flume.root.logger&#x3D;ERROR,LOGFILE</span><br><span class="line">flume.log.file&#x3D;flume-file-to-kafka.log</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-kafka-to-hdfs</span><br><span class="line">[root@bigdata04 conf_kafka_to_hdfs]# vi log4j.properties </span><br><span class="line">flume.root.logger&#x3D;ERROR,LOGFILE</span><br><span class="line">flume.log.file&#x3D;flume-kafka-to-hdfs.log</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">把刚才配置的两个Agent的配置文件复制到这两个目录下</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-file-to-kafka</span><br><span class="line">[root@bigdata04 conf-file-to-kafka]# vi file-to-kafka.conf</span><br><span class="line">.....把file-to-kafka.conf文件中的内容复制进来即可</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-kafka-to-hdfs&#x2F;</span><br><span class="line">[root@bigdata04 conf-kafka-to-hdfs]# vi kafka-to-hdfs.conf</span><br><span class="line">.....把kafka-to-hdfs.conf文件中的内容复制进来即可</span><br></pre></td></tr></table></figure><h3 id="启动Flume-Agent"><a href="#启动Flume-Agent" class="headerlink" title="启动Flume Agent"></a>启动Flume Agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">启动这两个Flume Agent</span><br><span class="line">确保zookeeper集群、kafka集群和Hadoop集群是正常运行的</span><br><span class="line">以及Kafka中的topic需要提前创建好</span><br><span class="line"></span><br><span class="line">创建topic</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 -partions 5 --replication-factor 2 --topic test_r2p5</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">先启动第二个Agent，再启动第一个Agent</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf-kafka-to-hdfs --conf-file conf-kafka-to-hdfs&#x2F;kafka-to-hdfs.conf</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf-file-to-kafka --conf-file conf-file-to-kafka&#x2F;file-to-kafka.conf</span><br><span class="line"></span><br><span class="line">模拟产生日志数据</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;log&#x2F;</span><br><span class="line">[root@bigdata04 log]# echo hello world &gt;&gt; &#x2F;data&#x2F;log&#x2F;test.log</span><br><span class="line"></span><br><span class="line">到HDFS上查看数据，验证结果：</span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -ls &#x2F;kafkaout</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r-- 2 root supergroup 12 2020-06-09 22:59 &#x2F;kafkaout&#x2F;data-.15</span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -cat &#x2F;kafkaout&#x2F;data-.1591714755267.tmp</span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line">此时Flume可以通过tail -F命令实时监控文件中的新增数据，发现有新数据就写入kafka，然后kafka后面的flume落盘程序，以及kafka后面的实时计算程序就可以使用这份数据了。</span><br></pre></td></tr></table></figure><h2 id="实战：Kafka集群平滑升级"><a href="#实战：Kafka集群平滑升级" class="headerlink" title="实战：Kafka集群平滑升级"></a>实战：Kafka集群平滑升级</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">之前我们在使用 Kafka 0.9.0.0版本的时候，遇到一个比较诡异的问题</span><br><span class="line">（背景：这个版本他们遇到一个问题，官方通过升级kafka版本解决了，但他们之前的版本工作中运用于直播平台，所以不可能将集群停了重新部署一套）</span><br><span class="line">针对消费者组增加消费者的时候可能会导致rebalance，进而导致部分consumer不能再消费分区数据</span><br><span class="line">意思就是之前针对这个topic的5个分区只有2个消费者消费数据，后期我动态的把消费者调整为了5个，这样可能会导致部分消费者无法消费分区中的数据。</span><br><span class="line"></span><br><span class="line">针对这个bug这里有一份详细描述：</span><br><span class="line">https:&#x2F;&#x2F;issues.apache.org&#x2F;jira&#x2F;browse&#x2F;KAFKA-2978</span><br><span class="line">此bug官方在0.9.0.1版本中进行了修复</span><br><span class="line">当时我们的线上集群使用的就是0.9.0.0的版本。</span><br><span class="line"></span><br><span class="line">所以我们需要对线上集群在不影响线上业务的情况下进行升级，称为平滑升级(滚动升级)，也就是升级的时候不影响线上的正常业务运行(但还是要选择在业务低峰期时进行升级)。</span><br><span class="line"></span><br><span class="line">接下来我们就查看了官网文档(0.9.0.0)，上面有针对集群平滑升级的一些信息</span><br><span class="line">http:&#x2F;&#x2F;kafka.apache.org&#x2F;090&#x2F;documentation.html#upgrade</span><br><span class="line">在验证这个升级流程的时候我们是在测试环境下，先模拟线上的集群环境，进行充分测试，可千万不能简单测试一下就直接搞到测试环境去做，这样是很危险的。</span><br><span class="line">由于当时这个kafka集群我们还没有移交给运维负责，并且运维当时对这个框架也不是很熟悉，所以才由我们开发人员来进行平滑升级，否则这种框架升级的事情肯定是交给运维去做的。</span><br><span class="line"></span><br><span class="line">那接下来看一下具体的平滑升级步骤</span><br><span class="line">小版本之间集群升级不需要额外修改集群的配置文件。只需要按照下面步骤去执行即可。</span><br><span class="line">假设kafka0.9.0.0集群在三台服务器上，需要把这三台服务器上的kafka集群升级到0.9.0.1版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：提前在集群的三台机器上把0.9.0.1的安装包，解压、配置好。</span><br><span class="line">主要是log.dirs这个参数，0.9.0.1中的这个参数和0.9.0.0的这个参数一定要保持一致，这样新版本的kafka才可以识别之前的kakfa中的数据。</span><br><span class="line">在集群升级的过程当中建议通过CMAK(kafkamanager)查看集群的状态信息，比较方便</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171529822.png" alt="image-20230417152957587"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1：先stop掉0.9.0.0集群中的第一个节点，然后去CMAK上查看集群的broker信息，确认节点确实已停掉。并且再查看一下，节点的副本下线状态。确认集群是否识别到副本下线状态。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171531285.png" alt="image-20230417153133889"></p><p> <img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171533475.png" alt="image-20230417153328093"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后在当前节点把kafka0.9.0.1启动起来。再回到CMAK中查看broker信息，确认刚启动的节点是否已正确显示，并且还要确认这个节点是否可以正常接收和发送数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171534542.png" alt="image-20230417153455185"></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2：按照第一步的流程去依次操作剩余节点即可，就是先把0.9.0.0版本的kafka停掉，再把0.9.0.1版本的kafka启动即可。</span><br><span class="line"></span><br><span class="line">注意：每操作一个节点，需要稍等一下，确认这个节点可以正常接收和发送数据之后，再处理下一个节点。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.html</id>
    <published>2023-04-15T16:08:29.000Z</published>
    <updated>2023-04-19T16:39:50.981Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="Kafka技巧篇"><a href="#Kafka技巧篇" class="headerlink" title="Kafka技巧篇"></a>Kafka技巧篇</h1><h2 id="Kafka集群参数调忧"><a href="#Kafka集群参数调忧" class="headerlink" title="Kafka集群参数调忧"></a>Kafka集群参数调忧</h2><h3 id="JVM参数调忧"><a href="#JVM参数调忧" class="headerlink" title="JVM参数调忧"></a>JVM参数调忧</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">默认启动的Broker进程只会使用1G内存，在实际使用中会导致进程频繁GC，影响Kafka集群的性能和稳</span><br><span class="line">定性</span><br><span class="line">通过 jstat -gcutil &lt;pid&gt; 1000 查看到kafka进程GC情况</span><br><span class="line">主要看 YGC,YGCT,FGC,FGCT 这几个参数，如果这几个值不是很大，就没什么问题</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">YGC：young gc发生的次数</span><br><span class="line">YGCT：young gc消耗的时间</span><br><span class="line">FGC：full gc发生的次数</span><br><span class="line">FGCT：full gc消耗的时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">13248 Kafka</span><br><span class="line">18087 Jps</span><br><span class="line">1679 QuorumPeerMain</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jstat -gcutil 13248 1000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162304926.png" alt="image-20230416230418172"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果你发现YGC很频繁，或者FGC很频繁，就说明内存分配的少了</span><br><span class="line">此时需要修改kafka-server-start.sh中的KAFKA_HEAP_OPTS</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export KAFKA_HEAP_OPTS&#x3D;&quot;-Xmx10g -Xms10g -XX:MetaspaceSize&#x3D;96m -XX:+UseG1GC -XX</span><br><span class="line"></span><br><span class="line">xms:初始化内存</span><br><span class="line">xmx:最大内存</span><br><span class="line">建议设置成一样大，否则可能进行内存交换</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个配置表示给kafka分配了10G内存</span><br></pre></td></tr></table></figure><h3 id="Replication参数调忧"><a href="#Replication参数调忧" class="headerlink" title="Replication参数调忧"></a>Replication参数调忧</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">replica.socket.timeout.ms&#x3D;60000</span><br><span class="line">这个参数的默认值是30秒，它是控制partiton副本之间socket通信的超时时间，如果设置的太小，有可能会由于网络原因导致造成误判，认为某一个partition副本连不上了。</span><br><span class="line"></span><br><span class="line">replica.lag.time.max.ms&#x3D;50000</span><br><span class="line">如果一个副本在指定的时间内没有向leader节点发送任何请求，或者在指定的时间内没有同步完leader中的数据，则leader会将这个节点从Isr列表中移除。</span><br><span class="line"></span><br><span class="line">这个参数的值默认为10秒</span><br><span class="line">如果网络不好，或者kafka压力较大，建议调大该值，否则可能会频繁出现副本丢失，进而导致集群需要频繁复制副本，导致集群压力更大，会陷入一个恶性循环</span><br></pre></td></tr></table></figure><h3 id="Log参数调优"><a href="#Log参数调优" class="headerlink" title="Log参数调优"></a>Log参数调优</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这块是针对Kafka中数据文件的删除时机进行设置，不是对kafka本身的日志参数配置</span><br><span class="line">log.retention.hours&#x3D;24</span><br><span class="line">这个参数默认值为168，单位是小时，就是7天，默认对数据保存7天，可以在这调整数据保存的时间，我们在实际工作中改为了只保存1天，因为kafka中的数据我们会在hdfs中进行备份，保存一份，所以就没有必要在kafka中保留太长时间了。</span><br><span class="line"></span><br><span class="line">在kafka中保留只是为了能够让你在指定的时间内恢复数据，或者重新消费数据，如果没有这种需求，那就没有必要设置太长时间。</span><br><span class="line"></span><br><span class="line">这里分析的Replication的参数和Log参数都是在server.properties文件中进行配置</span><br><span class="line"></span><br><span class="line">JVM参数是在kafka-server-start.sh脚本中配置</span><br><span class="line"></span><br><span class="line">broker参数调优更多在开发文档里有</span><br></pre></td></tr></table></figure><h2 id="Kafka-Topic命名小技巧"><a href="#Kafka-Topic命名小技巧" class="headerlink" title="Kafka Topic命名小技巧"></a>Kafka Topic命名小技巧</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">针对Kafka中Topic命名的小技巧</span><br><span class="line">建议在给topic命名的时候在后面跟上r2p10之类的内容</span><br><span class="line">r2：表示Partition的副本因子是2</span><br><span class="line">p10：表示这个Topic的分区数是10</span><br><span class="line"></span><br><span class="line">这样的好处是后期我们如果要写消费者消费指定topic的数据，通过topic的名称我们就知道应该设置多少个消费者消费数据效率最高。</span><br><span class="line">因为一个partition同时只能被一个消费者消费，所以效率最高的情况就是消费者的数量和topic的分区数量保持一致。在这里通过topic的名称就可以直接看到，一目了然。</span><br><span class="line"></span><br><span class="line">但是也有一个缺点，就是后期如果我们动态调整了topic的partiton，那么这个topic名称上的partition数量就不准了，针对这个topic，建议大家一开始的时候就提前预估一下，可以多设置一些partition，我们</span><br><span class="line">在工作中的时候针对一些数据量比较大的topic一般会设置40-50个partition，数据量少的topic一般设置5-10个partition，这样后期调整topic partiton数量的场景就比较少了。</span><br></pre></td></tr></table></figure><h2 id="Kafka集群监控管理工具"><a href="#Kafka集群监控管理工具" class="headerlink" title="Kafka集群监控管理工具"></a>Kafka集群监控管理工具</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">现在我们操作Kafka都是在命令行界面中通过脚本操作的，后面需要传很多参数，用起来还是比较麻烦的，那kafka没有提供web界面的支持吗？</span><br><span class="line">很遗憾的告诉你，Apache官方并没有提供，不过好消息是有一个由雅虎开源的一个工具，目前用起来还是不错的。</span><br><span class="line"></span><br><span class="line">它之前的名字叫KafkaManager，后来改名字了，叫CMAK</span><br><span class="line">CMAK是目前最受欢迎的Kafka集群管理工具，最早由雅虎开源，用户可以在Web界面上操作Kafka集群</span><br><span class="line">可以轻松检查集群状态(Topic、Consumer、Offset、Brokers、Replica、Partition)</span><br><span class="line"></span><br><span class="line">那下面我们先去下载这个CMAK</span><br><span class="line">需要到github上面去下载</span><br><span class="line">在github里面搜索CMAK即可</span><br></pre></td></tr></table></figure><h3 id="下载CMAK"><a href="#下载CMAK" class="headerlink" title="下载CMAK"></a>下载CMAK</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162330282.png" alt="image-20230416233039880"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162330225.png" alt="image-20230416233050053"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162331098.png" alt="image-20230416233123771"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162331745.png" alt="image-20230416233141274"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：由于cmak-3.0.0.4.zip版本是在java11这个版本下编译的，所以在运行的时候也需要使用java11这个版本，我们目前服务器上使用的是java8这个版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">我们为什么不使用java11版本呢？因为自2019年1月1日1起，java8之后的更新版本在商业用途的时候就需要收费授权了。</span><br><span class="line">在这针对cmak-3.0.0.4这个版本，如果我们想要使用的话有两种解决办法</span><br><span class="line">1：下载cmak的源码，使用jdk8编译</span><br><span class="line">2：额外安装一个jdk11(自己用不属于商业用途，现实公司很少有用java8以后的)</span><br><span class="line">如果想要编译的话需要安装sbt这个工具对源码进行编译，sbt是Scala 的构建工具, 类似于Maven。</span><br><span class="line"></span><br><span class="line">由于我们在这使用不属于商业用途，所以使用jdk11是没有问题的，那就不用重新编译了。</span><br><span class="line">下载jdk11，jdk-11.0.7_linux-x64_bin.tar.gz</span><br><span class="line">将jdk11的安装包上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">只需要解压即可，不需要配置环境变量，因为只有cmak这个工具才需要使用jdk11</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# tar -zxvf jdk-11.0.7_linux-x64_bin.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来把 cmak-3.0.0.4.zip 上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">1：解压</span><br><span class="line">[root@bigdata01 soft]# unzip cmak-3.0.0.4.zip </span><br><span class="line">-bash: unzip: command not found</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：如果提示-bash: unzip: command not found，则说明目前不支持unzip命令，可以使用yum在线安装</span><br><span class="line">建议先清空一下yum缓存，否则使用yum可能无法安装unzip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# yum clean all </span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Cleaning repos: base extras updates</span><br><span class="line">Cleaning up list of fastest mirrors</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# yum install -y unzip</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">.....</span><br><span class="line">Running transaction</span><br><span class="line"> Installing : unzip-6.0-21.el7.x86_64 1&#x2F;1 </span><br><span class="line"> Verifying : unzip-6.0-21.el7.x86_64 1&#x2F;1 </span><br><span class="line">Installed:</span><br><span class="line"> unzip.x86_64 0:6.0-21.el7 </span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">再重新解压</span><br><span class="line">[root@bigdata01 soft]# unzip cmak-3.0.0.4.zip</span><br></pre></td></tr></table></figure><h3 id="配置CMAK"><a href="#配置CMAK" class="headerlink" title="配置CMAK"></a>配置CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2：修改CMAK配置</span><br><span class="line">首先修改bin目录下的cmak脚本</span><br><span class="line">在里面配置JAVA_HOME指向jdk11的安装目录，否则默认会使用jdk8</span><br><span class="line">[root@bigdata01 soft]# cd cmak-3.0.0.4</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# cd bin&#x2F;</span><br><span class="line">[root@bigdata01 bin]# vi cmak</span><br><span class="line">....</span><br><span class="line">JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk-11.0.7</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">然后修改conf目录下的application.conf文件</span><br><span class="line">只需要在里面增加一行cmak.zkhosts参数的配置即可，指定zookeeper的地址</span><br><span class="line"></span><br><span class="line">注意：在这里指定zookeeper地址主要是为了让CMAK在里面保存数据，这个zookeeper地址不一定是kafka集群使用的那个zookeeper集群，随便哪个zookeeper集群都可以。(cmak需要报错它自己的东西)</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# cd conf&#x2F;</span><br><span class="line">[root@bigdata01 conf]# vi application.conf </span><br><span class="line">....</span><br><span class="line">cmak.zkhosts&#x3D;&quot;bigdata01:2181,bigdata02:2181,bigdata03:2181&quot;</span><br><span class="line">....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3：修改kafka启动配置</span><br><span class="line">想要在CMAK中查看kafka的一些指标信息，在启动kafka的时候需要指定JMX_PORT</span><br><span class="line"></span><br><span class="line">停止kafka集群</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh </span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh </span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">重新启动kafka集群，指定JXM_PORT</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br></pre></td></tr></table></figure><h3 id="启动CMAK"><a href="#启动CMAK" class="headerlink" title="启动CMAK"></a>启动CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">4：启动cmak</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# bin&#x2F;cmak -Dconfig.file&#x3D;conf&#x2F;application.conf -</span><br><span class="line"></span><br><span class="line">如果想把cmak放在后台执行的话需要添加上nohup和&amp;</span><br><span class="line">1 [root@bigdata01 cmak-3.0.0.4]# nohup bin&#x2F;cmak -Dconfig.file&#x3D;conf&#x2F;application.conf -Dhttp.port&#x3D;9001 &amp;</span><br><span class="line"></span><br><span class="line">cmak默认监听端口9000，但这样和hdfs的端口重复了</span><br></pre></td></tr></table></figure><h3 id="访问CMAK"><a href="#访问CMAK" class="headerlink" title="访问CMAK"></a>访问CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5：访问cmak</span><br><span class="line">http:&#x2F;&#x2F;bigdata01:9001&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162356804.png" alt="image-20230416235630637"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6：操作CMAK</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4%5C202304162355045.png" alt="image-20230416235520700"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162355502.png" alt="image-20230416235548395"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这几个参数配置好了以后还需要配置以下几个线程池相关的参数，这几个参数默认值是1，在保存的时候会提示需要大于1，所以可以都改为10</span><br><span class="line">最后点击Save按钮保存即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brokerViewThreadPoolSize：10</span><br><span class="line">offsetCacheThreadPoolSize：10</span><br><span class="line">kafkaAdminClientThreadPoolSize：10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后进来是这样的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170003610.png" alt="image-20230417000304032"></p><h4 id="查看kafak集群的所有broker信息"><a href="#查看kafak集群的所有broker信息" class="headerlink" title="查看kafak集群的所有broker信息"></a>查看kafak集群的所有broker信息</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170003829.png" alt="image-20230417000358325"></p><h4 id="查看kafak集群的所有topic信息"><a href="#查看kafak集群的所有topic信息" class="headerlink" title="查看kafak集群的所有topic信息"></a>查看kafak集群的所有topic信息</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170008382.png" alt="image-20230417000847024"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170007471.png" alt="image-20230417000718104"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170007023.png" alt="image-20230417000734624"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击topic的消费者信息是可以进来查看的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170016794.png" alt="image-20230417001617419"></p><h4 id="创建一个topic"><a href="#创建一个topic" class="headerlink" title="创建一个topic"></a>创建一个topic</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170016835.png" alt="image-20230417001657421"></p><h4 id="给topic增加分区"><a href="#给topic增加分区" class="headerlink" title="给topic增加分区"></a>给topic增加分区</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170017969.png" alt="image-20230417001716328"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是CMAK中常见的功能，当然了这里面还要一些我们没有说到的功能就留给大家以后来发掘了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.html</id>
    <published>2023-04-15T14:14:17.000Z</published>
    <updated>2023-04-19T15:35:18.244Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-3"><a href="#第十四周-消息队列之Kafka从入门到小牛-3" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-3"></a>第十四周 消息队列之Kafka从入门到小牛-3</h1><h2 id="Kafka核心之存储和容错机制"><a href="#Kafka核心之存储和容错机制" class="headerlink" title="Kafka核心之存储和容错机制"></a>Kafka核心之存储和容错机制</h2><h3 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在kafka中每个topic包含1到多个partition，每个partition存储一部分Message。每条Message包含三个属性，其中有一个是offset。</span><br><span class="line"></span><br><span class="line">问题来了：offset相当于partition中这个message的唯一id，那么如何通过id高效的找到message？</span><br><span class="line">两大法宝：分段+索引(分段表示一个partition会存储多个文件)</span><br><span class="line"></span><br><span class="line">kafak中数据的存储方式是这样的：</span><br><span class="line">1、每个partition由多个segment【片段】组成，每个segment文件中存储多条消息，</span><br><span class="line">2、每个partition在内存中对应一个index，记录每个segment文件中的第一条消息偏移量。</span><br><span class="line"></span><br><span class="line">Kafka中数据的存储流程是这样的：</span><br><span class="line">生产者生产的消息会被发送到topic的多个partition上，topic收到消息后往对应partition的最后一个segment上添加该消息，segment达到一定的大小后会创建新的segment。</span><br><span class="line">来看这个图，可以认为是针对topic中某个partition的描述</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160035193.png" alt="image-20230416003228787"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">图中左侧就是索引，右边是segment文件，左边的索引里面会存储每一个segment文件中第一条消息的偏移量，由于消息的偏移量都是递增的，这样后期查找起来就方便了，先到索引中判断数据在哪个</span><br><span class="line">segment文件中，然后就可以直接定位到具体的segment文件了，这样再找具体的那一条数据就很快了，因为都是有序的。</span><br></pre></td></tr></table></figure><h3 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h3><h4 id="Broker节点宕机"><a href="#Broker节点宕机" class="headerlink" title="Broker节点宕机"></a>Broker节点宕机</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当Kafka集群中的一个Broker节点宕机，会出现什么现象？</span><br><span class="line"></span><br><span class="line">下面来演示一下</span><br><span class="line">使用kill -9 杀掉bigdata01中的broker进程测试</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">7522 Jps</span><br><span class="line">2054 Kafka</span><br><span class="line">1679 QuorumPeerMain</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# kill 2054</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">我们可以先通过zookeeper来查看一下，因为当kafka集群中的broker节点启动之后，会自动向zookeeper中进行注册，保存当前节点信息</span><br><span class="line">....]# bin&#x2F;zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers</span><br><span class="line">[ids, seqid, topics]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[1, 2]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160046086.png" alt="image-20230416004647924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时发现zookeeper的&#x2F;brokers&#x2F;ids下面只有2个节点信息</span><br><span class="line">可以通过get命令查看节点信息，这里面会显示对应的主机名和端口号</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] get &#x2F;brokers&#x2F;ids&#x2F;1</span><br><span class="line">&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLA</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160050443.png" alt="image-20230416005045245"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">然后再使用describe查询topic的详细信息，会发现此时的分区的leader全部变成了目前存活的另外两个节点</span><br><span class="line"></span><br><span class="line">此时可以发现Isr中的内容和Replicas中的不一样了，因为Isr中显示的是目前正常运行的节点</span><br><span class="line"></span><br><span class="line">所以当Kafka集群中的一个Broker节点宕机之后，对整个集群而言没有什么特别的大影响，此时集群会给partition重新选出来一些新的Leader节点</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160053702.png" alt="image-20230416005338215"></p><h4 id="新增一个Broker节点"><a href="#新增一个Broker节点" class="headerlink" title="新增一个Broker节点"></a>新增一个Broker节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当Kafka集群中新增一个Broker节点，会出现什么现象？</span><br><span class="line">新加入一个broker节点，zookeeper会自动识别并在适当的机会选择此节点提供服务</span><br><span class="line"></span><br><span class="line">再次启动bigdata01节点中的broker进程测试</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">此时到zookeeper中查看一下</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line">.....</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers</span><br><span class="line">[ids, seqid, topics]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[0, 1, 2]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160058241.png" alt="image-20230416005822131"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">发现broker.id为0的这个节点信息也有了</span><br><span class="line"></span><br><span class="line">在通过describe查看topic的描述信息，Isr中的信息和Replicas中的内容是一样的了</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper localhost:2181 --topic hello</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160059016.png" alt="image-20230416005947958"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">但是启动后有个问题：发现新启动的这个节点不会是任何分区的leader？怎么重新均匀分配呢？</span><br><span class="line">1、Broker中的自动均衡策略（默认已经有）</span><br><span class="line">auto.leader.rebalance.enable&#x3D;true</span><br><span class="line">leader.imbalance.check.interval.seconds 默认值：300</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2、手动执行：</span><br><span class="line">bin&#x2F;kafka-leader-election.sh --bootstrap-server localhost:9092 --election-type pareferred --all-topic-partitions</span><br><span class="line"></span><br><span class="line">Successfully completed leader election (PREFERRED) for partitions hello-4, he</span><br><span class="line"></span><br><span class="line">执行后的效果如下，这样就实现了均匀分配</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160105050.png" alt="image-20230416010500020"></p><h2 id="Kafka生产消费者实战"><a href="#Kafka生产消费者实战" class="headerlink" title="Kafka生产消费者实战"></a>Kafka生产消费者实战</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们使用基于console的生产者和消费者对topic实现了数据的生产和消费，，这个基于控制台的生产者和消费者主要是让我们做测试用的</span><br><span class="line">在实际工作中，我们有时候需要将生产者和消费者功能集成到我们已有的系统中，此时就需要写代码实现生产者和消费者的逻辑了。</span><br><span class="line">在这我们使用java代码来实现生产者和消费者的功能</span><br></pre></td></tr></table></figure><h3 id="Kafka-Java代码编程"><a href="#Kafka-Java代码编程" class="headerlink" title="Kafka Java代码编程"></a>Kafka Java代码编程</h3><h4 id="Java代码实现生产者代码"><a href="#Java代码实现生产者代码" class="headerlink" title="Java代码实现生产者代码"></a>Java代码实现生产者代码</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160114267.png" alt="image-20230416011434393"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先创建maven项目， db_kafka</span><br><span class="line"></span><br><span class="line">添加kafka的maven依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.kafka&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;kafka-clients&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">开发生产者代码</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.kafka;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Java代码实现生产者代码</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerDemo</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">         <span class="comment">//指定kafka的broker地址</span></span><br><span class="line">         prop.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">         <span class="comment">//指定key-value数据的序列化格式(key就是之前讲的，如果指定了数据有key，则可以根据它来将数据放入哪一个partition，一般用不到；但这里要知道不然要报错)</span></span><br><span class="line">         prop.put(<span class="string">"key.serializer"</span>, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         prop.put(<span class="string">"value.serializer"</span>, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定topic</span></span><br><span class="line">         String topic = <span class="string">"hello"</span>; </span><br><span class="line">         <span class="comment">//创建kafka生产者</span></span><br><span class="line">         KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String,String&gt;(prop);</span><br><span class="line">         <span class="comment">//向topic中生产数据(这里也没有传入key，只传入了value)</span></span><br><span class="line">         producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="string">"hello kafka"</span>))</span><br><span class="line">         <span class="comment">//关闭链接</span></span><br><span class="line">         producer.close();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Java代码实现消费者代码"><a href="#Java代码实现消费者代码" class="headerlink" title="Java代码实现消费者代码"></a>Java代码实现消费者代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.kafka;</span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Collection;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Java代码实现消费者代码</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerDemo</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">         <span class="comment">//指定kafka的broker地址(之前控制台那里server没s)</span></span><br><span class="line">         prop.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">         <span class="comment">//指定key-value的反序列化类型</span></span><br><span class="line">         prop.put(<span class="string">"key.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         prop.put(<span class="string">"value.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定消费者组(之前控制台那里，会自动生成)</span></span><br><span class="line">         prop.put(<span class="string">"group.id"</span>, <span class="string">"con-1"</span>);</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//创建消费者</span></span><br><span class="line">         KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String,String&gt;(prop);</span><br><span class="line">        Collection&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">         topics.add(<span class="string">"hello"</span>);</span><br><span class="line">         <span class="comment">//订阅指定的topic</span></span><br><span class="line">         consumer.subscribe(topics);</span><br><span class="line">         <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">             <span class="comment">//消费数据【注意：需要修改jdk编译级别为1.8，否则Duration.ofSeconds(1)会语法报错</span></span><br><span class="line">             ConsumerRecords&lt;String, String&gt; poll = consumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">             <span class="keyword">for</span> (ConsumerRecord&lt;String,String&gt; consumerRecord : poll) &#123;</span><br><span class="line">             System.out.println(consumerRecord);</span><br><span class="line">             &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1. 关闭kafka服务器的防火墙</span><br><span class="line">2. 配置windows的hosts文件 添加kafka节点的hostname和ip的映射关系。[如果我们的hosts文件中没有对kafka节点的hostnam和ip的映射关系做配置，在这经过多次尝试连接不上就会报错]</span><br><span class="line"></span><br><span class="line">先开启消费者。</span><br><span class="line">发现没有消费到数据，这个topic中是有数据的，为什么之前的数据没有消费出来呢？(就是前面讲的，默认会从consumer生成后生成的数据读取)不要着急，先带着这个问题往下面看</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3%5Cimage-20230416014022148.png" alt="image-20230416014022148"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再开启生产者，生产者会生产一条数据，然后就结束</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160143273.png" alt="image-20230416014143241"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时回到kafka的消费者端就可以看到消费出来的数据了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160142342.png" alt="image-20230416014214263"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以这个时候我们发现，新产生的数据我们是可以消费到的，但是之前的数据我们就无法消费了，那下面我们来分析一下这个问题</span><br></pre></td></tr></table></figure><h4 id="消费者代码扩展"><a href="#消费者代码扩展" class="headerlink" title="消费者代码扩展"></a>消费者代码扩展</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x2F;&#x2F;开启消费者自动提交offset功能，默认就是开启的</span><br><span class="line">prop.put(&quot;enable.auto.commit&quot;,&quot;true&quot;);</span><br><span class="line">&#x2F;&#x2F;自动提交offset的时间间隔，单位是毫秒(在开启自动提交时，它默认开启，且默认值是5000)</span><br><span class="line">prop.put(&quot;auto.commit.interval.ms&quot;,&quot;5000&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">注意：正常情况下，kafka消费数据的流程是这样的</span><br><span class="line">先根据group.id指定的消费者组到kafka中查找之前保存的offset信息</span><br><span class="line"></span><br><span class="line">如果查找到了，说明之前使用这个消费者组消费过数据，则根据之前保存的offset继续进行消费</span><br><span class="line"></span><br><span class="line">如果没查找到(说明第一次消费)，或者查找到了，但是查找到的那个offset对应的数据已经不存</span><br><span class="line"></span><br><span class="line">这个时候消费者该如何消费数据？</span><br><span class="line">(因为kafka默认只会保存7天的数据，超过时间数据会被删除)</span><br><span class="line"></span><br><span class="line">此时会根据auto.offset.reset的值执行不同的消费逻辑</span><br><span class="line"></span><br><span class="line">这个参数的值有三种:[earliest,latest,none]</span><br><span class="line">earliest：表示从最早的数据开始消费(从头消费)</span><br><span class="line">latest【默认】：表示从最新的数据开始消费</span><br><span class="line">none：如果根据指定的group.id没有找到之前消费的offset信息，就会抛异常</span><br><span class="line"></span><br><span class="line">(工作中earliest和latest常用)</span><br><span class="line"></span><br><span class="line">解释：【查找到了，但是查找到的那个offset对应的数据已经不存在了】 </span><br><span class="line">假设你第一天使用一个消费者去消费了一条数据，然后就把消费者停掉了，等了7天之后，你又使用这个消费者去消费数据</span><br><span class="line">这个时候，这个消费者启动的时候会到kafka里面查询它之前保存的offset信息</span><br><span class="line">但是那个offset对应的数据已经被删了，所以此时再根据这个offset去消费是消费不到数据的</span><br><span class="line"></span><br><span class="line">总结，一般在实时计算的场景下，这个参数的值建议设置为latest，消费最新的数据</span><br><span class="line"></span><br><span class="line">这个参数只有在消费者第一次消费数据，或者之前保存的offset信息已过期的情况下才会生效</span><br><span class="line"> *&#x2F;</span><br><span class="line"></span><br><span class="line">prop.put(&quot;auto.offset.reset&quot;,&quot;latest&quot;);</span><br><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时我们来验证一下，</span><br><span class="line">先启动一次生产者，再启动一次消费者，看看消费者能不能消费到这条数据，如果能消费到，就说明此时是根据上次保存的offset信息进行消费了。结果发现是可以消费到的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：消费者消费到数据之后，不要立刻关闭程序，要至少等5秒，因为自动提交offset的时机是5秒提交一次</span><br><span class="line"></span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 4, leaderEpoch &#x3D; 5, offset &#x3D; 0, Cre</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">将auto.offset.reset置为earliest，修改一下group.id的值，相当于使用一个新的消费者，验证一下，看是否能把这个topic中的所有数据都取出来，因为新的消费者第一次肯定是获取不到offset信息的，</span><br><span class="line">所以就会根据auto.offset.reset的值来消费数据</span><br><span class="line"></span><br><span class="line">prop.put(&quot;group.id&quot;, &quot;con-2&quot;);</span><br><span class="line">prop.put(&quot;auto.offset.reset&quot;,&quot;earliest&quot;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 2, leaderEpoch &#x3D; 0, offset &#x3D; 0, Cre</span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 3, leaderEpoch &#x3D; 3, offset &#x3D; 0, Cre</span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 4, leaderEpoch &#x3D; 5, offset &#x3D; 0, Cre</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162130163.png" alt="image-20230416213026828"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时，关闭消费者(需要等待5秒，这样才会提交offset)，再重新启动，发现没有消费到数据，说明此时就</span><br><span class="line">根据上次保存的offset来消费数据了，因为没有新数据产生，所以就消费不到了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最后来处理一下程序输出的日志警告信息，这里其实示因为缺少依赖日志依赖</span><br><span class="line">在pom文件中添加log4j的依赖，然后将 log4j.properties 添加到 resources目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;info,stdout</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout &#x3D; org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Target &#x3D; System.out</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br></pre></td></tr></table></figure><h3 id="Consumer消费offset查询"><a href="#Consumer消费offset查询" class="headerlink" title="Consumer消费offset查询"></a>Consumer消费offset查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka0.9版本以前，消费者的offset信息保存在zookeeper中</span><br><span class="line">从kafka0.9开始，使用了新的消费API，消费者的信息会保存在kafka里面的__consumer_offsets这个topic中</span><br><span class="line"></span><br><span class="line">因为频繁操作zookeeper性能不高，所以kafka在自己的topic中负责维护消费者的offset信息。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162135924.png" alt="image-20230416213511507"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如何查询保存在kafka中的Consumer的offset信息呢？</span><br><span class="line">使用kafka-consumer-groups.sh这个脚本可以查看目前所有的consumer group</span><br><span class="line"></span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-consumer-groups.sh --list --bootstrap-server localhost:9092</span><br><span class="line"></span><br><span class="line">con-1</span><br><span class="line">con-2 (前面视频里修改过)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">具体查看某一个consumer group的信息</span><br><span class="line">GROUP：当前消费者组，通过group.id指定的值</span><br><span class="line">TOPIC：当前消费的topic</span><br><span class="line">PARTITION：消费的分区</span><br><span class="line">CURRENT-OFFSET：消费者消费到这个分区的offset</span><br><span class="line">LOG-END-OFFSET：当前分区中数据的最大offset</span><br><span class="line">LAG：当前分区未消费数据量</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group con-1</span><br><span class="line">GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG</span><br><span class="line">con-1 hello 4 1 1 0 </span><br><span class="line">con-1 hello 2 1 1 0 </span><br><span class="line">con-1 hello 3 1 1 0</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162141088.png" alt="image-20230416214127550"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">partition:是指消费者消费了哪些分区</span><br><span class="line">current-offset:当前消费了的数据的offset</span><br><span class="line">log-end-offset:最新数据的offset</span><br><span class="line">lag:还有多少条数据没消费</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时再执行一次生产者代码，生产一条数据，重新查看一下这个消费者的offset情况</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162148346.png" alt="image-20230416214839816"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如何分析生产的数据能不能及时消费掉：查看lag</span><br><span class="line">如果lag值比较大：就需要增加消费者个数，同一个代码执行多次(但group.id不能变)</span><br></pre></td></tr></table></figure><h3 id="Consumer消费顺序"><a href="#Consumer消费顺序" class="headerlink" title="Consumer消费顺序"></a>Consumer消费顺序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当一个消费者消费一个partition时候，消费的数据顺序和此partition数据的生产顺序是一致的</span><br><span class="line"></span><br><span class="line">当一个消费者消费多个partition时候，消费者按照partition的顺序，首先消费一个partition，当消费完一个partition最新的数据后再消费其它partition中的数据</span><br><span class="line"></span><br><span class="line">总之：如果一个消费者消费多个partiton，只能保证消费的数据顺序在一个partition内是有序的</span><br><span class="line"></span><br><span class="line">也就是说消费kafka中的数据只能保证消费partition内的数据是有序的，多个partition之间是无序的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162201198.png" alt="image-20230416220156464"></p><h3 id="Kafka的三种语义"><a href="#Kafka的三种语义" class="headerlink" title="Kafka的三种语义"></a>Kafka的三种语义</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka可以实现以下三种语义，这三种语义是针对消费者而言的：</span><br></pre></td></tr></table></figure><h4 id="至少一次：at-least-once"><a href="#至少一次：at-least-once" class="headerlink" title="至少一次：at-least-once"></a>至少一次：at-least-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这种语义有可能会对数据重复处理</span><br><span class="line">实现至少一次消费语义的消费者也很简单。</span><br><span class="line">1: 设置enable.auto.commit为false，禁用自动提交offset</span><br><span class="line">2: 消息处理完之后手动调用consumer.commitSync()提交offset</span><br><span class="line">这种方式是在消费数据之后，手动调用函数consumer.commitSync()异步提交offset，有可能处理多次的场景是消费者的消息处理完并输出到结果库，但是offset还没提交，这个时候消费者挂掉了，再重启的时候会重新消费并处理消息，所以至少会处理一次</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162213700.png" alt="image-20230416221328144"></p><h4 id="至多一次：at-most-once"><a href="#至多一次：at-most-once" class="headerlink" title="至多一次：at-most-once"></a>至多一次：at-most-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">这种语义有可能会丢失数据</span><br><span class="line">至多一次消费语义是kafka消费者的默认实现。配置这种消费者最简单的方式是</span><br><span class="line">1: enable.auto.commit设置为true。</span><br><span class="line">2: auto.commit.interval.ms设置为一个较低的时间范围。</span><br><span class="line">由于上面的配置，此时kafka会有一个独立的线程负责按照指定间隔提交offset。</span><br><span class="line"></span><br><span class="line">消费者的offset已经提交，但是消息还在处理中(还没有处理完)，这个时候程序挂了，导致数据没有被成功处理，再重启的时候会从上次提交的offset处消费，导致上次没有被成功处理的消息就丢失了。</span><br></pre></td></tr></table></figure><h4 id="仅一次：exactly-once"><a href="#仅一次：exactly-once" class="headerlink" title="仅一次：exactly-once"></a>仅一次：exactly-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这种语义可以保证数据只被消费处理一次。</span><br><span class="line">实现仅一次语义的思路如下：</span><br><span class="line">1: 将enable.auto.commit设置为false，禁用自动提交offset</span><br><span class="line">2: 使用consumer.seek(topicPartition，offset)来指定offset</span><br><span class="line">3: 在处理消息的时候，要同时保存住每个消息的offset。以原子事务的方式保存offset和处理的消息结果，这个时候相当于自己保存offset信息了，把offset和具体的数据绑定到一块，数据真正处理成功的时候才会保存offset信息</span><br><span class="line">这样就可以保证数据仅被处理一次了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.html</id>
    <published>2023-04-15T14:13:26.000Z</published>
    <updated>2023-04-16T03:01:48.465Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="大数据开发工程师-第十四周-消息队列之Kafka从入门到小牛-2"><a href="#大数据开发工程师-第十四周-消息队列之Kafka从入门到小牛-2" class="headerlink" title="大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2"></a>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2</h1><h2 id="Kafka使用初体验"><a href="#Kafka使用初体验" class="headerlink" title="Kafka使用初体验"></a>Kafka使用初体验</h2><h3 id="Kafka中Topic的操作"><a href="#Kafka中Topic的操作" class="headerlink" title="Kafka中Topic的操作"></a>Kafka中Topic的操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka集群安装好了以后我们就想向kafka中添加一些数据</span><br><span class="line">想要添加数据首先需要创建topic</span><br><span class="line">那接下来看一下针对topic的一些操作</span><br></pre></td></tr></table></figure><h4 id="新增Topic"><a href="#新增Topic" class="headerlink" title="新增Topic"></a>新增Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">指定2个分区，2个副本，注意：副本数不能大于集群中Broker的数量</span><br><span class="line"></span><br><span class="line">因为每个partition的副本必须保存在不同的broker，否则没有意义，如果partition的副本都保存在同一个broker，那么这个broker挂了，则partition数据依然会丢失</span><br><span class="line"></span><br><span class="line">在这里我使用的是3个节点的kafka集群，所以副本数我就暂时设置为2，最大可以设置为3</span><br><span class="line"></span><br><span class="line">如果你们用的是单机kafka的话，这里的副本数就只能设置为1了，这个需要注意一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 2 --replication-factor 2</span><br><span class="line">--topic hello</span><br><span class="line">Created topic hello.</span><br></pre></td></tr></table></figure><h4 id="查询Topic"><a href="#查询Topic" class="headerlink" title="查询Topic"></a>查询Topic</h4><h5 id="查询Kafka中的所有Topic列表"><a href="#查询Kafka中的所有Topic列表" class="headerlink" title="查询Kafka中的所有Topic列表"></a>查询Kafka中的所有Topic列表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">查询Kafka中的所有Topic列表以及查看指定Topic的详细信息</span><br><span class="line">查询kafka中所有的topic列表</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line">hello</span><br><span class="line"></span><br><span class="line">topic数据是存在zookeeper中，所以直接指定zookeeper地址就可以了(有些地方需要指定kafka地址)</span><br></pre></td></tr></table></figure><h5 id="查看指定Topic的详细信息"><a href="#查看指定Topic的详细信息" class="headerlink" title="查看指定Topic的详细信息"></a>查看指定Topic的详细信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看指定topic的详细信息</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper localhost:2181 --topic hello</span><br><span class="line">Topic: hello PartitionCount: 2 ReplicationFactor: 2 Configs: </span><br><span class="line">Topic: hello Partition: 0 Leader: 2 Replicas: 2,0 Is</span><br><span class="line">Topic: hello Partition: 1 Leader: 0 Replicas: 0,1 Is</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152231741.png" alt="image-20230415223043384"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">第一个行显示指定topic所有partitions的一个总结</span><br><span class="line">PartitionCount：表示这个Topic一共有多少个partition</span><br><span class="line">ReplicationFactor：表示这个topic中partition的副本因子是几个</span><br><span class="line">Config：这个表示创建Topic时动态指定的配置信息，在这我们没有额外指定配置信息</span><br><span class="line"></span><br><span class="line">下面每一行给出的是一个partition的信息，如果只有一个partition，则只显示一行。</span><br><span class="line">Topic：显示当前的topic名称</span><br><span class="line">Partition：显示当前topic的partition编号</span><br><span class="line">Leader：Leader partition所在的节点编号，这个编号其实就是broker.id的值，</span><br><span class="line">来看这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152234654.png" alt="image-20230415223420514"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这个图里面的hello这个topic有两个partition，其中partition1的leader所在的节点是broker1，partition2的leader所在的节点是broker2</span><br><span class="line"></span><br><span class="line">Replicas：当前partition所有副本所在的节点编号【包含Leader所在的节点】，如果设置多个副本的话，这里会显示多个，不管该节点是否是Leader以及是否存活。</span><br><span class="line"></span><br><span class="line">Isr：当前partition处于同步状态的所有节点，这里显示的所有节点都是存活状态的，并且跟Leader同步的(包含Leader所在的节点)</span><br><span class="line"></span><br><span class="line">所以说Replicas和Isr的区别就是</span><br><span class="line">如果某个partition的副本所在的节点宕机了，在Replicas中还是会显示那个节点，但是在Isr中就不会显示了，Isr中显示的都是处于正常状态的节点。</span><br></pre></td></tr></table></figure><h4 id="修改Topic"><a href="#修改Topic" class="headerlink" title="修改Topic"></a>修改Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">修改一般是修改Topic的partition数量，只能增加</span><br><span class="line"></span><br><span class="line">为什么partition只能增加？</span><br><span class="line">因为数据是存储在partition中的，如果可以减少partition的话，那么partition中的数据就丢了</span><br><span class="line"></span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --alter --zookeeper localhost:2181 --partitions 5 --topic hello </span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partitio</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line">修改之后再来查看一下topic的详细信息</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper </span><br><span class="line">Topic: hello PartitionCount: 5 ReplicationFactor: 2 Configs: </span><br><span class="line"> Topic: hello Partition: 0 Leader: 2 Replicas: 2,0 I</span><br><span class="line"> Topic: hello Partition: 1 Leader: 0 Replicas: 0,1 I</span><br><span class="line"> Topic: hello Partition: 2 Leader: 1 Replicas: 1,2 I</span><br><span class="line"> Topic: hello Partition: 3 Leader: 2 Replicas: 2,1 I</span><br><span class="line"> Topic: hello Partition: 4 Leader: 0 Replicas: 0,2</span><br></pre></td></tr></table></figure><h4 id="删除Topic"><a href="#删除Topic" class="headerlink" title="删除Topic"></a>删除Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">删除Kafka中的指定Topic</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --delete --zookeeper localhost 2181 --topic hello</span><br><span class="line">Topic hello is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br><span class="line"></span><br><span class="line">删除操作是不可逆的，删除Topic会删除它里面的所有数据</span><br><span class="line"></span><br><span class="line">注意：Kafka从1.0.0开始默认开启了删除操作，之前的版本只会把Topic标记为删除状态，需要设置delete.topic.enable为true才可以真正删除</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果不想开启删除功能，可以设置delete.topic.enable为false，这样删除topic的时候只会把它标记为删除状态，此时这个topic依然可以正常使用。</span><br><span class="line">delete.topic.enable可以配置在server.properties文件中</span><br></pre></td></tr></table></figure><h3 id="Kafka中的生产者和消费者"><a href="#Kafka中的生产者和消费者" class="headerlink" title="Kafka中的生产者和消费者"></a>Kafka中的生产者和消费者</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了Kafka中的topic的创建方式，下面我们可以向topic中生产数据以及消费数据了</span><br><span class="line">生产数据需要用到生产者</span><br><span class="line">消费数据需要用到消费者</span><br><span class="line"></span><br><span class="line">kafka默认提供了基于控制台的生产者和消费者，方便测试使用</span><br><span class="line">生产者： bin&#x2F;kafka-console-producer.sh</span><br><span class="line">消费者： bin&#x2F;kafka-console-consumer.sh</span><br></pre></td></tr></table></figure><h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">先来看一下如何向里面生产数据</span><br><span class="line">直接使用kafka提供的基于控制台的生产者</span><br><span class="line">先创建一个topic【5个分区，2个副本】：</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost 2181 --partitions 5 --replication-factor 2 --topic hello</span><br><span class="line">Created topic hello.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">向这个topic中生产数据</span><br><span class="line">broker-list：kafka的服务地址[多个用逗号隔开](这里需要用到kafka地址，上面创建topic时指定的是zookeeper，这里用本地地址和使用bigdata01:9092,bigdata02:9092,bigdata03:9092是一样的)</span><br><span class="line">topic：topic名称</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-producer.sh --broker-list localhost:9092 --topic hello</span><br><span class="line">&gt;hehe</span><br></pre></td></tr></table></figure><h4 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">下面来创建一个消费者消费topic中的数据</span><br><span class="line">bootstrap-server：kafka的服务地址</span><br><span class="line">topic:具体的topic下面来创建一个消费者消费topic中的数据</span><br><span class="line">1 [root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hello</span><br><span class="line"></span><br><span class="line">发现消费不到刚才生产的数据，为什么呢？</span><br><span class="line">因为kafka的消费者默认是消费最新生产的数据，如果想消费之前生产的数据需要添加一个参数--from-beginning，表示从头消费的意思</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hello --from-beginning</span><br><span class="line"></span><br><span class="line">hehe</span><br><span class="line"></span><br><span class="line">这里创建消费者的机器01、02、03都可以</span><br></pre></td></tr></table></figure><h4 id="案例：QQ群聊天"><a href="#案例：QQ群聊天" class="headerlink" title="案例：QQ群聊天"></a>案例：QQ群聊天</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">通过kafka可以模拟QQ群聊天的功能，我们来看一下</span><br><span class="line">首先在kafka中创建一个新的topic，可以认为是我们在QQ里面创建了一个群，群号是88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost 2181 --partitions 5 --replication-factor 2 --topic 88888888</span><br><span class="line">Created topic 88888888.</span><br><span class="line"></span><br><span class="line">然后我把你们都拉到这个群里面，这样我在群里面发消息你们就都能收到了</span><br><span class="line">在bigdata02和bigdata03上开启消费者，可以认为是把这两个人拉到群里面了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic 88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic 88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">然后我在bigdata01上开启生产者发消息，这样bigdata02和bigdata03都是可以收到的。</span><br><span class="line">这样就可以认为在群里的人都能收到我发的消息，类似于发广播。</span><br><span class="line">这个其实主要利用了kafka中的多消费者的特性，每个消费者都可以消费到相同的数据</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-producer.sh --broker-list localhost:9092 --topic 88888888 </span><br><span class="line">&gt;hello everyone</span><br></pre></td></tr></table></figure><h2 id="Kafka核心扩展内容"><a href="#Kafka核心扩展内容" class="headerlink" title="Kafka核心扩展内容"></a>Kafka核心扩展内容</h2><h3 id="Broker扩展"><a href="#Broker扩展" class="headerlink" title="Broker扩展"></a>Broker扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Broker的参数可以配置在server.properties这个配置文件中，Broker中支持的完整参数在官方文档中有体现</span><br><span class="line">具体链接为：</span><br><span class="line">http:&#x2F;&#x2F;kafka.apache.org&#x2F;24&#x2F;documentation.html#brokerconfigs</span><br><span class="line">针对Broker的参数，我们主要分析两块</span><br></pre></td></tr></table></figure><h4 id="Log-Flush-Policy"><a href="#Log-Flush-Policy" class="headerlink" title="Log Flush Policy"></a>Log Flush Policy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1：Log Flush Policy：设置数据flush到磁盘的时机</span><br><span class="line">为了减少磁盘写入的次数,broker会将消息暂时缓存起来,当消息的个数达到一定阀值或者过了一定的时间间隔后,再flush到磁盘,这样可以减少磁盘IO调用的次数。</span><br><span class="line"></span><br><span class="line">这块主要通过两个参数控制</span><br><span class="line">log.flush.interval.messages 一个分区的消息数阀值，达到该阈值则将该分区的数据flush到磁盘，注意这里是针对分区，因为topic是一个逻辑概念，分区是真实存在的，每个分区会在磁盘上产生一个目录</span><br><span class="line">[root@bigdata01 kafka-logs]# ll</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:12 88888888-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:12 88888888-3</span><br><span class="line">-rw-r--r--. 1 root root 4 Jun 8 15:23 cleaner-offset-checkpoint</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-12</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-15</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-18</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-21</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-24</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-27</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-3</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-30</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-33</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-36</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-39</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-42</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-45</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-48</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-6</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-9</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-1</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-4 </span><br><span class="line">hello topic有5个分区，但这里只有2个目录的原因是：没写几条数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160022785.png" alt="image-20230416002248398"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个参数的默认值为9223372036854775807，long的最大值</span><br><span class="line">默认值太大了，所以建议修改，可以使用server.properties中针对这个参数指定的值10000，需要去掉注释之后这个参数才生效。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">log.flush.interval.ms间隔指定时间</span><br><span class="line">默认间隔指定的时间将内存中缓存的数据flush到磁盘中，由文档可知，这个参数的默认值为null，此时会使用log.flush.scheduler.interval.ms参数的值，log.flush.scheduler.interval.ms参数的值默认是 9223372036854775807，long的最大值</span><br><span class="line"></span><br><span class="line">所以这个值也建议修改，可以使用server.properties中针对这个参数指定的值1000，单位是毫秒，表示每1秒写一次磁盘，这个参数也需要去掉注释之后才生效</span><br></pre></td></tr></table></figure><h4 id="Log-Retention-Policy"><a href="#Log-Retention-Policy" class="headerlink" title="Log Retention Policy"></a>Log Retention Policy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">设置数据保存周期，默认7天</span><br><span class="line">kafka中的数据默认会保存7天，如果kafka每天接收的数据量过大，这样是很占磁盘空间的，建议修改数据保存周期，我们之前在实际工作中是将数据保存周期改为了1天。</span><br><span class="line"></span><br><span class="line">数据保存周期主要通过这几个参数控制</span><br><span class="line">log.retention.hours，这个参数默认值为168，单位是小时，就是7天，可以在这调整数据保存的时间，超过这个时间数据会被自动删除</span><br><span class="line">log.retention.bytes，这个参数表示当分区的文件达到一定大小的时候会删除它，如果设置了按照指定周期删除数据文件，这个参数不设置也可以，这个参数默认是没有开启的</span><br><span class="line">log.retention.check.interval.ms，这个参数表示检测的间隔时间，单位是毫秒，默认值是300000，就是5分钟，表示每5分钟检测一次文件看是否满足删除的时机</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">默认根据时间就行了，后期在时间范围内还可以从kafka中恢复数据</span><br></pre></td></tr></table></figure><h3 id="Producer扩展"><a href="#Producer扩展" class="headerlink" title="Producer扩展"></a>Producer扩展</h3><h4 id="producer发送数据到partition的方式"><a href="#producer发送数据到partition的方式" class="headerlink" title="producer发送数据到partition的方式"></a>producer发送数据到partition的方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Producer默认是随机将数据发送到topic的不同分区中，也可以根据用户设置的算法来根据消息的key来计算输入到哪个partition里面</span><br><span class="line"></span><br><span class="line">此时需要通过partitioner来控制，这个知道就行了，因为在实际工作中一般在向kafka中生产数据的都是不带key的，只有数据内容，所以一般都是使用随机的方式发送数据</span><br></pre></td></tr></table></figure><h4 id="producer的数据通讯方式"><a href="#producer的数据通讯方式" class="headerlink" title="producer的数据通讯方式"></a>producer的数据通讯方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">在这里有一个需要注意的内容就是</span><br><span class="line">针对producer的数据通讯方式：同步发送和异步发送</span><br><span class="line"></span><br><span class="line">同步是指：生产者发出数据后，等接收方发回响应以后再发送下个数据的通讯方式。</span><br><span class="line">异步是指：生产者发出数据后，不等接收方发回响应，接着发送下个数据的通讯方式。</span><br><span class="line"></span><br><span class="line">具体的数据通讯策略是由acks参数控制的</span><br><span class="line">acks默认为1，表示需要Leader节点回复收到消息，这样生产者才会发送下一条数据</span><br><span class="line">acks：all，表示需要所有Leader+副本节点回复收到消息（acks&#x3D;-1），这样生产者才会发送下一条数据</span><br><span class="line">acks：0，表示不需要任何节点回复，生产者会继续发送下一条数据</span><br><span class="line">再来看一下这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152350669.png" alt="image-20230415235025189"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">我们在向hello这个topic生产数据的时候，可以在生产者中设置acks参数，</span><br><span class="line">acks设置为1，表示我们在向hello这个topic的partition1这个分区写数据的时候，只需要让leader所在的broker1这个节点回复确认收到的消息就可以了，这样生产者就可以发送下一条数据了</span><br><span class="line"></span><br><span class="line">如果acks设置为all，则需要partition1的这两个副本所在的节点(包含Leader)都回复收到消息，生产者才会发送下一条数据</span><br><span class="line"></span><br><span class="line">如果acks设置为0，表示生产者不会等待任何partition所在节点的回复，它只管发送数据，不管你有没有收到，所以这种情况丢失数据的概率比较高。</span><br></pre></td></tr></table></figure><h5 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对这块在面试的时候会有一个面试题：Kafka如何保证数据不丢？</span><br><span class="line">其实就是通过acks机制保证的，如果设置acks为all，则可以保证数据不丢，因为此时把数据发送给kafka之后，会等待对应partition所在的所有leader和副本节点都确认收到消息之后才会认为数据发送成功了，所以在这种策略下，只要把数据发送给kafka之后就不会丢了。</span><br><span class="line"></span><br><span class="line">如果acks设置为1，则当我们把数据发送给partition之后，partition的leader节点也确认收到了，但是leader回复完确认消息之后，leader对应的节点就宕机了，副本partition还没来得及将数据同步过去，所以会存在丢失的可能性。</span><br><span class="line">不过如果宕机的是副本partition所在的节点，则数据是不会丢的</span><br><span class="line"></span><br><span class="line">如果acks设置为0的话就表示是顺其自然了，只管发送，不管kafka有没有收到，这种情况表示对数据丢不丢都无所谓了。</span><br></pre></td></tr></table></figure><h3 id="Consumer扩展"><a href="#Consumer扩展" class="headerlink" title="Consumer扩展"></a>Consumer扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">在消费者中还有一个消费者组的概念</span><br><span class="line">每个consumer属于一个消费者组，通过group.id指定消费者组</span><br><span class="line"></span><br><span class="line">那组内消费和组间消费有什么区别吗？</span><br><span class="line">组内：消费者组内的所有消费者消费同一份数据；</span><br><span class="line"></span><br><span class="line">注意：在同一个消费者组中，一个partition同时只能有一个消费者消费数据</span><br><span class="line">如果消费者的个数小于分区的个数，一个消费者会消费多个分区的数据。</span><br><span class="line">如果消费者的个数大于分区的个数，则多余的消费者不消费数据</span><br><span class="line">所以，对于一个topic,同一个消费者组中推荐不能有多于分区个数的消费者,否则将意味着某些消费者将无法获得消息。</span><br><span class="line"></span><br><span class="line">组间：多个消费者组消费相同的数据，互不影响。</span><br><span class="line">来看下面这个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160000840.png" alt="image-20230416000001364"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Kafka集群有两个节点，Broker1和Broker2</span><br><span class="line">集群内有一个topic，这个topic有4个分区，P0,P1,P2,P3</span><br><span class="line"></span><br><span class="line">下面有两个消费者组</span><br><span class="line">Consumer Group A和Consumer Group B</span><br><span class="line">其中Consumer Group A中有两个消费者C1和C2，由于这个topic有4个分区，所以，C1负责消费两个分区的数据，C2负责消费两个分区的数据，这个属于组内消费</span><br><span class="line">Consumer Group B有5个消费者，C3~C7，其中C3,C4,C5,C6分别消费一个分区的数据，而C7就是多余出来的了，因为现在这个消费者组内的消费者的数量比对应的topic的分区数量还多，但是一个分区同时只能被一个消费者消费，所以就会有一个消费者处于空闲状态。这个也属于组内消费</span><br><span class="line">Consumer Group A和Consumer Group B这两个消费者组属于组间消费，互不影响。</span><br></pre></td></tr></table></figure><h3 id="Topic、Partition扩展"><a href="#Topic、Partition扩展" class="headerlink" title="Topic、Partition扩展"></a>Topic、Partition扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">每个partition在存储层面是append log文件。</span><br><span class="line">新消息都会被直接追加到log文件的尾部，每条消息在log文件中的位置称为offset(偏移量)。</span><br><span class="line">越多partitions可以容纳更多的consumer,有效提升并发消费的能力。</span><br><span class="line"></span><br><span class="line">具体什么时候增加topic的数量？什么时候增加partition的数量呢？</span><br><span class="line"></span><br><span class="line">业务类型增加需要增加topic、数据量大需要增加partition</span><br></pre></td></tr></table></figure><h3 id="Message扩展"><a href="#Message扩展" class="headerlink" title="Message扩展"></a>Message扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每条Message包含了以下三个属性：</span><br><span class="line">1. offset对应类型：long，表示此消息在一个partition中的起始的位置。可以认为offset是partition中Message的id，自增的</span><br><span class="line">2. MessageSize 对应类型：int32 此消息的字节大小。</span><br><span class="line">3. data，类型为bytes,是message的具体内容。</span><br><span class="line">看这个图，加深对Topic、Partition、Message的理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160016235.png" alt="image-20230416001618719"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里也体现了kafka高吞吐量的原因：磁盘顺序读写由于内存随机访问</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2%5C202304160022785.png" alt></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
</feed>
