<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2022-03-01T12:02:52.759Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发工程师-第九周 第4章 Scala函数式编程</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Scala%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Scala%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B.html</id>
    <published>2022-03-01T01:35:01.000Z</published>
    <updated>2022-03-01T12:02:52.759Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第九周-第4章-Scala函数式编程"><a href="#第九周-第4章-Scala函数式编程" class="headerlink" title="第九周 第4章 Scala函数式编程"></a>第九周 第4章 Scala函数式编程</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面我们来学习一下scala中最重要的内容，函数式编程，其实我们学习Scala这门语言就是因为它的这一个特性，咱们在最开始的时候给大家演示了，使用java代码实现函数式编程是很复杂的，而使用scala代码实现函数式编程就很轻松，很简单了。</span><br><span class="line">这块内容我们在后续工作中会经常使用，需要大家重点掌握。</span><br></pre></td></tr></table></figure><h2 id="什么是函数式编程"><a href="#什么是函数式编程" class="headerlink" title="什么是函数式编程"></a>什么是函数式编程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Scala是一门既面向对象，又面向过程的语言。</span><br><span class="line">因此在Scala中有非常好的面向对象的特性，可以使用Scala来基于面向对象的思想开发大型复杂的系统和工程；</span><br><span class="line">而且Scala也面向过程，因此Scala中有函数的概念。</span><br><span class="line">在Scala中，函数与类、对象一样，都是一等公民，所以说scala的面向过程其实就重在针对函数的编程了，所以称之为函数式编程</span><br></pre></td></tr></table></figure><h2 id="函数赋值给变量"><a href="#函数赋值给变量" class="headerlink" title="函数赋值给变量"></a>函数赋值给变量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Scala中的函数是一等公民，可以独立定义，独立存在，而且可以直接将函数作为值赋值给变量</span><br><span class="line">Scala的语法规定，将函数赋值给变量时，必须在函数后面加上空格和下划线</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>(name: <span class="type">String</span>) &#123; println(<span class="string">"Hello, "</span> + name) &#125;</span><br><span class="line">sayHello: (name: <span class="type">String</span>)<span class="type">Unit</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> sayHelloFunc = sayHello _</span><br><span class="line">sayHelloFunc: <span class="type">String</span> =&gt; <span class="type">Unit</span> = &lt;function1&gt;</span><br><span class="line">scala&gt; sayHelloFunc(<span class="string">"scala"</span>)</span><br><span class="line"><span class="type">Hello</span>, scala</span><br></pre></td></tr></table></figure><h2 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Scala中的函数也可以不需要命名，这种函数称为匿名函数</span><br><span class="line">匿名函数的语法格式：(参数名: 参数类型) &#x3D;&gt; 函数体</span><br><span class="line">(参数名: 参数类型) ：是函数的参数列表</span><br><span class="line"></span><br><span class="line">可以将匿名函数直接赋值给某个变量</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sayHelloFunc = (name: <span class="type">String</span>) =&gt; println(<span class="string">"Hello, "</span> + name)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> sayHelloFunc = (name: <span class="type">String</span>) =&gt; &#123;println(<span class="string">"Hello, "</span> + name)&#125;</span><br><span class="line"></span><br><span class="line">注意：如果函数体有多行代码，则需要添加&#123;&#125;</span><br></pre></td></tr></table></figure><h2 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">由于函数是一等公民，所以说我们可以直接将某个函数作为参数传入其它函数</span><br><span class="line">这个功能是极其强大的，也是Java这种面向对象的编程语言所不具备的</span><br><span class="line">这个功能在实际工作中是经常需要用到的</span><br><span class="line">接收其它函数作为当前函数的参数，当前这个函数也被称作高阶函数 (higher-order function)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">看一个例子：</span><br><span class="line">先定义一个匿名函数，赋值给变量sayHelloFunc</span><br><span class="line"><span class="keyword">val</span> sayHelloFunc = (name: <span class="type">String</span>) =&gt; println(<span class="string">"Hello, "</span> + name)</span><br><span class="line"></span><br><span class="line">再定义一个高阶函数，这个高阶函数的参数会接收一个函数</span><br><span class="line">参数： (<span class="type">String</span>) =&gt; <span class="type">Unit</span> 表示这个函数接收一个字符串，没有返回值</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greeting</span></span>(func: (<span class="type">String</span>) =&gt; <span class="type">Unit</span>, name: <span class="type">String</span>) &#123; func(name) &#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; greeting(sayHelloFunc, <span class="string">"scala"</span>)</span><br><span class="line"><span class="type">Hello</span>, scala</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">或者还可以这样用，直接把匿名函数的定义传过来也是可以</span><br><span class="line">scala&gt; greeting((name: <span class="type">String</span>) =&gt; println(<span class="string">"Hello, "</span> + name),<span class="string">"scala"</span>)</span><br><span class="line"><span class="type">Hello</span>, scala</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">高阶函数可以自动推断出它里面函数的参数类型，对于只有一个参数的函数，还可以省去小括号</span><br><span class="line"># 先定义一个高阶函数</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greeting</span></span>(func: (<span class="type">String</span>) =&gt; <span class="type">Unit</span>, name: <span class="type">String</span>) &#123; func(name) &#125;</span><br><span class="line"># 使用高阶函数：完整写法</span><br><span class="line">greeting((name: <span class="type">String</span>) =&gt; println(<span class="string">"Hello, "</span> + name), <span class="string">"scala"</span>)</span><br><span class="line"># 使用高阶函数：高阶函数可以自动推断出参数类型，而不需要写明类型</span><br><span class="line">greeting((name) =&gt; println(<span class="string">"Hello, "</span> + name), <span class="string">"scala"</span>)</span><br><span class="line"># 使用高阶函数：对于只有一个参数的函数，还可以省去其小括号</span><br><span class="line">greeting(name =&gt; println(<span class="string">"Hello, "</span> + name), <span class="string">"scala"</span>)</span><br></pre></td></tr></table></figure><h2 id="常用高阶函数"><a href="#常用高阶函数" class="headerlink" title="常用高阶函数"></a>常用高阶函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">刚才是我们自己实现的高阶函数，其实我们在工作中自己定义高阶函数的场景不多，大部分场景都是去使</span><br><span class="line">用已有的高阶函数</span><br><span class="line">下面我们来看几个常见的高阶函数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">map：对传入的每个元素都进行处理，返回一个元素</span><br><span class="line">flatMap：对传入的每个元素都进行处理，返回一个或者多个元素</span><br><span class="line">foreach：对传入的每个元素都进行处理，但是没有返回值</span><br><span class="line">filter：对传入的每个元素都进行条件判断，如果返回true，则保留该元素，否则过滤掉该元素</span><br><span class="line">reduceLeft：从左侧元素开始，进行reduce操作</span><br></pre></td></tr></table></figure><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).map(num=&gt;&#123;num * <span class="number">2</span>&#125;)</span><br><span class="line">res38: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">可以简写为</span><br><span class="line">scala&gt; <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).map(_ * <span class="number">2</span>)</span><br><span class="line">res40: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="type">Array</span>(<span class="string">"hello you"</span>,<span class="string">"hello me"</span>).flatMap(line=&gt;line.split(<span class="string">" "</span>))</span><br><span class="line">res53: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(hello, you, hello, me)</span><br><span class="line"></span><br><span class="line">可以简写为</span><br><span class="line">scala&gt; <span class="type">Array</span>(<span class="string">"hello you"</span>,<span class="string">"hello me"</span>).flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">res54: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(hello, you, hello, me)</span><br></pre></td></tr></table></figure><h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).map(_ * <span class="number">2</span>).foreach(num=&gt;println(num))</span><br><span class="line"><span class="number">2</span> <span class="number">4</span> <span class="number">6</span> <span class="number">8</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).map(_ * <span class="number">2</span>).foreach(println(_))</span><br><span class="line"><span class="number">2</span> <span class="number">4</span> <span class="number">6</span> <span class="number">8</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>).map(_ * <span class="number">2</span>).foreach(println _)</span><br><span class="line"><span class="number">2</span> <span class="number">4</span> <span class="number">6</span> <span class="number">8</span></span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; Array(1, 2, 3, 4, 5).filter(num&#x3D;&gt;num % 2 &#x3D;&#x3D; 0)</span><br><span class="line">res46: Array[Int] &#x3D; Array(2, 4)</span><br><span class="line"></span><br><span class="line">scala&gt; Array(1, 2, 3, 4, 5).filter(_ % 2 &#x3D;&#x3D; 0)</span><br><span class="line">res47: Array[Int] &#x3D; Array(2, 4)</span><br></pre></td></tr></table></figure><h3 id="reduceLeft"><a href="#reduceLeft" class="headerlink" title="reduceLeft"></a>reduceLeft</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">reduceLeft的使用</span><br><span class="line">表示先对元素1和元素2进行处理，然后将结果与元素3处理，再将结果与元素4处理，依次类推spark中有一个reduce函数，和这个函数的效果一致</span><br><span class="line"></span><br><span class="line">scala&gt; Array(1, 2, 3, 4, 5).reduceLeft((t1,t2)&#x3D;&gt;t1+t2)</span><br><span class="line">res50: Int &#x3D; 15</span><br><span class="line"></span><br><span class="line">scala&gt; Array(1, 2, 3, 4, 5).reduceLeft( _ + _)</span><br><span class="line">res49: Int &#x3D; 15</span><br><span class="line"></span><br><span class="line">注意：这里的两个_代表是两个元素</span><br></pre></td></tr></table></figure><h2 id="案例：函数式编程"><a href="#案例：函数式编程" class="headerlink" title="案例：函数式编程"></a>案例：函数式编程</h2> <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">统计多个文本内的单词总数</span><br><span class="line">使用scala的io包读取文本文件内的数据</span><br><span class="line"></span><br><span class="line">使用<span class="type">List</span>的伴生对象，将多个文件内的内容创建为一个<span class="type">List</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines01 = scala.io.<span class="type">Source</span>.fromFile(<span class="string">"D://a.txt"</span>).mkString</span><br><span class="line"><span class="keyword">val</span> lines02 = scala.io.<span class="type">Source</span>.fromFile(<span class="string">"D://b.txt"</span>).mkString</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = <span class="type">List</span>(lines01, lines02)</span><br><span class="line"></span><br><span class="line">注意：下面这一行是核心代码，使用了链式调用的函数式编程</span><br><span class="line">lines.flatMap(_.split( <span class="string">" "</span>)).map((_, <span class="number">1</span>)).map(_._2).reduceLeft(_ + _)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines.flatMap(_.split( <span class="string">" "</span>)) ：表示对每一行数据使用空格进行切割，返回每一个单词</span><br><span class="line">.map((_, <span class="number">1</span>)) ：针对每一个单词，都转成tuple类型，tuple中的第<span class="number">1</span>个元素是这个单词，第<span class="number">2</span>个元素表示单词出现的次数<span class="number">1</span></span><br><span class="line">.map(_._2) ：迭代每一个tuple，获取tuple中的第<span class="number">2</span>个元素</span><br><span class="line">.reduceLeft(_ + _) ：对前面获取到的元素进行累加求和</span><br></pre></td></tr></table></figure><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Scala" scheme="http://tianyong.fun/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第九周 第5章 Scala高级特性</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Scala%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Scala%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7.html</id>
    <published>2022-03-01T01:35:01.000Z</published>
    <updated>2022-03-01T13:17:32.015Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第九周-第5章-Scala高级特性"><a href="#第九周-第5章-Scala高级特性" class="headerlink" title="第九周 第5章 Scala高级特性"></a>第九周 第5章 Scala高级特性</h1><h2 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先看一下模式匹配</span><br><span class="line">模式匹配是<span class="type">Scala</span>中非常有特色，非常强大的一种功能。</span><br><span class="line">模式匹配，其实类似于<span class="type">Java</span>中的 switch <span class="keyword">case</span> 语法，即对一个值进行条件判断，然后针对不同的条件，进</span><br><span class="line">行不同的处理</span><br><span class="line">不过<span class="type">Scala</span>没有<span class="type">Java</span>中的 switch <span class="keyword">case</span> 语法，但是，<span class="type">Scala</span>提供了更加强大的 <span class="keyword">match</span> <span class="keyword">case</span> 语法，就是这个</span><br><span class="line">模式匹配</span><br><span class="line"><span class="type">Java</span>的 switch <span class="keyword">case</span> 仅能匹配变量的值，而<span class="type">Scala</span>的 <span class="keyword">match</span> <span class="keyword">case</span> 可以匹配各种情况，比如：变量的类型、</span><br><span class="line">集合的元素，有值没值</span><br></pre></td></tr></table></figure><h3 id="对变量的值进行模式匹配"><a href="#对变量的值进行模式匹配" class="headerlink" title="对变量的值进行模式匹配"></a>对变量的值进行模式匹配</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">match case语法格式：变量 match &#123; case 值 &#x3D;&gt; 代码 &#125;</span><br><span class="line">如果值为下划线，则代表了不满足以上所有情况下的默认处理</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo1</span></span>(day: <span class="type">Int</span>) &#123;</span><br><span class="line">day <span class="keyword">match</span> &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="number">1</span> =&gt; println(<span class="string">"Monday"</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="number">2</span> =&gt; println(<span class="string">"Tuesday"</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="number">3</span> =&gt; println(<span class="string">"Wednesday"</span>)</span><br><span class="line"><span class="keyword">case</span> _ =&gt; println(<span class="string">"none"</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; demo1(<span class="number">1</span>)</span><br><span class="line"><span class="type">Monday</span></span><br><span class="line">scala&gt; demo1(<span class="number">4</span>)</span><br><span class="line">none</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：match case中，只要一个case分支满足并处理了，就不会继续判断下一个case分支了，这一点与Java不同，java的switch case需要用break停止向下执行</span><br></pre></td></tr></table></figure><h3 id="变量类型的模式匹配"><a href="#变量类型的模式匹配" class="headerlink" title="变量类型的模式匹配"></a>变量类型的模式匹配</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Scala</span>的模式匹配一个强大之处就在于，可以直接匹配类型，而不是值！！！这点是java的switch <span class="keyword">case</span>绝对做不到的</span><br><span class="line">语法格式：变量 <span class="keyword">match</span> &#123; <span class="keyword">case</span> 变量: 类型 =&gt; 代码 &#125;</span><br><span class="line">典型的一个应用场景就是针对异常的处理</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io._</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processException</span></span>(e: <span class="type">Exception</span>) &#123;</span><br><span class="line">e <span class="keyword">match</span> &#123;</span><br><span class="line"><span class="keyword">case</span> e1: <span class="type">IllegalArgumentException</span> =&gt; println(<span class="string">"IllegalArgumentException "</span></span><br><span class="line"><span class="keyword">case</span> e2: <span class="type">FileNotFoundException</span> =&gt; println(<span class="string">"FileNotFoundException "</span> + e2)</span><br><span class="line"><span class="keyword">case</span> e3: <span class="type">IOException</span> =&gt; println(<span class="string">"IOException "</span> + e3)</span><br><span class="line"><span class="keyword">case</span> _: <span class="type">Exception</span> =&gt; println(<span class="string">"Exception "</span> )</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; processException(<span class="keyword">new</span> <span class="type">Exception</span>())</span><br><span class="line"><span class="type">Exception</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">在<span class="keyword">try</span>-<span class="keyword">catch</span>异常中的应用</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="keyword">val</span> lines02 = scala.io.<span class="type">Source</span>.fromFile(<span class="string">"D://test02.txt"</span>).mkString</span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line"><span class="keyword">case</span> ex: <span class="type">FileNotFoundException</span> =&gt; println(<span class="string">"no file"</span>)</span><br><span class="line"><span class="keyword">case</span> ex: <span class="type">IOException</span> =&gt; println(<span class="string">"io exception"</span>)</span><br><span class="line"><span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; println(<span class="string">"exception"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="case-class与模式匹配"><a href="#case-class与模式匹配" class="headerlink" title="case class与模式匹配"></a>case class与模式匹配</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Scala</span>中提供了一种特殊的类，用 <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">进行声明，中文可以称为样例类</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">其实有点类似于Java中的JavaBean的概念</span></span></span><br><span class="line"><span class="class"><span class="title">即只定义field，会由Scala在编译时自动提供get和set方法，但是没有其它的method</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">的主构造函数接收的参数通常不需要使用var或val修饰，Scala自动就会使用val修饰（但是如果你自己使用var修饰，那么还是会按照var来，在这用哪个区别都不大）</span></span></span><br><span class="line"><span class="class"><span class="title">Scala自动为</span> <span class="title">case</span> <span class="title">class</span> <span class="title">定义了伴生对象，也就是object，并且定义了apply</span>(<span class="params"></span>)<span class="title">方法，该方法接收主构造函数中相同的参数，并返回</span> <span class="title">case</span> <span class="title">class</span> <span class="title">对象</span></span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Teacher</span>(<span class="params">name: <span class="type">String</span>, sub: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Student</span>(<span class="params">name: <span class="type">String</span>, cla: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">check</span>(<span class="params">p: <span class="type">Person</span></span>) </span>&#123;</span><br><span class="line">p <span class="keyword">match</span> &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">Teacher</span>(name, sub) =&gt; println(<span class="string">"Teacher, name is "</span> + name + <span class="string">", sub is "</span> + sub)</span><br><span class="line"><span class="keyword">case</span> <span class="type">Student</span>(name, cla) =&gt; println(<span class="string">"Student, name is "</span> + name + <span class="string">", cla is "</span> + cla)</span><br><span class="line"><span class="keyword">case</span> _ =&gt; println(<span class="string">"none"</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">执行</span><br><span class="line">scala&gt; check(<span class="keyword">new</span> <span class="type">Student</span>(<span class="string">"tom"</span>,<span class="string">"class1"</span>))</span><br><span class="line"><span class="type">Student</span>, name is tom, cla is class1</span><br><span class="line">scala&gt; check(<span class="keyword">new</span> <span class="type">Person</span>())</span><br><span class="line">none</span><br></pre></td></tr></table></figure><h3 id="Option与模式匹配"><a href="#Option与模式匹配" class="headerlink" title="Option与模式匹配"></a>Option与模式匹配</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Scala有一种特殊的数据类型，叫做Option。</span><br><span class="line">Option有两种值，一种是Some，表示有值，一种是None，表示没有值</span><br><span class="line">Option通常会用于模式匹配中，用于判断某个变量是有值还是没有值，这比null来的更加简洁明了</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/b1Mnh9" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/03/01/b1Mnh9.md.png" alt="b1Mnh9.md.png"></a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ages = <span class="type">Map</span>(<span class="string">"jack"</span> -&gt; <span class="number">18</span>, <span class="string">"tom"</span> -&gt; <span class="number">30</span>, <span class="string">"jessic"</span> -&gt; <span class="number">27</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAge</span></span>(name: <span class="type">String</span>) &#123;</span><br><span class="line"><span class="keyword">val</span> age = ages.get(name)</span><br><span class="line">age <span class="keyword">match</span> &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">Some</span>(age) =&gt; println(<span class="string">"your age is "</span> + age)</span><br><span class="line"><span class="keyword">case</span> <span class="type">None</span> =&gt; println(<span class="string">"none"</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; getAge(<span class="string">"jack"</span>)</span><br><span class="line">your age is <span class="number">18</span></span><br><span class="line">scala&gt; getAge(<span class="string">"hehe"</span>)</span><br><span class="line">none</span><br></pre></td></tr></table></figure><h2 id="隐式转换"><a href="#隐式转换" class="headerlink" title="隐式转换"></a>隐式转换</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Scala的隐式转换，允许手动指定将某种类型的对象转换成其它类型的对象</span><br><span class="line">Scala的隐式转换，最核心的就是定义隐式转换函数，即implicit conversion function</span><br><span class="line">隐式转换函数与普通函数唯一的语法区别是要以implicit开头而且最好要定义函数返回类型</span><br><span class="line">隐式转换非常强大的一个功能，就是可以在不知不觉中加强现有类型的功能。也就是说，我们可以为某个普通类定义一个加强类，并定义对应的隐式转换函数，这样我们在使用加强类里面的方法的时候，Scala会自动进行隐式转换，把普通类转换为加强类，然后再调用加强类中的方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Scala默认会自动使用两种隐式转换</span><br><span class="line">1：源类型，或者目标类型的伴生对象里面的隐式转换函数</span><br><span class="line">2：当前程序作用域内可以用唯一标识符表示的隐式转换函数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果隐式转换函数不在上述两种情况下的话，那么就必须手动使用import引入对应的隐式转换函数</span><br><span class="line">通常建议，仅仅在需要进行隐式转换的地方，比如某个函数内，用import导入隐式转换函数，这样可以缩小隐式转换函数的作用域，避免不需要的隐式转换</span><br></pre></td></tr></table></figure><h2 id="案例：狗也能抓老鼠"><a href="#案例：狗也能抓老鼠" class="headerlink" title="案例：狗也能抓老鼠"></a>案例：狗也能抓老鼠</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">通过隐式转换实现，狗也具备猫抓老鼠的功能</span><br><span class="line"></span><br><span class="line">class cat(val name: String)&#123;</span><br><span class="line">def catchMouse()&#123;println(name+&quot; catch mouse&quot;)&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class dog(val name: String)</span><br><span class="line"></span><br><span class="line">implicit def object2Cat (obj: Object): cat &#x3D; &#123;</span><br><span class="line">if (obj.getClass &#x3D;&#x3D; classOf[dog]) &#123;</span><br><span class="line">val dog &#x3D; obj.asInstanceOf[dog]</span><br><span class="line">new cat(dog.name)</span><br><span class="line">&#125;</span><br><span class="line">else Nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> d = <span class="keyword">new</span> dog(<span class="string">"d1"</span>)</span><br><span class="line">d: dog = dog@<span class="number">7</span>f0e0db3</span><br><span class="line">scala&gt; d.catchMouse()</span><br><span class="line">d1 <span class="keyword">catch</span> mouse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">我们后续在工作中一般很少需要我们自己去定义隐式转换函数，大部分的场景是我们只需要使用<span class="keyword">import</span></span><br><span class="line">导入对应的隐式转换函数就可以了，在这个案例中我们是自己手工实现了一个隐私转换函数，因为他们都在一个作用域内，所以就不需要<span class="keyword">import</span>了</span><br></pre></td></tr></table></figure><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Scala" scheme="http://tianyong.fun/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第九周 第3章 Scala面向对象</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Scala%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Scala%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html</id>
    <published>2022-03-01T01:35:01.000Z</published>
    <updated>2022-03-01T06:56:15.897Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第九周-第3章-Scala面向对象"><a href="#第九周-第3章-Scala面向对象" class="headerlink" title="第九周 第3章 Scala面向对象"></a>第九周 第3章 Scala面向对象</h1><h2 id="Scala面向对象编程"><a href="#Scala面向对象编程" class="headerlink" title="Scala面向对象编程"></a>Scala面向对象编程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在这里我们主要学习Scala中的类、对象和接口</span><br><span class="line">注意：</span><br><span class="line">Scala中类和java中的类基本是类似的</span><br><span class="line">Scala中的对象时需要定义的，而java中的对象是通过class new出来的</span><br><span class="line">Scala中的接口是trait，java中的接口是interface</span><br></pre></td></tr></table></figure><h3 id="类-class"><a href="#类-class" class="headerlink" title="类-class"></a>类-class</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先看一下类</span><br><span class="line">Scala中定义类和Java一样，都是使用 class 关键字</span><br><span class="line">和Java一样，使用new关键字创建对象</span><br><span class="line">那下面来看一个具体案例</span><br><span class="line">定义Person类，创建对象并调用其方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Person&#123;</span><br><span class="line">var name &#x3D; &quot;scala&quot;</span><br><span class="line">def sayHello()&#123;</span><br><span class="line">println(&quot;Hello,&quot;+name)</span><br><span class="line">&#125;</span><br><span class="line">def getName&#x3D; name</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：如果在定义方法的时候指定了()，那么在调用的时候()可写可不写，如果在定义方法的时候</span><br><span class="line">没指定()，则调用方法时肯定不能带()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val p &#x3D; new Person()</span><br><span class="line">p: Person &#x3D; Person@23b8d9f3</span><br><span class="line">scala&gt; p.sayHello()</span><br><span class="line">Hello,scala</span><br><span class="line">scala&gt; p.sayHello</span><br><span class="line">Hello,scala</span><br><span class="line">scala&gt; println(p.getName)</span><br><span class="line">scala</span><br><span class="line">scala&gt; println(p.getName())</span><br><span class="line">&lt;console&gt;:10: error: not enough arguments for method apply: (index: Int)Char</span><br></pre></td></tr></table></figure><h4 id="constructor-NaN"><a href="#constructor-NaN" class="headerlink" title="constructor"></a>constructor</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">类创建好了，下面我们来看一下类中的构造函数，</span><br><span class="line">Scala类中的构造函数可以分为主构造函数和辅助构造函数</span><br><span class="line">这两种构造函数有什么区别呢？</span><br><span class="line">主constructor：类似Java的默认构造函数 this()</span><br><span class="line">辅助constructor：类似Java的重载构造函数 this(name,age)</span><br></pre></td></tr></table></figure><h5 id="主constructor"><a href="#主constructor" class="headerlink" title="主constructor"></a>主constructor</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那先来看一下主构造函数</span><br><span class="line">Scala的主constructor是与类名放在一起的，与Java不同，Java中的构造函数是写在类内部的</span><br><span class="line"></span><br><span class="line">注意：在类中，没有定义在任何方法或者是代码块之中的代码就是主constructor的代码</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">val name:<span class="type">String</span>,val age:<span class="type">Int</span></span>)</span>&#123;</span><br><span class="line">println(<span class="string">"your name is "</span> + name + <span class="string">", your age is "</span> + age)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">new</span> <span class="type">Student</span>(<span class="string">"zs"</span>,<span class="number">19</span>)</span><br><span class="line">your name is zs, your age is <span class="number">19</span></span><br><span class="line">res8: <span class="type">Student</span> = <span class="type">Student</span>@<span class="number">3134153</span>d</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">在创建对象的时候，类中的println语句执行了，说明这个语句属于主构造函数中的代码</span><br><span class="line">主constructor中还可以通过使用默认参数，来给参数设置默认值</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">val name:<span class="type">String</span> = "jack",val age:<span class="type">Int</span> = 20</span>)</span>&#123;</span><br><span class="line">println(<span class="string">"your name is "</span> + name + <span class="string">", your age is "</span> + age)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">new</span> <span class="type">Student</span>()</span><br><span class="line">your name is jack, your age is <span class="number">20</span></span><br><span class="line">res10: <span class="type">Student</span> = <span class="type">Student</span>@<span class="number">7</span>ddd84b5</span><br><span class="line">scala&gt; <span class="keyword">new</span> <span class="type">Student</span>(<span class="string">"tom"</span>,<span class="number">18</span>)</span><br><span class="line">your name is tom, your age is <span class="number">18</span></span><br><span class="line">res11: <span class="type">Student</span> = <span class="type">Student</span><span class="meta">@a</span>09303</span><br></pre></td></tr></table></figure><h5 id="辅构造函数"><a href="#辅构造函数" class="headerlink" title="辅构造函数"></a>辅构造函数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Scala中，可以给类定义多个辅助constructor，类似于java中的构造函数重载</span><br><span class="line">辅助constructor之间可以互相调用，但是第一行必须调用主constructor</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line"><span class="keyword">var</span> name = <span class="string">"jack"</span></span><br><span class="line"><span class="keyword">var</span> age = <span class="number">10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name: <span class="type">String</span>) &#123;</span><br><span class="line"><span class="keyword">this</span>()</span><br><span class="line"><span class="keyword">this</span>.name = name</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name: <span class="type">String</span>, age: <span class="type">Int</span>) &#123;</span><br><span class="line"><span class="keyword">this</span>(name)</span><br><span class="line"><span class="keyword">this</span>.age = age</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> s = <span class="keyword">new</span> <span class="type">Student</span>(<span class="string">"tom"</span>)</span><br><span class="line">s: <span class="type">Student</span> = <span class="type">Student</span>@<span class="number">1</span>a538ed8</span><br><span class="line">scala&gt; <span class="keyword">val</span> s = <span class="keyword">new</span> <span class="type">Student</span>(<span class="string">"mick"</span>,<span class="number">30</span>)</span><br><span class="line">s: <span class="type">Student</span> = <span class="type">Student</span>@<span class="number">319642</span>db</span><br></pre></td></tr></table></figure><h3 id="对象-object"><a href="#对象-object" class="headerlink" title="对象-object"></a>对象-object</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习的scala中的<span class="class"><span class="keyword">class</span><span class="title">和java中的class是类似的，下面来看一个java中没有的内容，叫Object</span></span></span><br><span class="line"><span class="class"><span class="title">那大家可能有疑问了，Java中也有object，通过class就可以创建object</span></span></span><br><span class="line"><span class="class"><span class="title">但是注意了，在scala中，我们可以直接定义一个object，就像定义class一样。</span></span></span><br><span class="line"><span class="class"><span class="title">object：相当于class的单个实例，通常在里面放一些静态的field或者method</span></span></span><br><span class="line"><span class="class"><span class="title">object不能定义带参数的constructor，只有空参的constructor</span></span></span><br><span class="line"><span class="class"><span class="title">第一次调用object的方法时，会执行object的constructor，也就是执行object内部不在任何方法中的代码，因为它只有空参的构造函数</span></span></span><br><span class="line"><span class="class"><span class="title">但是注意，object的constructor的代码只会在他第一次被调用时执行一次，以后再次调用就不会再执行了</span></span></span><br><span class="line"><span class="class"><span class="title">object通常用于作为单例模式的实现，或者放class的一些静态成员，比如工具方法</span></span></span><br><span class="line"><span class="class"><span class="title">object可以直接使用，不能new</span></span></span><br><span class="line"><span class="class"><span class="title">创建一个object，使用object关键字</span></span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"><span class="keyword">var</span> age = <span class="number">1</span></span><br><span class="line">println(<span class="string">"this Person object!"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAge</span> </span>= age</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">执行，直接通过<span class="type">Object</span>的名称调用属性或者方法即可，类似于<span class="type">Java</span>中的静态类</span><br><span class="line">res18: <span class="type">Person</span><span class="class">.<span class="keyword">type</span> </span>= <span class="type">Person</span>$@<span class="number">73e776</span>b7</span><br><span class="line">scala&gt; <span class="type">Person</span>.age</span><br><span class="line">res19: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line">scala&gt; <span class="type">Person</span>.getAge</span><br><span class="line">res20: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><h4 id="伴生对象"><a href="#伴生对象" class="headerlink" title="伴生对象"></a>伴生对象</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">前面学习了class和object，那下面再来看一个特殊的概念，伴生对象</span><br><span class="line">如果有一个class，还有一个与class同名的object，那么就称这个object是class的 伴生对象 ，class是object的 伴生类</span><br><span class="line">注意：伴生类和伴生对象必须存放在一个.scala文件之中</span><br><span class="line">伴生类和伴生对象最大特点在于可以互相访问private field</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> fdNum= <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getFdNum</span> </span>= fdNum</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span>, val age: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sayHello</span> </span>= println(<span class="string">"Hi, "</span> + name + <span class="string">",you are "</span> + age + <span class="string">" years old!"</span> +</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"tom"</span>,<span class="number">20</span>).sayHello</span><br><span class="line"><span class="type">Hi</span>, tom,you are <span class="number">20</span> years old!, and you have <span class="number">1</span> friend.</span><br><span class="line">scala&gt; <span class="type">Person</span>.fdNum</span><br><span class="line">&lt;console&gt;:<span class="number">9</span>: error: value fdNum is not a member of <span class="class"><span class="keyword">object</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"><span class="title">Person</span>.<span class="title">fdNum</span></span></span><br><span class="line"><span class="class"><span class="title">^</span></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">Person</span>.<span class="title">getFdNum</span></span></span><br><span class="line"><span class="class"><span class="title">res26</span></span>: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><h4 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apply是object中非常重要的一个特殊方法，通常在伴生对象中实现apply方法，并在其中实现构造伴生类</span><br><span class="line">对象的功能</span><br><span class="line">在创建对象的时候，就不需要使用new Class的方式，而是使用Class()的方式，隐式调用伴生对象的apply方法，这样会让对象创建更加简洁</span><br><span class="line">例如：Array的伴生对象的apply方法就实现了接收可变数量的参数，以及会创建一个Array对象</span><br><span class="line">val a &#x3D; Array(1, 2, 3, 4, 5)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">从Array object的源码中可以看出来，它里面就是在apply方法内部使用new Array创建的对象</span><br><span class="line">下面我们来自己定义一个伴生类和伴生对象</span><br><span class="line">class Person(val name: String)&#123;</span><br><span class="line">println(&quot;my name is,&quot;+name)</span><br><span class="line">&#125;</span><br><span class="line">object Person &#123;</span><br><span class="line">def apply(name: String) &#x3D; &#123;</span><br><span class="line">println(&quot;apply exec...&quot;)</span><br><span class="line">new Person(name)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; new Person(&quot;tom&quot;)</span><br><span class="line">my name is,tom</span><br><span class="line">res29: Person &#x3D; Person@63917fe1</span><br><span class="line">scala&gt; Person(&quot;tom&quot;)</span><br><span class="line">apply exec...</span><br><span class="line">my name is,tom</span><br><span class="line">res30: Person &#x3D; Person@35e74e08</span><br><span class="line"></span><br><span class="line">注意：在这里new Person(“zhang”) 等于 Person(“zhang”)，都是获取Person的对象</span><br><span class="line">只不过Person(“zhang”)是用的object中apply方法</span><br><span class="line">而new Person(“zhang”)是直接基于class创建的</span><br></pre></td></tr></table></figure><h3 id="main方法"><a href="#main方法" class="headerlink" title="main方法"></a>main方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下scala中的main方法</span><br><span class="line">和Java一样，在Scala中如果要运行一个应用程序，必须有一个main方法，作为入口</span><br><span class="line">Scala中的main方法必须定义在object中，格式为 def main(args: Array[String])</span><br><span class="line">这就需要在编辑器中操作了，我们可以使用eclipse或者idea，但是eclipse对scala的支持不太好，所以建议使用idea</span><br><span class="line">首先确认一下idea中是否集成了scala语言插件</span><br><span class="line">打开idea，点击 configure--&gt;plugins</span><br><span class="line"></span><br><span class="line">确认scala的插件是否已经安装</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接着创建maven项目</span><br><span class="line"></span><br><span class="line">到这还没完，因为此时我们是无法创建scala代码的，这个项目中也没有集成scala的sdk，只有java的</span><br><span class="line">接下来就需要给这个项目添加scala的sdk了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以后再创建想要创建scala的maven项目，只需要进入到这个界面确认项目中是否有scala的依赖，没有的话直接点击右边的加号按钮添加即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">好，idea的scala开发环境配置好了，但是我一般还是喜欢再增加一些配置</span><br><span class="line">目前项目的src目录下有一个java目录，这个目录表示是放java代码的，当然了你在里面写scala代码肯定</span><br><span class="line">是没有问题的。</span><br><span class="line">只是针对我这种稍微有点强迫症的用起来就有点别扭了</span><br><span class="line">在实际工作中可能我们一个项目既需要使用到java代码，也需要使用到scala代码，所以最好还是建议把</span><br><span class="line">java代码和scala代码分开存放，这样比较清晰</span><br><span class="line">所以我们需要在这里比葫芦画瓢，增加一个scala目录</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">但是注意了，这样创建的scala目录是有问题的，你有没有发现这个目录的颜色和java目录的颜色都不一样</span><br><span class="line">因为你在这直接创建的scala目录是一个普通的目录，而java那个目录是一个source根目录</span><br><span class="line">所以我们也需要把scala目录变为source根目录</span><br></pre></td></tr></table></figure><h3 id="接口-trait"><a href="#接口-trait" class="headerlink" title="接口-trait"></a>接口-trait</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">接下来看一个scala中的接口，这个接口也是比较特殊的</span><br><span class="line">Scala中的接口称为trait，trait类似于Java中的interface</span><br><span class="line">在triat中可以定义抽象方法</span><br><span class="line">类可以使用 extends 关键字继承trait，无论继承类还是trait统一都是使用 extends 这个关键字</span><br><span class="line">类继承trait后，必须实现trait中的抽象方法，实现时不需要使用 override 关键字</span><br><span class="line">scala不支持对类进行多继承，但是支持对trait进行多重继承，使用 with 关键字即可</span><br><span class="line">下面我们就来看一个接口多继承的案例</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Created by xuwei</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PersonDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="keyword">val</span> p1 = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"tom"</span>)</span><br><span class="line"><span class="keyword">val</span> p2 = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"jack"</span>)</span><br><span class="line">p1.sayHello(p2.name)</span><br><span class="line">p1.makeFriends(p2)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">HelloTrait</span> </span>&#123; <span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>(name: <span class="type">String</span>)&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">MakeFriendsTrait</span> </span>&#123; <span class="function"><span class="keyword">def</span> <span class="title">makeFriends</span></span>(p: <span class="type">Person</span>)&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">val name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">HelloTrait</span> <span class="keyword">with</span> <span class="title">MakeFriendsTrait</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sayHello</span></span>(name: <span class="type">String</span>) = println(<span class="string">"Hello, "</span> + name)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeFriends</span></span>(p: <span class="type">Person</span>) = println(<span class="string">"Hello, my name is "</span> + name + <span class="string">", your name is"</span>+p.name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Scala" scheme="http://tianyong.fun/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第九周 第2章 Scala基础语法</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC2%E7%AB%A0-Scala%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC2%E7%AB%A0-Scala%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95.html</id>
    <published>2022-02-28T06:18:55.000Z</published>
    <updated>2022-03-01T02:16:22.190Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第九周-第2章-Scala基础语法"><a href="#第九周-第2章-Scala基础语法" class="headerlink" title="第九周 第2章 Scala基础语法"></a>第九周 第2章 Scala基础语法</h1><h2 id="Scala的基本使用"><a href="#Scala的基本使用" class="headerlink" title="Scala的基本使用"></a>Scala的基本使用</h2><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cala中的变量分为两种：可变 var 和 不可变 val</span><br><span class="line">可变var：可以随时修改var声明的变量的值</span><br><span class="line">不可变val：val声明的变量，值不能被修改，否则会报错： error: reassignment to val</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var a &#x3D; 1</span><br><span class="line">a: Int &#x3D; 1</span><br><span class="line">scala&gt; a &#x3D; 2</span><br><span class="line">a: Int &#x3D; 2</span><br><span class="line">scala&gt; val b &#x3D; 1</span><br><span class="line">b: Int &#x3D; 1</span><br><span class="line">scala&gt; b &#x3D; 2</span><br><span class="line">&lt;console&gt;:8: error: reassignment to val</span><br><span class="line">b &#x3D; 2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">注意：在实际工作中，针对一些不需要改变值的变量，通常建议使用val，这样可以不用担心值被错误的修改(等于java中的final类型)。这样可以提高系统的稳定性和健壮性！</span><br><span class="line"></span><br><span class="line">无论声明val变量，还是声明var变量，都可以手动指定变量的类型</span><br><span class="line">如果不指定，Scala会自动根据值，进行类型推断</span><br><span class="line">val c &#x3D; 1 等价于 val c: Int &#x3D; 1</span><br><span class="line"></span><br><span class="line">scala&gt; val c &#x3D; 1</span><br><span class="line">c: Int &#x3D; 1</span><br><span class="line">scala&gt; val c: Int &#x3D; 1</span><br><span class="line">c: Int &#x3D; 1</span><br></pre></td></tr></table></figure><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Scala中的数据类型可以分为两种，基本数据类型和增强版数据类型</span><br><span class="line">基本数据类型有： Byte、Char、Short、Int、Long、Float、Double、Boolean</span><br><span class="line">增强版数据类型有： StringOps、RichInt、RichDouble、RichChar 等</span><br><span class="line">scala使用这些增强版数据类给基本数据类型增加了上百种增强的功能</span><br><span class="line">例如：RichInt提供的有一个to函数， 1.to(10) ，此处Int会先隐式转换为RichInt，然后再调用其to函数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; 1.to(10)</span><br><span class="line">res2: scala.collection.immutable.Range.Inclusive &#x3D; Range(1, 2, 3, 4, 5, 6, 7,</span><br><span class="line"></span><br><span class="line">注意，to函数还可以这样写</span><br><span class="line">scala&gt; 1 to 10</span><br><span class="line">res3: scala.collection.immutable.Range.Inclusive &#x3D; Range(1, 2, 3, 4, 5, 6, 7,</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用基本数据类型，直接就可以调用RichInt中对应的函数</span><br><span class="line">scala&gt; 1.toString() &#x2F;&#x2F;空号也可以不用</span><br><span class="line">res4: String &#x3D; 1</span><br></pre></td></tr></table></figure><h3 id="操作符"><a href="#操作符" class="headerlink" title="操作符"></a>操作符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Scala的算术操作符与Java的算术操作符没有什么区别</span><br><span class="line">比如 +、-、*、&#x2F;、% 等，以及 &amp;、|、^、&gt;&gt;、&lt;&lt; 等</span><br><span class="line">注意：Scala中没有提供++、–操作符</span><br><span class="line">我们只能使用+和-，比如count &#x3D; 1，count++是错误的，必须写做count +&#x3D; 1</span><br></pre></td></tr></table></figure><h3 id="if-表达式"><a href="#if-表达式" class="headerlink" title="if 表达式"></a>if 表达式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在Scala中，if表达式是有返回值的，就是if或者else中最后一行语句返回的值，这一点和java中的if是不一样的，java中的if表达式是没有返回值的</span><br><span class="line"></span><br><span class="line">scala&gt; val age &#x3D; 20</span><br><span class="line">age: Int &#x3D; 20</span><br><span class="line">scala&gt; if(age &gt; 18) 1 else 0</span><br><span class="line">res9: Int &#x3D; 1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在这因为if表达式是有返回值的，所以可以将if表达式赋予一个变量</span><br><span class="line"></span><br><span class="line">scala&gt; val res &#x3D; if(age &gt; 18) 1 else 0</span><br><span class="line">res: Int &#x3D; 1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">由于if表达式是有值的，而if和else子句的值的类型可能还不一样，此时if表达式的值是什么类型呢？</span><br><span class="line">注意：Scala会自动进行推断，取两个类型的公共父类型</span><br><span class="line"></span><br><span class="line">例如，if(age &gt; 18) 1 else 0，表达式的类型是Int，因为1和0都是Int</span><br><span class="line">例如，if(age &gt; 18) “old” else 0，此时if和else的值分别是String和Int，则表达式的值是Any类型，Any是</span><br><span class="line">String和Int的公共父类型</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; if(age &gt; 18) 1 else 0</span><br><span class="line">res12: Int &#x3D; 1</span><br><span class="line">scala&gt; if(age &gt; 18) &quot;old&quot; else 0</span><br><span class="line">res13: Any &#x3D; old</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如果if后面没有跟else，则默认else的值是Unit，也可以用()表示，类似于java中的void或者null</span><br><span class="line">例如，val age &#x3D; 12; if(age &gt; 18) “old”。此时就相当于if(age &gt; 18) “old” else ()。</span><br><span class="line">此时表达式的值是Any</span><br><span class="line"></span><br><span class="line">scala&gt; if(age &gt; 18) &quot;old&quot; else ()</span><br><span class="line">res17: Any &#x3D; ()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果想在scala REPL中执行多行代码，该如何操作？</span><br><span class="line">使用 :paste 和 ctrl+D 的方式</span><br><span class="line">:paste 表示代码块的开始</span><br><span class="line">ctrl+D 表示代码块的结束</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line">&#x2F;&#x2F; Entering paste mode (ctrl-D to finish)</span><br><span class="line">val age &#x3D; 20</span><br><span class="line">if(age &gt; 18)</span><br><span class="line">1</span><br><span class="line">else</span><br><span class="line">0</span><br><span class="line">&#x2F;&#x2F; Exiting paste mode, now interpreting.</span><br><span class="line">age: Int &#x3D; 20</span><br><span class="line">res18: Int &#x3D; 1</span><br></pre></td></tr></table></figure><h3 id="语句终结符"><a href="#语句终结符" class="headerlink" title="语句终结符"></a>语句终结符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Scala默认不需要语句终结符，它将每一行作为一个语句</span><br><span class="line">如果一行要放多条语句，则前面的语句必须使用语句终结符</span><br><span class="line">语句终结符和Java中的一样，就是我们平时使用的分号</span><br><span class="line"></span><br><span class="line">scala&gt; val age &#x3D; 20; if(age &gt; 18) 1 else 0</span><br><span class="line">age: Int &#x3D; 20</span><br><span class="line">res0: Int &#x3D; 1</span><br></pre></td></tr></table></figure><h3 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h3><h4 id="print和println"><a href="#print和println" class="headerlink" title="print和println"></a>print和println</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在讲循环之前，先来看一下打印命令print和println</span><br><span class="line">print打印时不会加换行符，而println打印时会加一个换行符，这个特性和Java中的打印语句的特性是一样的</span><br></pre></td></tr></table></figure><h4 id="for循环"><a href="#for循环" class="headerlink" title="for循环"></a>for循环</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for循环本身的特性就没什么好说的了，直接上案例，主要注意一下scala中的for和java中的for在语法层面的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line">&#x2F;&#x2F; Entering paste mode (ctrl-D to finish)</span><br><span class="line">val n &#x3D; 10</span><br><span class="line">for(i &lt;- 1 to n) &#x2F;&#x2F;1.to(n) 一样的</span><br><span class="line">println(i)</span><br><span class="line">&#x2F;&#x2F; Exiting paste mode, now interpreting.</span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5 </span><br><span class="line">6 </span><br><span class="line">7 </span><br><span class="line">8 </span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">n: Int &#x3D; 10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">这里面的to可以换成until</span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line">&#x2F;&#x2F; Entering paste mode (ctrl-D to finish)</span><br><span class="line">val n &#x3D; 10</span><br><span class="line">for(i &lt;- 1 until 10)</span><br><span class="line">println(i)</span><br><span class="line">&#x2F;&#x2F; Exiting paste mode, now interpreting.</span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5 </span><br><span class="line">6 </span><br><span class="line">7 </span><br><span class="line">8 </span><br><span class="line">9</span><br><span class="line">n: Int &#x3D; 10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">对比两次执行的结果发现</span><br><span class="line">1 to 10 可以获取1~10之间的所有数字</span><br><span class="line">1 until 10可以获取1~9之间的所有数字</span><br><span class="line"></span><br><span class="line">所以在这需要注意了，to 和 until 其实都是函数，一个是闭区间，一个是开区间</span><br><span class="line">具体用哪个就要看你的需求了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">for循环针对字符串还可以用</span><br><span class="line">scala&gt; for(c &lt;- &quot;hello scala&quot;) println(c)</span><br><span class="line">h </span><br><span class="line">e </span><br><span class="line">l </span><br><span class="line">l </span><br><span class="line">o </span><br><span class="line">s </span><br><span class="line">c </span><br><span class="line">a </span><br><span class="line">l</span><br><span class="line">a</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">注意：在这里我在for循环后面没有使用花括号，都省略了，主要是因为for循环的循环体代码就只有一行，如果有多行，就需要使用花括号了，否则，最终执行的结果就不是我们想要的</span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line">&#x2F;&#x2F; Entering paste mode (ctrl-D to finish)</span><br><span class="line">for(i &lt;- 1 to 5)</span><br><span class="line">println(i)</span><br><span class="line">println(&quot;hehe&quot;)</span><br><span class="line">&#x2F;&#x2F; Exiting paste mode, now interpreting.</span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5</span><br><span class="line">hehe</span><br></pre></td></tr></table></figure><h4 id="while循环"><a href="#while循环" class="headerlink" title="while循环"></a>while循环</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">while循环，它的用法和java中的while也是很像的，主要看一下语法层面的区别</span><br><span class="line"></span><br><span class="line">scala&gt; :paste</span><br><span class="line">&#x2F;&#x2F; Entering paste mode (ctrl-D to finish)</span><br><span class="line">var n &#x3D; 10</span><br><span class="line">while(n&gt;0)&#123;</span><br><span class="line">println(n)</span><br><span class="line">n -&#x3D; 1</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; Exiting paste mode, now interpreting.</span><br><span class="line">10</span><br><span class="line">9 </span><br><span class="line">8 </span><br><span class="line">7 </span><br><span class="line">6 </span><br><span class="line">5 </span><br><span class="line">4 </span><br><span class="line">3 </span><br><span class="line">2 </span><br><span class="line">1</span><br><span class="line">n: Int &#x3D; 0</span><br></pre></td></tr></table></figure><h4 id="高级for循环"><a href="#高级for循环" class="headerlink" title="高级for循环"></a>高级for循环</h4><h5 id="if守卫模式"><a href="#if守卫模式" class="headerlink" title="if守卫模式"></a>if守卫模式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">最后来看一下高级for循环的用法</span><br><span class="line">if守卫</span><br><span class="line">if守卫模式，假设我们想要获取1~10之间的所有偶数，使用普通的for循环，需要把每一个数字都循环出来，然后判断是否是偶数</span><br><span class="line">如果在for循环里面使用if守卫，可以在循环的时候就执行一定的逻辑，判断数值是否是偶数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; for(i &lt;- 1 to 10 if i % 2 &#x3D;&#x3D; 0) println(i)</span><br><span class="line">2 </span><br><span class="line">4 </span><br><span class="line">6 </span><br><span class="line">8</span><br><span class="line">10</span><br></pre></td></tr></table></figure><h5 id="for推导式"><a href="#for推导式" class="headerlink" title="for推导式"></a>for推导式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for推导式，一个典型例子是构造集合</span><br><span class="line"></span><br><span class="line">我们在使用for循环迭代数字的时候，可以使用yield指定一个规则，对迭代出来的数字进行处理，并且创建一个新的集合</span><br><span class="line"></span><br><span class="line">scala&gt; for(i &lt;- 1 to 10) yield i *2</span><br><span class="line">res16: scala.collection.immutable.IndexedSeq[Int] &#x3D; Vector(2, 4, 6, 8, 10, 12</span><br></pre></td></tr></table></figure><h3 id="Scala的集合体系"><a href="#Scala的集合体系" class="headerlink" title="Scala的集合体系"></a>Scala的集合体系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来整体学习一下Scala中的集合体系，集合在工作中属于经常使用的数据结构</span><br></pre></td></tr></table></figure><h4 id="集合体系"><a href="#集合体系" class="headerlink" title="集合体系"></a>集合体系</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">首先看一下整个集合体系结构，这个结构与Java的集合体系非常相似</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bKJxIg" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/28/bKJxIg.md.png" alt="bKJxIg.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">集合的顶层接口是Iterable，Iterable接口下面还有一些子接口， Set、Seq、Map</span><br><span class="line">这几个子接口下面有具体的实现类</span><br><span class="line">set下面有HashSet、LinkedHashSet、SortedSet等等</span><br><span class="line">seq下面有List、Buffer、Range等等</span><br><span class="line">Map下面有HashMap、SortedMap、LinkedHashMap等等</span><br><span class="line">其中Buffer下面还有两个常用的，ArrayBuffer、ListBuffer</span><br><span class="line">这是集合中一些常见的实现类</span><br><span class="line">在讲这个集合体系的时候，还会关联讲到 Array和Tuple 这两个数据结构</span><br></pre></td></tr></table></figure><h4 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Scala中的集合是分成可变和不可变两类集合的</span><br><span class="line">其中可变集合就是说，集合的元素可以动态修改</span><br><span class="line">而不可变集合就是说，集合的元素在初始化之后，就无法修改了</span><br><span class="line">可变集合：在 scala.collection.mutable 这个包下面</span><br><span class="line">不可变集合：在 scala.collection.immutable 这个包下面</span><br><span class="line">我们在创建集合的时候，如果不指定具体的包名，默认会使用不可变集合</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">先来看一下Set，Set代表一个没有重复元素的集合</span><br><span class="line">这个集合的特性和Java中Set集合的特性基本一样</span><br><span class="line">Set集合分为可变的和不可变的集合，默认情况下使用的是不可变集合</span><br><span class="line">Set可以直接使用，并且不需要使用new关键字，来看一下</span><br><span class="line"></span><br><span class="line">scala&gt; val s &#x3D; Set(1,2,3)</span><br><span class="line">s: scala.collection.immutable.Set[Int] &#x3D; Set(1, 2, 3)</span><br></pre></td></tr></table></figure><h5 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h5><h6 id="不可变set"><a href="#不可变set" class="headerlink" title="不可变set"></a>不可变set</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这是不是很奇怪，本来Set是一个接口，但是却可以创建对象，更神奇的是竟然还不需要使用new关键</span><br><span class="line">字，这就有点颠覆我们的认知了</span><br><span class="line">注意了，大家在学习Scala的时候，可以拿Java来进行对比，加深理解，但是不要全部拿Java里面的知识点来硬套，因为它们两个有些地方还是不一样的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">来看一下Scala的文档，你会发现这个Set不仅仅是一个接口，它还是一个Object，具体这个Object类型我</span><br><span class="line">们在后面会详细分析，在这大家先知道这个东西就行了。</span><br><span class="line"></span><br><span class="line">注意：本来是应该看对应版本2.12.11的文档的，但是2.12.11文档的格式看起来不是很清晰，所以在这我们就是要2.11.12这个版本了，主要的是没有什么变化的，不影响我们使用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在这大家可以这样理解，只要前面带有object的，可以直接创建对象，并且不需要使用new关键字</span><br><span class="line">所以set可以直接使用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注意：默认情况下直接创建的set集合是一个不可变集合，在这可以看到是在immutable包里面的，不可变集合中的元素一经初始化，就不能改变了，所以初始化后再向里面添加元素就报错了</span><br><span class="line"></span><br><span class="line">scala&gt; val s &#x3D; Set(1,2,3)</span><br><span class="line">s: scala.collection.immutable.Set[Int] &#x3D; Set(1, 2, 3)</span><br><span class="line">scala&gt; s +&#x3D; 4</span><br><span class="line">&lt;console&gt;:9: error: value +&#x3D; is not a member of scala.collection.immutable.Set</span><br><span class="line">s +&#x3D; 4</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">但是注意，我使用s + 4 这种操作是可以的</span><br><span class="line">scala&gt; val s &#x3D; Set(1,2,3)</span><br><span class="line">s: scala.collection.immutable.Set[Int] &#x3D; Set(1, 2, 3)</span><br><span class="line">scala&gt; s + 4</span><br><span class="line">res33: scala.collection.immutable.Set[Int] &#x3D; Set(1, 2, 3, 4)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这样是不是和我们刚才说的自相矛盾？</span><br><span class="line">不是的，因为 s + 4 返回的是一个新的集合了，相当于在之前的集合的基础上，创建一个新的集合，新的集合包含之前集合的元素和我们新增的4这个元素</span><br><span class="line">这个大家需要能够区分开</span><br></pre></td></tr></table></figure><h6 id="可变set"><a href="#可变set" class="headerlink" title="可变set"></a>可变set</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如果想要创建一个可变的set集合，可以使用mutable包下面的set集合，显式指定包名</span><br><span class="line"></span><br><span class="line">scala&gt; val s &#x3D; scala.collection.mutable.Set(1,2,3)</span><br><span class="line">s: scala.collection.mutable.Set[Int] &#x3D; Set(1, 2, 3)</span><br><span class="line">scala&gt; s +&#x3D; 4</span><br><span class="line">res34: s.type &#x3D; Set(1, 2, 3, 4)</span><br><span class="line"> 12345</span><br></pre></td></tr></table></figure><h6 id="子类HashSet"><a href="#子类HashSet" class="headerlink" title="子类HashSet"></a>子类HashSet</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HashSet：这个集合的特点是：集合中的元素不重复、无序</span><br><span class="line"></span><br><span class="line">HashSet集合分为可变和不可变之分， immutable 包下面的是不可变的，后期无法新增元素</span><br><span class="line">在这里可以使用new关键字，也可以不使用，因为HashSet既是class，又是object，但是包名需要指定，否则无法识别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val s &#x3D; new scala.collection.mutable.HashSet[Int]()</span><br><span class="line">s: scala.collection.mutable.HashSet[Int] &#x3D; Set()</span><br><span class="line">scala&gt; s +&#x3D;1</span><br><span class="line">res35: s.type &#x3D; Set(1)</span><br><span class="line">scala&gt; s +&#x3D;2</span><br><span class="line">res36: s.type &#x3D; Set(1, 2)</span><br><span class="line">scala&gt; s +&#x3D;5 &#x2F;&#x2F;＋&#x3D;是一个方法，s.+&#x3D;(5)一样</span><br><span class="line">res38: s.type &#x3D; Set(1, 5, 2)</span><br><span class="line"></span><br><span class="line">如果在创建集合的时候就初始化了元素，则可以省略泛型的定义，集合会自动识别元素的类型</span><br></pre></td></tr></table></figure><h6 id="子类LinkedHashSet"><a href="#子类LinkedHashSet" class="headerlink" title="子类LinkedHashSet"></a>子类LinkedHashSet</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LinkedHashSet：这个集合的特点是：集合中的元素不重复、有序，它会用一个链表维护插入顺序，可以保证集合中元素是有序的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">LinkedHashSet只有可变的，没有不可变的</span><br><span class="line"></span><br><span class="line">scala&gt; val s &#x3D; new scala.collection.mutable.LinkedHashSet[Int]()</span><br><span class="line">s: scala.collection.mutable.LinkedHashSet[Int] &#x3D; Set()</span><br><span class="line">scala&gt; s +&#x3D;1</span><br><span class="line">res42: s.type &#x3D; Set(1)</span><br><span class="line">scala&gt; s +&#x3D;2</span><br><span class="line">res43: s.type &#x3D; Set(1, 2)</span><br><span class="line">scala&gt; s +&#x3D;5</span><br><span class="line">res44: s.type &#x3D; Set(1, 2, 5)</span><br></pre></td></tr></table></figure><h6 id="子类SortedSet"><a href="#子类SortedSet" class="headerlink" title="子类SortedSet"></a>子类SortedSet</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SortedSet：这个集合的特点是：集合中的元素不重复、有序，它会自动根据元素来进行排序</span><br><span class="line"></span><br><span class="line">SortedSet分为可变集合和不可变集合</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bKDyTI" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/28/bKDyTI.md.png" alt="bKDyTI.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">下面的那两个SortedSet是上面collection.SortedSet接口的子接口，一般会使用下面那两个。</span><br><span class="line"></span><br><span class="line">scala&gt; val s &#x3D; scala.collection.mutable.SortedSet[String]()</span><br><span class="line">s: scala.collection.mutable.SortedSet[String] &#x3D; TreeSet()</span><br><span class="line">scala&gt; s +&#x3D;(&quot;c&quot;)</span><br><span class="line">res45: s.type &#x3D; TreeSet(c)</span><br><span class="line">scala&gt; s +&#x3D;(&quot;a&quot;)</span><br><span class="line">res46: s.type &#x3D; TreeSet(a, c)</span><br><span class="line">scala&gt; s +&#x3D;(&quot;b&quot;)</span><br><span class="line">res47: s.type &#x3D; TreeSet(a, b, c)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">从这可以看出来SortedSet集合中的元素是按照元素的字典顺序排序的</span><br><span class="line">针对里面这些Set集合，如果想要迭代他们里面的元素，可以使用for循环直接迭代</span><br><span class="line">以SortedSet为例，其它的 Set、HashSet、LinkedHashSet 都是一样的</span><br><span class="line"></span><br><span class="line">scala&gt; for(i &lt;- s ) println(i)</span><br><span class="line">a b c</span><br></pre></td></tr></table></figure><h5 id="List"><a href="#List" class="headerlink" title="List"></a>List</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下List，List属于Seq接口的子接口</span><br><span class="line">List代表一个不可变的列表</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">创建一个list</span><br><span class="line">scala&gt; val l &#x3D; List(1, 2, 3, 4)</span><br><span class="line">l: List[Int] &#x3D; List(1, 2, 3, 4)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：为什么有的地方需要写类的全路径，而有的不需要呢？</span><br><span class="line">由于immutable包是默认导入的，所以不需要导包，但是也会有个别虽然在immutable包下面的，但是不写全路径还是报错，原谅它把，反正你都带全路径肯定是没有问题的，后期我们会使用idea来开发，也不需要考虑包名的问题，不过在这为了演示起来更加清晰，就使用scala的命令行了</span><br></pre></td></tr></table></figure><h6 id="head和tail"><a href="#head和tail" class="headerlink" title="head和tail"></a>head和tail</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对List有 head 、 tail 以及 :: 这几个操作</span><br><span class="line">先演示一下 head、tail 操作</span><br><span class="line"></span><br><span class="line">scala&gt; l.head</span><br><span class="line">res49: Int &#x3D; 1</span><br><span class="line">scala&gt; l.tail</span><br><span class="line">res51: List[Int] &#x3D; List(2, 3, 4)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">head：表示获取List中的第一个元素</span><br><span class="line">tail：表示获取List中第一个元素之后的所有元素</span><br><span class="line">那其实head和tail就可以获取list中的所有元素了</span><br></pre></td></tr></table></figure><h6 id><a href="#" class="headerlink" title=": :"></a>: :</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">通过 :: 操作符，可以将head和tail的结果合并成一个List</span><br><span class="line"></span><br><span class="line">scala&gt; l.head :: l.tail</span><br><span class="line">res52: List[Int] &#x3D; List(1, 2, 3, 4)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">:: 这种操作符要清楚，在spark源码中是有体现的，一定要能够看懂</span><br><span class="line"></span><br><span class="line">针对List中的元素进行迭代和前面讲的Set集合的迭代是一样的</span><br></pre></td></tr></table></figure><h5 id="ListBuffer"><a href="#ListBuffer" class="headerlink" title="ListBuffer"></a>ListBuffer</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在这里List是不可变的列表，在实际工作中使用的时候会很不方便，因为我们很多场景下都是需要向列表中动态添加元素，这个时候该怎么办呢？</span><br><span class="line">Scala还提供的有一个ListBuffer</span><br><span class="line">ListBuffer：可以支持动态增加或者移除元素</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val lb &#x3D; scala.collection.mutable.ListBuffer[Int]()</span><br><span class="line">lb: scala.collection.mutable.ListBuffer[Int] &#x3D; ListBuffer()</span><br><span class="line">scala&gt; lb +&#x3D;1</span><br><span class="line">res56: lb.type &#x3D; ListBuffer(1)</span><br><span class="line">scala&gt; lb +&#x3D;2</span><br><span class="line">res57: lb.type &#x3D; ListBuffer(1, 2)</span><br><span class="line">scala&gt; lb +&#x3D;5</span><br><span class="line">res58: lb.type &#x3D; ListBuffer(1, 2, 5)</span><br><span class="line">scala&gt; lb -&#x3D;5</span><br><span class="line">res59: lb.type &#x3D; ListBuffer(1, 2)</span><br><span class="line"></span><br><span class="line">ListBuffer也可以直接使用for循环迭代</span><br></pre></td></tr></table></figure><h5 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Map是一种可迭代的键值对（key&#x2F;value）结构</span><br><span class="line">Map分为可变和不可变，默认情况下使用的是不可变Map</span><br></pre></td></tr></table></figure><h6 id="不可变Map"><a href="#不可变Map" class="headerlink" title="不可变Map"></a>不可变Map</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">创建一个不可变的Map</span><br><span class="line">创建一个可变的Map</span><br><span class="line">还有一种创建Map的简易方式，这种方式创建的是不可变Map</span><br><span class="line">查询操作</span><br><span class="line">获取指定key对应的value，如果key不存在，会报错</span><br><span class="line">所以在实际工作中这样直接获取不太好，如果遇到了不存在的key程序会报错，导致程序异常退出。</span><br><span class="line">那是不是可以考虑在获取key的值之前，先判断key是否存在</span><br><span class="line">可以使用contains函数检查key是否存在、</span><br><span class="line">使用if-else语句，如果指定的key不存在，则返回一个默认值</span><br><span class="line">这样是没问题的，就是写起来有点麻烦了，有没有方便一点的用法呢？</span><br><span class="line">map中还有一个getOrElse函数</span><br><span class="line">scala&gt; val ages &#x3D; Map(&quot;jack&quot;-&gt;30,&quot;tom&quot;-&gt;25,&quot;jessic&quot;-&gt;23)</span><br><span class="line">ages: scala.collection.immutable.Map[String,Int] &#x3D; Map(jack -&gt; 30, tom -&gt; 25,</span><br><span class="line">scala&gt; ages(&quot;jack&quot;)</span><br><span class="line">res100: Int &#x3D; 30</span><br></pre></td></tr></table></figure><h6 id="可变Map"><a href="#可变Map" class="headerlink" title="可变Map"></a>可变Map</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val ages &#x3D; scala.collection.mutable.Map(&quot;jack&quot;-&gt;30,&quot;tom&quot;-&gt;25,&quot;jessic&quot;-</span><br><span class="line">ages: scala.collection.mutable.Map[String,Int] &#x3D; Map(jessic -&gt; 23, jack -&gt; 30</span><br><span class="line">scala&gt; ages(&quot;jack&quot;)</span><br><span class="line">res101: Int &#x3D; 30</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">还有一种创建Map的简易方式，这种方式创建的是不可变Map</span><br><span class="line"></span><br><span class="line">scala&gt; val ages &#x3D; Map((&quot;jack&quot;,30),(&quot;tom&quot;,25),(&quot;jessic&quot;-&gt;23))</span><br><span class="line">ages: scala.collection.immutable.Map[String,Int] &#x3D; Map(jack -&gt; 30, tom -&gt; 25,</span><br></pre></td></tr></table></figure><h6 id="查询操作"><a href="#查询操作" class="headerlink" title="查询操作"></a>查询操作</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.获取指定key对应的value，如果key不存在，会报错</span><br><span class="line"></span><br><span class="line">scala&gt; val ages &#x3D; scala.collection.mutable.Map((&quot;jack&quot;,30),(&quot;tom&quot;,25),(&quot;jessi</span><br><span class="line">ages: scala.collection.mutable.Map[String,Int] &#x3D; Map(jessic -&gt; 23, jack -&gt; 30</span><br><span class="line">scala&gt; val age &#x3D; ages(&quot;jack&quot;)</span><br><span class="line">age: Int &#x3D; 30</span><br><span class="line">scala&gt; val age &#x3D; ages(&quot;jack1&quot;)</span><br><span class="line">java.util.NoSuchElementException: key not found: jack</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">所以在实际工作中这样直接获取不太好，如果遇到了不存在的key程序会报错，导致程序异常退出。</span><br><span class="line">那是不是可以考虑在获取key的值之前，先判断key是否存在</span><br><span class="line">可以使用contains函数检查key是否存在、</span><br><span class="line">使用if-else语句，如果指定的key不存在，则返回一个默认值</span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line">scala&gt; val age &#x3D; if (ages.contains(&quot;jack1&quot;)) ages(&quot;jack1&quot;) else 0</span><br><span class="line">age: Int &#x3D; 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">这样是没问题的，就是写起来有点麻烦了，有没有方便一点的用法呢？</span><br><span class="line">map中还有一个getOrElse函数</span><br><span class="line">3.</span><br><span class="line">scala&gt; val age &#x3D; ages.getOrElse(&quot;jack1&quot;, 0)</span><br><span class="line">age: Int &#x3D; 0</span><br><span class="line"></span><br><span class="line">建议后期从map中获取数据都使用这个 getOrElse 函数</span><br></pre></td></tr></table></figure><h6 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.更新map中的元素(首先确保创建时使用的是mutable)</span><br><span class="line">scala&gt; ages(&quot;jack&quot;) &#x3D; 31</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2.增加多个元素</span><br><span class="line">scala&gt; ages +&#x3D; (&quot;hehe&quot; -&gt; 35, &quot;haha&quot; -&gt; 40)</span><br><span class="line">res105: ages.type &#x3D; Map(hehe -&gt; 35, jessic -&gt; 23, jack -&gt; 31, tom -&gt; 25, haha</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3.移除元素</span><br><span class="line">scala&gt; ages -&#x3D; &quot;hehe&quot;</span><br><span class="line">res106: ages.type &#x3D; Map(jessic -&gt; 23, jack -&gt; 31, tom -&gt; 25, haha -&gt; 40)</span><br></pre></td></tr></table></figure><h6 id="遍历"><a href="#遍历" class="headerlink" title="遍历"></a>遍历</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.遍历map的entrySet</span><br><span class="line">scala&gt; for ((key, value) &lt;- ages) println(key + &quot; &quot; + value)</span><br><span class="line">jessic 23</span><br><span class="line">jack 31</span><br><span class="line">tom 25</span><br><span class="line">haha 40</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2.遍历map的key</span><br><span class="line">scala&gt; for (key &lt;- ages.keySet) println(key)</span><br><span class="line">jessic</span><br><span class="line">jack</span><br><span class="line">tom</span><br><span class="line">haha</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3.遍历map的value</span><br><span class="line">scala&gt; for (value &lt;- ages.values) println(value)</span><br><span class="line">23</span><br><span class="line">31</span><br><span class="line">25</span><br><span class="line">40</span><br></pre></td></tr></table></figure><h6 id="子类HashMap"><a href="#子类HashMap" class="headerlink" title="子类HashMap"></a>子类HashMap</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HashMap：是一个按照key的hash值进行排列存储的map</span><br></pre></td></tr></table></figure><h6 id="子类SortedMap"><a href="#子类SortedMap" class="headerlink" title="子类SortedMap"></a>子类SortedMap</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SortedMap：可以自动对Map中的key进行排序【有序的map】</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HashMap分为可变和不可变的，没有什么特殊之处</span><br><span class="line">在这主要演示一下SortedMap和LinkedHashMap</span><br><span class="line">SortedMap是不可变的</span><br><span class="line"></span><br><span class="line">scala&gt; val ages &#x3D; scala.collection.immutable.SortedMap(&quot;b&quot; -&gt; 30, &quot;a&quot; -&gt; 15,</span><br><span class="line">ages: scala.collection.immutable.SortedMap[String,Int] &#x3D; Map(a -&gt; 15, b -&gt; 30</span><br></pre></td></tr></table></figure><h6 id="子类LinkedHashMap"><a href="#子类LinkedHashMap" class="headerlink" title="子类LinkedHashMap"></a>子类LinkedHashMap</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LinkedHashMap：可以记住插入的key-value的顺序</span><br><span class="line"></span><br><span class="line">LinkedHashMap是可变的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val ages &#x3D; new scala.collection.mutable.LinkedHashMap[String, Int]()</span><br><span class="line">ages: scala.collection.mutable.LinkedHashMap[String,Int] &#x3D; Map()</span><br><span class="line">scala&gt; ages(&quot;b&quot;)&#x3D;30</span><br><span class="line">scala&gt; ages(&quot;a&quot;)&#x3D;15</span><br><span class="line">scala&gt; ages(&quot;c&quot;)&#x3D;25</span><br><span class="line">scala&gt; ages</span><br><span class="line">res116: scala.collection.mutable.LinkedHashMap[String,Int] &#x3D; Map(b -&gt; 30, a</span><br></pre></td></tr></table></figure><h5 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Scala中Array的含义与Java中的数组类似，长度不可变</span><br><span class="line">由于Scala和Java都是运行在JVM中，双方可以互相调用，因此Scala数组的底层实际上就是Java数组</span><br><span class="line">数组初始化后，长度就固定下来了，而且元素全部根据其类型进行初始化</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a &#x3D; new Array[Int](10)</span><br><span class="line">a: Array[Int] &#x3D; Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)</span><br><span class="line">scala&gt; a(0)</span><br><span class="line">res65: Int &#x3D; 0</span><br><span class="line">scala&gt; a(0)&#x3D;1</span><br><span class="line">scala&gt; a(0)</span><br><span class="line">res67: Int &#x3D; 1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">也可以直接使用Array()创建数组，元素类型自动推断</span><br><span class="line">scala&gt; val a &#x3D; Array(&quot;hello&quot;, &quot;world&quot;)</span><br><span class="line">a: Array[String] &#x3D; Array(hello, world)</span><br><span class="line"></span><br><span class="line">scala&gt; a(0)</span><br><span class="line">res68: String &#x3D; hello</span><br><span class="line">scala&gt; val a1 &#x3D; Array(&quot;hello&quot;, 30)</span><br><span class="line">a1: Array[Any] &#x3D; Array(hello, 30)</span><br></pre></td></tr></table></figure><h5 id="ArrayBuffer"><a href="#ArrayBuffer" class="headerlink" title="ArrayBuffer"></a>ArrayBuffer</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Scala中ArrayBuffer与Java中的ArrayList类似，长度可变</span><br><span class="line">ArrayBuffer：添加元素、移除元素</span><br><span class="line">如果不想每次都使用全限定名，则可以预先导入ArrayBuffer类</span><br><span class="line"></span><br><span class="line">scala&gt; import scala.collection.mutable.ArrayBuffer</span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">使用ArrayBuffer()的方式可以创建一个空的ArrayBuffer</span><br><span class="line">注意：也支持直接创建并且初始化ArrayBuffer(1,2,3,4)</span><br><span class="line"></span><br><span class="line">scala&gt; val b &#x3D; new ArrayBuffer[Int]()</span><br></pre></td></tr></table></figure><h6 id="添加元素"><a href="#添加元素" class="headerlink" title="添加元素"></a>添加元素</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.使用+&#x3D;操作符，可以添加一个元素，或者多个元素</span><br><span class="line">b +&#x3D; 1 或者 b +&#x3D; (2, 3, 4, 5)</span><br><span class="line"></span><br><span class="line">2.使用insert()函数可以在指定位置插入元素，但是这种操作效率很低，因为需要移动指定位置后的所有元素 向3号角标的位置添加一个元素 30</span><br><span class="line">scala&gt; b.insert(3,30)</span><br></pre></td></tr></table></figure><h6 id="移除元素"><a href="#移除元素" class="headerlink" title="移除元素"></a>移除元素</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">使用 remove() 函数可以移除指定位置的元素</span><br><span class="line">移除1号角标的元素</span><br><span class="line">scala&gt; b.remove(1)</span><br><span class="line">res73: Int &#x3D; 2</span><br></pre></td></tr></table></figure><h6 id="Array和ArrayBuffer转化"><a href="#Array和ArrayBuffer转化" class="headerlink" title="Array和ArrayBuffer转化"></a>Array和ArrayBuffer转化</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b.toArray：ArrayBuffer转Array</span><br><span class="line">a.toBuffer：Array转ArrayBuffer</span><br></pre></td></tr></table></figure><h5 id="数组常见操作"><a href="#数组常见操作" class="headerlink" title="数组常见操作"></a>数组常见操作</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面看一下针对数据的常见操作</span><br><span class="line">遍历Array和ArrayBuffer的两种方式</span><br><span class="line">由于Array和ArrayBuffer都是有角标的，所以在迭代数组中元素的时候除了可以使用前面迭代集合的方式还可以使用角标迭代</span><br></pre></td></tr></table></figure><h6 id="遍历-1"><a href="#遍历-1" class="headerlink" title="遍历"></a>遍历</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.scala&gt; for(i &lt;- b) println(i)</span><br><span class="line">1 2 3 4 5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2.scala&gt; for(i &lt;- 0 until b.length ) println(b(i))</span><br><span class="line">1 2 3 4 5</span><br></pre></td></tr></table></figure><h6 id="求和，最大值"><a href="#求和，最大值" class="headerlink" title="求和，最大值"></a>求和，最大值</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val sum &#x3D; a.sum</span><br><span class="line">sum: Int &#x3D; 15</span><br><span class="line">scala&gt; val max &#x3D; a.max</span><br><span class="line">max: Int &#x3D; 5</span><br></pre></td></tr></table></figure><h6 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; scala.util.Sorting.quickSort(a)</span><br></pre></td></tr></table></figure><h5 id="Tuple"><a href="#Tuple" class="headerlink" title="Tuple"></a>Tuple</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tuple：称之为元组，它与Array类似，都是不可变的，但与数组不同的是元组可以包含不同类型的元素</span><br><span class="line">Tuple中的元素角标从 1 开始</span><br><span class="line"></span><br><span class="line">注意：目前 Scala 支持的元组最大长度为 22 ，对于更大长度可以使用集合或数组</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val t &#x3D; (1, 3.14, &quot;hehe&quot;)</span><br><span class="line">t: (Int, Double, String) &#x3D; (1,3.14,hehe)</span><br><span class="line">scala&gt; t._1</span><br><span class="line">res117: Int &#x3D; 1</span><br><span class="line">scala&gt; t._3</span><br><span class="line">res118: String &#x3D; hehe</span><br></pre></td></tr></table></figure><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">前面讲了很多集合体系中的数据结构，有的是可变的，有的是不可变的，有的是既是可变的又是不可变的，听起来有点乱，在这里我们总结一下</span><br><span class="line">可变集合： LinkedHashSet、ListBuffer、ArrayBuffer、LinkedHashMap</span><br><span class="line">不可变集合： List、SortedMap</span><br><span class="line">可变+不可变集合： Set、HashSet、SortedSet、Map、HashMap</span><br><span class="line">还有两个编外人员：</span><br><span class="line">Array、Tuple</span><br><span class="line">Array：长度不可变，里面的元素可变</span><br><span class="line">Tuple：长度不可变，里面的元素也不可变</span><br></pre></td></tr></table></figure><h3 id="Scala中函数的使用"><a href="#Scala中函数的使用" class="headerlink" title="Scala中函数的使用"></a>Scala中函数的使用</h3><h4 id="函数的定义"><a href="#函数的定义" class="headerlink" title="函数的定义"></a>函数的定义</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">先来看一下函数的定义</span><br><span class="line">在Scala中定义函数需要使用 def 关键字，函数包括函数名、参数、函数体</span><br><span class="line">Scala要求必须给出函数所有参数的类型，但是函数返回值的类型不是必须的，因为Scala可以自己根据函数体中的表达式推断出返回值类型。</span><br><span class="line">函数中最后一行代码的返回值就是整个函数的返回值，不需要使用return，这一点与Java不同，java中函数的返回值是必须要使用return的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">单行函数</span><br><span class="line">scala&gt; def sayHello(name: String) &#x3D; print(&quot;Hello, &quot; + name)</span><br><span class="line">sayHello: (name: String)Unit</span><br><span class="line">scala&gt; sayHello(&quot;Scala&quot;)</span><br><span class="line">Hello, Scala</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">多行函数</span><br><span class="line">scala&gt; :paste</span><br><span class="line">&#x2F;&#x2F; Entering paste mode (ctrl-D to finish)</span><br><span class="line">def sayHello(name: String, age: Int) &#x3D; &#123;</span><br><span class="line">println(&quot;My name is &quot;+name+&quot;,age is &quot;+age)</span><br><span class="line">age</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; Exiting paste mode, now interpreting.</span><br><span class="line">sayHello: (name: String, age: Int)Int</span><br><span class="line">scala&gt; sayHello(&quot;Scala&quot;,18)</span><br><span class="line">My name is Scala,age is 18</span><br><span class="line">res120: Int &#x3D; 18</span><br></pre></td></tr></table></figure><h4 id="函数的参数"><a href="#函数的参数" class="headerlink" title="函数的参数"></a>函数的参数</h4><h5 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a>默认参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在Scala中，有时候我们调用某些函数时，不希望给出参数的具体值，而是希望使用参数自身默认的值，此时就需要在定义函数时使用默认参数。</span><br><span class="line">如果给出的参数不够，则会从左往右依次应用参数。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def sayHello(fName: String, mName: String &#x3D; &quot;mid&quot;, lName: String &#x3D; &quot;la</span><br><span class="line">sayHello: (fName: String, mName: String, lName: String)String</span><br><span class="line">scala&gt; sayHello(&quot;zhang&quot;,&quot;san&quot;)</span><br><span class="line">res122: String &#x3D; zhang san last</span><br></pre></td></tr></table></figure><h5 id="带名参数"><a href="#带名参数" class="headerlink" title="带名参数"></a>带名参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在调用函数时，也可以不按照函数定义的参数顺序来传递参数，而是使用带名参数的方式来传递。</span><br><span class="line">scala&gt; def sayHello(fName: String, mName: String &#x3D; &quot;mid&quot;, lName: String &#x3D; &quot;la</span><br><span class="line">sayHello: (fName: String, mName: String, lName: String)String</span><br><span class="line">scala&gt; sayHello(fName &#x3D; &quot;Mick&quot;, lName &#x3D; &quot;Tom&quot;, mName &#x3D; &quot;Jack&quot;)</span><br><span class="line">res127: String &#x3D; Mick Jack Tom</span><br></pre></td></tr></table></figure><h5 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Scala中，有时我们需要将函数定义为参数个数可变的形式，则此时可以使用变长参数来定义函数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; :paste</span><br><span class="line">&#x2F;&#x2F; Entering paste mode (ctrl-D to finish)</span><br><span class="line">def sum(nums: Int*) &#x3D; &#123;  &#x2F;&#x2F;Int的I要大写</span><br><span class="line">var res &#x3D; 0</span><br><span class="line">for (num &lt;- nums) res +&#x3D; num</span><br><span class="line">res</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; Exiting paste mode, now interpreting.</span><br><span class="line">sum: (nums: Int*)Int</span><br><span class="line">scala&gt; sum(1,2,3,4,5)</span><br><span class="line">res129: Int &#x3D; 15</span><br></pre></td></tr></table></figure><h4 id="特殊的函数-过程"><a href="#特殊的函数-过程" class="headerlink" title="特殊的函数-过程"></a>特殊的函数-过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在Scala中，定义函数时，如果函数体直接在花括号里面而没有使用&#x3D;连接，则函数的返回值类型就是Unit，这样的函数称之为过程</span><br><span class="line">过程通常用于不需要返回值的函数</span><br><span class="line">过程还有一种写法，就是将函数的返回值类型显式定义为Unit</span><br><span class="line">比较一下这四种写法的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">有返回值</span><br><span class="line">def sayHello(name: String) &#x3D; &quot;Hello, &quot; + name</span><br><span class="line">def sayHello(name: String): String &#x3D; &quot;Hello, &quot; + name</span><br><span class="line"></span><br><span class="line">无返回值</span><br><span class="line">def sayHello(name: String) &#123; &quot;Hello, &quot; + name &#125;</span><br><span class="line">def sayHello(name: String): Unit &#x3D; &quot;Hello, &quot; + name</span><br><span class="line"></span><br><span class="line">前面两种写法的效果是一样的，都是函数</span><br><span class="line">后面两种写法的效果是一样的，都是过程</span><br></pre></td></tr></table></figure><h4 id="lazy"><a href="#lazy" class="headerlink" title="lazy"></a>lazy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Scala提供了lazy特性，如果将一个变量声明为lazy，则只有在第一次使用该变量时，变量对应的表达式才会发生计算</span><br><span class="line">什么场景下需要使用lazy特性呢？</span><br><span class="line">这种特性对于特别耗时的操作特别有用，比如打开文件这个操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import scala.io.Source._</span><br><span class="line">import scala.io.Source._</span><br><span class="line">scala&gt; lazy val lines &#x3D; fromFile(&quot;D:&#x2F;&#x2F;test.txt&quot;).mkString</span><br><span class="line">lines: String &#x3D; &lt;lazy&gt;</span><br><span class="line"></span><br><span class="line">即使D:&#x2F;&#x2F;test.txt文件不存在，代码也不会报错，只有变量使用时才会报错，这就是lazy这个特性</span><br><span class="line">scala&gt; lines</span><br><span class="line">java.io.FileNotFoundException: D:\test.txt (系统找不到指定的文件。)</span><br><span class="line">.......</span><br></pre></td></tr></table></figure><h4 id="用法积累"><a href="#用法积累" class="headerlink" title="用法积累"></a>用法积累</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.字符串不能用单引号代替双引号</span><br><span class="line">2.过程不用等号那种形式时，语句只能放在花括号里</span><br><span class="line">3.数据类型Int的I只能大写</span><br><span class="line">4.只能lazy val xxx；不能lazy var</span><br><span class="line">5</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Scala" scheme="http://tianyong.fun/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第九周 第1章 Scala极速入门</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC1%E7%AB%A0-Scala%E6%9E%81%E9%80%9F%E5%85%A5%E9%97%A8.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B9%9D%E5%91%A8-%E7%AC%AC1%E7%AB%A0-Scala%E6%9E%81%E9%80%9F%E5%85%A5%E9%97%A8.html</id>
    <published>2022-02-28T06:16:58.000Z</published>
    <updated>2022-02-28T06:35:48.168Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第九周-第1章-Scala极速入门"><a href="#第九周-第1章-Scala极速入门" class="headerlink" title="第九周 第1章 Scala极速入门"></a>第九周 第1章 Scala极速入门</h1><h2 id="为什么要学习Scala语言"><a href="#为什么要学习Scala语言" class="headerlink" title="为什么要学习Scala语言"></a>为什么要学习Scala语言</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先我们来分析一下为什么要学习Scala语言</span><br><span class="line">最直接的一点就是因为我们后面要学的Spark框架需要用到Scala这门语言</span><br><span class="line">但是Spark其实是同时支持Scala语言和Java语言的，为什么非要学Scala呢，使用java它难道不香吗？</span><br><span class="line">这就要说第二点了：看下面的代码，使用Spark统计文件内单词出现的次数这个需求，使用java代码和scala代码的区别是有多么的明显，在代码量上来说，scala是完胜java的，所以在实际工作中开发spark代码，我们都是需要使用scala的，使用java实现函数式编程太别扭了，代码量太大，这个就是我们学习</span><br><span class="line">scala的最直接的原因</span><br></pre></td></tr></table></figure><h2 id="什么是Scala"><a href="#什么是Scala" class="headerlink" title="什么是Scala"></a>什么是Scala</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Scala是一门多范式的编程语言，它是一种类似Java的编程语言，它设计的初衷是为了实现可伸缩的语言、并集成面向对象编程和函数式编程的各种特性</span><br><span class="line">Scala基于Java虚拟机，也就是基于JVM的一门编程语言。所有Scala代码，都需要编译为字节码，然后交由Java虚拟机来运行</span><br><span class="line">Scala和Java可以无缝相互操作，Scala可以任意调用Java代码，这个特性是非常好的</span><br></pre></td></tr></table></figure><h2 id="如何快速掌握Scala语言"><a href="#如何快速掌握Scala语言" class="headerlink" title="如何快速掌握Scala语言"></a>如何快速掌握Scala语言</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在这先给大家打个预防针，虽然前面我们说了使用scala实现spark代码会很简洁，但是scala语言本身是</span><br><span class="line">很反人类的，特别是你熟悉了例如java之类的编程语言之后再来学scala，会感觉到既爱又恨</span><br><span class="line">那我们如何快速掌握Scala语言的使用呢？</span><br><span class="line">首先大家在学习的过程中需要对比分析Java和Scala在使用上的区别，这样可以加深我们的理解</span><br><span class="line">然后没事的时候可以尝试着用Scala代码改写你之前的Java代码</span><br><span class="line">最后的最后，交给大家一个绝招，一个终极方案，那就是多练！多练！多练。</span><br><span class="line">因为针对编程语言，其实就是需要掌握它的一些基本语法，大家都是有其它语言编程基础的，在这学习Scala语言，其实主要就是掌握这门语言和其它语言的一些不同之处，只要把这些东西捋顺了，那上手也是很容易的。</span><br></pre></td></tr></table></figure><h2 id="Scala环境安装配置"><a href="#Scala环境安装配置" class="headerlink" title="Scala环境安装配置"></a>Scala环境安装配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面我们就来具体学习使用一下Scala这门神奇的语言。</span><br><span class="line">在具体使用之前需要先安装scala的开发环境，就类似安装java环境一样</span><br><span class="line"></span><br><span class="line">注意：由于Scala是基于Java虚拟机的，所以使用 Scala 之前必须先安装 Java，Java我们已经安装过了。</span><br><span class="line">那在这里我们先到官网下载Scala安装包</span><br><span class="line">Scala现在有三个主要在使用的版本， 2.11，2.12，2.13</span><br><span class="line">目前的话2.12使用的比较多，所以我们就使用这个版本</span><br></pre></td></tr></table></figure><h2 id="Scala命令行"><a href="#Scala命令行" class="headerlink" title="Scala命令行"></a>Scala命令行</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">咱们刚才进入的就是Scala命令行</span><br><span class="line">Scala命令行也称为Scala解释器(REPL)，它会快速编译Scala代码为字节码，然后交给JVM来执行</span><br><span class="line">这里的REPL表示：Read（取值）-&gt; Evaluation（求值）-&gt; Print（打印）-&gt; Loop（循环）</span><br><span class="line">在Scala命令行内，输入Scala代码，解释器会直接返回结果</span><br><span class="line">如果你没有指定变量来存放计算的值，那么值默认的名称会显示为res开头的变量，而且会显示结果的数据类型</span><br><span class="line">scala&gt; 1+1</span><br><span class="line">res0: Int &#x3D; 2</span><br><span class="line">在后面可以继续使用res0这个变量，以及它里面存放的值</span><br><span class="line">scala&gt; 5 * res0</span><br><span class="line">res1: Int &#x3D; 10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala的命令行也有自动补全功能，使用起来还是比较方便的</span><br><span class="line">输入res，按键盘上的tab键，下面就会列出目前以res开头的变量名称</span><br><span class="line">scala&gt; res</span><br><span class="line">res0 res1</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Scala" scheme="http://tianyong.fun/tags/Scala/"/>
    
  </entry>
  
  <entry>
    <title>maven相关</title>
    <link href="http://tianyong.fun/maven%E7%9B%B8%E5%85%B3.html"/>
    <id>http://tianyong.fun/maven%E7%9B%B8%E5%85%B3.html</id>
    <published>2022-02-21T15:35:12.000Z</published>
    <updated>2022-02-21T15:57:06.154Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="maven相关"><a href="#maven相关" class="headerlink" title="maven相关"></a>maven相关</h1><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.打jar包</span><br><span class="line">在项目根目录下</span><br><span class="line">mvn clean package -DskipTests</span><br><span class="line"></span><br><span class="line">2.为项目下载pom上配置的依赖</span><br><span class="line">在项目根目录下(用cmd或IDEA)</span><br><span class="line">mvn clean compile</span><br><span class="line"></span><br><span class="line">下载后，在项目上右键-&gt;maven-&gt;reload project</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="maven" scheme="http://tianyong.fun/categories/maven/"/>
    
    
  </entry>
  
  <entry>
    <title>mysql安装</title>
    <link href="http://tianyong.fun/mysql%E5%AE%89%E8%A3%85.html"/>
    <id>http://tianyong.fun/mysql%E5%AE%89%E8%A3%85.html</id>
    <published>2022-02-20T10:02:24.000Z</published>
    <updated>2022-02-20T14:59:21.445Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="mysql安装"><a href="#mysql安装" class="headerlink" title="mysql安装"></a>mysql安装</h1><p><a href="https://www.cnblogs.com/ivy-zheng/p/11088644.html" target="_blank" rel="external nofollow noopener noreferrer">url1</a></p><p><a href="https://blog.csdn.net/t15263857960/article/details/83590484" target="_blank" rel="external nofollow noopener noreferrer">url2</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.安装包是目免安装型的</span><br><span class="line">2.在系统环境变量里添加mysql的路径到xxx\bin(也可以用管理员权限打开的cmd，切到这个路径下，再执行命令)</span><br><span class="line">3.用管理员权限打开cmd，输入</span><br><span class="line">mysqld --initialize --console  (注意一定要看之前是否安装过mysql,在环境变量里，一眼就可以看出，不然总是报各种错误; 加上console可以看到初始化后为root创建的临时密码)</span><br><span class="line">4.mysqld -install 将mysql服务安装到win服务</span><br><span class="line">5.net start mysql (启动服务)</span><br><span class="line">6.mysql -uroot -p 再输入之前生成的临时密码</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 如果安装时已有mysql服务，删除：sc delete mysql</span><br></pre></td></tr></table></figure><p><a href="https://www.jianshu.com/p/b70a2cb5d4be" target="_blank" rel="external nofollow noopener noreferrer">url</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改临时密码</span><br><span class="line">ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39;; &#x2F;&#x2F;记得修改自己的账户</span><br><span class="line">flush privileges; &#x2F;&#x2F;修改成功后刷新权限</span><br><span class="line">quit; &#x2F;&#x2F;最后退出</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第6章 Hive技巧与核心复盘</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC6%E7%AB%A0-Hive%E6%8A%80%E5%B7%A7%E4%B8%8E%E6%A0%B8%E5%BF%83%E5%A4%8D%E7%9B%98.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC6%E7%AB%A0-Hive%E6%8A%80%E5%B7%A7%E4%B8%8E%E6%A0%B8%E5%BF%83%E5%A4%8D%E7%9B%98.html</id>
    <published>2022-02-20T04:02:41.000Z</published>
    <updated>2022-02-21T08:37:46.068Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第5章 Hive高级函数实战</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Hive%E9%AB%98%E7%BA%A7%E5%87%BD%E6%95%B0%E5%AE%9E%E6%88%98%202.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Hive%E9%AB%98%E7%BA%A7%E5%87%BD%E6%95%B0%E5%AE%9E%E6%88%98%202.html</id>
    <published>2022-02-20T04:02:11.000Z</published>
    <updated>2022-02-28T03:27:57.755Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第5章-Hive高级函数实战"><a href="#第八周-第5章-Hive高级函数实战" class="headerlink" title="第八周 第5章 Hive高级函数实战"></a>第八周 第5章 Hive高级函数实战</h1><h2 id="一个SQL语句分析"><a href="#一个SQL语句分析" class="headerlink" title="一个SQL语句分析"></a>一个SQL语句分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.Key</span><br><span class="line">, SUM(a.Cnt) AS Cnt</span><br><span class="line">FROM (</span><br><span class="line">SELECT Key, COUNT(*) AS Cnt</span><br><span class="line">FROM TableName</span><br><span class="line">GROUP BY Key,</span><br><span class="line">CASE</span><br><span class="line">WHEN Key &#x3D; &#39;KEY001&#39; THEN Hash(Random()) % 50</span><br><span class="line">ELSE 0</span><br><span class="line">END</span><br><span class="line">) a</span><br><span class="line">GROUP BY a.Key;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">解释：这个SQL其实是一个解决数据倾斜的SQL</span><br><span class="line">先看里面的select语句，里面的select语句其实是根据key进行分组，但是这个key对应的数据存在数据倾</span><br><span class="line">斜，key&#x3D;KEY001的数据占了整份数据的90%，所以直接针对key进行分组肯定会出现数据倾斜，应该计</span><br><span class="line">算效率，所以在这里就实现了曲线救国，先把key&#x3D;KEY001的数据打散，分成50份，进行局部聚合</span><br><span class="line">最后再通过外面的select进行全局的聚合，这样就可以显著提高计算效率</span><br></pre></td></tr></table></figure><h2 id="hue"><a href="#hue" class="headerlink" title="hue"></a>hue</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CHD的hue界面的使用</span><br></pre></td></tr></table></figure><h2 id="【扩展内容】Hive数据倾斜的解决方案"><a href="#【扩展内容】Hive数据倾斜的解决方案" class="headerlink" title="【扩展内容】Hive数据倾斜的解决方案"></a>【扩展内容】Hive数据倾斜的解决方案</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">可能会触发Hive数据倾斜的几种情况</span><br><span class="line"></span><br><span class="line">关键字 情形 后果</span><br><span class="line">join 其中一个表较小，但是key集中 分发到某一个或几个Reduce</span><br><span class="line">大表与小表，但是分桶的判断字段0值或空值过多 这些空值都由一个reduce处</span><br><span class="line">group by group by维度过小，处理的数量过多 处理某值的reduce非常</span><br><span class="line">count distinct 某特殊值过多 处理此特殊值的reduc</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">原因：</span><br><span class="line">1)、key分布不均匀</span><br><span class="line">2)、业务数据本身的特性</span><br><span class="line">3)、建表时考虑不周</span><br><span class="line">4)、某些SQL语句本身就有数据倾斜</span><br><span class="line">表现：</span><br><span class="line">任务进度长时间维持在99%（或100</span><br><span class="line">务未完成。因为其处理的数据量和其</span><br><span class="line">单一reduce的记录数与平均记录数差</span><br><span class="line">数据倾斜的解决方案</span><br><span class="line">参数调节：</span><br><span class="line">hive.map.aggr&#x3D;true</span><br><span class="line">Map 端部分聚合，相当于Combiner</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive.groupby.skewindata&#x3D;true</span><br><span class="line">有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR</span><br><span class="line">Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这</span><br><span class="line">样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第</span><br><span class="line">二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的</span><br><span class="line">Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。</span><br><span class="line">SQL语句调节：</span><br><span class="line">大小表Join：</span><br><span class="line">使用map join让小的维度表（1000条以下的记录条数） 先进内存。在map端完成reduce.</span><br><span class="line">大表Join大表：</span><br><span class="line">把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。</span><br><span class="line">count distinct大量相同特殊值</span><br><span class="line">count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。</span><br><span class="line">针对去重求和的需求还可以这样做：</span><br><span class="line">采用sum() group by的方式来替换count(distinct)完成计算。</span><br><span class="line">特殊情况特殊处理：</span><br><span class="line">在业务逻辑优化效果的不大情况下，有些时候是可以将倾斜的数据单独拿出来处理。最后union回去。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第5章 Hive高级函数实战</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Hive%E9%AB%98%E7%BA%A7%E5%87%BD%E6%95%B0%E5%AE%9E%E6%88%98%201.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Hive%E9%AB%98%E7%BA%A7%E5%87%BD%E6%95%B0%E5%AE%9E%E6%88%98%201.html</id>
    <published>2022-02-20T04:02:11.000Z</published>
    <updated>2022-02-28T03:09:50.010Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第5章-Hive高级函数实战"><a href="#第八周-第5章-Hive高级函数实战" class="headerlink" title="第八周 第5章 Hive高级函数实战"></a>第八周 第5章 Hive高级函数实战</h1><h2 id="函数的基本操作"><a href="#函数的基本操作" class="headerlink" title="函数的基本操作"></a>函数的基本操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">和mysql一样的，hive也是一个主要做统计的工具，所以为了满足各种各样的统计需要，他也内置了相当</span><br><span class="line">多的函数，我们可以通过 show functions; 来查看hive中的内置函数</span><br><span class="line">hive (default)&gt; show functions;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查看指定函数的描述信息我们可以使用： desc function functionName;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; desc function year;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">显示函数的扩展内容</span><br><span class="line">hive (default)&gt; desc function extended year;</span><br></pre></td></tr></table></figure><h2 id="Hive高级函数应用"><a href="#Hive高级函数应用" class="headerlink" title="Hive高级函数应用"></a>Hive高级函数应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">普通的就不说了，mysql中支持的函数这里面大部分都支持，并且hive支持的函数比mysql还要多，在这里我们主要挑几个典型的说一下</span><br></pre></td></tr></table></figure><h3 id="分组排序取TopN"><a href="#分组排序取TopN" class="headerlink" title="分组排序取TopN"></a>分组排序取TopN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">一个典型的应用场景，分组排序取TopN操作</span><br><span class="line">主要需要使用到ROW_NUMBER() 和 OVER()函数</span><br><span class="line">row_number和over函数通常搭配在一起使用</span><br><span class="line">row_number会对数据编号，编号从1开始</span><br><span class="line">over可以理解为把数据划分到一个窗口内，里面可以加上partition by，表示按照字段对数据进行分组，</span><br><span class="line">还可以加上order by 表示对每个分组内的数据按照某个字段进行排序</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">我们的需求是这样，有一份学生的考试分数信息，语文、数学、英语这三门，需要计算出班级中单科排名前三名学生的姓名</span><br><span class="line">基础数据是这样的</span><br><span class="line">[root@bigdata04 hivedata]# more student_score.data</span><br><span class="line">1 zs1 chinese 80</span><br><span class="line">2 zs1 math 90</span><br><span class="line">3 zs1 english 89</span><br><span class="line">4 zs2 chinese 60</span><br><span class="line">5 zs2 math 75</span><br><span class="line">6 zs2 english 80</span><br><span class="line">7 zs3 chinese 79</span><br><span class="line">8 zs3 math 83</span><br><span class="line">9 zs3 english 72</span><br><span class="line">10 zs4 chinese 90</span><br><span class="line">11 zs4 math 76</span><br><span class="line">12 zs4 english 80</span><br><span class="line">13 zs5 chinese 98</span><br><span class="line">14 zs5 math 80</span><br><span class="line">15 zs5 english 70</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">建表</span><br><span class="line">create external table student_score(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">sub string,</span><br><span class="line">score int</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">location &#39;&#x2F;data&#x2F;student_score&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">加载数据</span><br><span class="line">[root@bigdata04 hivedata]# hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;student_score.dat</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我们先使用row_number对数据编号，看一下是什么样子，row_number不能单独使用，在这里需要加上over</span><br><span class="line"></span><br><span class="line">select *,row_number() over() from student_score;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">结果如下：在这里相当于给表里面的所有数据编了一个号，从1开始</span><br><span class="line">15 zs5 english 70 1</span><br><span class="line">14 zs5 math 80 2</span><br><span class="line">13 zs5 chinese 98 3</span><br><span class="line">12 zs4 english 80 4</span><br><span class="line">11 zs4 math 76 5</span><br><span class="line">10 zs4 chinese 90 6</span><br><span class="line">9 zs3 english 72 7</span><br><span class="line">8 zs3 math 83 8</span><br><span class="line">7 zs3 chinese 79 9</span><br><span class="line">6 zs2 english 80 10</span><br><span class="line">5 zs2 math 75 11</span><br><span class="line">4 zs2 chinese 60 12</span><br><span class="line">3 zs1 english 89 13</span><br><span class="line">2 zs1 math 90 14</span><br><span class="line">1 zs1 chinese 80 15</span><br><span class="line"></span><br><span class="line">但是我们是希望对这些数据，先分组，再对组内数据进行排序，再编号</span><br><span class="line">所以就需要在over函数内部添加partiton by进行分组，添加order by 进行排序，最终给生成的编号起了</span><br><span class="line">换一个别名num</span><br></pre></td></tr></table></figure><h3 id="row-number"><a href="#row-number" class="headerlink" title="row_number()"></a>row_number()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">执行sql</span><br><span class="line">select *, row_number() over (partition by sub order by score desc) as num from student_score</span><br><span class="line"></span><br><span class="line">13 zs5 chinese 98 1</span><br><span class="line">10 zs4 chinese 90 2</span><br><span class="line">1 zs1 chinese 80 3</span><br><span class="line">7 zs3 chinese 79 4</span><br><span class="line">4 zs2 chinese 60 5</span><br><span class="line">3 zs1 english 89 1</span><br><span class="line">6 zs2 english 80 2</span><br><span class="line">12 zs4 english 80 3</span><br><span class="line">9 zs3 english 72 4</span><br><span class="line">15 zs5 english 70 5</span><br><span class="line">2 zs1 math 90 1</span><br><span class="line">8 zs3 math 83 2</span><br><span class="line">14 zs5 math 80 3</span><br><span class="line">11 zs4 math 76 4</span><br><span class="line">5 zs2 math 75 5</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">接着就可以获取前三名了，</span><br><span class="line">执行sql</span><br><span class="line"></span><br><span class="line">select * from (</span><br><span class="line">select *, row_number() over (partition by sub order by score desc) as num</span><br><span class="line">from student_score</span><br><span class="line">) s where s.num&lt;&#x3D;3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">13 zs5 chinese 98 1</span><br><span class="line">10 zs4 chinese 90 2</span><br><span class="line">1 zs1 chinese 80 3</span><br><span class="line">3 zs1 english 89 1</span><br><span class="line">6 zs2 english 80 2</span><br><span class="line">12 zs4 english 80 3</span><br><span class="line">2 zs1 math 90 1</span><br><span class="line">8 zs3 math 83 2</span><br><span class="line">14 zs5 math 80 3</span><br></pre></td></tr></table></figure><h3 id="rank"><a href="#rank" class="headerlink" title="rank()"></a>rank()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面SQL中的 row_number() 可以替换为 rank() 或者 dense_rank()</span><br><span class="line">其中： rank() 表示上下两条记录的score相等时，记录的行号是一样的，但下一个score值的行号递增N</span><br><span class="line">（N是重复的次数），比如：有两条并列第一，下一个是第三，没有第二</span><br><span class="line"></span><br><span class="line">执行sql</span><br><span class="line">select *, rank() over (partition by sub order by score desc) as num from student_score</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">13 zs5 chinese 98 1</span><br><span class="line">10 zs4 chinese 90 2</span><br><span class="line">1 zs1 chinese 80 3</span><br><span class="line">7 zs3 chinese 79 4</span><br><span class="line">4 zs2 chinese 60 5</span><br><span class="line">3 zs1 english 89 1</span><br><span class="line">6 zs2 english 80 2</span><br><span class="line">12 zs4 english 80 2</span><br><span class="line">9 zs3 english 72 4</span><br><span class="line">15 zs5 english 70 5</span><br><span class="line">2 zs1 math 90 1</span><br><span class="line">8 zs3 math 83 2</span><br><span class="line">14 zs5 math 80 3</span><br><span class="line">11 zs4 math 76 4</span><br><span class="line">5 zs2 math 75 5</span><br></pre></td></tr></table></figure><h3 id="dense-rank"><a href="#dense-rank" class="headerlink" title="dense_rank()"></a>dense_rank()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dense_rank() 表示上下两条记录的score相等时，下一个score值的行号递增1，比如：有两条并列第一，下一个是第二</span><br><span class="line">执行sql</span><br><span class="line">select *, dense_rank() over (partition by sub order by score desc) as num from student_score;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">13 zs5 chinese 98 1</span><br><span class="line">10 zs4 chinese 90 2</span><br><span class="line">1 zs1 chinese 80 3</span><br><span class="line">7 zs3 chinese 79 4</span><br><span class="line">4 zs2 chinese 60 5</span><br><span class="line">3 zs1 english 89 1</span><br><span class="line">6 zs2 english 80 2</span><br><span class="line">12 zs4 english 80 2</span><br><span class="line">9 zs3 english 72 3</span><br><span class="line">15 zs5 english 70 4</span><br><span class="line">2 zs1 math 90 1</span><br><span class="line">8 zs3 math 83 2</span><br><span class="line">14 zs5 math 80 3</span><br><span class="line">11 zs4 math 76 4</span><br><span class="line">5 zs2 math 75 5</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">row_number() over() 是正常排序</span><br><span class="line">rank() over() 是跳跃排序，有两个第一名时接下来就是第三名（在各个分组内）</span><br><span class="line">dense_rank() over() 是连续排序，有两个第一名时仍然跟着第二名（在各个分组内）</span><br></pre></td></tr></table></figure><h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">行转列就是把多行数据转为一列数据</span><br><span class="line">针对行转列这种需求主要需要使用到 CONCAT_WS()、COLLECT_SET() 、COLLECT_LIST() 函数</span><br><span class="line">先看一下这几个函数的描述信息，注意，有的函数在帮助文档里面没有描述信息</span><br></pre></td></tr></table></figure><h4 id="CONCAT-WS"><a href="#CONCAT-WS" class="headerlink" title="CONCAT_WS()"></a>CONCAT_WS()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc function CONCAT_WS;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">CONCAT_WS(separator, [string | array(string)]+) - returns the concatenation of</span><br><span class="line">Time taken: 0.019 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONCAT_WS() 函数可以实现根据指定的分隔符拼接多个字段的值，最终转化为一个带有分隔符的字符串</span><br><span class="line">它可以接收多个参数，第一个参数是分隔符，后面的参数可以是字符串或者字符串数组，最终就是使用分隔符把后面的所有字符串拼接到一块</span><br></pre></td></tr></table></figure><h4 id="COLLECT-LIST"><a href="#COLLECT-LIST" class="headerlink" title="COLLECT_LIST()"></a>COLLECT_LIST()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc function COLLECT_LIST;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">COLLECT_LIST(x) - Returns a list of objects with duplicates</span><br><span class="line">Time taken: 0.014 seconds, Fetched: 1 row(s)</span><br><span class="line"></span><br><span class="line">这个函数可以返回一个list集合，集合中的元素会重复，一般和group by 结合在一起使用，一会再演示</span><br></pre></td></tr></table></figure><h4 id="COLLECT-SET"><a href="#COLLECT-SET" class="headerlink" title="COLLECT_SET()"></a>COLLECT_SET()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc function COLLECT_SET;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">COLLECT_SET(x) - Returns a set of objects with duplicate elements eliminated</span><br><span class="line">Time taken: 0.014 seconds, Fetched: 1 row(s)</span><br><span class="line"></span><br><span class="line">这个函数可以返回一个set集合，集合汇中的元素不重复，一般和group by 结合在一起使用，一会再演示</span><br><span class="line">根据前面的分析，使用这几个函数我们就可以实现行转列这个需求了</span><br></pre></td></tr></table></figure><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 hivedata]# more student_favors.data</span><br><span class="line">zs swing</span><br><span class="line">zs footbal</span><br><span class="line">zs sing</span><br><span class="line">zs codeing</span><br><span class="line">zs swing</span><br><span class="line"></span><br><span class="line">期望的结果是这样的</span><br><span class="line">zs swing,footbal,sing,codeing,swing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">分析一下：</span><br><span class="line">在这其实就是对数据进行了分组，分组之后可以把相同人的爱好保存到一个数组中，再把数组中的数据转成使用逗号分割的字符串</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">开始对原始数据建表</span><br><span class="line">create external table student_favors(</span><br><span class="line">name string,</span><br><span class="line">favor string</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">location &#39;&#x2F;data&#x2F;student_favors&#39;;</span><br><span class="line"></span><br><span class="line">上传数据</span><br><span class="line">[root@bigdata04 hivedata]# hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;student_favors.data &#x2F;data&#x2F;student_favors</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">查看数据</span><br><span class="line">hive (default)&gt; select * from student_favors;</span><br><span class="line">OK</span><br><span class="line">student_favors.name student_favors.favor</span><br><span class="line">zs swing</span><br><span class="line">zs footbal</span><br><span class="line">zs sing</span><br><span class="line">zs codeing</span><br><span class="line">zs swing</span><br><span class="line">Time taken: 0.175 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">先对name字段进行分组，把favor转成一个数组</span><br><span class="line">select name,collect_list(favor) as favor_list from student_favors group by name;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">结果如下</span><br><span class="line"> zs [&quot;swing&quot;,&quot;footbal&quot;,&quot;sing&quot;,&quot;codeing&quot;,&quot;swing&quot;]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">然后再使用 concat_ws 把数组中的元素按照指定分隔符转成字符串</span><br><span class="line">这样就实现了多行数据转为一列数据了</span><br><span class="line">执行行转列操作</span><br><span class="line">select name,concat_ws(&#39;,&#39;,collect_list(favor)) as favor_list from student_favors group by name;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">结果如下</span><br><span class="line">zs swing,footbal,sing,codeing,swing</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我们发现这里面有一些爱好是重复的，如果不希望出现重复的话可以使用COLLECT_SET()</span><br><span class="line">执行sql</span><br><span class="line">select name,concat_ws(&#39;,&#39;,collect_set(favor)) as favor_list from student_favors group by name;</span><br></pre></td></tr></table></figure><h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">列转行是和刚才的行转列反着来的，列转行可以把一列数据转成多行</span><br><span class="line">主要使用到 SPLIT()、EXPLODE()和LATERAL VIEW</span><br><span class="line">看一下这几个函数</span><br></pre></td></tr></table></figure><h4 id="SPLIT"><a href="#SPLIT" class="headerlink" title="SPLIT()"></a>SPLIT()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc function SPLIT;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">SPLIT(str, regex) - Splits str around occurances that match regex</span><br><span class="line">Time taken: 0.011 seconds, Fetched: 1 row(s)</span><br><span class="line"></span><br><span class="line">split函数，接受一个字串符和切割规则，就类似于java中的split函数，使用切割规则对字符串中的数据进行切割，最终返回一个array数组</span><br></pre></td></tr></table></figure><h4 id="EXPLODE"><a href="#EXPLODE" class="headerlink" title="EXPLODE()"></a>EXPLODE()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc function EXPLODE;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">EXPLODE(a) - separates the elements of array a into multiple rows, or the ele</span><br><span class="line">Time taken: 0.013 seconds, Fetched: 1 row(s)</span><br><span class="line"></span><br><span class="line">explode函数可以接受array或者map</span><br><span class="line">explode(ARRAY)：表示把数组中的每个元素转成一行</span><br><span class="line">explode(MAP) ：表示把map中每个key-value对，转成一行，key为一列，value为一列</span><br></pre></td></tr></table></figure><h4 id="LATERAL-VIEW"><a href="#LATERAL-VIEW" class="headerlink" title="LATERAL VIEW"></a>LATERAL VIEW</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Lateral view 通常和split， explode等函数一起使用。</span><br><span class="line">split可以对表中的某一列进行切割，返回一个数组类型的字段，explode可以对这个数组中的每一个元素</span><br><span class="line">转为一行，lateral view可以对这份数据产生一个支持别名的虚拟表</span><br><span class="line">原始数据如下</span><br></pre></td></tr></table></figure><h4 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 hivedata]# more student_favors_2.data</span><br><span class="line">zs swing,footbal,sing</span><br><span class="line">ls codeing,swing</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">希望的结果是这样的</span><br><span class="line">zs swing</span><br><span class="line">zs footbal</span><br><span class="line">zs sing</span><br><span class="line">ls codeing</span><br><span class="line">ls swing</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">接着建表</span><br><span class="line">create external table student_favors_2(</span><br><span class="line">name string,</span><br><span class="line">favorlist string</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">location &#39;&#x2F;data&#x2F;student_favors_2&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">上传数据</span><br><span class="line">[root@bigdata04 hivedata]# hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;student_favors_2</span><br><span class="line"></span><br><span class="line">查看数据</span><br><span class="line">hive (default)&gt; select * from student_favors_2;</span><br><span class="line">OK</span><br><span class="line">student_favors_2.name student_favors_2.favorlist</span><br><span class="line">zs swing,footbal,sing</span><br><span class="line">ls codeing,swing</span><br><span class="line">Time taken: 0.131 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先使用split对favorlist字段进行切割</span><br><span class="line">hive (default)&gt; select split(favorlist,&#39;,&#39;) from student_favors_2;</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line">_c1</span><br><span class="line">[&quot;swing&quot;,&quot;footbal&quot;,&quot;sing&quot;]</span><br><span class="line">[&quot;codeing&quot;,&quot;swing&quot;]</span><br><span class="line">Time taken: 0.224 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">再使用explode对数据进行操作</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select explode(split(favorlist,&#39;,&#39;)) from student_favors_2;</span><br><span class="line">OK</span><br><span class="line">col</span><br><span class="line">swing</span><br><span class="line">footbal</span><br><span class="line">sing</span><br><span class="line">codeing</span><br><span class="line">swing</span><br><span class="line">Time taken: 0.185 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实到这里已经实现了列转行了，但是还需要把name字段拼接上，这时候就需要使用later view了，否则直接查询name字段会报错</span><br><span class="line">laterview相当于把explode返回的数据作为一个虚拟表来使用了，起名字为table1，然后给这个表里面的那一列数据起一个名字叫favor_new，如果有多个字段，可以再后面指定多个。这样在select后面就可以使用这个名字了，有点类似join操作了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select name,favor_new from student_favors_2 lateral view explode(split(favorlist,&#39;,&#39;)) table1 as favor_new;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">结果如下：</span><br><span class="line">zs swing</span><br><span class="line">zs footbal</span><br><span class="line">zs sing</span><br><span class="line">ls codeing</span><br><span class="line">ls swing</span><br></pre></td></tr></table></figure><h2 id="Hive排序相关函数"><a href="#Hive排序相关函数" class="headerlink" title="Hive排序相关函数"></a>Hive排序相关函数</h2><h3 id="ORDER-BY"><a href="#ORDER-BY" class="headerlink" title="ORDER BY"></a>ORDER BY</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hive中的order by跟传统的sql语言中的order by作用是一样的，会对查询的结果做一次全局排序，使用这个语句的时候生成的reduce任务只有一个</span><br></pre></td></tr></table></figure><h3 id="SORT-BY"><a href="#SORT-BY" class="headerlink" title="SORT BY"></a>SORT BY</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Hive中指定了sort by，如果有多个reduce，那么在每个reducer端都会做排序，也就是说保证了局部有序（每个reducer出来的数据是有序的，但是不能保证所有的数据是全局有序的，除非只有一个reducer）</span><br><span class="line">使用前面的t2_bak表，表中的数据如下：</span><br><span class="line">hive (default)&gt; select * from t2_bak;</span><br><span class="line">OK</span><br><span class="line">t2_bak.id t2_bak.name</span><br><span class="line">1 NULL</span><br><span class="line">2 NULL</span><br><span class="line">3 NULL</span><br><span class="line">4 NULL</span><br><span class="line">5 NULL</span><br><span class="line">1 NULL</span><br><span class="line">2 NULL</span><br><span class="line">3 NULL</span><br><span class="line">4 NULL</span><br><span class="line">5 NULL</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">执行排序SQL</span><br><span class="line">hive (default)&gt; select id from t2_bak sort by id;</span><br><span class="line">......</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers:</span><br><span class="line">......</span><br><span class="line">OK</span><br><span class="line">id</span><br><span class="line">1 </span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">2 </span><br><span class="line">3</span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">4 </span><br><span class="line">5 </span><br><span class="line">5</span><br><span class="line">Time taken: 24.562 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">刚才我们说sort by是局部有序，为什么最终的结果还是全局有序呢？</span><br><span class="line">看里面的日志，现在只有一个reduce任务，所以最终结果还是有序的</span><br><span class="line">动态设置reduce任务数量为2，然后再执行排序的SQL</span><br><span class="line"></span><br><span class="line">hive (default)&gt; set mapreduce.job.reduces &#x3D; 2;</span><br><span class="line">hive (default)&gt; select id from t2_bak sort by id;</span><br><span class="line">.......</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers:</span><br><span class="line">......</span><br><span class="line">OK</span><br><span class="line">id</span><br><span class="line">1 3 3 4 5 5 1 2 2 4</span><br><span class="line">Time taken: 27.943 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时会发现数据就没有全局排序了，因为有多个reduce了。</span><br><span class="line">不过针对ORDER BY来说，你动态设置再多的reduce数量都没有用，最后还是只产生1个reduce。</span><br></pre></td></tr></table></figure><h3 id="DISTRIBUTE-BY"><a href="#DISTRIBUTE-BY" class="headerlink" title="DISTRIBUTE BY"></a>DISTRIBUTE BY</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ditribute by是控制map的输出到reducer是如何划分的</span><br><span class="line"></span><br><span class="line">ditribute by：只会根据指定的key对数据进行分区，但是不会排序。</span><br><span class="line">一般情况下可以和sort by 结合使用，先对数据分区，再进行排序</span><br><span class="line">两者结合使用的时候distribute by必须要写在sort by之前</span><br><span class="line">先来看一下单独ditribute by的使用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces &#x3D; 2;</span><br><span class="line">hive (default)&gt; select id from t2_bak distribute by id;</span><br><span class="line">.......</span><br><span class="line">Number of reduce tasks not specified. Defaulting to jobconf value of: 2</span><br><span class="line">.......</span><br><span class="line">OK</span><br><span class="line">id</span><br><span class="line">4 </span><br><span class="line">2 </span><br><span class="line">4 </span><br><span class="line">2 </span><br><span class="line">5 </span><br><span class="line">3 </span><br><span class="line">1 </span><br><span class="line">5 </span><br><span class="line">3 </span><br><span class="line">1</span><br><span class="line">Time taken: 25.395 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">可以结合sort by实现分区内的排序，默认是升序，可以通过desc来设置倒序</span><br><span class="line"></span><br><span class="line">hive (default)&gt; set mapreduce.job.reduces &#x3D; 2;</span><br><span class="line">hive (default)&gt; select id from t2_bak distribute by id sort by id;</span><br><span class="line">.......</span><br><span class="line">Number of reduce tasks not specified. Defaulting to jobconf value of: 2</span><br><span class="line">.......</span><br><span class="line">id</span><br><span class="line">2 2 4 4 1 1 3 3 5 5</span><br><span class="line">Time taken: 24.468 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select id from t2_bak distribute by id sort by id desc;</span><br></pre></td></tr></table></figure><h3 id="CLUSTER-BY"><a href="#CLUSTER-BY" class="headerlink" title="CLUSTER BY"></a>CLUSTER BY</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cluster by的功能就是distribute by和sort by的简写形式</span><br><span class="line">也就是 cluster by id 等于 distribute by id sort by id</span><br><span class="line">注意被cluster by指定的列只能是升序，不能指定asc和desc</span><br><span class="line"></span><br><span class="line">hive (default)&gt; set mapreduce.job.reduces &#x3D; 2;</span><br><span class="line">hive (default)&gt; select id from t2_bak cluster by id;</span><br><span class="line">.....</span><br><span class="line">Number of reduce tasks not specified. Defaulting to jobconf value of: 2</span><br><span class="line"></span><br><span class="line">.....</span><br><span class="line">OK</span><br><span class="line">id</span><br><span class="line">2 </span><br><span class="line">2 </span><br><span class="line">4 </span><br><span class="line">4 </span><br><span class="line">1 </span><br><span class="line">1 </span><br><span class="line">3 </span><br><span class="line">3 </span><br><span class="line">5 </span><br><span class="line">5</span><br><span class="line">Time taken: 25.495 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure><h2 id="Hive-的分组和去重函数"><a href="#Hive-的分组和去重函数" class="headerlink" title="Hive 的分组和去重函数"></a>Hive 的分组和去重函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GROUP BY ：对数据按照指定字段进行分组</span><br><span class="line">DISTINCT：对数据中指定字段的重复值进行去重</span><br><span class="line"></span><br><span class="line">第一种：select count(distinct name) from order</span><br><span class="line">第二种：select count(tmp.name) from (select name from order group by name) tmp</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第一种：使用distinct会将所有的name都shuffle到一个reducer里面，性能较低</span><br><span class="line">第二种：先对name分组，因为分组的同时其实就是去重，此时是可以并行计算的，然后再计算count</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第4章 Hive核心实战2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%204.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%204.html</id>
    <published>2022-02-20T04:01:53.000Z</published>
    <updated>2022-02-24T04:34:32.668Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第4章-Hive核心实战"><a href="#第八周-第4章-Hive核心实战" class="headerlink" title="第八周 第4章 Hive核心实战"></a>第八周 第4章 Hive核心实战</h1><a href="/hadoop%E5%AE%9E%E9%AA%8C%E8%AF%BE-hive.html" title="hadoop实验课-hive">hadoop实验课-hive</a><h2 id="综合案例"><a href="#综合案例" class="headerlink" title="综合案例"></a>综合案例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一个综合案例，主要使用外部分区表和视图实现</span><br><span class="line">需求：Flume按天把日志数据采集到HDFS中的对应目录中，使用SQL按天统计每天数据的相关指标</span><br><span class="line">分析一下：</span><br><span class="line">Flume按天把日志数据保存到HDFS中的对应目录中</span><br><span class="line">针对Flume的source可以使用execsource、channel可以使用基于文件的或者内存的，sink使用hdfssink，在hdfssink的path路径中需要使用%Y%m%d获取日期，将每天的日志数据采集到指定的hdfs目录中</span><br><span class="line">这个是我们在前面学习Flume的时候已经讲过的了，这个倒不难</span><br><span class="line">后面就是需要对按天采集的日志数据建表，由于这份数据可能会被多种计算引擎使用，所以建议使用外部表，这样就算我们不小心把表删了，数据也还是在的，不影响其他人使用，还有就是这份数据是按天分目录存储的，在实际工作中，离线计算的需求大部分都是按天计算的，所以在这里最好在表中增加日期这个</span><br><span class="line">分区字段，所以最终决定使用外部分区表。</span><br><span class="line">前面FLume采集数据的流程我们就不再演示了，在这我就直接使用之前我们使用hdfs落盘的数据了。</span><br><span class="line">我们之前有一个案例是分类型，分目录，把多种类型的数据存储到不同的目录下</span><br><span class="line">目录结构是这样的，首先是按天，然后是按照类型</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第4章 Hive核心实战</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98.html</id>
    <published>2022-02-20T04:01:53.000Z</published>
    <updated>2022-02-22T12:45:48.826Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第4章-Hive核心实战"><a href="#第八周-第4章-Hive核心实战" class="headerlink" title="第八周 第4章 Hive核心实战"></a>第八周 第4章 Hive核心实战</h1><a href="/hadoop%E5%AE%9E%E9%AA%8C%E8%AF%BE-hive.html" title="hadoop实验课-hive">hadoop实验课-hive</a><h2 id="Hive中数据库的操作"><a href="#Hive中数据库的操作" class="headerlink" title="Hive中数据库的操作"></a>Hive中数据库的操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">show databases;</span><br><span class="line">use xxx;</span><br><span class="line">create database xxx;</span><br><span class="line"></span><br><span class="line">drop database xxxx;</span><br></pre></td></tr></table></figure><h2 id="Hive中表的操作"><a href="#Hive中表的操作" class="headerlink" title="Hive中表的操作"></a>Hive中表的操作</h2><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table xxx(id int);</span><br></pre></td></tr></table></figure><h3 id="查看创建的表"><a href="#查看创建的表" class="headerlink" title="查看创建的表"></a>查看创建的表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">查看表信息</span><br><span class="line">show tables;###</span><br></pre></td></tr></table></figure><h3 id="查看表结构信息"><a href="#查看表结构信息" class="headerlink" title="查看表结构信息"></a>查看表结构信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc xxx;</span><br></pre></td></tr></table></figure><h3 id="查看表的创建信息"><a href="#查看表的创建信息" class="headerlink" title="查看表的创建信息"></a>查看表的创建信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">show create table xxx;</span><br><span class="line">从这里的location可以看到这个表在hdfs上的位置。</span><br><span class="line">注意了：表中的数据是存储在hdfs中的，但是表的名称、字段信息是存储在metastore中的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">到metastore中看一下</span><br><span class="line">先看tbls表，这个里面中存储的都是在hive中创建的表</span><br><span class="line">可以看到DB_ID 为1</span><br><span class="line">可以到dbs表中看到默认default数据库的id就是1。</span><br><span class="line">TBL_NAME 是这个表的名称。</span><br><span class="line"></span><br><span class="line">在表COLUMNS_V2中存储的是Hive表的字段信息(包含字段注释、字段名称、字段类型、字段顺序)</span><br><span class="line">其中的CD_ID和tbls中的TBL_ID相等</span><br></pre></td></tr></table></figure><h3 id="修改表名"><a href="#修改表名" class="headerlink" title="修改表名"></a>修改表名</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table t2 rename to t2_bak;</span><br><span class="line"></span><br><span class="line">hdfs中对应的目录名称也同步变化了</span><br></pre></td></tr></table></figure><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">咱们前面向表中添加数据是使用的insert命令，其实使用insert向表里面添加数据只是在测试的时候使用，实际中向表里面添加数据很少使用insert命令的</span><br><span class="line">具体原因我们后面再分析，在这大家先带着这个问题。</span><br><span class="line">insert into test2(id, name) values(1,&quot;zhangsan&quot;);</span><br><span class="line"></span><br><span class="line">向表中加载数据可以使用load命令</span><br><span class="line">以t2_bak为例，在bigdata04机器的 &#x2F;data&#x2F;soft&#x2F;hivedata 下有一个 t2.data 文件，将其加载到 t2_bak表中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 hivedata]# pwd</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;hivedata</span><br><span class="line">[root@bigdata04 hivedata]# more t2.data</span><br><span class="line">1 2 3 4 5</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t2.data&#39; into table t2_bak</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from t2_bak;</span><br><span class="line">OK</span><br><span class="line">t2_bak.id</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们到hdfs上去看一下这个表，发现刚才的文件其实就是上传到了t2_bak目录中</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HzRNWV" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/22/HzRNWV.md.png" alt="HzRNWV.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那我们自己手工通过put命令把数据上传到t2_bak目录中可以吗？</span><br><span class="line">可以的！</span><br><span class="line">hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t2_bak.data &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;t2_bak</span><br><span class="line"></span><br><span class="line">到hdfs上确认一下，可以看到刚才上传的文件</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HzWY0H" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/22/HzWY0H.md.png" alt="HzWY0H.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">再查询一下这个表的数据，可以发现数据多了一份，说明刚才使用hdfs的put命令上传的是可以的。</span><br><span class="line">hive (default)&gt; select * from t2_bak;</span><br><span class="line">OK</span><br><span class="line">t2_bak.id</span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5 </span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5</span><br></pre></td></tr></table></figure><h3 id="表增加字段及注释、删除表"><a href="#表增加字段及注释、删除表" class="headerlink" title="表增加字段及注释、删除表"></a>表增加字段及注释、删除表</h3><h4 id="增加字段"><a href="#增加字段" class="headerlink" title="增加字段"></a>增加字段</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在工作中会有给已存在的表增加字段的需求，需要使用alter命令</span><br><span class="line">在这里我们给t2_bak表增加一个name字段，重新查看表结构信息，再查询一下这个表中的数据，结果发现，第二列为null，这是正常的，因为我们的数据数据文件中就只有一列，第二列查询不到，就显示为null，不会报错，这一点要注意一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table t2_bak add columns (name string);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.175 seconds</span><br><span class="line">hive (default)&gt; desc t2_bak;</span><br><span class="line">OK</span><br><span class="line">col_name data_type comment</span><br><span class="line">id int</span><br><span class="line">name string</span><br><span class="line">Time taken: 0.121 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; select * from t2_bak;</span><br><span class="line">OK</span><br><span class="line">t2_bak.id t2_bak.name</span><br><span class="line">1 NULL</span><br><span class="line">2 NULL</span><br><span class="line">3 NULL</span><br><span class="line">4 NULL</span><br><span class="line">5 NULL</span><br><span class="line">1 NULL</span><br><span class="line">2 NULL</span><br><span class="line">3 NULL</span><br><span class="line">4 NULL</span><br><span class="line">5 NULL</span><br><span class="line">Time taken: 0.199 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure><h4 id="增加注释"><a href="#增加注释" class="headerlink" title="增加注释"></a>增加注释</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">现在我们通过desc查询表中的字段信息发现都没有注释，所以想要给字段加一些注释，以及表本身也可以增加注释，都是使用comment关键字</span><br><span class="line">重新创建一个表t2</span><br><span class="line">注意：在建表语句中，缩进不要使用tab制表符，否则拷贝到hive命令行下执行会提示语句错误，这里的缩进需要使用空格</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create table t2(</span><br><span class="line">age int comment &#39;年龄&#39;</span><br><span class="line">) comment &#39;测试&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">查看这个表的信息，结果发现我们添加的中文注释都是乱码</span><br><span class="line"></span><br><span class="line">hive (default)&gt; desc t2;</span><br><span class="line">OK</span><br><span class="line">col_name data_type comment</span><br><span class="line">age int ??</span><br><span class="line">Time taken: 0.086 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; show create table t2;</span><br><span class="line">OK</span><br><span class="line">createtab_stmt</span><br><span class="line">CREATE TABLE &#96;t2&#96;(</span><br><span class="line">&#96;age&#96; int COMMENT &#39;??&#39;)</span><br><span class="line">COMMENT &#39;??&#39;</span><br><span class="line">ROW FORMAT SERDE</span><br><span class="line">&#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">原因是什么？怎么破？</span><br><span class="line">中文乱码的原因是因为hive数据库里面的表都是latin1编码的，中文本来就会显示乱码，但是又不能修改整个数据库里面所有表的编码，否则在使用hive的时候会出问题，那么只有考虑把存储字段注释和表注释相关的表的编码改为utf8。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">登陆Mysql数据库切换到Hive库：</span><br><span class="line"></span><br><span class="line">C:\Users\yehua&gt;mysql -uroot -padmin</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecu</span><br><span class="line">Welcome to the MySQL monitor. Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 736</span><br><span class="line">Server version: 8.0.16 MySQL Community Server - GPL</span><br><span class="line">Copyright (c) 2000, 2019, Oracle and&#x2F;or its affiliates. All rights reserved.</span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and&#x2F;or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line">Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement</span><br><span class="line">mysql&gt; use hive;</span><br><span class="line">Database changed</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">确认一下现在表COLUMNS_V2和TABLE_PARAMS的编码，都是latin1</span><br><span class="line"></span><br><span class="line">mysql&gt; show create table COLUMNS_V2;</span><br><span class="line">+------------+---------------------------------------------------------------</span><br><span class="line">| Table | Create Table</span><br><span class="line">+------------+---------------------------------------------------------------</span><br><span class="line">| COLUMNS_V2 | CREATE TABLE &#96;columns_v2&#96; (</span><br><span class="line">&#96;CD_ID&#96; bigint(20) NOT NULL,</span><br><span class="line">&#96;COMMENT&#96; varchar(256) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL</span><br><span class="line">&#96;COLUMN_NAME&#96; varchar(767) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL</span><br><span class="line">&#96;TYPE_NAME&#96; mediumtext,</span><br><span class="line">&#96;INTEGER_IDX&#96; int(11) NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;CD_ID&#96;,&#96;COLUMN_NAME&#96;),</span><br><span class="line">KEY &#96;COLUMNS_V2_N49&#96; (&#96;CD_ID&#96;),</span><br><span class="line">CONSTRAINT &#96;COLUMNS_V2_FK1&#96; FOREIGN KEY (&#96;CD_ID&#96;) REFERENCES &#96;cds&#96; (&#96;CD_ID&#96;</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;latin1 |</span><br><span class="line">+------------+---------------------------------------------------------------</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; show create table TABLE_PARAMS;</span><br><span class="line">+--------------+-------------------------------------------------------------</span><br><span class="line">| Table | Create Table</span><br><span class="line">+--------------+-------------------------------------------------------------</span><br><span class="line">| TABLE_PARAMS | CREATE TABLE &#96;table_params&#96; (</span><br><span class="line">&#96;TBL_ID&#96; bigint(20) NOT NULL,</span><br><span class="line">&#96;PARAM_KEY&#96; varchar(256) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL,</span><br><span class="line">&#96;PARAM_VALUE&#96; mediumtext CHARACTER SET latin1 COLLATE latin1_bin,</span><br><span class="line">PRIMARY KEY (&#96;TBL_ID&#96;,&#96;PARAM_KEY&#96;),</span><br><span class="line">KEY &#96;TABLE_PARAMS_N49&#96; (&#96;TBL_ID&#96;),</span><br><span class="line">CONSTRAINT &#96;TABLE_PARAMS_FK1&#96; FOREIGN KEY (&#96;TBL_ID&#96;) REFERENCES &#96;tbls&#96; (&#96;TB</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;latin1 |</span><br><span class="line">+--------------+-------------------------------------------------------------</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">修改这两张表的编码即可；</span><br><span class="line">alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;</span><br><span class="line">alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果你的表创建了分区的话就要再执行两条命令：</span><br><span class="line">alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br><span class="line">alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这样修改之后以后就可以看到中文注释了。</span><br><span class="line">注意：需要先把之前创建的t2表删除掉，因为之前存储的中文已经是乱码了，无法恢复，删除之后重新创建就可以了</span><br></pre></td></tr></table></figure><h3 id="指定列和行分隔符的指定"><a href="#指定列和行分隔符的指定" class="headerlink" title="指定列和行分隔符的指定"></a>指定列和行分隔符的指定</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">在我们实际工作中，肯定不会像上面一样创建一张非常简单的表，实际中表的字段会比较多，下面我们就来创建一个多字段的表t3</span><br><span class="line">create table t3(</span><br><span class="line">id int comment &#39;ID&#39;,</span><br><span class="line">stu_name string comment &#39;name&#39;,</span><br><span class="line">stu_birthday date comment &#39;birthday&#39;,</span><br><span class="line">online boolean comment &#39;is online&#39;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这样创建没有问题，我们来加载对应的数据文件 &#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t3.data ，表中的多列内容之间是使用制表符分割的</span><br><span class="line">看一下表中的数据，会不会有问题呢？</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# pwd</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;hivedata</span><br><span class="line">[root@bigdata04 hivedata]# more t3.data</span><br><span class="line">1 张三 2020-01-01 true</span><br><span class="line">2 李四 2020-02-01 false</span><br><span class="line">3 王五 2020-03-01 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t3.data&#39; into table t3;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from t3;</span><br><span class="line">OK</span><br><span class="line">t3.id t3.stu_name t3.stu_birthday t3.online</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">Time taken: 0.229 seconds, Fetched: 3 row(s)</span><br><span class="line"></span><br><span class="line">在这里发现不是我们想要的结果，都是 NULL ，说明数据没有被识别，这是为什么？</span><br><span class="line">注意了，hive在创建表的时候，需要我们指定相应的行分隔符，列分隔符。而我们在创建mysql表的时候，这些都是不需要的，因为它在组织数据的时候，已经规定好了数据的表现形式。</span><br><span class="line">我们刚才在创建t3的时候没有指定相应的分隔符，所以导致使用制表符分割的数据无法被解析。</span><br><span class="line">实际上呢，hive是有默认的分隔符的，默认的行分隔符是 &#39;\n&#39; ，就是换行符，而默认的列分隔符呢，是\001 。</span><br><span class="line">\001 这个是ASCII码中的特殊不常使用的不可见字符，在文本中我们可以通过 ctrl+v 和 ctrl+a 来输入\001 ，这里我们在将 t3.data 改一下，重新上传，再查看表t3。</span><br><span class="line">修改t3.data</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# vi t3.data</span><br><span class="line">1^A张三^A2020-01-01^Atrue</span><br><span class="line">2 李四 2020-02-01 false</span><br><span class="line">3 王五 2020-03-01 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">重新加载数据，查询表数据，这个时候发现刚才修改的那条数据被成功解析了</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t3.data&#39; into table t3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Loading data to table default.t3</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.367 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from t3;</span><br><span class="line">OK</span><br><span class="line">t3.id t3.stu_name t3.stu_birthday t3.online</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">1 张三 2020-01-01 true</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">Time taken: 0.149 seconds, Fetched: 6 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">那么问题来了，为了我们能够在上传数据之后让hive表正确识别数据，那我们该如何修改hive表的默认分隔符呢？</span><br><span class="line">其实也是非常的简单，只要我们在创建表的时候指定一下分隔符就可以了，我们把建表语句修改一下，重新创建一个表t3_new</span><br><span class="line"></span><br><span class="line">create table t3_new(</span><br><span class="line">id int comment &#39;ID&#39;,</span><br><span class="line">stu_name string comment &#39;name&#39;,</span><br><span class="line">stu_birthday date comment &#39;birthday&#39;,</span><br><span class="line">online boolean comment &#39;is online&#39;</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br><span class="line"></span><br><span class="line">在这需要注意的是， lines terminated by 行分隔符可以忽略不写，但是如果要写的话，只能写到最后面！</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">把t3.data文件中的字段分隔符都恢复为制表符，然后重新把数据加载到t3_new表中。</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t3.data&#39; into table t3_new</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">查看t3_new表中的数据，注意，针对无法识别的数据显示为NULL，因为最后一列为boolean类型，但是在数据中我故意指定了一个数字，所以导致无法解析，但是不会导致数据加载失败，也不会导致查询失败，这就是hive的特性，他不会提前检查数据，只有在使用的时候才会检查数据，如果数据有问题就显示为null，也不报错。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from t3_new;</span><br><span class="line">OK</span><br><span class="line">t3_new.id t3_new.stu_name t3_new.stu_birthday t3_new.online</span><br><span class="line">1 张三 2020-01-01 true</span><br><span class="line">2 李四 2020-02-01 false</span><br><span class="line">3 王五 2020-03-01 NULL</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第4章 Hive核心实战2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%202.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%202.html</id>
    <published>2022-02-20T04:01:53.000Z</published>
    <updated>2022-02-23T05:05:43.370Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第4章-Hive核心实战"><a href="#第八周-第4章-Hive核心实战" class="headerlink" title="第八周 第4章 Hive核心实战"></a>第八周 第4章 Hive核心实战</h1><a href="/hadoop%E5%AE%9E%E9%AA%8C%E8%AF%BE-hive.html" title="hadoop实验课-hive">hadoop实验课-hive</a><h2 id="Hive中的数据类型"><a href="#Hive中的数据类型" class="headerlink" title="Hive中的数据类型"></a>Hive中的数据类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive作为一个类似数据库的框架，也有自己的数据类型，便于存储、统计、分析。</span><br><span class="line">Hive中主要包含两大数据类型</span><br><span class="line">一类是基本数据类型</span><br><span class="line">一类是复合数据类型</span><br><span class="line">基本数据类型：常用的有INT,STRING,BOOLEAN,DOUBLE等</span><br><span class="line">复合数据类型：常用的有ARRAY,MAP,STRUCT等</span><br></pre></td></tr></table></figure><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">看这个表，一般数字类型我们可以试验int，小数可以使用double，日期可以使用date类型、还有就是</span><br><span class="line">boolean类型，这些算是比较常见的了，前面我们在建表的时候基本都用过了。</span><br><span class="line">这些基本数据类型倒没有什么特殊之处</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据类型 开始支持版本 数据类型 开始支持版本</span><br><span class="line">TINYINT ~ TIMESTAMP 0.8.0</span><br><span class="line">SMALLINT ~ DATE 0.12.0</span><br><span class="line">INT&#x2F;INTEGER ~ STRING ~</span><br><span class="line">BIGINT ~ VARCHAR 0.12.0</span><br><span class="line">FLOAT ~ CHAR 0.13.0</span><br><span class="line">DOUBLE ~ BOOLEAN ~</span><br><span class="line">DECIMAL 0.11.0</span><br></pre></td></tr></table></figure><h3 id="复合数据类型"><a href="#复合数据类型" class="headerlink" title="复合数据类型"></a>复合数据类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面主要看一下复合数据类型，在这里我们主要分析这三个， array，map和struct</span><br><span class="line"></span><br><span class="line">数据类型 开始支持版本 格式</span><br><span class="line">ARRAY 0.14.0 ARRAY&lt;data_type&gt;</span><br><span class="line">MAP 0.14.0 MAP&lt;primitive_type, data_type&gt;</span><br><span class="line">STRUCT ~ STRUCT&lt;col_name : data_type, ...&gt;</span><br></pre></td></tr></table></figure><h4 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先来看Array，这个表示是一个数组结构</span><br><span class="line">在这里举一个例子：学生有多个爱好，有两个学生，zhangsan、lisi，</span><br><span class="line">zhangsan的爱好是swing、sing、coding</span><br><span class="line">lisi的爱好是music、football</span><br><span class="line">每个学生的爱好都是不固定的，有多有少，如果根据学生的每一个爱好都在表里面增加一列，这样就不合适了，后期可能要经常增加列存储不同的爱好</span><br><span class="line">如果我们如果把每个学生的爱好都拼接为一个字符串保存到一个字段中，这样针对存储层面来说是没有问题的，但是后期需要根据爱好的增加而修改字段，这样操作起来很不方便，如果想获取每个学生的1个爱好，这样是没办法直接获取的，因为这些爱好是以字符串的形式保存在一个字段中的</span><br><span class="line">为了方便存储和使用，我们针对学生的爱好这种数据个数不固定的场景，可以使用数组的形式来存储</span><br><span class="line">测试数据是这样的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 hivedata]# more stu.data</span><br><span class="line">1 zhangsan swing,sing,coding</span><br><span class="line">2 lisi music,football</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">来建一张表，指定了一个array数组类型的字段叫favors，数组中存储字符串，数组中的元素怎么分割呢？通过 collection items terminated by &#39;,&#39; 指定的</span><br><span class="line"></span><br><span class="line">create table stu(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">favors array&lt;string&gt;</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;stu.data&#39; into ta</span><br><span class="line">Loading data to table default.stu</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.478 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">查询数组中的某一个元素，使用 arrayName[index]</span><br><span class="line">hive (default)&gt; select * from stu;</span><br><span class="line">OK</span><br><span class="line">stu.id stu.name stu.favors</span><br><span class="line">1 zhangsan [&quot;swing&quot;,&quot;sing&quot;,&quot;coding&quot;]</span><br><span class="line">2 lisi [&quot;music&quot;,&quot;football&quot;]</span><br><span class="line">Time taken: 1.547 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; select id,name,favors[1] from stu;</span><br><span class="line">OK</span><br><span class="line">id name _c2</span><br><span class="line"></span><br><span class="line">1 zhangsan sing</span><br><span class="line">2 lisi football</span><br><span class="line">Time taken: 0.631 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">角标是从0开始的，如果获取到了不存在的角标则返回null</span><br><span class="line">这就是Array类型的使用了</span><br></pre></td></tr></table></figure><h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面来说一下另外一种常见的集合——map，我们知道map集合里面存储的是键值对，每一个键值对属于Map集合的一个item，</span><br><span class="line">这里给大家举个例子，有两个学生zhangsan、lisi，每个学生有语文、数学、英语，成绩如下：</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# more stu2.data</span><br><span class="line">1 zhangsan chinese:80,math:90,english:100</span><br><span class="line">2 lisi chinese:89,english:70,math:88</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">针对学生的成绩信息最好也是存储到一个字段中，方便管理和使用，发现学生的成绩都是key-value类型的，所以非常适合使用map类型</span><br><span class="line">建表语句如下：指定scores字段类型为map格式</span><br><span class="line">通过 collection items terminated by &#39;,&#39; 指定了map中元素之间的分隔符</span><br><span class="line">通过 map keys terminated by &#39;:&#39; 指定了key和value之间的分隔符</span><br><span class="line"></span><br><span class="line">create table stu2(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">scores map&lt;string,int&gt;</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;stu2.data&#39; into t</span><br><span class="line">Loading data to table default.stu2</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.521 seconds</span><br><span class="line"></span><br><span class="line">查看表中的数据</span><br><span class="line">hive (default)&gt; select * from stu2;</span><br><span class="line">OK</span><br><span class="line">stu2.id stu2.name stu2.scores</span><br><span class="line">1 zhangsan &#123;&quot;chinese&quot;:80,&quot;math&quot;:90,&quot;english&quot;:100&#125;</span><br><span class="line">2 lisi &#123;&quot;chinese&quot;:89,&quot;english&quot;:70,&quot;math&quot;:88&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">查询所有学生的语文和数学成绩</span><br><span class="line">hive (default)&gt; select id,name,scores[&#39;chinese&#39;] as ch_score ,scores[&#39;math&#39;] as math_score from stu2;</span><br><span class="line"></span><br><span class="line">id name ch_score math_score</span><br><span class="line">1  zhangsan  80    90</span><br><span class="line">2 lisi       89    88</span><br><span class="line">Time taken: 0.232 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在这注意一下，我们取数据是根据元素中的key获取的，和map结构中元素的位置没有关系</span><br></pre></td></tr></table></figure><h4 id="Struct"><a href="#Struct" class="headerlink" title="Struct"></a>Struct</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">再来介绍最后一种复合类型struct，有点像java中的对象，举个例子说明一下，</span><br><span class="line">某学校有2个实习生，zhangsan、lisi，每个实习生都有地址信息，一个是户籍地所在的城市，一个是公司所在的城市，</span><br><span class="line">我们来组织一下数据</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# more stu3.data</span><br><span class="line">1 zhangsan bj,sh</span><br><span class="line">2 lisi gz,sz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">针对这里面的地址信息，不能懒省事使用字符串，否则后期想要获取他们对应的户籍地城市或者公司所在</span><br><span class="line">的城市信息时就比较麻烦了</span><br><span class="line">所以在这我们可以考虑使用Struct类型</span><br><span class="line">建表语句如下：</span><br><span class="line"></span><br><span class="line">create table stu3(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">address struct&lt;home_addr:string,office_addr:string&gt;</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;stu3.data&#39; into t</span><br><span class="line">Loading data to table default.stu3</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.447 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from stu3;</span><br><span class="line">OK</span><br><span class="line">stu3.id stu3.name stu3.address</span><br><span class="line">1 zhangsan &#123;&quot;home_addr&quot;:&quot;bj&quot;,&quot;office_addr&quot;:&quot;sh&quot;&#125;</span><br><span class="line">2 lisi &#123;&quot;home_addr&quot;:&quot;gz&quot;,&quot;office_addr&quot;:&quot;sz&quot;&#125;</span><br><span class="line">Time taken: 0.189 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; select id,name,address.home_addr from stu3;</span><br><span class="line">OK</span><br><span class="line">id name home_addr</span><br><span class="line">1 zhangsan bj</span><br><span class="line">2 lisi gz</span><br><span class="line">Time taken: 0.201 seconds, Fetched: 2 row(s)</span><br><span class="line"></span><br><span class="line">在这里大家会发现其实这个需求，我们使用Array也是可以搞定的吧，只不过是在查询的时候只能通过角</span><br><span class="line">标访问，不太方便而已。</span><br></pre></td></tr></table></figure><h4 id="Struct和Map的区别"><a href="#Struct和Map的区别" class="headerlink" title="Struct和Map的区别"></a>Struct和Map的区别</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">如果从建表语句上来分析，其实这个Struct和Map还是有一些相似之处的</span><br><span class="line">来总结一下：</span><br><span class="line">map中可以随意增加k-v对的个数</span><br><span class="line">struct中的k-v个数是固定的</span><br><span class="line">map在建表语句中需要指定k-v的类型</span><br><span class="line">struct在建表语句中需要指定好所有的属性名称和类型</span><br><span class="line">map中通过[]取值</span><br><span class="line">struct中通过.取值，类似java中的对象属性引用</span><br><span class="line">map的源数据中需要带有k-v</span><br><span class="line">struct的源数据中只需要有v即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">总体而言还是map比较灵活，但是会额外占用磁盘空间，因为他比struct多存储了数据的key</span><br><span class="line">struct只需要存储value，比较节省空间，但是灵活性有限，后期无法动态增加k-v</span><br></pre></td></tr></table></figure><h2 id="案例：复合数据类型的使用"><a href="#案例：复合数据类型的使用" class="headerlink" title="案例：复合数据类型的使用"></a>案例：复合数据类型的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在这里我们把前面学到的这三种复合数据类型结合到一块来使用一下。</span><br><span class="line">有一份数据是这样的</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# more student.data</span><br><span class="line">1 zhangsan english,sing,swing chinese:80,math:90,english:10</span><br><span class="line">2 lisi games,coding chinese:89,english:70,math:88 gz,sz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">根据这份数据建表，根据咱们前面的学习，这里面这几个字段分别是int类型、string类型，array类型，map类型，struct类型</span><br><span class="line">其实也不一定非要使用这些复合类型，主要是需要根据具体业务分析，使用复合数据类型可以更方便的操作数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table student (</span><br><span class="line">id int comment &#39;id&#39;,</span><br><span class="line">name string comment &#39;name&#39;,</span><br><span class="line">favors array&lt;string&gt; ,</span><br><span class="line">scores map&lt;string, int&gt;,</span><br><span class="line">address struct&lt;home_addr:string,office_addr:string&gt;</span><br><span class="line">) row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;student.data&#39; into table student;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from student;</span><br><span class="line">OK</span><br><span class="line">student.id student.name student.favors student.scores student.addre</span><br><span class="line">1 zhangsan [&quot;english&quot;,&quot;sing&quot;,&quot;swing&quot;] &#123;&quot;chinese&quot;:80,&quot;math&quot;:</span><br><span class="line">2 lisi [&quot;games&quot;,&quot;coding&quot;] &#123;&quot;chinese&quot;:89,&quot;english&quot;:70,&quot;math&quot;:88&#125;</span><br><span class="line">Time taken: 0.168 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">问：在mysql中有一张表student(id,name)，还有一张表address(stu_id,</span><br><span class="line">home,school)，还有联系方式表contact(stu_id,mine,parents,others)。如果把这三张表迁移到hive中，</span><br><span class="line">如何迁移？</span><br><span class="line">答：</span><br><span class="line">可以一一对应迁移，优点是迁移成本非常低，包括DDL和业务逻辑，几乎不需要修改，可以直接使用。缺点是产生大量的表连接，造成查询慢。</span><br><span class="line">可以一对多，mysql中的多张关联表可以创建为hive中的一张表。优点是减少表连接操作。缺点是迁移成本高，需要修改原有的业务逻辑。</span><br><span class="line">实际上，在我们日常的开发过程中遇到这样的问题，要想比较完美、顺利的解决，一般都分为两个阶段，</span><br><span class="line">第一个阶段，现在快捷迁移，就是上面说的一一对应，让我们的系统能跑起来，在此基础之上呢，再做一张大表，尽量包含以上所有字段，例如：</span><br><span class="line">stu(id, name, address struct&lt;home,school&gt;, contact struct&lt;…&gt;);</span><br><span class="line">等第二个阶段完工之后了，就可以跑在新的系统里面了。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第4章 Hive核心实战2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%203.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%203.html</id>
    <published>2022-02-20T04:01:53.000Z</published>
    <updated>2022-02-24T04:16:13.215Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第4章-Hive核心实战"><a href="#第八周-第4章-Hive核心实战" class="headerlink" title="第八周 第4章 Hive核心实战"></a>第八周 第4章 Hive核心实战</h1><a href="/hadoop%E5%AE%9E%E9%AA%8C%E8%AF%BE-hive.html" title="hadoop实验课-hive">hadoop实验课-hive</a><h2 id="Hive中的表类型"><a href="#Hive中的表类型" class="headerlink" title="Hive中的表类型"></a>Hive中的表类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在Mysql中没有表类型这个概念，因为它就只有一种表。</span><br><span class="line">但是Hive中是有多种表类型的，我们可以分为四种，内部表、外部表、分区表、桶表</span><br><span class="line">下面来一个一个学习一下这些类型的表</span><br></pre></td></tr></table></figure><h3 id="内部表"><a href="#内部表" class="headerlink" title="内部表"></a>内部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">首先看内部表</span><br><span class="line">内部表也可以称为受控表</span><br><span class="line">它是Hive中的默认表类型，表数据默认存储在 warehouse 目录中</span><br><span class="line">在加载数据的过程中，实际数据会被移动到warehouse目录中，就是咱们前面在使用load加载数据的时候，数据就会被加载到warehouse中表对应的目录中</span><br><span class="line">当我们删除表时，表中的数据和元数据将会被同时删除</span><br><span class="line">实际上，我们前面创建的表都属于受控表，前面我们已经演示了，创建一张表，其对应就，在metastore中存储表的元数据信息，当我们一旦从hive中删除一张表之后，表中的数据会被删除，在metastore中存储的元数据信息也会被删除。</span><br><span class="line">这就是内部表的特性。</span><br></pre></td></tr></table></figure><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">建表语句中包含 External 的表叫外部表</span><br><span class="line">外部表在加载数据的时候，实际数据并不会移动到warehouse目录中，只是与外部数据建立一个链接(映射关系)</span><br><span class="line">表的定义和数据的生命周期互相不约束，数据只是表对hdfs上的某一个目录的引用而已，当删除表定义的时候，数据依然是存在的。仅删除表和数据之间引用关系，所以这种表是比较安全的，就算是我们误删表了，数据还是没丢的</span><br><span class="line">我们来创建一张外部表，看一下外部表的建表语句该如何来写</span><br><span class="line">看一下官方文档</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">那根据这个格式我们自己来创建一个外部表</span><br><span class="line">create external table external_table (</span><br><span class="line">key string</span><br><span class="line">) location &#39;&#x2F;data&#x2F;external&#39;;</span><br><span class="line"></span><br><span class="line">表创建完以后到hdfs上查询，如果指定的目录不存在会自动创建</span><br><span class="line">此时到hdfs的 &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F; 目录下查看，是看不到这个表的目录的，因为这个表的目录是我们刚才通过location指定的目录</span><br><span class="line"></span><br><span class="line">我们再来看一下metastore中的tbls表，这里看到external_table的类型是外部表。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bCVdy9" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bCVdy9.md.png" alt="bCVdy9.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 hivedata]# more external_table.data</span><br><span class="line">a </span><br><span class="line">b </span><br><span class="line">c </span><br><span class="line">d </span><br><span class="line">e</span><br><span class="line"></span><br><span class="line">加载数据</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;external_table.dat</span><br><span class="line">Loading data to table default.external_table</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.364 seconds</span><br><span class="line"></span><br><span class="line">此时加载的数据会存储到hdfs的 &#x2F;data&#x2F;external 目录下</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bCZgA0" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bCZgA0.md.png" alt="bCZgA0.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来尝试删除这个表，看看会发生什么现象</span><br><span class="line">到hdfs上查看数据，发现之前上传上去的数据还在</span><br><span class="line"></span><br><span class="line">这个其实就是前面我们所的外部表的特性，外部表被删除时，只会删除表的元数据，表中的数据不会被删除。</span><br></pre></td></tr></table></figure><h4 id="内部表和外部表相互转化"><a href="#内部表和外部表相互转化" class="headerlink" title="内部表和外部表相互转化"></a>内部表和外部表相互转化</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">注意：实际上内外部表是可以互相转化的，需要我们做一下简单的设置即可。</span><br><span class="line">内部表转外部表</span><br><span class="line">alter table tblName set tblproperties (‘external’&#x3D;‘true’);</span><br><span class="line">外部表转内部表</span><br><span class="line">alter table tblName set tblproperties (‘external’&#x3D;‘false’);</span><br><span class="line">(不要根据数据是否在warehouse中，来判断是否是内外部表；外部表不指定location时，删除外部表时，warehouse里的数据也不会被删除；内部表也可指指定数据不放到warehouse里，但删除表时，数据还是会被删除)</span><br><span class="line">在实际工作中，我们在hive中创建的表95%以上的都是外部表</span><br><span class="line">因为大致流程是这样的，我们先通过flume采集数据，把数据上传到hdfs中，然后在hive中创建外部表和hdfs上的数据绑定关系，就可以使用sql查询数据了，所以连load数据那一步都可以省略了，因为是先有数据，才创建的表。</span><br><span class="line">画图分析一下。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bCn2Nt" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bCn2Nt.md.png" alt="bCn2Nt.md.png"></a></p><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><h4 id="单个分区字段"><a href="#单个分区字段" class="headerlink" title="单个分区字段"></a>单个分区字段</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  假设我们的web服务器每天都产生一个日志数据文件，Flume把数据采集到HDFS中，每一天的数据存储到一个日期目录中。我们如果想查询某一天的数据的话，hive执行的时候默认会对所有文件都扫描一遍，然后再过滤出来我们想要查询的那一天的数据</span><br><span class="line">如果你已经采集了一年的数据，这样每次计算都需要把一年的数据取出来，再过滤出来某一天的数据，效率就太低了，会非常浪费资源，所以我们可以让hive在查询的时候，根据你要查询的日期，直接定位到对应的日期目录。这样就可以直接查询满足条件的数据了，效率提升可不止一点点啊，是质的提升。</span><br><span class="line">想要实现这个功能，就需要使用分区表了</span><br><span class="line"></span><br><span class="line">  分区可以理解为分类，通过分区把不同类型的数据放到不同目录中</span><br><span class="line">  分区的标准就是指定分区字段，分区字段可以有一个或多个，根据咱们刚才举的例子，分区字段就是日期</span><br><span class="line">  分区表的意义在于优化查询，查询时尽量利用分区字段，如果不使用分区字段，就会全表扫描，最典型的一个场景就是把天作为分区字段，查询的时候指定天</span><br><span class="line">按照上面的分析，我们来创建一个分区表，使用partitioned by指定区分字段，分区字段的名称为dt，类型为string</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">create table partition_1 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) partitioned by (dt string)</span><br><span class="line">row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;;</span><br><span class="line"></span><br><span class="line">查看表的信息，可以看到分区信息</span><br><span class="line">hive (default)&gt; desc partition_1;</span><br><span class="line">OK</span><br><span class="line">col_name data_type comment</span><br><span class="line">id int</span><br><span class="line">name string</span><br><span class="line">dt string</span><br><span class="line"># Partition Information</span><br><span class="line"># col_name data_type comment</span><br><span class="line">dt string</span><br><span class="line">Time taken: 0.745 seconds, Fetched: 7 row(s)</span><br></pre></td></tr></table></figure><h5 id="加载数据时自动创建分区"><a href="#加载数据时自动创建分区" class="headerlink" title="加载数据时自动创建分区"></a>加载数据时自动创建分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">数据格式是这样的</span><br><span class="line">[root@bigdata04 hivedata]# more partition_1.data</span><br><span class="line">1 zhangsan</span><br><span class="line">2 lisi</span><br><span class="line"></span><br><span class="line">向分区表中加载数据【注意，在这里添加数据的同时需要指定分区信息】</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_1.data&#39; into table partition_1 partition(dt&#x3D;&#39;20200101&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">来查看一下hdfs中的信息，刚才创建的分区信息在hdfs中的体现是一个目录。</span><br><span class="line">由于这个分区表属于内部表， 所以目录还在warehouse这个目录中</span><br></pre></td></tr></table></figure><p><img src="C:%5CUsers%5CTy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220223165814509.png" alt="image-20220223165814509"></p><h5 id="手动创建分区"><a href="#手动创建分区" class="headerlink" title="手动创建分区"></a>手动创建分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">当然我也可以手动在表中只创建分区：</span><br><span class="line">hive (default)&gt; alter table partition_1 add partition (dt&#x3D;&#39;2020-01-02&#39;);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.295 seconds</span><br><span class="line"></span><br><span class="line">此时会发现hdfs中又多了一个目录，只不过这个分区目录中是没有数据的</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bClx0g" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bClx0g.md.png" alt="bClx0g.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">向这个分区中添加数据，可以使用刚才的load命令或者hdfs的put命令都可以</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_1.data&#39;</span><br><span class="line">Loading data to table default.partition_1 partition (dt&#x3D;2020-01-02)</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bC1YHe" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bC1YHe.md.png" alt="bC1YHe.md.png"></a></p><h5 id="查看表的分区"><a href="#查看表的分区" class="headerlink" title="查看表的分区"></a>查看表的分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如何查看我的表中目前有哪些分区呢，语法为： show partitions tblName</span><br><span class="line"></span><br><span class="line">hive (default)&gt; show partitions partition_1;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">dt&#x3D;2020-01-01</span><br><span class="line">dt&#x3D;2020-01-02</span><br><span class="line">Time taken: 0.246 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><h5 id="删除分区"><a href="#删除分区" class="headerlink" title="删除分区"></a>删除分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">那问题来了，刚才增加了一个分区，那我能删除一个分区吗？</span><br><span class="line">必须是可以的</span><br><span class="line"></span><br><span class="line">hive (default)&gt; alter table partition_1 drop partition(dt&#x3D;&#39;2020-01-02&#39;);</span><br><span class="line">Dropped the partition dt&#x3D;2020-01-02</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.771 seconds</span><br><span class="line">hive (default)&gt; show partitions partition_1;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">dt&#x3D;2020-01-01</span><br><span class="line">Time taken: 0.174 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，此时分区删除之后，分区中对应的数据也就没有了，因为是内部表，所以分区的数据是会被删掉的</span><br></pre></td></tr></table></figure><h4 id="多个分区字段"><a href="#多个分区字段" class="headerlink" title="多个分区字段"></a>多个分区字段</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">刚才呢，我们创建了一个分区，但是有的业务需求，需要创建多个分区，可以吗？</span><br><span class="line">当然是可以的！</span><br><span class="line">这里再举一个例子。某学校，有若干二级学院，每年都招很多学生，学校的统计需求大部分会根据年份和学院名称作为条件</span><br><span class="line">所以为了提高后期的统计效率，我们最好是使用年份和学院名称作为分区字段</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table partition_2 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) partitioned by (year int, school string)</span><br><span class="line">row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc partition_2;</span><br><span class="line">OK</span><br><span class="line">col_name data_type comment</span><br><span class="line">id int</span><br><span class="line">name string</span><br><span class="line">year int</span><br><span class="line">school string</span><br><span class="line"># Partition Information</span><br><span class="line"># col_name data_type comment</span><br><span class="line">year int</span><br><span class="line">school string</span><br><span class="line">Time taken: 0.097 seconds, Fetched: 9 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据文件内容</span><br><span class="line">[root@bigdata04 hivedata]# more partition_2.data</span><br><span class="line">1 zhangsan</span><br><span class="line">2 lisi</span><br><span class="line">3 wangwu</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：数据文件中只需要有id和name这两个字段的值就可以了，具体year和school这两个分区字段是在加载分区的时候指定的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_2.data&#39; into partition_2 patition(year&#x3D;2020, school&#x3D;&quot;xk&quot;)</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_2.data&#39; into partition_2 patition(year&#x3D;2020, school&#x3D;&quot;english&quot;)</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_2.data&#39; into partition_2 patition(year&#x3D;2019, school&#x3D;&quot;xk&quot;)</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_2.data&#39; into partition_2 patition(year&#x3D;2019, school&#x3D;&quot;english&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">查看分区信息</span><br><span class="line">hive (default)&gt; show partitions partition_2;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">year&#x3D;2019&#x2F;school&#x3D;english</span><br><span class="line">year&#x3D;2019&#x2F;school&#x3D;xk</span><br><span class="line">year&#x3D;2020&#x2F;school&#x3D;english</span><br><span class="line">year&#x3D;2020&#x2F;school&#x3D;xk</span><br></pre></td></tr></table></figure><h4 id="查询分区表"><a href="#查询分区表" class="headerlink" title="查询分区表"></a>查询分区表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">前面我们讲了如何创建、增加和删除分区</span><br><span class="line">还有一个比较重要的是我们该如何查询分区中的数据呢？其实非常简单，分区相当于我们的一个查询条</span><br><span class="line">件，直接跟在where后面就可以了。</span><br><span class="line"></span><br><span class="line">select * from partition_2; 【全表扫描，没有用到分区的特性】</span><br><span class="line">select * from partition_2 where year &#x3D; 2019;【用到了一个分区字段进行过滤】</span><br><span class="line">select * from partition_2 where year &#x3D; 2019 and school &#x3D; &#39;xk&#39;;【用到了两个分区字段进行过滤】</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是分区表的主要操作</span><br><span class="line">其实我们在这使用的分区表可以认为是内部分区表，内部分区表的应用场景也不多，外部分区表的应用场景才多，外部分区表就是在外部表的基础上又增加了分区。</span><br></pre></td></tr></table></figure><h4 id="外部分区表"><a href="#外部分区表" class="headerlink" title="外部分区表"></a>外部分区表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">外部分区表示工作中最常用的表</span><br><span class="line">我们先来创建一个外部分区表</span><br><span class="line"></span><br><span class="line">create external table ex_par(</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line">row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">location &#39;&#x2F;data&#x2F;ex_par&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">其它的操作和前面操作普通分区表是一样的，我们主要演示一下添加分区数据和删除分区的操作添加分区数据</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;ex_par.data&#39; into table ex_par partition(dt&#x3D;20200101)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive (default)&gt; show partitions ex_par;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">dt&#x3D;2020-01-01</span><br><span class="line">Time taken: 0.415 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">删除分区(删除后，此时hdfs上的分区目录还在)</span><br><span class="line">hive (default)&gt; alter table ex_par drop partition(dt&#x3D;&#39;2020-01-01&#39;);</span><br><span class="line">Dropped the partition dt&#x3D;2020-01-01</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.608 seconds</span><br><span class="line">hive (default)&gt; show partitions ex_par;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">Time taken: 0.229 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：此时分区目录的数据还是在的，因为这个是外部表，所以删除分区也只是删除分区的定义，分区中的数据还是在的，这个和内部分区表就不一样了</span><br><span class="line"></span><br><span class="line">虽然这个分区目录还在，但是刚才我们通过，show partitions 已经查不到分区信息了，所以查询表数据是查不出来的，虽然这个目录确实在这个表对应的hdfs目录中，但是由于这个是一个分区表，这份数据没有和任何分区绑定，所以就查询不出来</span><br><span class="line">这个一定要注意，在实际工作中新手最容易遇到的一个问题就是，针对分区表，通过hdfs的put命令把数据上传上去了，但是却查不到数据，就是因为没有在表中添加分区信息，也就是说你们现在虽然在一起了，但是还没有领结婚证，国家还不承认。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果数据已经上传上去了，如何给他们绑定关系呢？就是使用前面咱们讲的alter add partition命令，注</span><br><span class="line">意在这里需要通过location指定分区目录</span><br><span class="line">hive (default)&gt; alter table ex_par add partition(dt&#x3D;&#39;2020-01-01&#39;) location &#39;&#x2F;data&#x2F;ex_par&#x2F;dt&#x3D;20200101&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">此时再查询分区数据和表数据，就正常了。</span><br><span class="line">hive (default)&gt; show partitions ex_par;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">dt&#x3D;2020-01-01</span><br><span class="line">Time taken: 0.19 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; select * from ex_par;</span><br><span class="line">OK</span><br><span class="line">ex_par.id ex_par.name ex_par.dt</span><br><span class="line">1 zhangsan 2020-01-01</span><br><span class="line">2 lisi 2020-01-01</span><br><span class="line">3 wangwu 2020-01-01</span><br><span class="line">Time taken: 0.432 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;ex_par.data&#39; into table ex_par  partition(dt&#x3D;&#39;20200101&#39;);</span><br><span class="line">load data .... partition 这条命令做了两个事情，1：上传数据，2：添加分区(绑定数据和分区之间的关系)</span><br><span class="line"></span><br><span class="line">hdfs dfs -mkdir &#x2F;data&#x2F;ex_par&#x2F;20200101</span><br><span class="line">hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;ex_par.data &#x2F;data&#x2F;ex_par&#x2F;20200101</span><br><span class="line"></span><br><span class="line">alter table ex_par add partition(dt&#x3D;&#39;20200101&#39;) location &#39;&#x2F;data&#x2F;ex_par&#x2F;dt&#x3D;20200101&#39;;</span><br><span class="line">上面这三条命令做了两件事情，1：上传数据 2：添加分区(绑定数据和分区之间的关系)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">外部分区表是工作中最常见的表</span><br></pre></td></tr></table></figure><h3 id="桶表"><a href="#桶表" class="headerlink" title="桶表"></a>桶表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">桶表是对数据进行哈希取值，然后放到不同文件中存储</span><br><span class="line">物理上，每个桶就是表(或分区）里的一个文件</span><br><span class="line">什么时候会用到桶表呢？</span><br><span class="line">举个例子，针对中国的人口，主要集中河南、江苏、山东、广东、四川，其他省份就少的多了，你像西藏</span><br><span class="line">就三四百万，海南也挺少的，如果使用分区表，我们把省份作为分区字段，数据会集中在某几个分区，其</span><br><span class="line">他分区数据就不会很多，那这样对数据存储以及查询不太友好，在计算的时候会出现数据倾斜的问题，计</span><br><span class="line">算效率也不高，我们应该相对均匀的存放数据，从源头上解决，这个时候我们就可以采用分桶的概念，也</span><br><span class="line">就是使用桶表</span><br></pre></td></tr></table></figure><h4 id="创建桶表"><a href="#创建桶表" class="headerlink" title="创建桶表"></a>创建桶表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下面来建立一个桶表：</span><br><span class="line">这个表的意思是按照id进行分桶，分成4个桶。</span><br><span class="line">create table bucket_tb(</span><br><span class="line">id int</span><br><span class="line">) clustered by (id) into 4 buckets;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create table bucket_tb(</span><br><span class="line">id int</span><br><span class="line">) clustered by (id) into 4 buckets;</span><br><span class="line"></span><br><span class="line">这个时候往桶中加载数据的时候，就不能使用load data的方式了，而是需要使用其它表中的数据，那么给桶表加载数据的写法就有新的变化了。</span><br><span class="line">类似这样的写法</span><br><span class="line">insert into table … select … from …;</span><br><span class="line">注意，在插入数据之前需要先设置开启桶操作，不然数据无法分到不同的桶里面</span><br><span class="line">其实这里的分桶就是设置reduce任务的数量，因为你分了多少个桶，最终结果就会产生多少个文件，最终结果中文件的数量就和reduce任务的数量是挂钩的</span><br><span class="line">设置完 set hive.enforce.bucketing &#x3D; true 可以自动控制reduce的数量从而适配bucket的个数</span><br><span class="line"></span><br><span class="line">hive (default)&gt; set hive.enforce.bucketing&#x3D;true;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">初始化一个表，用于向桶表中加载数据</span><br><span class="line">原始数据文件是这样的</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# more b_source.data</span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5 </span><br><span class="line">6 </span><br><span class="line">7 </span><br><span class="line">8 </span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table b_source(id int);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.262 seconds</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;b_source.data&#39; into table b_source;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from b_source;</span><br><span class="line">OK</span><br><span class="line">b_source.id</span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5 </span><br><span class="line">6 </span><br><span class="line">7 </span><br><span class="line">8 </span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">Time taken: 0.187 seconds, Fetched: 12 row(s)</span><br></pre></td></tr></table></figure><h4 id="向桶表加载数据"><a href="#向桶表加载数据" class="headerlink" title="向桶表加载数据"></a>向桶表加载数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">向桶表中加载数据</span><br><span class="line">hive (default)&gt; insert into table bucket_tb select id from b_source where id !&#x3D; NULL;</span><br></pre></td></tr></table></figure><h4 id="查看结果"><a href="#查看结果" class="headerlink" title="查看结果"></a>查看结果</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from bucket_tb;</span><br><span class="line">OK</span><br><span class="line">bucket_tb.id</span><br><span class="line">12</span><br><span class="line">8 </span><br><span class="line">4 </span><br><span class="line">9 </span><br><span class="line">5 </span><br><span class="line">1</span><br><span class="line">10</span><br><span class="line">6 </span><br><span class="line">2</span><br><span class="line">11</span><br><span class="line">7 </span><br><span class="line">3</span><br><span class="line">Time taken: 0.183 seconds, Fetched: 12 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">按照我们设置的桶的数量为4，这样在hdfs中会存在4个对应的文件，每个文件的大小是相似的</span><br></pre></td></tr></table></figure><p><img src="C:%5CUsers%5CTy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220223230539806.png" alt="image-20220223230539806"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到hdfs上查看桶表中的文件内容，可以看出是通过对buckets取模确定的</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bPt0SJ" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bPt0SJ.md.png" alt="bPt0SJ.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这样就实现了数据分桶存储。</span><br><span class="line">桶表的主要作用：</span><br></pre></td></tr></table></figure><h4 id="桶表的作用"><a href="#桶表的作用" class="headerlink" title="桶表的作用"></a>桶表的作用</h4><h5 id="数据抽样"><a href="#数据抽样" class="headerlink" title="数据抽样"></a>数据抽样</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据抽样</span><br><span class="line">假如我们使用的是一个大规模的数据集,我们只想去抽取部分数据进行查看.使用bucket表可以变得更加的高效</span><br><span class="line">select * from bucket_tb tablesample(bucket 1 out of 4 on id);</span><br><span class="line">tablesample是抽样语句</span><br><span class="line">语法解析：TABLESAMPLE(BUCKET x OUT OF y ON column)</span><br><span class="line">y尽可能是桶表的bucket数的倍数或者因子，而且y必须要大于等于x</span><br><span class="line">y表示是把桶表中的数据随机分为多少桶</span><br><span class="line">x表示取出第几桶的数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bucket 1 out of 4 on id：根据id对桶表中的数据重新分桶，分成4桶，取出第1桶的数据</span><br><span class="line">bucket 2 out of 4 on id：根据id对桶表中的数据重新分桶，分成4桶，取出第2桶的数据</span><br><span class="line">bucket 3 out of 4 on id：根据id对桶表中的数据重新分桶，分成4桶，取出第3桶的数据</span><br><span class="line">bucket 4 out of 4 on id：根据id对桶表中的数据重新分桶，分成4桶，取出第4桶的数据</span><br><span class="line">验证一下效果，这里面四个SQL语句，每个SQL语句取出一个桶的数据，最终的总和就是表中的所有数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from bucket_tb tablesample(bucket 1 out of 4 on id);</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from bucket_tb tablesample(bucket 2 out of 4 on id);</span><br></pre></td></tr></table></figure><h5 id="提高某些查询效率"><a href="#提高某些查询效率" class="headerlink" title="提高某些查询效率"></a>提高某些查询效率</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">例如：join查询,可以避免产生笛卡尔积的操作</span><br><span class="line">select a.id,a.name,b.addr from a join b on a.id &#x3D; b.id;</span><br><span class="line">如果a表和b表已经是分桶表，而且分桶的字段是id字段，那么做这个操作的时候就不需要再进行全表笛卡尔积了，因为分桶之后相同规则的id已经在相同的文件里面了。</span><br></pre></td></tr></table></figure><h3 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hive中，也有视图的概念，那我们都知道视图实际上是一张虚拟的表，是对数据的逻辑表示，它的主要作用是为了降低查询的复杂度。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那我们在Hive中如何来创建一个视图呢？</span><br><span class="line">需要使用create view命令，</span><br><span class="line">下面我们来创建一个视图</span><br></pre></td></tr></table></figure><h4 id="创建视图"><a href="#创建视图" class="headerlink" title="创建视图"></a>创建视图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create view v1 as select t3_new.id,t3_new.stu_name from t3_new;</span><br><span class="line"></span><br><span class="line">此时通过 show tables 也可以查看到这个视图</span><br><span class="line">hive (default)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">b_source</span><br><span class="line">bucket_tb</span><br><span class="line">ex_par</span><br><span class="line">partition_1</span><br><span class="line">partition_2</span><br><span class="line">stu</span><br><span class="line">stu2</span><br><span class="line">stu3</span><br><span class="line">student</span><br><span class="line">t1</span><br><span class="line">t2</span><br><span class="line">t2_bak</span><br><span class="line">t3</span><br><span class="line">t3_new</span><br><span class="line">v1</span><br></pre></td></tr></table></figure><h4 id="查询视图"><a href="#查询视图" class="headerlink" title="查询视图"></a>查询视图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">查看视图的结构，显示的内容和表显示的内容是没有区别的</span><br><span class="line"></span><br><span class="line">hive (default)&gt; desc v1;</span><br><span class="line">OK</span><br><span class="line">col_name data_type comment</span><br><span class="line">id int</span><br><span class="line">stu_name string</span><br><span class="line">Time taken: 0.041 seconds, Fetched: 2 row(s)</span><br><span class="line"></span><br><span class="line">通过视图查询数据</span><br><span class="line">hive (default)&gt; select * from v1;</span><br><span class="line">OK</span><br><span class="line">v1.id v1.stu_name</span><br><span class="line">1 张三</span><br><span class="line">2 李四</span><br><span class="line">3 王五</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：视图在&#x2F;user&#x2F;hive&#x2F;warehouse中是不存在的。因为它只是一个虚拟的表</span><br><span class="line"></span><br><span class="line">在元数据metastore中的体现</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/biY1Tf" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/24/biY1Tf.md.png" alt="biY1Tf.md.png"></a></p><h4 id="删除视图"><a href="#删除视图" class="headerlink" title="删除视图"></a>删除视图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop view v1;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第3章 Hive基础使用</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Hive%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Hive%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8.html</id>
    <published>2022-02-20T04:01:26.000Z</published>
    <updated>2022-02-22T04:12:08.754Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第3章-Hive基础使用"><a href="#第八周-第3章-Hive基础使用" class="headerlink" title="第八周 第3章 Hive基础使用"></a>第八周 第3章 Hive基础使用</h1><h2 id="Hive的使用方式"><a href="#Hive的使用方式" class="headerlink" title="Hive的使用方式"></a>Hive的使用方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">操作Hive可以在Shell命令行下操作，或者是使用JDBC代码的方式操作</span><br><span class="line">下面先来看一下在命令行中操作的方式</span><br></pre></td></tr></table></figure><h3 id="命令行方式"><a href="#命令行方式" class="headerlink" title="命令行方式"></a>命令行方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">针对命令行这种方式，其实还有两种使用</span><br><span class="line">第一个是使用bin目录下的hive命令，这个是从hive一开始就支持的使用方式</span><br><span class="line">后来又出现一个beeline命令，它是通过HiveServer2服务连接hive，它是一个轻量级的客户端工具，所以</span><br><span class="line">后来官方开始推荐使用这个。</span><br><span class="line">具体使用哪个我觉得属于个人的一个习惯问题，特别是一些做了很多年大数据开发的人，已经习惯了使用</span><br><span class="line">hive命令，如果让我使用beeline会感觉有点别扭</span><br><span class="line">针对我们写的hive sql通过哪一种客户端去执行结果都是一样的，没有任何区别，所以在这里我们使用哪</span><br><span class="line">个就无所谓了。</span><br></pre></td></tr></table></figure><h4 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">先看第一种，这种直接就可以连进去</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;hive</span><br><span class="line"></span><br><span class="line">这里有一行信息提示，从Hive2开始Hive-on-MR就过时了，并且在以后的版本中可能就不维护了，建议</span><br><span class="line">使用其它的计算引擎，例如：spark或者tez</span><br><span class="line">如果你确实想使用MapReduce引擎，那建议你使用Hive1.x的版本。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">hive&gt; create table t1(id int,name string);</span><br><span class="line"></span><br><span class="line">hive&gt; insert into t1(id,name) values(1,&quot;zs&quot;);</span><br><span class="line">此时会产生mapreduce任务</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from t1;</span><br><span class="line"></span><br><span class="line">查询数据，为什么这时没有产生mapreduce任务呢？因为这个计算太简单了，不需要经过mapreduce任</span><br><span class="line">务就可以获取到结果，直接读取表对应的数据文件就可以了。</span><br><span class="line"></span><br><span class="line">hive&gt; drop table t1;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">退出</span><br><span class="line">exit;</span><br><span class="line">quit;</span><br><span class="line">ctrl C</span><br></pre></td></tr></table></figure><h4 id="beeline"><a href="#beeline" class="headerlink" title="beeline"></a>beeline</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.启动hiveserver2服务</span><br><span class="line">&#x2F;bin&#x2F;hiveserver2</span><br><span class="line">2.bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">注意了，启动hiveserver2服务之后，最下面会输出几行Hive Session ID的信息，一定要等到输出4行以后再使用beeline去连接，否则会提示连接拒绝</span><br><span class="line"></span><br><span class="line">当hiveserver2服务没有真正启动成功之前连接会提示这样的信息</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost</span><br><span class="line">Connecting to jdbc:hive2:&#x2F;&#x2F;localhost:10000</span><br><span class="line">20&#x2F;05&#x2F;06 16:44:21 [main]: WARN jdbc.HiveConnection: Failed to connect to loca</span><br><span class="line">Could not open connection to the HS2 server. Please check the server URI and</span><br><span class="line">Error: Could not open client transport with JDBC Uri: jdbc:hive2:&#x2F;&#x2F;localhost:</span><br><span class="line">Beeline version 3.1.2 by Apache Hive</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">等待hiveserver2服务真正启动之后再连接，此时就可以连接进去了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;localhost:10000&gt; create table t1(id int,name string);</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;localhost:10000&gt; insert into t1(id,name) values(1,&quot;zs&quot;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">发现添加数据报错，提示匿名用户对&#x2F;tmp&#x2F;hadoop-yarn没有写权限</span><br><span class="line">解决方法有两个</span><br><span class="line">1. 给hdfs中的&#x2F;tmp&#x2F;hadoop-yarn设置777权限，让匿名用户具备权限</span><br><span class="line">可以直接给tmp及下面的所有目录设置777权限</span><br><span class="line">hdfs dfs -chmod -R 777 &#x2F;tmp</span><br><span class="line">2. 在启动beeline的时候指定一个对这个目录有操作权限的用户</span><br><span class="line">bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000 -n root</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时使用bin&#x2F;hive命令行查看也是可以的，这两种方式维护的是同一份Metastore</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在beeline后面指定hiveserver2的地址的时候，可以指定当前机器的内网ip也是可以的。(其它机器也可连)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">后面我们使用的时候我还是使用hive命令，已经习惯用这个了，还有一个就是大家如果也用这个的话，别</span><br><span class="line">人是不是感觉你也是老司机了，但是你要知道官方目前是推荐使用beeline命令的</span><br><span class="line">在工作中我们如果遇到了每天都需要执行的命令，那我肯定想要把具体的执行sql写到脚本中去执行，但</span><br><span class="line">是现在这种用法每次都需要开启一个会话，好像还没办法把命令写到脚本中。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意了，hive后面可以使用 -e 命令，这样这条hive命令就可以放到脚本中定时调度执行了因为这样每次hive都会开启一个新的会话，执行完毕以后再关闭这个会话。</span><br><span class="line"></span><br><span class="line">当然了beeline也可以，后面也是跟一个-e参数</span><br></pre></td></tr></table></figure><h3 id="JDBC方式"><a href="#JDBC方式" class="headerlink" title="JDBC方式"></a>JDBC方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">JDBC这种方式也需要连接hiveserver2服务，前面我们已经启动了hiveserver2服务，在这里直接使用就可以了</span><br><span class="line">创建maven项目 db_hive</span><br><span class="line">在pom中添加hive-jdbc的依赖(百度mvn可以下载依赖)</span><br><span class="line"></span><br><span class="line">&lt;!-- hive-jdbc驱动 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-jdbc&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;version&gt;3.1.2&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发代码，创建包名： com.imooc.hive</span><br><span class="line">创建类名： HiveJdbcDemo</span><br></pre></td></tr></table></figure><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.hive;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.Statement;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * JDBC代码操作 Hive</span></span><br><span class="line"><span class="comment"> * 注意：需要先启动hiveserver2服务</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveJdbcDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="comment">//指定hiveserver2的连接</span></span><br><span class="line">        String jdbcUrl = <span class="string">"jdbc:hive2://192.168.206.132:10000"</span>;</span><br><span class="line">    <span class="comment">//获取jdbc连接，这里的user使用root，就是linux中的用户名，password随便指定即</span></span><br><span class="line">        Connection conn = DriverManager.getConnection(jdbcUrl, <span class="string">"root"</span>, <span class="string">"any"</span>)</span><br><span class="line">    <span class="comment">//获取Statement</span></span><br><span class="line">        Statement stmt = conn.createStatement();</span><br><span class="line">    <span class="comment">//指定查询的sql</span></span><br><span class="line">        String sql = <span class="string">"select * from t1"</span>;</span><br><span class="line">    <span class="comment">//执行sql</span></span><br><span class="line">        ResultSet res = stmt.executeQuery(sql);</span><br><span class="line">    <span class="comment">//循环读取结果</span></span><br><span class="line">        <span class="keyword">while</span> (res.next())&#123;</span><br><span class="line">            System.out.println(res.getInt(<span class="string">"id"</span>)+<span class="string">"\t"</span>+res.getString(<span class="string">"name"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;D:&#x2F;.m2&#x2F;org&#x2F;apache&#x2F;logging&#x2F;log4j&#x2F;log4j-slf4</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;D:&#x2F;.m2&#x2F;org&#x2F;slf4j&#x2F;slf4j-log4j12&#x2F;1.6.1&#x2F;slf4j</span><br><span class="line">SLF4J: See http:&#x2F;&#x2F;www.slf4j.org&#x2F;codes.html#multiple_bindings for an explanati</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory</span><br><span class="line"></span><br><span class="line">ERROR StatusLogger No log4j2 configuration file found. Using default configur</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">分析上面的警告信息，发现现在是有两个log4j的实现类，需要去掉一个，还有就是缺少log4j2的配置文</span><br><span class="line">件，注意log4j2的配置文件是xml格式的，不是properties格式的</span><br><span class="line">1: 去掉多余的log4j依赖，从日志中可以看到日志的路径</span><br><span class="line">这两个去掉哪个都可以，这两个都是hive-jdbc这个依赖带过来的，所以需要修改pom文件中hive-jdbc中</span><br><span class="line">的依赖</span><br><span class="line">&lt;!-- hive-jdbc驱动 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-jdbc&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;version&gt;3.1.2&lt;&#x2F;version&gt;</span><br><span class="line">&lt;exclusions&gt;</span><br><span class="line">&lt;!-- 去掉 log4j依赖 --&gt;</span><br><span class="line">&lt;exclusion&gt;</span><br><span class="line">&lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">&lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;exclusion&gt;</span><br><span class="line">&lt;&#x2F;exclusions&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">2：在项目的resources目录中增加log4j2.xml配置文件</span><br><span class="line"></span><br><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;Configuration status&#x3D;&quot;INFO&quot;&gt;</span><br><span class="line">&lt;Appenders&gt;</span><br><span class="line">&lt;Console name&#x3D;&quot;Console&quot; target&#x3D;&quot;SYSTEM_OUT&quot;&gt;</span><br><span class="line">&lt;PatternLayout pattern&#x3D;&quot;%d&#123;YYYY-MM-dd HH:mm:ss&#125; [%t] %-5p %c&#123;1&#125;:%</span><br><span class="line">&lt;&#x2F;Console&gt;</span><br><span class="line">&lt;&#x2F;Appenders&gt;</span><br><span class="line">&lt;Loggers&gt;</span><br><span class="line">&lt;Root level&#x3D;&quot;info&quot;&gt;</span><br><span class="line">&lt;AppenderRef ref&#x3D;&quot;Console&quot; &#x2F;&gt;</span><br><span class="line">&lt;&#x2F;Root&gt;</span><br><span class="line">&lt;&#x2F;Loggers&gt;</span><br><span class="line">&lt;&#x2F;Configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="Set命令的使用"><a href="#Set命令的使用" class="headerlink" title="Set命令的使用"></a>Set命令的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在hive命令行中可以使用set命令临时设置一些参数的值，</span><br><span class="line">其实就是临时修改hive-site.xml中参数的值</span><br><span class="line">不过通过set命令设置的参数只在当前会话有效，退出重新打开就无效了</span><br><span class="line">如果想要对当前机器上的当前用户有效的话可以把命令配置在 ~&#x2F;.hiverc文件中</span><br><span class="line">所以总结一下，使用set命令配置的参数是当前会话有效，在~&#x2F;.hiverc文件中配置的是当前机器中的当前用户有效，而在hive-site.xml中配置的则是永久有效了，</span><br><span class="line">在hive-site.xml中有一个参数是 hive.cli.print.current.db ，这个参数可以显示当前所在的数据库名称，默认值为 false 。</span><br></pre></td></tr></table></figure><h4 id="显示数据库名"><a href="#显示数据库名" class="headerlink" title="显示数据库名"></a>显示数据库名</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.cli.print.current.db &#x3D; true;</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure><h4 id="显示列属性"><a href="#显示列属性" class="headerlink" title="显示列属性"></a>显示列属性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from t1;</span><br><span class="line">OK</span><br><span class="line">1 zs</span><br><span class="line">Time taken: 0.184 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; set hive.cli.print.header &#x3D; true;</span><br><span class="line">hive (default)&gt; select * from t1;</span><br><span class="line">OK</span><br><span class="line">t1.id t1.name</span><br><span class="line">1 zs</span><br><span class="line">Time taken: 0.202 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">些参数属于我自己的个人习惯，所以我希望把这个配置放到我个人用户下面</span><br><span class="line">修改 ~&#x2F;.hiverc ，我们每次在进入hive命令行的时候都会加载当前用户目录下的 .hiverc 文件中的内容</span><br></pre></td></tr></table></figure><h4 id="历史命令"><a href="#历史命令" class="headerlink" title="历史命令"></a>历史命令</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如果我们想查看一下hive的历史操作命令如何查看呢？</span><br><span class="line">linux中有一个history命令可以查看历史操作命令hive中也有类似的功能，hive中的历史命令会存储在当前用户目录下的 .hivehistory 目录中</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# more ~&#x2F;.hivehistory</span><br><span class="line">show tables;</span><br><span class="line">exit</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><h2 id="Hive的日志配置"><a href="#Hive的日志配置" class="headerlink" title="Hive的日志配置"></a>Hive的日志配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;lib&#x2F;log4jSLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0&#x2F;share&#x2F;hadoop&#x2F;common</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">我们每次进入hive命令行的时候都会出现这么一坨日志，看着很恶心，想要去掉，怎么办呢？</span><br><span class="line">通过分析日志可知，现在也是有重复的日志依赖，所以需要删除一个，</span><br><span class="line">这里是hive中的一个日志依赖包和hadoop中的日志依赖包冲入了，那我们只能去掉Hive的了，因为hadoop是共用的，尽量不要删它里面的东西。</span><br><span class="line">为了保险起见，我们可以使用mv给这个日志依赖包重命名，这样它就不生效了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">还有就是当我们遇到Hive执行发生错误的时候，我们要学会去查看Hive的日志信息，通过日志的提示来分析，找到错误的根源，帮助我们及时解决错误。</span><br><span class="line">那我们在哪里查看Hive日志呢，我们可以通过配置文件来找到默认日志文件所在的位置。</span><br><span class="line">在hive的conf目录下有一些log4j的模板配置文件，我们需要去修改一下，让它生效。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">首先是 hive-log4j.properties.template 这个文件，去掉 .template 后缀，修改里面的 property.hive.log.level 和 property.hive.log.dir 参数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# mv hive-log4j2.properties.template hive-log4j2.propert</span><br><span class="line">[root@bigdata04 conf]# vi hive-log4j2.properties</span><br><span class="line">property.hive.log.level &#x3D; WARN</span><br><span class="line">property.hive.root.logger &#x3D; DRFA</span><br><span class="line">property.hive.log.dir &#x3D; &#x2F;data&#x2F;hive_repo&#x2F;log</span><br><span class="line">property.hive.log.file &#x3D; hive.log</span><br><span class="line">property.hive.perflogger.log.level &#x3D; INFO</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">然后修改 hive-exec-log4j2.properties.template 这个文件，去掉 .template 后缀修改里面的</span><br><span class="line"></span><br><span class="line">[root@bigdata04 conf]# mv hive-exec-log4j2.properties.template hive-exec-log4</span><br><span class="line">[root@bigdata04 conf]# vi hive-exec-log4j2.properties</span><br><span class="line">property.hive.log.level &#x3D; WARN</span><br><span class="line">property.hive.root.logger &#x3D; FA</span><br><span class="line">property.hive.query.id &#x3D; hadoop</span><br><span class="line">property.hive.log.dir &#x3D; &#x2F;data&#x2F;hive_repo&#x2F;log</span><br><span class="line">property.hive.log.file &#x3D; $&#123;sys:hive.query.id&#125;.log</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这样后期分析日志就可以到 &#x2F;data&#x2F;hive_repo&#x2F;log 目录下去查看了。</span><br><span class="line"></span><br><span class="line">提交任务后，在mapreduce产生的一些日志，还是要去yarn的weibu界面看</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第2章 数据库与数据仓库的区别</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB.html</id>
    <published>2022-02-20T04:01:04.000Z</published>
    <updated>2022-02-21T08:47:05.460Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第2章-数据库与数据仓库的区别"><a href="#第八周-第2章-数据库与数据仓库的区别" class="headerlink" title="第八周 第2章 数据库与数据仓库的区别"></a>第八周 第2章 数据库与数据仓库的区别</h1><h2 id="Hive-VS-Mysql"><a href="#Hive-VS-Mysql" class="headerlink" title="Hive VS Mysql"></a>Hive VS Mysql</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">为了加深对Hive的理解，下面我们拿Hive和我们经常使用的Mysql做一个对比</span><br><span class="line"></span><br><span class="line">           HIVE MySQL</span><br><span class="line">数据存储位置 HDFS 本地磁盘</span><br><span class="line">数据格式 用户定义 系统决定</span><br><span class="line">数据更新 不支持(不支持修改和删除,新增) 支持(支持增删</span><br><span class="line">索引   有，但较弱，一般很少用 有，经常使用</span><br><span class="line">执行   MapReduce Executor</span><br><span class="line">执行延迟 高 低</span><br><span class="line">可扩展性 高 低</span><br><span class="line">数据规模 大 小</span><br></pre></td></tr></table></figure><h2 id="数据库-VS-数据仓库"><a href="#数据库-VS-数据仓库" class="headerlink" title="数据库 VS 数据仓库"></a>数据库 VS 数据仓库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">前面我们说了Hive是一个数据仓库，咱们平时经常使用的mysql属于数据库，那数据库和数据仓库到底有什么区别呢？</span><br><span class="line">下面我们来分析一下</span><br><span class="line">数据库：传统的关系型数据库主要应用在基本的事务处理，例如银行交易之类的场景</span><br><span class="line">数据库支持增删改查这些常见的操作。</span><br><span class="line">数据仓库：主要做一些复杂的分析操作，侧重决策支持，相对数据库而言，数据仓库分析的数据规模要大得多。但是数据仓库只支持查询操作，不支持修改和删除</span><br><span class="line">这些都是明面上的一些区别</span><br><span class="line">其实数据库与数据仓库的本质区别就是 OLTP与OLAP 的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive和关系数据库存储文件的系统不同，hive使用的是hadoop的HDFS（hadoop的分布式文件系统），关系数据库则是服务器本地的文件 系统</span><br></pre></td></tr></table></figure><h3 id="OLTP-VS-OLAP"><a href="#OLTP-VS-OLAP" class="headerlink" title="OLTP VS OLAP"></a>OLTP VS OLAP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那这里的OLTO和OLAP又是什么意思呢？</span><br><span class="line">OLTP(On-Line Transaction Processing)：操作型处理，称为联机事务处理，也可以称为面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性等问题</span><br><span class="line">OLAP(On-Line Analytical Processing)：分析型处理，称为联机分析处理，一般针对某些主题历史数据进行分析，支持管理决策。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">其实从字面上来对比，OLTP 和 OLAP 只有一个单词不一样</span><br><span class="line">OLTP侧重于事务，OLAP侧重于分析</span><br><span class="line">所以数据库和数据仓库的特性是不一样的，不过我们平时在使用的时候，可以把Hive作为一个数据库来操作，但是你要知道他们两个是不一样的。数据仓库的概念是比数据库要大的</span><br></pre></td></tr></table></figure><h2 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">现在我们对Hive有了基本的了解，就想使用Hive分析一下HDFS中的数据，在分析数据之前，我们需要先把Hive安装部署起来</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">想要安装Hive，那首先要下载Hive的安装包，进入Hive的官网，找到download下载链接</span><br><span class="line"></span><br><span class="line">发现目前hive主要有三大版本，Hive1.x、Hive2.x、Hive3.x</span><br><span class="line">Hive1.x已经2年没有更新了，所以这个版本后续基本不会再维护了，不过这个版本已经迭代了很多年了，也是比较稳定的</span><br><span class="line">Hive2.x最近一直在更新</span><br><span class="line">Hive3.x上次是19年8月份更新的，也算是一直在维护</span><br><span class="line">那我们到底选择哪个版本呢？注意了，在选择Hive版本的时候我们需要注意已有的Hadoop集群的版本。</span><br><span class="line">因为Hive会依赖于Hadoop，所以版本需要兼容才可以。</span><br><span class="line">具体Hive和Hadoop的版本对应关系可以在download页面下的news列表里面看到。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">按照这里面说的hive2.x的需要在hadoop2.x版本中运行，hive3.x的需要在hadoop3.x版本中运行。</span><br><span class="line">所以在这里我们最好是使用Hive3.x的版本</span><br><span class="line">那我们就下载hive-3.1.2这个版本，如果想要下载其它历史版本的话这里面还找不到，不过可以使用</span><br><span class="line">apache的一个通用archive地址</span><br><span class="line">https:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;hive&#x2F;</span><br><span class="line">在这里面就可以找到hive的所有历史版本了</span><br></pre></td></tr></table></figure><h3 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面开始安装Hive</span><br><span class="line">Hive相当于Hadoop的客户端工具，安装时不一定非要放在集群的节点中，可以放在任意一个集群客户端节点上都可以</span><br><span class="line">1.下载，上传，解压</span><br><span class="line">2.修改配置文件</span><br><span class="line">需要修改配置文件，进入hive的conf目录中，先对这两个模板文件重命名</span><br><span class="line"></span><br><span class="line">[root@bigdata04 conf]# mv hive-env.sh.template hive-env.sh</span><br><span class="line">[root@bigdata04 conf]# mv hive-default.xml.template hive-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">3：然后再修改这两个文件的内容</span><br><span class="line">注意：在 hive-env.sh 文件的末尾直接增加下面三行内容，【根据实际的路径配置】</span><br><span class="line"></span><br><span class="line">[root@bigdata04 conf]# vi hive-env.sh</span><br><span class="line">.....</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注意：在hive-site.xml文件中根据下面property中的name属性的值修改对应value的值，这些属</span><br><span class="line">性默认里面都是有的，所以都是修改对应的value的值即可</span><br><span class="line">由于这里面需要指定Metastore的地址，Metastore我们使用Mysql，所以需要大家提前安装好</span><br><span class="line">Mysql，我这里使用的是Mysql8.0.16版本，Mysql安装包会提供给大家，建议大家直接在自己的</span><br><span class="line">windows机器中安装Mysql即可，当然了，你在Linux中安装也可以。</span><br><span class="line">我这里Mysql的用户名是root、密码是admin，在下面的配置中会使用到这些信息，大家可以根据</span><br><span class="line">自己实际的用户名和密码修改这里面的value的值</span><br></pre></td></tr></table></figure><h3 id="mysql安装"><a href="#mysql安装" class="headerlink" title="mysql安装"></a>mysql安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于这里面需要指定Metastore的地址，Metastore我们使用Mysql，所以需要大家提前安装好Mysql，我这里使用的是Mysql8.0.16版本，Mysql安装包会提供给大家，建议大家直接在自己的windows机器中安装Mysql即可，当然了，你在Linux中安装也可以。</span><br></pre></td></tr></table></figure><a href="/mysql%E5%AE%89%E8%A3%85.html" title="mysql安装">mysql安装</a><h3 id="hive配置修改"><a href="#hive配置修改" class="headerlink" title="hive配置修改"></a>hive配置修改</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我这里Mysql的用户名是root、密码是admin，在下面的配置中会使用到这些信息，大家可以根据自己实际的用户名和密码修改这里面的value的值</span><br></pre></td></tr></table></figure><h4 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面这个ip应该错了，应该填自己主机ip；并在mysql中创建hive数据库</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# vi hive-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;192.168.0.10:3306&#x2F;hive?serverTimezone&#x3D;Asia&#x2F;Shanghai&lt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.cj.jdbc.Driver&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;root&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;admin&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.querylog.location&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;data&#x2F;hive_repo&#x2F;querylog&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.exec.local.scratchdir&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;data&#x2F;hive_repo&#x2F;scratchdir&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.downloaded.resources.dir&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;data&#x2F;hive_repo&#x2F;resources&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h4 id="导入mysql驱动"><a href="#导入mysql驱动" class="headerlink" title="导入mysql驱动"></a>导入mysql驱动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意mysql驱动包的版本，要和我们安装的版本保持一致：mysql-connector-java-8.0.16.jar</span><br><span class="line"></span><br><span class="line">[root@bigdata04 lib]# ll</span><br><span class="line">........</span><br><span class="line">-rw-r--r--. 1 root root 2293144 Mar 20 2019 mysql-connector-java-8.0.16.jar</span><br></pre></td></tr></table></figure><h4 id="修改core-site-xml"><a href="#修改core-site-xml" class="headerlink" title="修改core-site.xml"></a>修改core-site.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改bigdata01中的core-site.xml，然后同步到集群中的另外两个节点上(客户端节点不用修改)</span><br><span class="line">如果不增加这个配置，使用beeline连接hive的时候会报错</span><br><span class="line"></span><br><span class="line">注意：bigdata04这个客户端节点上不需要修改这个配置就可以了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 hadoop]# vi core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h3 id="初始化Hive的Metastore"><a href="#初始化Hive的Metastore" class="headerlink" title="初始化Hive的Metastore"></a>初始化Hive的Metastore</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava&#x2F;lang&#x2F;String;Ljava&#x2F;lang&#x2F;Object;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.hadoop和hive的两个guava.jar版本不一致</span><br><span class="line">两个位置分别位于下面两个目录：</span><br><span class="line">- &#x2F;usr&#x2F;local&#x2F;hive&#x2F;lib&#x2F;guava-19.0.jar</span><br><span class="line">- &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;guava-27.0-jre.jar</span><br><span class="line"></span><br><span class="line">直接把hive里的移走，把hadoop里的复制过来，完美解决</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2.SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;lib&#x2F;log4j-slf4j-impl-2.10.0.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.2&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;slf4j-log4j12-1.7.25.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这两个jar不同，但方法有相同的，不用管</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">3.</span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;schematool -dbType mysql -initSch</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.RuntimeException: com.ctc.wstx.exc.WstxP</span><br><span class="line">at [row,col,system-id]: [3215,96,&quot;file:&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;conf</span><br><span class="line"></span><br><span class="line">但是执行之后发现报错了，提示hive-site.xml文件中的第3215行内容有问题</span><br><span class="line">其实这个是原始配置文件本身就有的问题，最直接的就是把这一行直接删掉，删除之后的效果如下：其实就是把hive.txn.xlock.iow对应的description标签内容删掉，这样就可以了</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.txn.xlock.iow&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;description&gt;</span><br><span class="line">&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">4.</span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;schematool -dbType mysql -initSchema</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;lib&#x2F;log4j-slf4j-impl-2.10.0.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.2&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;slf4j-log4j12-1.7.25.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http:&#x2F;&#x2F;www.slf4j.org&#x2F;codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Metastore connection URL:        jdbc:mysql:&#x2F;&#x2F;192.168.206.1:3306&#x2F;hive?serverTimezone&#x3D;Asia&#x2F;Shanghai</span><br><span class="line">Metastore Connection Driver :    com.mysql.cj.jdbc.Driver</span><br><span class="line">Metastore connection User:       root</span><br><span class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.</span><br><span class="line">Underlying cause: com.mysql.cj.jdbc.exceptions.CommunicationsException : Communications link failure</span><br><span class="line"></span><br><span class="line">The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.</span><br><span class="line">SQL Error code: 0</span><br><span class="line">Use --verbose for detailed stacktrace.</span><br><span class="line">*** schemaTool failed ***</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.打开&#x2F;usr&#x2F;local&#x2F;hive&#x2F;conf&#x2F;hive-site.xml</span><br><span class="line"></span><br><span class="line">hive配置文件hive-site.xml中ConnectionURL中加上serverTimezone&#x3D;GMT就可以了。</span><br><span class="line"></span><br><span class="line">2.&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;192.168.52.26:3306&#x2F;hive&lt;&#x2F;value&gt;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第1章 快速了解Hive</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC1%E7%AB%A0-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3Hive.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC1%E7%AB%A0-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3Hive.html</id>
    <published>2022-02-20T04:00:28.000Z</published>
    <updated>2022-02-20T05:09:02.153Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第1章-快速了解Hive"><a href="#第八周-第1章-快速了解Hive" class="headerlink" title="第八周 第1章 快速了解Hive"></a>第八周 第1章 快速了解Hive</h1><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Hive是建立在Hadoop上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载，可以简称为ETL。</span><br><span class="line">Hive 定义了简单的类SQL查询语言，称为HQL，它允许熟悉SQL的用户直接查询Hadoop中的数据，同时，这个语言也允许熟悉MapReduce的开发者开发自定义的mapreduce任务来处理内建的SQL函数无法完成的复杂的分析任务。</span><br><span class="line">Hive中包含的有SQL解析引擎，它会将SQL语句转译成M&#x2F;R Job,然后在Hadoop中执行。</span><br><span class="line">通过这里的分析我们可以了解到Hive可以通过sql查询Hadoop中的数据，并且sql底层也会转化成mapreduce任务，所以hive是基于hadoop的。</span><br></pre></td></tr></table></figure><h2 id="Hive的数据存储"><a href="#Hive的数据存储" class="headerlink" title="Hive的数据存储"></a>Hive的数据存储</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Hive的数据存储基于Hadoop的 HDFS</span><br><span class="line">Hive没有专门的数据存储格式</span><br><span class="line">Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile等文件格式</span><br><span class="line">针对普通文本数据，我们在创建表时，只需要指定数据的列分隔符与行分隔符，Hive即可解析里面的数据</span><br></pre></td></tr></table></figure><h2 id="Hive的系统架构"><a href="#Hive的系统架构" class="headerlink" title="Hive的系统架构"></a>Hive的系统架构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面我们来分析一下Hive的系统架构</span><br><span class="line">看这个图，下面表示是Hadoop集群，上面是Hive，从这也可以看出来Hive是基于Hadoop的。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HLJa4O" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/20/HLJa4O.md.png" alt="HLJa4O.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">用户接口，包括 CLI、JDBC&#x2F;ODBC、WebGUI</span><br><span class="line">CLI，即Shell命令行，表示我们可以通过shell命令行操作Hive</span><br><span class="line">JDBC&#x2F;ODBC 是 Hive 的Java操作方式，与使用传统数据库JDBC的方式类似</span><br><span class="line">WebUI是通过浏览器访问 Hive</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">元数据存储(Metastore)，注意：这里的存储是名词，Metastore表示是一个存储系统</span><br><span class="line">Hive中的元数据包括表的相关信息，Hive会将这些元数据存储在Metastore中，目前Metastore只支持 mysql、derby</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Driver：包含：编译器、优化器、执行器</span><br><span class="line">编译器、优化器、执行器可以完成 Hive的 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划最终存储在 HDFS 中，并在随后由MapReduce 调用执行</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Hadoop：Hive会使用 HDFS 进行存储，利用 MapReduce 进行计算</span><br><span class="line">Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（特例 select * from table 不会生</span><br><span class="line">成 MapRedcue 任务，如果在SQL语句后面再增加where过滤条件就会生成MapReduce任务了。）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  在这有一点需要注意的，就是从Hive2开始，其实官方就不建议默认使用MapReduce引擎了，而是建议使用Tez引擎或者是Spark引擎，不过目前一直到最新的3.x版本中mapreduce还是默认的执行引擎</span><br><span class="line">  其实大数据计算引擎是有几个发展阶段的，</span><br><span class="line">  首先是第一代大数据计算引擎：MapReduce</span><br><span class="line">  接着是第二代大数据计算引擎：Tez，Tez的存在感比较低，它是源于MapReduce，主要和Hive结合在一起使用，它的核心思想是将Map和Reduce两个操作进一步拆分，这些分解后的元操作可以灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可以形成一个大的作业，这样可以提高计算效率，我们在实际工作中Hive使用的就是 Tez引擎，替换Hive的执行引擎也很简单，只需要把Tez安装好（Tez也是支持在YARN上执行的），然后到Hive中配置一下就可以了，不管使用什么引擎，不会对我们使用hive造</span><br><span class="line">成什么影响，也就说对上层的使用没有影响</span><br><span class="line">  接着是第三代大数据计算引擎：Spark，Spark在当时属于一个划时代的产品，改变了之前基于磁盘的计算思路，而是采用内存计算，就是说Spark把数据读取过来以后，中间的计算结果是不会进磁盘的，一直到出来最终结果，才会写磁盘，这样就大大提高了计算效率，而MapReduce的中间结果是会写磁盘的，所以效率没有Spark高。Spark的执行效率号称比MapReduce 快100倍，当然这需要在一定数据规模下才会差这么多，如果我们就计算几十兆或者几百兆的文件，你去对比发现其实也不会差多少，后面我们也会学到Spark这个基于内存的大数据计算引擎</span><br><span class="line"></span><br><span class="line">注意：spark也是支持在YARN上执行的</span><br><span class="line"></span><br><span class="line">其实目前还有第四代大数据计算引擎，：Flink，Flink是一个可以支持纯实时数据计算的计算引擎，在实时计算领域要优于Saprk，Flink和Spark其实是有很多相似之处，在某些方面他们两个属于互相参考，互相借鉴，互相成长，Flink后面我们也会学到，等后面我们讲到这个计算引擎的时候再详细分析。</span><br><span class="line"></span><br><span class="line">注意：Flink也是支持在YARN上执行的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">所以发现没有，MapReduce、Tez、Spark、Flink这些计算引擎都是支持在yarn上执行的，所以说Hdoop2中对架构的拆分是非常明智的。</span><br><span class="line">解释完这些名词之后其实我们就对这个架构有了一个基本理解，</span><br><span class="line">再看来这个图</span><br><span class="line">用户通过接口传递Hive SQL，然后经过Driver对SQL进行分析、编译，生成查询计划，查询计划会存储在HDFS中，然后再通过MapReduce进行计算出结果，这就是整个大的流程。</span><br><span class="line">其实在这里我们可以发现，Hive这个哥们是既不存储数据，也不计算数据，这些活都给了Hadoop来干，Hive底层最核心的东西其实就是Driver这一块，将SQL语句解析为最终的查询计划。</span><br></pre></td></tr></table></figure><h3 id="Metastore"><a href="#Metastore" class="headerlink" title="Metastore"></a>Metastore</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">接着来看一下Hive中的元数据存储，Metastore</span><br><span class="line">Metastore是Hive元数据的集中存放地。</span><br><span class="line">Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在的hdfs目录等</span><br><span class="line">Metastore默认使用内嵌的derby数据库</span><br><span class="line">Derby数据库的缺点：在同一个目录下一次只能打开一个会话</span><br><span class="line">使 用 derby 存 储 方 式 时 ， Hive 会 在 当 前 目 录 生 成 一 个 derby.log 文 件 和 一 个 metastore_db 目 录 ，metastore_db里面会存储具体的元数据信息</span><br><span class="line">如果下次切换到一个另一个新目录访问Hive，则会重新生成derby.log文件metastore_db目录，这样就没有办法使用之前的元数据信息了。</span><br><span class="line">推荐使用MySQL作为外置存储引擎，可以支持多用户同时访问以及元数据共享。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>写linux的shell脚本方法积累</title>
    <link href="http://tianyong.fun/%E5%86%99linux%E7%9A%84shell%E8%84%9A%E6%9C%AC%E6%96%B9%E6%B3%95%E7%A7%AF%E7%B4%AF.html"/>
    <id>http://tianyong.fun/%E5%86%99linux%E7%9A%84shell%E8%84%9A%E6%9C%AC%E6%96%B9%E6%B3%95%E7%A7%AF%E7%B4%AF.html</id>
    <published>2022-02-18T06:58:58.000Z</published>
    <updated>2022-02-20T03:00:27.913Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="写linux的shell脚本方法积累"><a href="#写linux的shell脚本方法积累" class="headerlink" title="写linux的shell脚本方法积累"></a>写linux的shell脚本方法积累</h1><h1 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 循环向文件中生成数据</span><br><span class="line"># 下面括号里没空格会报错</span><br><span class="line">while [ &quot;1&quot;&#x3D;&quot;1&quot; ]</span><br><span class="line">do</span><br><span class="line">        # 获取当前时间戳</span><br><span class="line">        curr_time&#x3D;&#96;date +%s&#96;</span><br><span class="line">        # 获取当前主机名</span><br><span class="line">        name&#x3D;&#96;hostname&#96;</span><br><span class="line">        echo $&#123;name&#125;_$&#123;curr_time&#125; &gt;&gt; &#x2F;data&#x2F;log&#x2F;networkLogUploadToHdfsExample&#x2F;access.l</span><br><span class="line">og</span><br><span class="line">        sleep 1</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h2 id="案例二-写Flume进程的监控脚本"><a href="#案例二-写Flume进程的监控脚本" class="headerlink" title="案例二 写Flume进程的监控脚本"></a>案例二 写Flume进程的监控脚本</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">monlist &#x3D; &#96;cat monlist.conf&#96;</span><br><span class="line">echo &quot;start check&quot;</span><br><span class="line">for item in $&#123;monlist&#125;</span><br><span class="line">do</span><br><span class="line">    # 设置字段分隔符</span><br><span class="line">    OLD_IFS&#x3D;$IFS</span><br><span class="line">    IFS&#x3D;&quot;&#x3D;&quot;</span><br><span class="line">    # 把一行内容转成多列[数组] </span><br><span class="line">    arr&#x3D;($item) </span><br><span class="line">    # 获取等号左边的内容</span><br><span class="line">    name&#x3D;$&#123;arr[0]&#125;</span><br><span class="line">    # 获取等号右边的内容</span><br><span class="line">    script&#x3D;$&#123;arr[1]&#125;</span><br><span class="line">    echo &quot;time is:&quot;&#96;date +&quot;%Y-%m-%d %H:%M:%S&quot;&#96;&quot; check &quot;$name</span><br><span class="line">    if [ &#96;jps -m|grep $name | wc -l&#96; -eq 0 ]</span><br><span class="line">    then</span><br><span class="line">    # 发短信或者邮件告警</span><br><span class="line">    echo &#96;date +&quot;%Y-%m-%d %H:%M:%S&quot;&#96;$name &quot;is none&quot;</span><br><span class="line">    sh -x .&#x2F;$&#123;script&#125;</span><br><span class="line">    fi</span><br><span class="line">    done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 myconfFile]# vim monlist.conf </span><br><span class="line">load-failover.conf &#x3D; startExample.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 myconfFile]# vim startExample.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">flume_path &#x3D; &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin</span><br><span class="line">nohup $&#123;flume_path&#125;&#x2F;bin&#x2F;flume-ng --name a1 --conf $&#123;flume_path&#125;&#x2F;conf --conf-file $&#123;flume_</span><br><span class="line">path&#125;&#x2F;conf&#x2F;myconfFile&#x2F;load-failover.conf &amp;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="linux" scheme="http://tianyong.fun/categories/linux/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第七周 第2章 极速上手Flume使用 采集网络日志上传到HDFS</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8-%E9%87%87%E9%9B%86%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97%E4%B8%8A%E4%BC%A0%E5%88%B0HDFS.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8-%E9%87%87%E9%9B%86%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97%E4%B8%8A%E4%BC%A0%E5%88%B0HDFS.html</id>
    <published>2022-02-18T04:47:46.000Z</published>
    <updated>2022-02-25T04:08:23.341Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第七周-第2章-极速上手Flume使用-采集网络日志上传到HDFS"><a href="#第七周-第2章-极速上手Flume使用-采集网络日志上传到HDFS" class="headerlink" title="第七周 第2章 极速上手Flume使用 采集网络日志上传到HDFS"></a>第七周 第2章 极速上手Flume使用 采集网络日志上传到HDFS</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面我们讲了两个案例的使用，接下来看一个稍微复杂一点的案例：</span><br><span class="line">需求是这样的，</span><br><span class="line">1. 将A和B两台机器实时产生的日志数据汇总到机器C中</span><br><span class="line">2. 通过机器C将数据统一上传至HDFS的指定目录中</span><br><span class="line">注意：HDFS中的目录是按天生成的，每天一个目录</span><br><span class="line">看下面这个图，来详细分析一下</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HTp8o9" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTp8o9.md.png" alt="HTp8o9.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">根据刚才的需求分析可知，我们一共需要三台机器</span><br><span class="line">这里使用bigdata02和bigdata03采集当前机器上产生的实时日志数据，统一汇总到bigdata04机器上。</span><br><span class="line">其中bigdata02和bigdata03中的source使用基于file的source，ExecSource，因为要实时读取文件中的新增数据channel在这里我们使用基于内存的channel，因为这里是采集网站的访问日志，就算丢一两条数据对整体结果影响也不大，我们只希望采集到的数据可以快读进入hdfs中，所以就选择了基于内存的channel。</span><br><span class="line">由于bigdata02和bigdata03的数据需要快速发送到bigdata04中，为了快速发送我们可以通过网络直接传输，sink建议使用avrosink，avro是一种数据序列化系统，经过它序列化的数据传输起来效率更高，并且它对应的还有一个avrosource，avrosink的数据可以直接发送给avrosource，所以他们可以无缝衔接。</span><br><span class="line">这样bigdata04的source就确定了 使用avrosource、channel还是基于内存的channel，sink就使用hdfssink，因为是要向hdfs中写数据的。</span><br><span class="line">这里面的组件，只有execsource、avrosource、avrosink我们还没有使用过，其他的组件都使用过了。</span><br><span class="line">最终需要在每台机器上启动一个agent，启动的时候需要注意先后顺序，先启动bigdata04上面的，再启动bigdata02和bigdata03上面的。</span><br></pre></td></tr></table></figure><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><h3 id="bigdata02"><a href="#bigdata02" class="headerlink" title="bigdata02"></a>bigdata02</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h3 id="bigdata03"><a href="#bigdata03" class="headerlink" title="bigdata03"></a>bigdata03</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：bigdata02和bigdata03中配置的a1.sinks.k1.port 的值45454需要和bigdata04中配置的一致</span><br></pre></td></tr></table></figure><h3 id="bigdata02-conf"><a href="#bigdata02-conf" class="headerlink" title="bigdata02 conf"></a>bigdata02 conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata02 conf]# vim file-to-avro-101.conf</span><br><span class="line"></span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;networkLogUploadToHdfsExample&#x2F;access.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type &#x3D; avro</span><br><span class="line"># ip也行</span><br><span class="line">a1.sinks.k1.hostname &#x3D; bigdata04</span><br><span class="line"># 端口没用过的就行</span><br><span class="line">a1.sinks.k1.port &#x3D; 45454</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels&#x3D;c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h3 id="bigdata03-conf"><a href="#bigdata03-conf" class="headerlink" title="bigdata03 conf"></a>bigdata03 conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata03 conf]# vim file-to-avro-102.conf</span><br><span class="line"></span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;networkLogUploadToHdfsExample&#x2F;access.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type &#x3D; avro</span><br><span class="line"># ip也行</span><br><span class="line">a1.sinks.k1.hostname &#x3D; bigdata04</span><br><span class="line"># 端口没用过的就行</span><br><span class="line">a1.sinks.k1.port &#x3D; 45454</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels&#x3D;c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h3 id="bigdata04-conf"><a href="#bigdata04-conf" class="headerlink" title="bigdata04 conf"></a>bigdata04 conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这台机器我们已经安装过Flume了，所以直接配置Agent即可</span><br><span class="line">在指定Agent中sink配置的时候注意，我们的需求是需要按天在hdfs中创建目录，并把当天的数据上传到当天的日期目录中，这也就意味着hdfssink中的path不能写死，需要使用变量，动态获取时间，查看官方文档可知，在hdfs的目录中需要使用%Y%m%d</span><br><span class="line">在这还有一点需要注意的，因为我们这里需要抽取时间，这个时间其实是需要从数据里面抽取，咱们前面说过数据的基本单位是Event，Event是一个对象，后面我们会详细分析，在这里大家先知道它里面包含的既有我们采集到的原始的数据，还有一个header属性，这个header属性是一个key-value结构的，我们现在抽取时间就需要到event的header中抽取，但是默认情况下event的header中是没有日期的，强行抽取是会报错的，会提示抽取不到，返回空指针异常。</span><br><span class="line">java.lang.NullPointerException: Expected timestamp in the Flume event headers, but it was n</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那如何向header中添加日期呢？ 其实官方文档中也说了，可以使用hdfs.useLocalTimeStamp或者时间拦截器，时间拦截器我们后面会讲，暂时最简单直接的方式就是使用hdfs.useLocalTimeStamp，这个属性的值默认为false，需要改为true。</span><br></pre></td></tr></table></figure><p><img src="C:%5CUsers%5CTy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220218163730584.png" alt="image-20220218163730584"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# vim avro-to-hdfs.conf </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sources.r1.type &#x3D; avro</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 45454</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flume&#x2F;networkLogUploadToHdfsExaple&#x2F;%Y%m%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; access-</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix&#x3D;.log</span><br><span class="line">a1.sinks.k1.hdfs.fileType&#x3D;DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat&#x3D;Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval&#x3D;3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize&#x3D;134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount&#x3D;0</span><br><span class="line">#(不设置前面使用时间变量会报错)使用event header里的timestamp要麻烦些</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp&#x3D;true</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels&#x3D;c1</span><br><span class="line">a1.sinks.k1.channel&#x3D;c1</span><br></pre></td></tr></table></figure><h2 id="模拟数据源"><a href="#模拟数据源" class="headerlink" title="模拟数据源"></a>模拟数据源</h2><h3 id="bigdata02-1"><a href="#bigdata02-1" class="headerlink" title="bigdata02"></a>bigdata02</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata02 log]# vim SimulateData.sh </span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 循环向文件中生成数据</span><br><span class="line">while [ &quot;1&quot;&#x3D;&quot;1&quot; ]</span><br><span class="line">do</span><br><span class="line">        # 获取当前时间戳</span><br><span class="line">        curr_time&#x3D;&#96;date +%s&#96;</span><br><span class="line">        # 获取当前主机名</span><br><span class="line">        name&#x3D;&#96;hostname&#96;</span><br><span class="line">        echo $&#123;name&#125;_$&#123;curr_time&#125; &gt;&gt; &#x2F;data&#x2F;log&#x2F;networkLogUploadToHdfsExample&#x2F;access.log</span><br><span class="line">        sleep 1</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="bigdata03-1"><a href="#bigdata03-1" class="headerlink" title="bigdata03"></a>bigdata03</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">同上</span><br></pre></td></tr></table></figure><h2 id="启动进程"><a href="#启动进程" class="headerlink" title="启动进程"></a>启动进程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来开始启动相关的服务进程</span><br><span class="line">首先启动bigdata04上的agent服务</span><br><span class="line">接下来启动bigdata-02上的agent服务和shell脚本</span><br><span class="line">最后启动bigdata-03上的agent服务和shell脚本</span><br></pre></td></tr></table></figure><h3 id="启动bigdata04"><a href="#启动bigdata04" class="headerlink" title="启动bigdata04"></a>启动bigdata04</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h3 id="启动bigdata02"><a href="#启动bigdata02" class="headerlink" title="启动bigdata02"></a>启动bigdata02</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h3 id="启动bigdata03"><a href="#启动bigdata03" class="headerlink" title="启动bigdata03"></a>启动bigdata03</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.shell脚本</span><br><span class="line">while [ 1&#x3D;1 ] </span><br><span class="line"># 两个空格不能少</span><br><span class="line"></span><br><span class="line">2.flume配置</span><br><span class="line">注释不要写在语句的同一行后面</span><br><span class="line"></span><br><span class="line">3.启动agent时</span><br><span class="line">先启动bigdata02,bigdata03和先关bigdata04都会造成数据流失</span><br></pre></td></tr></table></figure><h2 id="结果查看"><a href="#结果查看" class="headerlink" title="结果查看"></a>结果查看</h2><p><a href="https://imgtu.com/i/HTfiPs" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTfiPs.md.png" alt="HTfiPs.md.png"></a></p><p><a href="https://imgtu.com/i/HTfGM6" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTfGM6.md.png" alt="HTfGM6.md.png"></a><br><a href="https://imgtu.com/i/HTf3xx" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTf3xx.md.png" alt="HTf3xx.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：启动之后稍等一会就可以看到数据了，我们观察数据的变化，会发现hdfs中数据增长的不是很快，它会每隔一段时间添加一批数据，实时性好像没那么高？</span><br><span class="line">这是因为avrosink中有一个配置batch-size，它的默认值是100，也就是每次发送100条数据，如果数据不够100条，则不发送。</span><br><span class="line">具体这个值设置多少合适，要看你source数据源大致每秒产生多少数据，以及你希望的延迟要达到什么程度，如果这个值设置太小的话，会造成sink频繁向外面写数据，这样也会影响性能。</span><br><span class="line">最终，依次停止bigdata02、bigdata03中的服务，最后停止bigdata04中的服务</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HTIFTU" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTIFTU.md.png" alt="HTIFTU.md.png"></a></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flume" scheme="http://tianyong.fun/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第七周 第2章 极速上手Flume使用 采集文件内容到HDFS</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8-%E9%87%87%E9%9B%86%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E5%88%B0HDFS.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8-%E9%87%87%E9%9B%86%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E5%88%B0HDFS.html</id>
    <published>2022-02-18T04:18:46.000Z</published>
    <updated>2022-02-22T04:18:01.302Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第2章-极速上手Flume使用-采集文件内容到HDFS"><a href="#第2章-极速上手Flume使用-采集文件内容到HDFS" class="headerlink" title="第2章 极速上手Flume使用 采集文件内容到HDFS"></a>第2章 极速上手Flume使用 采集文件内容到HDFS</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一个工作中的典型案例：</span><br><span class="line">采集文件内容上传至HDFS</span><br><span class="line">需求：采集目录中已有的文件内容，存储到HDFS</span><br><span class="line">分析：source是要基于目录的，channel建议使用file，可以保证不丢数据，sink使用hdfs</span><br><span class="line">下面要做的就是配置Agent了，可以把example.conf拿过来修改一下，新的文件名为file-to-hdfs.conf</span><br><span class="line">首先是基于目录的source，咱们前面说过，Spooling Directory Source可以实现目录监控来看一下这个Spooling Directory Source</span><br></pre></td></tr></table></figure><h2 id="source"><a href="#source" class="headerlink" title="source"></a>source</h2><h3 id="Spooling-Directory-Source"><a href="#Spooling-Directory-Source" class="headerlink" title="Spooling Directory Source"></a>Spooling Directory Source</h3><p><a href="https://imgtu.com/i/HI4GDS" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HI4GDS.md.png" alt="HI4GDS.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">channels和type肯定是必填的，还有一个是spoolDir，就是指定一个监控的目录</span><br><span class="line">看他下面的案例，里面还多指定了一个fileHeader，这个我们暂时也用不到，后面等我们讲了Event之后</span><br><span class="line">大家就知道这个fileHeader可以干什么了，先记着有这个事把。</span><br></pre></td></tr></table></figure><h2 id="channel"><a href="#channel" class="headerlink" title="channel"></a>channel</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来是channel了</span><br><span class="line">channel在这里使用基于文件的，可以保证数据的安全性</span><br><span class="line">如果针对采集的数据，丢个一两条对整体结果影响不大，只要求采集效率，那么这个时候完全可以使用基于内存的channel</span><br><span class="line">咱们前面的例子中使用的是基于内存的channel，下面我们到文档中找一下基于文件的channel</span><br></pre></td></tr></table></figure><h3 id="File-Channel"><a href="#File-Channel" class="headerlink" title="File Channel"></a>File Channel</h3><p><a href="https://imgtu.com/i/HI5YM6" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HI5YM6.md.png" alt="HI5YM6.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">根据这里的例子可知，主要配置checkpointDir和dataDir，因为这两个目录默认会在用户家目录下生成，建议修改到其他地方</span><br><span class="line">checkpointDir是存放检查点目录</span><br><span class="line">data是存放数据的目录</span><br></pre></td></tr></table></figure><h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最后是sink</span><br><span class="line">因为要向hdfs中输出数据，所以可以使用hdfssink</span><br></pre></td></tr></table></figure><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p><a href="https://imgtu.com/i/HIozKx" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HIozKx.md.png" alt="HIozKx.md.png"></a></p><p><a href="https://imgtu.com/i/HITpqK" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HITpqK.md.png" alt="HITpqK.md.png"></a></p><p><a href="https://imgtu.com/i/HITCVO" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HITCVO.md.png" alt="HITCVO.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs.path是必填项，指定hdfs上的存储目录</span><br><span class="line">看这里例子中还指定了filePrefix参数，这个是一个文件前缀，会在hdfs上生成的文件前面加上这个前缀，这个属于可选项，有需求的话可以加上</span><br><span class="line">一般在这我们需要设置writeFormat和fileType这两个参数</span><br><span class="line">默认情况下writeFormat的值是Writable，建议改为Text，看后面的解释，如果后期想使用hive或者impala操作这份数据的话，必须在生成数据之前设置为Text，Text表示是普通文本数据</span><br><span class="line">fileType默认是SequenceFile，还支持DataStream 和 CompressedStream ，DataStream 不会对输出数据进行压缩，CompressedStream 会对输出数据进行压缩，在这里我们先不使用压缩格式的，所以选择DataStream</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">除了这些参数以外，还有三个也比较重要</span><br><span class="line">hdfs.rollInterval、hdfs.rollSize和hdfs.rollCount</span><br><span class="line">hdfs.rollInterval默认值是30，单位是秒，表示hdfs多长时间切分一个文件，因为这个采集程序是一直运行的，只要有新数据，就会被采集到hdfs上面，hdfs默认30秒钟切分出来一个文件，如果设置为0表示不按时间切文件</span><br><span class="line">hdfs.rollSize默认是1024，单位是字节，最终hdfs上切出来的文件大小都是1024字节，如果设置为0表示不按大小切文件</span><br><span class="line">hdfs.rollCount默认设置为10，表示每隔10条数据切出来一个文件，如果设置为0表示不按数据条数切文件</span><br><span class="line">这三个参数，如果都设置的有值，哪个条件先满足就按照哪个条件都会执行。</span><br><span class="line">在实际工作中一般会根据时间或者文件大小来切分文件，我们之前在工作中是设置的时间和文件大小相结合，时间设置的是一小时，文件大小设置的128M，这两个哪个满足执行哪个</span><br><span class="line">所以针对hdfssink的配置最终是这样的</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; spooldir</span><br><span class="line">a1.sources.r1.spoolDir &#x3D; &#x2F;data&#x2F;log&#x2F;studentDir</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; file</span><br><span class="line">a1.channels.c1.checkpointDir &#x3D; &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;studentDir&#x2F;chec</span><br><span class="line">kpoint</span><br><span class="line">a1.channels.c1.dataDirs &#x3D; &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;studentDir&#x2F;data</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;192.168.206.129:9000&#x2F;flume&#x2F;studentDir</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; stu-</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h2 id="测试文件"><a href="#测试文件" class="headerlink" title="测试文件"></a>测试文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">下面就可以启动agent了，在启动agent之前，先初始化一下测试数据</span><br><span class="line">创建&#x2F;data&#x2F;log&#x2F;studentDir目录，然后在里面添加一个文件，class1.dat</span><br><span class="line">class1.dat中存储的是学生信息，学生姓名、年龄、性别</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ~]# mkdir -p &#x2F;data&#x2F;log&#x2F;studentDir</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;log&#x2F;studentDir</span><br><span class="line">[root@bigdata04 studentDir]# more class1.dat</span><br><span class="line">jack 18 male</span><br><span class="line">jessic 20 female</span><br><span class="line">tom 17 male</span><br></pre></td></tr></table></figure><h2 id="启动Agent"><a href="#启动Agent" class="headerlink" title="启动Agent"></a>启动Agent</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">启动之前，先启动hadoop</span><br><span class="line">启动Agent，使用在前台启动的方式，方便观察现象</span><br><span class="line"></span><br><span class="line">apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf --conf-file conf&#x2F;file-to-hdfs.conf -Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2020-05-02 15:36:58,283 (conf-file-poller-0) [ERROR - org.apache.flume.node.P</span><br><span class="line">java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;io&#x2F;SequenceFile$Compression</span><br><span class="line">at org.apache.flume.sink.hdfs.HDFSEventSink.configure(HDFSEventSink.j</span><br><span class="line">at org.apache.flume.conf.Configurables.configure(Configurables.java:4</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.SequenceFil</span><br><span class="line">at java.net.URLClassLoader.findClass(URLClassLoader.java:382)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">... 12 more</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">但是发现在启动的时候报错，提示找不到SequenceFile，但是我们已经把fileType改为了DataStream，</span><br><span class="line">但是Flume默认还是会加载这个类</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.SequenceFile$CompressionT</span><br><span class="line">ype</span><br><span class="line">就算你把SequenceFile相关的jar包都拷贝到flume的lib目录下解决了这个问题，但是还是会遇到找不到</span><br><span class="line">找不到HDFS这种文件类型，还是缺少hdfs相关的jar包</span><br><span class="line">No FileSystem for scheme: hdfs</span><br><span class="line">当然这个问题也可以通过拷贝jar包来解决这个问题，但是这样其实太费劲了，并且后期我们有很大可能需要在这个节点上操作HDFS，所以其实最简单直接的方法就是把这个节点设置为hadoop集群的一个客户端节点，这样操作hdfs就没有任何问题了。</span><br><span class="line">咱们之前在讲Hadoop的时候讲了客户端节点的特性，其实很简单，我们直接把集群中修改好配置的hadoop目录远程拷贝到bigdata04上就可以了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# scp -rq hadoop-3.2.0 192.168.182.103:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line"></span><br><span class="line">由于bigdata01和bigdata04没有做免密码登录，也不认识它的主机名，所以就使用ip，并且输入密码了。</span><br><span class="line">拷贝完成之后到bigdata04节点上验证一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意：还需要修改环境变量，配置HADOOP_HOME，否则启动Agent的时候还是会提示找不到SequenceFile</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hadoop-3.2.0]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">.....</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br><span class="line">export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$PATH</span><br><span class="line">[root@bigdata04 hadoop-3.2.0]# source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><h2 id="再次启动Agent"><a href="#再次启动Agent" class="headerlink" title="再次启动Agent"></a>再次启动Agent</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">此时可以看到Agent正常启动</span><br><span class="line"></span><br><span class="line">到hdfs上验证结果</span><br><span class="line">[root@bigdata01 lib]# hdfs dfs -ls &#x2F;flume&#x2F;studentDir</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 root supergroup         47 2022-02-18 11:46 &#x2F;flume&#x2F;studentDir&#x2F;stu-.1645155976762.tmp</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">此时发现文件已经生成了，只不过默认情况下现在的文件是 .tmp 结尾的，表示它在被使用，因为Flume只要采集到数据就会向里面写，这个后缀默认是由 hdfs.inUseSuffix 参数来控制的。</span><br><span class="line">文件名上还拼接了一个当前时间戳，这个是默认文件名的格式，当达到文件切割时机的时候会给文件改名字，去掉.tmp</span><br><span class="line">这个文件现在也是可以查看的，里面的内容其实就是class1.dat文件中的内容</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -cat hdfs:&#x2F;&#x2F;192.168.182.100:9000&#x2F;flume&#x2F;studentDi</span><br><span class="line">jack 18 male</span><br><span class="line">jessic 20 female</span><br><span class="line">tom 17 male</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">所以此时Flume就会监控linux中的&#x2F;data&#x2F;log&#x2F;studentDir目录，当发现里面有新文件的时候就会把数据采集过来。</span><br><span class="line">那Flume怎么知道哪些文件是新文件呢？它会不会重复读取同一个文件的数据呢？</span><br><span class="line">不会的，我们到&#x2F;data&#x2F;log&#x2F;studentDir目录看一下你就知道了</span><br><span class="line"></span><br><span class="line">[root@bigdata04 studentDir]# cd checkpoint&#x2F;</span><br><span class="line">[root@bigdata04 checkpoint]# ls</span><br><span class="line">checkpoint  checkpoint.meta  inflightputs  inflighttakes  in_use.lock  queueset</span><br><span class="line">[root@bigdata04 checkpoint]# pwd</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;studentDir&#x2F;checkpoint</span><br><span class="line">You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 studentDir]# cd data&#x2F;</span><br><span class="line">[root@bigdata04 data]# ls</span><br><span class="line">in_use.lock  log-2       log-3.meta  log-5       log-6.meta</span><br><span class="line">log-1        log-2.meta  log-4       log-5.meta  log-7</span><br><span class="line">log-1.meta   log-3       log-4.meta  log-6       log-7.meta</span><br><span class="line">[root@bigdata04 data]# pwd</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;studentDir&#x2F;data</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">发现里面有一个 log-1 的文件，这个文件中存储的其实就是读取到的内容，只不过在这无法直接查看。</span><br><span class="line">现在我们想看一下Flume最终生成的文件是什么样子的，难道要根据配置等待1个小时或者弄一个128M的文件过来吗，</span><br><span class="line">其实也没必要，我们可以暴力操作一下</span><br><span class="line">停止Agent就可以看到了，当Agent停止的时候就会去掉 .tmp 标志了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那我再重启Agent之后，会不会再给加上.tmp呢，不会了，每次停止之前都会把所有的文件解除占用状态，下次启动的时候如果有新数据，则会产生新的文件，这其实就模拟了一下自动切文件之后的效果。</span><br><span class="line">但是这个文件看起来比较别扭，连个后缀都没有，没有后缀倒不影响使用，就是看起来不好看</span><br><span class="line">在这给大家留一个作业，下一次再生成新文件的时候我希望文件名有一个后缀是.log，大家下去之后自己查看官网文档资料，修改Agent配置，添加测试数据，验证效果。</span><br><span class="line">答案：其实就是使用hdfs sink中的hdfs.fileSuffix参数</span><br></pre></td></tr></table></figure><h2 id="异常-1"><a href="#异常-1" class="headerlink" title="异常"></a>异常</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Flume v1.9.0启动报错ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:459)</span><br><span class="line"></span><br><span class="line">Hadoop 3.3.0 中的 guava 版本和 Flume 1.9.0 中的版本不一致</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/llwy1428/article/details/112169028" target="_blank" rel="external nofollow noopener noreferrer">解决</a></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flume" scheme="http://tianyong.fun/tags/Flume/"/>
    
  </entry>
  
</feed>
