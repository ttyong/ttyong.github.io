<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2023-06-11T06:55:43.749Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发工程师-全文检索引擎Elasticsearch-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8EElasticsearch-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8EElasticsearch-3.html</id>
    <published>2023-06-02T10:03:09.000Z</published>
    <updated>2023-06-11T06:55:43.749Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="全文检索引擎Elasticsearch-3"><a href="#全文检索引擎Elasticsearch-3" class="headerlink" title="全文检索引擎Elasticsearch-3"></a>全文检索引擎Elasticsearch-3</h1><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周-</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8EElasticsearch-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8EElasticsearch-2.html</id>
    <published>2023-06-02T10:03:09.000Z</published>
    <updated>2023-06-02T10:03:09.649Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周-</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink1.15%E4%B9%8BFlink%20SQL%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%201.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink1.15%E4%B9%8BFlink%20SQL%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%201.html</id>
    <published>2023-06-02T10:03:09.000Z</published>
    <updated>2023-06-02T10:03:09.649Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周-</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink1.15%E4%B9%8BFlink%20SQL%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%202.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink1.15%E4%B9%8BFlink%20SQL%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%202.html</id>
    <published>2023-06-02T10:03:09.000Z</published>
    <updated>2023-06-02T10:03:09.649Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-全文检索引擎Elasticsearch-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8EElasticsearch-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8EElasticsearch-1.html</id>
    <published>2023-06-02T10:03:09.000Z</published>
    <updated>2023-06-11T06:51:18.760Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="全文检索引擎Elasticsearch-1"><a href="#全文检索引擎Elasticsearch-1" class="headerlink" title="全文检索引擎Elasticsearch-1"></a>全文检索引擎Elasticsearch-1</h1><h2 id="1-快速了解Elasticsearch"><a href="#1-快速了解Elasticsearch" class="headerlink" title="1 快速了解Elasticsearch"></a>1 快速了解Elasticsearch</h2><h3 id="为什么要学Elasticsearch？"><a href="#为什么要学Elasticsearch？" class="headerlink" title="为什么要学Elasticsearch？"></a>为什么要学Elasticsearch？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">针对海量数据计算分析，前面我们学习了MapReduce、Hive、Spark、Flink这些计算引擎和分析工具，但是它们侧重的都是对数据的清洗、聚合之类的需求。</span><br><span class="line">如果想要在海量数据里面快速查询出一批满足条件的数据，这些计算引擎都需要生成一个任务，提交到集群中去执行，这样中间消耗的时间就长了。</span><br><span class="line"></span><br><span class="line">并且针对多条件组合查询需求，这些计算引擎在查询的时候基本上都要实现全表扫描了，这样查询效率也是比较低的。</span><br><span class="line"></span><br><span class="line">所以，为了解决海量数据下的快速检索，以及多条件组合查询需求，Elasticsearch就应运而生了。</span><br></pre></td></tr></table></figure><h3 id="Elasticsearch简介"><a href="#Elasticsearch简介" class="headerlink" title="Elasticsearch简介"></a>Elasticsearch简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Elasticsearch是一个分布式的全文检索引擎，它是对lucene的功能做了封装，能够达到实时搜索，稳定，可靠，快速等特点。</span><br><span class="line">如果大家对Lucene有所了解的话，那么针对Elasticsearch其实就好理解了。</span><br></pre></td></tr></table></figure><h3 id="常见的全文检索引擎"><a href="#常见的全文检索引擎" class="headerlink" title="常见的全文检索引擎"></a>常见的全文检索引擎</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Lucene</span><br><span class="line">Lucene是Java家族中最为出名的一个开源搜索引擎，在Java世界中属于标准的全文检索程序，它提供了完整的查询引擎和索引引擎。</span><br><span class="line">但是它也存在一些缺点</span><br><span class="line">1：不支持分布式，无法扩展，海量数据下会存在瓶颈。</span><br><span class="line">2：提供的都是低级API，使用繁琐。</span><br><span class="line">3：没有提供web界面，不便于管理。</span><br><span class="line"></span><br><span class="line">Solr</span><br><span class="line">Solr是一个用java开发的独立的企业级搜索应用服务器，它是基于Lucene的。</span><br><span class="line">它解决了Lucene的一些痛点，提供了web界面，以及高级API接口。</span><br><span class="line">并且从Solr4.0版本开始，Solr开始支持分布式，称之为Solrcloud。</span><br><span class="line"></span><br><span class="line">Elasticsearch</span><br><span class="line">Elasticsearch是一个采用Java语言开发的，基于Lucene的开源、分布式的搜索引擎,能够实现实时搜索。</span><br><span class="line">它最重要的一个特点是天生支持分布式，可以这样说，Elasticsearch就是为了分布式而生的。</span><br><span class="line">它对外提供REST API接口，便于使用，通过外部插件实现web界面支持，便于管理集群。</span><br></pre></td></tr></table></figure><h3 id="Solr-vs-Elasticsearch"><a href="#Solr-vs-Elasticsearch" class="headerlink" title="Solr vs Elasticsearch"></a>Solr vs Elasticsearch</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Elasticsearch一般我们会简称为ES。</span><br><span class="line"></span><br><span class="line">从这里可以看出来，Solr和ES的功能基本是类似的，那在工作中该如何选择呢？</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111104774.png" alt="image-20230611110351736"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Solr从2007年就出现了，在传统企业中应用的还是比较广泛的，并且在2013年的时候，Solr推出了4.0版本，提供了Solrcloud，开始正式支持分布式集群。</span><br><span class="line">ES在2014年的时候才正式推出1.0版本，所以它的出现要比Solr晚很多年。</span><br><span class="line">但是ES从一开始就是为了解决海量数据下的全文检索，所以在分布式集群相关特性层面，ES会优于Solrcloud。</span><br><span class="line">建议：</span><br><span class="line"></span><br><span class="line">如果之前公司里面已经深度使用了Solr，现在为了解决海量数据检索问题，建议优先考虑使用Solrcloud。</span><br><span class="line">如果之前没有使用过Solr，那么在海量数据的场景下，建议优先考虑使用ES。</span><br></pre></td></tr></table></figure><h3 id="MySQL-VS-Elasticsearch"><a href="#MySQL-VS-Elasticsearch" class="headerlink" title="MySQL VS Elasticsearch"></a>MySQL VS Elasticsearch</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">为了便于理解ES，在这里我们拿MySQL和ES做一个对比分析：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111106927.png" alt="image-20230611110616238"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">1： MySQL中有Database（数据库）的概念，对应的在ES中有Index（索引库）的概念。</span><br><span class="line">2：MySQL中有Table（表）的概念，对应的在ES中有Type（类型）的概念，不过需要注意，ES在1.x~5.x版本中是正常支持Type的，每一个Index下面可以有多个Type。</span><br><span class="line"></span><br><span class="line">从6.0版本开始，每一个Index中只支持1个Type，属于过渡阶段。</span><br><span class="line">从7.0版本开始，取消了Type，也就意味着每一个Index中存储的数据类型可以认为都是同一种，不再区分类型了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">为何要取消Type？</span><br><span class="line"></span><br><span class="line">主要还是基于性能方面的考虑。</span><br><span class="line">因为ES设计初期，是直接参考了关系型数据库的设计模型，存在了 Type（表）的概念。</span><br><span class="line">但是，ES的搜索引擎是基于 Lucene 的，这种基因决定了 Type 是多余的。</span><br><span class="line">在关系型数据库中Table是独立的，但是在ES中同一个Index中不同Type的数据在底层是存储在同一个Lucene的索引文件中的。</span><br><span class="line">如果在同一个Index中的不同Type中都有一个id字段，那么ES会认为这两个id字段是同一个字段，你必须在不同的Type中给这个id字段定义相同的字段类型，否则，不同Type中的相同字段名称就会在处理的时候出现冲突，导致Lucene处理效率下降。</span><br><span class="line">除此之外，在同一个Index的不同Type下，存储字段个数不一样的数据，会导致存储中出现稀疏数据，影响Lucene压缩文档的能力，最终导致ES查询效率降低。</span><br><span class="line"></span><br><span class="line">3：MySQL中有Row（行）的概念，表示一条数据，在ES中对应的有Document（文档）。</span><br><span class="line">4：MySQL中有Column（列）的概念，表示一条数据中的某个列，在ES中对应的有Field（字段）。</span><br></pre></td></tr></table></figure><h3 id="Elasticsearch核心概念"><a href="#Elasticsearch核心概念" class="headerlink" title="Elasticsearch核心概念"></a>Elasticsearch核心概念</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111111465.png" alt="image-20230611111149099"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ES中几个比较核心的概念：</span><br><span class="line">Cluster：集群</span><br><span class="line">Shard：分片</span><br><span class="line">Replica：副本</span><br><span class="line">Recovery：数据恢复</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">接下来具体分析一下这几个概念：</span><br><span class="line"></span><br><span class="line">Cluster</span><br><span class="line">代表ES集群，集群中有多个节点，其中有一个为主节点，这个主节点是通过选举产生的。</span><br><span class="line"></span><br><span class="line">主从节点是对于集群内部来说的，ES的一个核心特性就是去中心化，字面上理解就是无中心节点，这是对于集群外部来说的，因为从外部来看ES集群，在逻辑上是一个整体，我们与任何一个节点的通信和与整个ES集群通信是等价的。</span><br><span class="line"></span><br><span class="line">主节点的职责是负责管理集群状态，包括管理分片的状态和副本的状态，以及节点的发现和删除。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Shard</span><br><span class="line">代表索引库分片，ES集群可以把一个索引库分成多个分片。</span><br><span class="line"></span><br><span class="line">这样的好处是可以把一个大的索引库水平拆分成多个分片，分布到不同的节点上，构成分布式搜索，进而提高性能和吞吐量。</span><br><span class="line"></span><br><span class="line">注意：分片的数量只能在创建索引库的时候指定，索引库创建后不能更改。</span><br><span class="line"></span><br><span class="line">默认情况下一个索引库有1个分片。</span><br><span class="line"></span><br><span class="line">每个分片中最多存储2,147,483,519条数据，其实就是Integer.MAX_VALUE-128。</span><br><span class="line">因为每一个ES的分片底层对应的都是Lucene索引文件，单个Lucene索引文件最多存储Integer.MAX_VALUE-128个文档（数据）。</span><br><span class="line"></span><br><span class="line">注意：在ES7.0版本之前，每一个索引库默认是有5个分片的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Replica</span><br><span class="line">代表分片的副本，ES集群可以给分片设置副本。</span><br><span class="line"></span><br><span class="line">副本的第一个作用是提高系统的容错性，当某个分片损坏或丢失时可以从副本中恢复。第二个作用是提高ES的查询效率，ES会自动对搜索请求进行负载均衡。</span><br><span class="line"></span><br><span class="line">注意：分片的副本数量可以随时修改。</span><br><span class="line">默认情况下，每一个索引库只有1个主分片和1个副本分片（前提是ES集群有2个及以上节点，如果ES集群只有1个节点，那么索引库就只有1个主分片，不会产生副本分片，因为主分片和副本分片在一个节点里面是没有意义的）。</span><br><span class="line">为了保证数据安全，以及提高查询效率，建议副本数量设置为2或者3。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Recovery</span><br><span class="line">代表数据恢复或者数据重新分布。</span><br><span class="line"></span><br><span class="line">ES集群在有节点加入或退出时会根据机器的负载对分片进行重新分配，挂掉的节点重新启动时也会进行数据恢复。</span><br></pre></td></tr></table></figure><h2 id="2-快速上手使用Elasticsearch"><a href="#2-快速上手使用Elasticsearch" class="headerlink" title="2 快速上手使用Elasticsearch"></a>2 快速上手使用Elasticsearch</h2><h3 id="ES安装部署"><a href="#ES安装部署" class="headerlink" title="ES安装部署"></a>ES安装部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ES支持单机和集群，在使用层面是完全一样的。</span><br><span class="line">首先下载ES的安装包，目前ES最新版本是7.x，在这使用7.13.4版本。</span><br><span class="line"></span><br><span class="line">下载地址：</span><br><span class="line">https:&#x2F;&#x2F;www.elastic.co&#x2F;cn&#x2F;downloads&#x2F;past-releases#elasticsearch</span><br><span class="line">选择ES的对应版本。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111120893.png" alt="image-20230611112053604"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111121533.png" alt="image-20230611112108886"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：目前ES中自带的有open JDK，不用单独安装部署Oracle JDK。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111123514.png" alt="image-20230611112339787"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在具体安装集群之前，先来分析一下ES中的核心配置文件：</span><br><span class="line">在ES_HOME的config目录下有一个elasticsearch.yml配置文件，这个文件是一个yaml格式的文件。</span><br><span class="line">elasticsearch.yml文件内容如下：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"># &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; Elasticsearch Configuration &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">#</span><br><span class="line"># NOTE: Elasticsearch comes with reasonable defaults for most settings.</span><br><span class="line">#       Before you set out to tweak and tune the configuration, make sure you</span><br><span class="line">#       understand what are you trying to accomplish and the consequences.</span><br><span class="line">#</span><br><span class="line"># The primary way of configuring a node is via this file. This template lists</span><br><span class="line"># the most important settings you may want to configure for a production cluster.</span><br><span class="line">#</span><br><span class="line"># Please consult the documentation for further information on configuration options:</span><br><span class="line"># https:&#x2F;&#x2F;www.elastic.co&#x2F;guide&#x2F;en&#x2F;elasticsearch&#x2F;reference&#x2F;index.html</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- Cluster -----------------------------------</span><br><span class="line">#</span><br><span class="line"># Use a descriptive name for your cluster:</span><br><span class="line"># 集群名称，默认是elasticsearch，如果想要将多个ES实例组成一个集群，那么它们的cluster.name必须一致</span><br><span class="line">#cluster.name: my-application</span><br><span class="line">#</span><br><span class="line"># ------------------------------------ Node ------------------------------------</span><br><span class="line">#</span><br><span class="line"># Use a descriptive name for the node:</span><br><span class="line"># 节点名称，可以手工指定，默认也会自动生成</span><br><span class="line">#node.name: node-1</span><br><span class="line">#</span><br><span class="line"># Add custom attributes to the node:</span><br><span class="line"># 给节点指定一些自定义的参数信息</span><br><span class="line">#node.attr.rack: r1</span><br><span class="line">#</span><br><span class="line"># ----------------------------------- Paths ------------------------------------</span><br><span class="line">#</span><br><span class="line"># Path to directory where to store the data (separate multiple locations by comma):</span><br><span class="line"># 可以指定ES的数据存储目录，默认存储在ES_HOME&#x2F;data目录下</span><br><span class="line">#path.data: &#x2F;path&#x2F;to&#x2F;data</span><br><span class="line">#</span><br><span class="line"># Path to log files:</span><br><span class="line"># 可以指定ES的日志存储目录，默认存储在ES_HOME&#x2F;logs目录下</span><br><span class="line">#path.logs: &#x2F;path&#x2F;to&#x2F;logs</span><br><span class="line">#</span><br><span class="line"># ----------------------------------- Memory -----------------------------------</span><br><span class="line">#</span><br><span class="line"># Lock the memory on startup:</span><br><span class="line"># 锁定物理内存地址，防止ES内存被交换出去,也就是避免ES使用swap交换分区中的内存</span><br><span class="line">#bootstrap.memory_lock: true</span><br><span class="line"># 确保ES_HEAP_SIZE参数设置为系统可用内存的一半左右</span><br><span class="line"># Make sure that the heap size is set to about half the memory available</span><br><span class="line"># on the system and that the owner of the process is allowed to use this</span><br><span class="line"># limit.</span><br><span class="line"># 当系统进行内存交换的时候，会导致ES的性能变的很差</span><br><span class="line"># Elasticsearch performs poorly when the system is swapping the memory.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- Network -----------------------------------</span><br><span class="line">#</span><br><span class="line"># By default Elasticsearch is only accessible on localhost. Set a different</span><br><span class="line"># address here to expose this node on the network:</span><br><span class="line"># 为ES设置绑定的IP，默认是127.0.0.1，也就是默认只能通过127.0.0.1 或者localhost才能访问</span><br><span class="line"># ES 1.x版本默认绑定的是0.0.0.0，但是从ES 2.x版本之后默认绑定的是127.0.0.1</span><br><span class="line">#network.host: 192.168.0.1</span><br><span class="line">#</span><br><span class="line"># By default Elasticsearch listens for HTTP traffic on the first free port it</span><br><span class="line"># finds starting at 9200. Set a specific HTTP port here:</span><br><span class="line"># 为ES服务设置监听的端口，默认是9200</span><br><span class="line"># 如果想要在一台机器上启动多个ES实例，需要修改此处的端口号</span><br><span class="line">#http.port: 9200</span><br><span class="line">#</span><br><span class="line"># For more information, consult the network module documentation.</span><br><span class="line">#</span><br><span class="line"># --------------------------------- Discovery ----------------------------------</span><br><span class="line"># </span><br><span class="line"># Pass an initial list of hosts to perform discovery when this node is started:</span><br><span class="line"># The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]</span><br><span class="line"># </span><br><span class="line"># 当启动新节点时，通过这个ip列表进行节点发现，组建集群</span><br><span class="line"># 默认ip列表：</span><br><span class="line"># 127.0.0.1，表示ipv4的本地回环地址。</span><br><span class="line">#[::1]，表示ipv6的本地回环地址。</span><br><span class="line"># 在ES 1.x中默认使用的是组播(multicast)协议，默认会自动发现同一网段的ES节点组建集群。</span><br><span class="line"># 从ES 2.x开始默认使用的是单播(unicast)协议，想要组建集群的话就需要在这指定要发现的节点信息了。</span><br><span class="line"># </span><br><span class="line"># 指定想要组装成一个ES集群的多个节点信息</span><br><span class="line">#discovery.seed_hosts: [&quot;host1&quot;, &quot;host2&quot;]</span><br><span class="line">#</span><br><span class="line"># Bootstrap the cluster using an initial set of master-eligible nodes:</span><br><span class="line"># 初始化一批具备成为主节点资格的节点【在选择主节点的时候会优先在这一批列表中进行选择】</span><br><span class="line">#cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;]</span><br><span class="line">#</span><br><span class="line"># For more information, consult the discovery and cluster formation module documentation.</span><br><span class="line">#</span><br><span class="line"># ---------------------------------- Various -----------------------------------</span><br><span class="line">#</span><br><span class="line"># Require explicit names when deleting indices:</span><br><span class="line"># 禁止使用通配符或_all删除索引, 必须使用名称或别名才能删除该索引。</span><br><span class="line">#action.destructive_requires_name: true</span><br></pre></td></tr></table></figure><h4 id="ES单机"><a href="#ES单机" class="headerlink" title="ES单机"></a>ES单机</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1：将ES的安装包上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2：在Linux中添加一个普通用户：es。</span><br><span class="line">因为ES目前不支持root用户启动。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# useradd -d &#x2F;home&#x2F;es -m es</span><br><span class="line">[root@bigdata01 soft]# passwd es</span><br><span class="line">Changing password for user es.</span><br><span class="line">New password: bigdata1234</span><br><span class="line">Retype new password: bigdata1234</span><br><span class="line">passwd: all authentication tokens updated successfully.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">3：修改Linux中最大文件描述符以及最大虚拟内存的参数</span><br><span class="line">因为ES对Linux的最大文件描述符以及最大虚拟内存有一定要求，所以需要修改，否则ES无法正常启动。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# vi &#x2F;etc&#x2F;security&#x2F;limits.conf </span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072</span><br><span class="line">* soft nproc 2048</span><br><span class="line">* hard nproc 4096</span><br><span class="line">[root@bigdata01 soft]# vi &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">vm.max_map_count&#x3D;262144</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">4：重启Linux系统。</span><br><span class="line">前面修改的参数需要重启系统才会生效。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# reboot -h now</span><br><span class="line"></span><br><span class="line">5：解压ES安装包。</span><br><span class="line">[root@bigdata01 soft]# tar -zxvf elasticsearch-7.13.4-linux-x86_64.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">6：配置ES_JAVA_HOME环境变量，指向ES中内置的JDK。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">......</span><br><span class="line">export ES_JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4&#x2F;jdk</span><br><span class="line">......</span><br><span class="line">[root@bigdata01 soft]# source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">7：修改elasticsearch-7.13.4目录的权限</span><br><span class="line">因为前面是使用root用户解压的，elasticsearch-7.13.4目录下的文件es用户是没有权限的。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# chmod 777 -R &#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">8：切换到es用户</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# su es</span><br><span class="line"></span><br><span class="line">9：修改elasticsearch.yml配置文件内容</span><br><span class="line">主要修改network.host、discovery.seed_hosts这两个参数。</span><br><span class="line"></span><br><span class="line">注意：yaml文件的格式，参数和值之间需要有一个空格。</span><br><span class="line"></span><br><span class="line">例如：network.host: bigdata01</span><br><span class="line">bigdata01前面必须要有一个空格，否则会报错。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[es@bigdata01 soft]$ cd elasticsearch-7.13.4</span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ vi config&#x2F;elasticsearch.yml </span><br><span class="line">......</span><br><span class="line">network.host: bigdata01</span><br><span class="line">discovery.seed_hosts: [&quot;bigdata01&quot;]</span><br><span class="line">......</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">10：启动ES服务【前台启动】</span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ bin&#x2F;elasticsearch</span><br><span class="line"></span><br><span class="line">按ctrl+c停止服务。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">11：启动ES服务【后台启动】</span><br><span class="line">在实际工作中需要将ES放在后台运行。</span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ bin&#x2F;elasticsearch -d</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">12：验证ES服务。</span><br><span class="line">通过jps命令验证进程是否存在。</span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ jps</span><br><span class="line">1849 Elasticsearch</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">通过web界面验证服务是否可以正常访问，端口为9200。</span><br><span class="line">http:&#x2F;&#x2F;bigdata01:9200&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111139441.png" alt="image-20230611113938806"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：需要关闭防火墙。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">13：停止ES服务。</span><br><span class="line">使用kill命令停止。</span><br><span class="line"></span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ jps</span><br><span class="line">1849 Elasticsearch</span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ kill</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">14：日志排查方式。</span><br><span class="line">如果发现ES服务启动有问题，需要查看ES的日志。</span><br><span class="line">ES的相关日志都在ES_HOME的logs目录下，ES服务的核心日志在elasticsearch.log日志文件中。</span><br></pre></td></tr></table></figure><h4 id="ES集群"><a href="#ES集群" class="headerlink" title="ES集群"></a>ES集群</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ES集群规划：</span><br><span class="line">bigdata01</span><br><span class="line">bigdata02</span><br><span class="line">bigdata03</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1：在bigdata01、bigdata02、bigdata03中创建普通用户：es。</span><br><span class="line">具体创建步骤参考ES单机中的操作。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# useradd -d &#x2F;home&#x2F;es -m es</span><br><span class="line">[root@bigdata01 soft]# passwd es</span><br><span class="line">Changing password for user es.</span><br><span class="line">New password: bigdata1234</span><br><span class="line">Retype new password: bigdata1234</span><br><span class="line">passwd: all authentication tokens updated successfully.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">useradd es和useradd -d &#x2F;home&#x2F;es -m es的区别是，前者会创建一个名为es的用户，但不会指定或创建它的家目录，后者会创建一个名为es的用户，并指定它的家目录为&#x2F;home&#x2F;es，并且使用-m选项来创建这个目录¹。如果你想修改一个已经存在的用户的家目录，你可以使用usermod -d命令¹。</span><br><span class="line"></span><br><span class="line">不一定。useradd es的默认家目录取决于&#x2F;etc&#x2F;default&#x2F;useradd文件中的HOME参数³。如果没有指定或修改这个参数，那么默认家目录就是&#x2F;home&#x2F;es⁴。但是，如果你没有使用-m或--create-home选项，那么useradd es不会创建这个家目录⁵。你需要手动创建或者使用usermod -m -d命令来移动已有的内容到新的家目录¹。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2：在bigdata01、bigdata02、bigdata03中修改Linux中最大文件描述符以及最大虚拟内存的参数。</span><br><span class="line">具体修改步骤参考ES单机中的操作。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# vi &#x2F;etc&#x2F;security&#x2F;limits.conf </span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072</span><br><span class="line">* soft nproc 2048</span><br><span class="line">* hard nproc 4096</span><br><span class="line">[root@bigdata01 soft]# vi &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">vm.max_map_count&#x3D;262144</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">3：重启bigdata01、bigdata02、bigdata03，让前面修改的参数生效。</span><br><span class="line">具体操作步骤参考ES单机中的操作。</span><br><span class="line">4：在bigdata01、bigdata02、bigdata03中配置ES_JAVA_HOME环境变量，指向ES中内置的JDK。</span><br><span class="line">具体配置步骤参考ES单机中的操作。</span><br><span class="line">5：在bigdata01中重新解压ES的安装包以及修改目录权限</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# tar -zxvf elasticsearch-7.13.4-linux-x86_64.tar.gz</span><br><span class="line">[root@bigdata01 soft]# chmod 777 -R &#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">6：修改elasticsearch.yml配置文件</span><br><span class="line">主要修改network.host、discovery.seed_hosts和cluster.initial_master_nodes这三个参数。</span><br><span class="line"></span><br><span class="line">7：将bigdata01中修改好配置的elasticsearch-7.13.4目录远程拷贝到bigdata02和bigdata03。</span><br><span class="line">[root@bigdata01 soft]# scp -rq elasticsearch-7.13.4 bigdata02:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">[root@bigdata01 soft]# scp -rq elasticsearch-7.13.4 bigdata03:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line"></span><br><span class="line">8：分别修改bigdata02和bigdata03中ES的elasticsearch.yml配置文件。</span><br><span class="line">修改bigdata02中的elasticsearch.yml配置文件，主要修改network.host参数的值为当前节点主机名。</span><br><span class="line">[root@bigdata02 elasticsearch-7.13.4]# vi config&#x2F;elasticsearch.yml </span><br><span class="line">......</span><br><span class="line">network.host: bigdata02</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">修改bigdata03中的elasticsearch.yml配置文件，主要修改network.host参数的值为当前节点主机名。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">9：在bigdata01、bigdata02、bigdata03中分别启动ES。</span><br><span class="line">在bigdata01上启动。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# su es</span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ bin&#x2F;elasticsearch -d</span><br><span class="line"></span><br><span class="line">在bigdata02上启动。</span><br><span class="line">在bigdata03上启动。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">10：验证集群中的进程。</span><br><span class="line">分别在bigdata01、bigdata02、bigdata03中验证进程是否存在。</span><br><span class="line"></span><br><span class="line">11：验证这几个节点是否组成一个集群。</span><br><span class="line">通过ES的REST API可以很方便的查看集群中的节点信息。</span><br><span class="line">http:&#x2F;&#x2F;bigdata01:9200&#x2F;_nodes&#x2F;_all?pretty</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111207382.png" alt="image-20230611120738840"></p><h3 id="ES集群监控管理工具-cerebro"><a href="#ES集群监控管理工具-cerebro" class="headerlink" title="ES集群监控管理工具-cerebro"></a>ES集群监控管理工具-cerebro</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">为了便于我们管理监控ES集群，推荐使用cerebro这个工具。</span><br><span class="line">1：首先到github上下载cerebro的安装包。</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;lmenezes&#x2F;cerebro&#x2F;releases</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111208875.png" alt="image-20230611120816538"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2：将下载好的cerebro-0.9.4.zip安装包上传到bigdata01的&#x2F;data&#x2F;soft目录中并且解压。</span><br><span class="line"></span><br><span class="line">注意：cerebro部署在任意节点上都可以，只要能和ES集群通信即可。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# ll cerebro-0.9.4.zip </span><br><span class="line">-rw-r--r--. 1 root root 57251010 Sep 11  2021 cerebro-0.9.4.zip</span><br><span class="line">[root@bigdata01 soft]# unzip cerebro-0.9.4.zip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3：启动cerebro。</span><br><span class="line">将cerebro放在后台启动。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 cerebro-0.9.4]# nohup bin&#x2F;cerebro 2&gt;&amp;1 &gt;&#x2F;dev&#x2F;null &amp;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：默认cerebro监听的端口是9000，如果出现端口冲突，需要修改cerebro监控的端口</span><br><span class="line"></span><br><span class="line">在启动cerebro的时候可以通过http.port参数指定端口号，如下命令：</span><br><span class="line">bin&#x2F;cerebro -Dhttp.port&#x3D;1234</span><br><span class="line"></span><br><span class="line">默认通过9000端口可以访问cerebro的web界面。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111210790.png" alt="image-20230611121030491"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4：使用cerebro。</span><br><span class="line">在Node address中输入ES集群任意一个节点的连接信息即可。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111211165.png" alt="image-20230611121101515"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5：使用cerebro监控管理ES集群。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111211190.png" alt="image-20230611121121089"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：集群有三种状态，green、yellow、red。</span><br><span class="line"></span><br><span class="line">green：表示集群处于健康状态，可以正常使用。</span><br><span class="line">yellow：表示集群处于风险状态，可以正常使用，可能是分片的副本个数不完整。例如：分片的副本数为2，但是现在分片的副本只有1份。</span><br><span class="line">red：表示集群处于故障状态，无法正常使用，可能是集群分片不完整。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6：cerebro的所有功能。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111212741.png" alt="image-20230611121226272"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6.1：查看节点信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111212780.png" alt="image-20230611121253312"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">6.2：rest功能。</span><br><span class="line">便于在页面中操作REST API接口</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111213303.png" alt="image-20230611121329984"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">6.3：更多功能。</span><br><span class="line">包括创建索引、集群参数、别名、分词功能、索引模板等。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111214032.png" alt="image-20230611121359073"></p><h3 id="ES的基本操作"><a href="#ES的基本操作" class="headerlink" title="ES的基本操作"></a>ES的基本操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">针对ES的操作，官方提供了很多种操作方式。</span><br><span class="line">https:&#x2F;&#x2F;www.elastic.co&#x2F;guide&#x2F;index.html</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111214584.png" alt="image-20230611121434249"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在实际工作中使用ES的时候，如果想屏蔽语言的差异，建议使用REST API，这种兼容性比较好，但是个人感觉有的操作使用起来比较麻烦，需要拼接组装各种数据字符串。</span><br><span class="line"></span><br><span class="line">针对Java程序员而言，还有一种选择是使用Java API，这种方式相对于REST API而言，代码量会大一些，但是代码层面看起来是比较清晰的。</span><br><span class="line"></span><br><span class="line">下面在操作ES的时候，分别使用一下这两种方式。</span><br></pre></td></tr></table></figure><h4 id="使用REST-API的方式操作ES"><a href="#使用REST-API的方式操作ES" class="headerlink" title="使用REST API的方式操作ES"></a>使用REST API的方式操作ES</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果想要在Linux命令行中使用REST API操作ES，需要借助于CURL工具。</span><br><span class="line">CURL是利用URL语法在命令行下工作的开源文件传输工具，使用CURL可以简单实现常见的get&#x2F;post请求。</span><br><span class="line"></span><br><span class="line">curl后面通过-X参数指定请求类型，通过-d指定要传递的参数。</span><br></pre></td></tr></table></figure><h5 id="索引库的操作（创建、删除）"><a href="#索引库的操作（创建、删除）" class="headerlink" title="索引库的操作（创建、删除）"></a>索引库的操作（创建、删除）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">创建索引库：</span><br><span class="line">curl -XPUT &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;&#39;</span><br><span class="line">这里使用PUT或者POST都可以。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl  -XPUT &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;&#39;</span><br><span class="line">&#123;&quot;acknowledged&quot;:true,&quot;shards_acknowledged&quot;:true,&quot;index&quot;:&quot;test&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：索引库名称必须要全部小写，不能以_、 -、 +开头，也不能包含逗号。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# curl  -XDELETE &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;&#39;</span><br><span class="line">&#123;&quot;acknowledged&quot;:true&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：索引库可以提前创建，也可以在后期添加数据的时候直接指定一个不存在的索引库，ES默认会自动创建这个索引库。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">手工创建索引库和自动创建索引库的区别就是，手工创建可以自定义索引库的分片数量。</span><br><span class="line">下面创建一个具有3个分片的索引库。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPUT &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;&#39; -d&#39;&#123;&quot;settings&quot;:&#123;&quot;index.number_of_shards&quot;:3&#125;&#125;&#39;</span><br><span class="line">&#123;&quot;acknowledged&quot;:true,&quot;shards_acknowledged&quot;:true,&quot;index&quot;:&quot;test&quot;&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111223294.png" alt="image-20230611122310001"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其中实线的框表示是主分片，虚线框是副本分片。</span><br><span class="line">索引分片编号是从0开始的，并且索引分片在物理层面是存在的，可以到集群中查看一下，从界面中也看到test索引库的1号和2号分片是在bigdata01节点上的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">到bigdata01节点中看一下，ES中的所有数据都在ES的数据存储目录中，默认是在ES_HOME下的data目录里面：</span><br><span class="line">[root@bigdata01 1IQ2r-vqRxSsicd8BzWPtg]# pwd</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4&#x2F;data&#x2F;nodes&#x2F;0&#x2F;indices&#x2F;1IQ2r-vqRxSsicd8BzWPtg</span><br><span class="line">[root@bigdata01 1IQ2r-vqRxSsicd8BzWPtg]# ll</span><br><span class="line">total 0</span><br><span class="line">drwxrwxr-x. 5 es es 49 Feb 26 18:01 1</span><br><span class="line">drwxrwxr-x. 5 es es 49 Feb 26 18:01 2</span><br><span class="line">drwxrwxr-x. 2 es es 24 Feb 26 18:01 _state</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里面的1IQ2r-vqRxSsicd8BzWPtg表示的是索引库的UUID。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111225755.png" alt="image-20230611122533512"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111225521.png" alt="image-20230611122545144"></p><h5 id="索引的操作（增、删、改、查）"><a href="#索引的操作（增、删、改、查）" class="headerlink" title="索引的操作（增、删、改、查）"></a>索引的操作（增、删、改、查）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">添加索引</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_doc&#x2F;1&#39; -d &#39;&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:20&#125;&#39;</span><br><span class="line">&#123;&quot;_index&quot;:&quot;emp&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line"></span><br><span class="line">1.这里emp索引库是不存在的，在使用的时候ES会自动创建，只不过索引分片数量默认是1。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111230257.png" alt="image-20230611123003969"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2.为了兼容之前的API，虽然ES现在取消了Type，但是API中Type的位置还是预留出来了，官方建议统一使用_doc 。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：在添加索引的时候，如果没有指定数据的ID，那么ES会自动生成一个随机的唯一ID。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_doc&#39; -d &#39;&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:30&#125;&#39; </span><br><span class="line">&#123;&quot;_index&quot;:&quot;emp&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;EFND8aMBpApLBooiIWda&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:2,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:1,&quot;_primary_term&quot;:1&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">查询索引：</span><br><span class="line">查看id&#x3D;1的索引数据。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl -XGET &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_doc&#x2F;1?pretty&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot; : &quot;emp&quot;,</span><br><span class="line">  &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">  &quot;_id&quot; : &quot;1&quot;,</span><br><span class="line">  &quot;_version&quot; : 1,</span><br><span class="line">  &quot;_seq_no&quot; : 0,</span><br><span class="line">  &quot;_primary_term&quot; : 1,</span><br><span class="line">  &quot;found&quot; : true,</span><br><span class="line">  &quot;_source&quot; : &#123;</span><br><span class="line">    &quot;name&quot; : &quot;tom&quot;,</span><br><span class="line">    &quot;age&quot; : 20</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">只获取部分字段内容。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl -XGET &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_doc&#x2F;1?_source&#x3D;name&amp;pretty&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot; : &quot;emp&quot;,</span><br><span class="line">  &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">  &quot;_id&quot; : &quot;1&quot;,</span><br><span class="line">  &quot;_version&quot; : 1,</span><br><span class="line">  &quot;_seq_no&quot; : 0,</span><br><span class="line">  &quot;_primary_term&quot; : 1,</span><br><span class="line">  &quot;found&quot; : true,</span><br><span class="line">  &quot;_source&quot; : &#123;</span><br><span class="line">    &quot;name&quot; : &quot;tom&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">[root@bigdata01 soft]# curl -XGET &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_doc&#x2F;1?_source&#x3D;name,age&amp;pretty&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot; : &quot;emp&quot;,</span><br><span class="line">  &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">  &quot;_id&quot; : &quot;1&quot;,</span><br><span class="line">  &quot;_version&quot; : 1,</span><br><span class="line">  &quot;_seq_no&quot; : 0,</span><br><span class="line">  &quot;_primary_term&quot; : 1,</span><br><span class="line">  &quot;found&quot; : true,</span><br><span class="line">  &quot;_source&quot; : &#123;</span><br><span class="line">    &quot;name&quot; : &quot;tom&quot;,</span><br><span class="line">    &quot;age&quot; : 20</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">查询指定索引库中所有数据。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl -XGET &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_search?pretty&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;took&quot; : 2,</span><br><span class="line">  &quot;timed_out&quot; : false,</span><br><span class="line">  &quot;_shards&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : 1,</span><br><span class="line">    &quot;successful&quot; : 1,</span><br><span class="line">    &quot;skipped&quot; : 0,</span><br><span class="line">    &quot;failed&quot; : 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : &#123;</span><br><span class="line">      &quot;value&quot; : 2,</span><br><span class="line">      &quot;relation&quot; : &quot;eq&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;max_score&quot; : 1.0,</span><br><span class="line">    &quot;hits&quot; : [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot; : &quot;emp&quot;,</span><br><span class="line">        &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">        &quot;_id&quot; : &quot;1&quot;,</span><br><span class="line">        &quot;_score&quot; : 1.0,</span><br><span class="line">        &quot;_source&quot; : &#123;</span><br><span class="line">          &quot;name&quot; : &quot;tom&quot;,</span><br><span class="line">          &quot;age&quot; : 20</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot; : &quot;emp&quot;,</span><br><span class="line">        &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">        &quot;_id&quot; : &quot;EVPO8aMBpApLBooib2e7&quot;,</span><br><span class="line">        &quot;_score&quot; : 1.0,</span><br><span class="line">        &quot;_source&quot; : &#123;</span><br><span class="line">          &quot;name&quot; : &quot;jack&quot;,</span><br><span class="line">          &quot;age&quot; : 30</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：针对这种查询操作，可以在浏览器里面执行，或者在cerebo中查询都是可以的，看起来更加清晰。</span><br></pre></td></tr></table></figure><h5 id="更新索引"><a href="#更新索引" class="headerlink" title="更新索引"></a>更新索引</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">可以分为全部更新和局部更新</span><br><span class="line">全部更新：同添加索引，如果指定id的索引数据（文档）已经存在，则执行更新操作。</span><br><span class="line"></span><br><span class="line">注意：执行更新操作的时候，ES首先将旧的文标记为删除状态，然后添加新的文档</span><br><span class="line"></span><br><span class="line">旧的文档不会立即消失，但是你也无法访问，ES会在你继续添加更多数据的时候在后台清理已经标记为删除状态的文档。</span><br><span class="line"></span><br><span class="line">局部更新：可以添加新字段或者更新已有字段，必须使用POST请求。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_doc&#x2F;1&#x2F;_update&#39; -d &#39;&#123;&quot;doc&quot;:&#123;&quot;age&quot;:25&#125;&#125;&#39;</span><br><span class="line">&#123;&quot;_index&quot;:&quot;emp&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:2,&quot;result&quot;:&quot;updated&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:2,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:2,&quot;_primary_term&quot;:1&#125;</span><br></pre></td></tr></table></figure><h5 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">删除id&#x3D;1的索引数据。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl -XDELETE &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_doc&#x2F;1&#39;</span><br><span class="line">&#123;&quot;_index&quot;:&quot;emp&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:3,&quot;result&quot;:&quot;deleted&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:2,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:3,&quot;_primary_term&quot;:1&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# curl -XDELETE &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_doc&#x2F;1&#39;</span><br><span class="line">&#123;&quot;_index&quot;:&quot;emp&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:4,&quot;result&quot;:&quot;not_found&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:2,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:4,&quot;_primary_term&quot;:1&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如果索引数据（文档）存在，ES返回的数据中，result属性值为deleted，_version（版本）属性的值+1。</span><br><span class="line"></span><br><span class="line">如果索引数据不存在，ES返回的数据中，result属性值为not_found，但是_version属性的值依然会+1，这属于ES的版本控制系统，它保证了我们在多个节点间的不同操作的顺序都被正确标记了。</span><br><span class="line">对于索引数据的每次写操作，无论是index，update还是delete，ES都会将_version增加 1。该增加是原子的，并且保证在操作成功返回时会发生。</span><br><span class="line"></span><br><span class="line">注意：删除一条索引数据（文档）也不会立即生效，它只是被标记成已删除状态。ES将会在你之后添加更多索引数据的时候才会在后台清理标记为删除状态的内容。</span><br></pre></td></tr></table></figure><h5 id="Bulk批量操作"><a href="#Bulk批量操作" class="headerlink" title="Bulk批量操作"></a>Bulk批量操作</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Bulk API可以帮助我们同时执行多个请求，提高效率。</span><br><span class="line">格式：</span><br><span class="line">&#123; action: &#123; metadata &#125;&#125;</span><br><span class="line">&#123; request body &#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line"></span><br><span class="line">action：index&#x2F;create&#x2F;update&#x2F;delete</span><br><span class="line">metadata：_index,_type,_id</span><br><span class="line">request body：_source(删除操作不需要)</span><br><span class="line"></span><br><span class="line">create 和index的区别：如果数据存在，使用create操作失败，会提示文档已经存在，使用index则可以成功执行。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面来看一个案例，假设在MySQL中有一批数据，首先需要从MySQL中把数据读取出来，然后将数据转化为Bulk需要的数据格式。</span><br><span class="line"></span><br><span class="line">在这直接手工生成Bulk需要的数据格式。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 elasticsearch-7.13.4]# vi request </span><br><span class="line">&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;</span><br><span class="line">&#123; &quot;field1&quot; : &quot;value1&quot; &#125;</span><br><span class="line">&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;</span><br><span class="line">&#123; &quot;field1&quot; : &quot;value1&quot; &#125;</span><br><span class="line">&#123; &quot;delete&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;</span><br><span class="line">&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;</span><br><span class="line">&#123; &quot;field1&quot; : &quot;value1&quot; &#125;</span><br><span class="line">&#123; &quot;update&quot; : &#123;&quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;,&quot;_id&quot; : &quot;1&quot; &#125; &#125;</span><br><span class="line">&#123; &quot;doc&quot; : &#123;&quot;field2&quot; : &quot;value2&quot;&#125; &#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">执行Bulk API</span><br><span class="line"></span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# curl -H &quot;Content-Type: application&#x2F;json&quot;  -XPUT &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;_doc&#x2F;_bulk&#39; --data-binary @request</span><br><span class="line">&#123;&quot;took&quot;:167,&quot;errors&quot;:false,&quot;items&quot;:[&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:2,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;2&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:2,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:0,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;2&quot;,&quot;_version&quot;:2,&quot;result&quot;:&quot;deleted&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:2,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:1,&quot;_primary_term&quot;:1,&quot;status&quot;:200&#125;&#125;,&#123;&quot;create&quot;:&#123;&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;3&quot;,&quot;_version&quot;:1,&quot;result&quot;:&quot;created&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:2,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:2,&quot;_primary_term&quot;:1,&quot;status&quot;:201&#125;&#125;,&#123;&quot;update&quot;:&#123;&quot;_index&quot;:&quot;test&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;:2,&quot;result&quot;:&quot;updated&quot;,&quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:2,&quot;failed&quot;:0&#125;,&quot;_seq_no&quot;:1,&quot;_primary_term&quot;:1,&quot;status&quot;:200&#125;&#125;]&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 elasticsearch-7.13.4]# curl -XGET &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;_search?pretty&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;took&quot; : 6,</span><br><span class="line">  &quot;timed_out&quot; : false,</span><br><span class="line">  &quot;_shards&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : 3,</span><br><span class="line">    &quot;successful&quot; : 3,</span><br><span class="line">    &quot;skipped&quot; : 0,</span><br><span class="line">    &quot;failed&quot; : 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : &#123;</span><br><span class="line">      &quot;value&quot; : 2,</span><br><span class="line">      &quot;relation&quot; : &quot;eq&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;max_score&quot; : 1.0,</span><br><span class="line">    &quot;hits&quot; : [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot; : &quot;test&quot;,</span><br><span class="line">        &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">        &quot;_id&quot; : &quot;3&quot;,</span><br><span class="line">        &quot;_score&quot; : 1.0,</span><br><span class="line">        &quot;_source&quot; : &#123;</span><br><span class="line">          &quot;field1&quot; : &quot;value1&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot; : &quot;test&quot;,</span><br><span class="line">        &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">        &quot;_id&quot; : &quot;1&quot;,</span><br><span class="line">        &quot;_score&quot; : 1.0,</span><br><span class="line">        &quot;_source&quot; : &#123;</span><br><span class="line">          &quot;field1&quot; : &quot;value1&quot;,</span><br><span class="line">          &quot;field2&quot; : &quot;value2&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Bulk一次最大可以处理多少数据量？</span><br><span class="line"></span><br><span class="line">Bulk会把将要处理的数据加载到内存中，所以数据量是有限制的，最佳的数据量不是一个确定的数值，它取决于集群硬件，文档大小、文档复杂性，索引以及ES集群的负载。</span><br><span class="line"></span><br><span class="line">一般建议是1000-5000个文档，如果文档很大，可以适当减少，文档总大小建议是5-15MB，默认不能超过100M。</span><br><span class="line">如果想要修改最大限制大小，可以在ES的配置文件中修改http.max_content_length: 100mb，但是不建议，因为太大的话Bulk操作也会慢。</span><br></pre></td></tr></table></figure><h4 id="使用Java-API的方式操作ES"><a href="#使用Java-API的方式操作ES" class="headerlink" title="使用Java API的方式操作ES"></a>使用Java API的方式操作ES</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">针对Java API，目前ES提供了两个Java REST Client版本:</span><br><span class="line"></span><br><span class="line">1.Java Low Level REST Client：</span><br><span class="line">低级别的REST客户端，通过HTTP与集群交互，用户需自己组装请求JSON串，以及解析响应JSON串。兼容所有Elasticsearch版本。</span><br><span class="line">这种方式其实就相当于使用Java对前面讲的REST API做了一层简单的封装，前面我们是使用的CURL这个工具执行的，现在是使用Java代码模拟执行HTTP请求了。</span><br><span class="line"></span><br><span class="line">2.Java High Level REST Client：</span><br><span class="line">高级别的REST客户端，基于低级别的REST客户端进行了封装，增加了组装请求JSON串、解析响应JSON串等相关API，开发代码使用的ES版本需要和集群中的ES版本一致，否则会有版本冲突问题。</span><br><span class="line">这种方式是从ES 6.0版本开始加入的，目的是以Java面向对象的方式进行请求、响应处理。</span><br><span class="line">高级别的REST客户端会兼容高版本的ES集群，例如：使用ES7.0版本开发的代码可以和任何7.x版本的ES集群交互。</span><br><span class="line">如果ES集群后期升级到了8.x版本，那么也要升级之前基于ES 7.0版本开发的代码。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果考虑到代码后期的兼容性，建议使用Java Low Level REST Client。</span><br><span class="line">如果考虑到易用性，建议使用Java High Level REST Client。</span><br><span class="line">在这我们使用Java High Level REST Client。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">创建maven项目：db_elasticsearch</span><br><span class="line">创建包：com.imooc.es</span><br><span class="line">在pom.xml文件中添加ES的依赖和日志的依赖。</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.elasticsearch.client&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;version&gt;7.13.4&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.logging.log4j&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;log4j-core&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.14.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">在resources目录下添加log4j2.properties。</span><br><span class="line"></span><br><span class="line">appender.console.type &#x3D; Console</span><br><span class="line">appender.console.name &#x3D; console</span><br><span class="line">appender.console.layout.type &#x3D; PatternLayout</span><br><span class="line">appender.console.layout.pattern &#x3D; [%d&#123;ISO8601&#125;][%-5p][%-25c] %marker%m%n</span><br><span class="line"></span><br><span class="line">rootLogger.level &#x3D; info</span><br><span class="line">rootLogger.appenderRef.console.ref &#x3D; console</span><br></pre></td></tr></table></figure><h5 id="索引库的操作（创建、删除）-1"><a href="#索引库的操作（创建、删除）-1" class="headerlink" title="索引库的操作（创建、删除）"></a>索引库的操作（创建、删除）</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.es;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RequestOptions;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestHighLevelClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.indices.CreateIndexRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.settings.Settings;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 针对ES中索引库的操作</span></span><br><span class="line"><span class="comment"> * 1：创建索引库</span></span><br><span class="line"><span class="comment"> * 2：删除索引库</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EsIndexOp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//获取RestClient连接</span></span><br><span class="line">        RestHighLevelClient client = <span class="keyword">new</span> RestHighLevelClient(</span><br><span class="line">                RestClient.builder(</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata01"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata02"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata03"</span>, <span class="number">9200</span>, <span class="string">"http"</span>)));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建索引库</span></span><br><span class="line">        <span class="comment">//createIndex(client);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//删除索引库</span></span><br><span class="line">        <span class="comment">//deleteIndex(client);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        client.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteIndex</span><span class="params">(RestHighLevelClient client)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        DeleteIndexRequest deleteRequest = <span class="keyword">new</span> DeleteIndexRequest(<span class="string">"java_test"</span>);</span><br><span class="line">        <span class="comment">//执行</span></span><br><span class="line">        client.indices().delete(deleteRequest, RequestOptions.DEFAULT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createIndex</span><span class="params">(RestHighLevelClient client)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        CreateIndexRequest createRequest = <span class="keyword">new</span> CreateIndexRequest(<span class="string">"java_test"</span>);</span><br><span class="line">        <span class="comment">//指定索引库的配置信息</span></span><br><span class="line">        createRequest.settings(Settings.builder()</span><br><span class="line">                .put(<span class="string">"index.number_of_shards"</span>, <span class="number">3</span>)<span class="comment">//指定分片个数</span></span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行</span></span><br><span class="line">        client.indices().create(createRequest, RequestOptions.DEFAULT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行代码的时候会有一个警告信息，提示ES集群没有开启权限校验机制，其实在企业中只要在运维层面控制好了ES集群IP和端口的访问其实就足够了。</span><br></pre></td></tr></table></figure><h5 id="索引的操作（增、删、改、查、Bulk批量操作）"><a href="#索引的操作（增、删、改、查、Bulk批量操作）" class="headerlink" title="索引的操作（增、删、改、查、Bulk批量操作）"></a>索引的操作（增、删、改、查、Bulk批量操作）</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.es;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.logging.LogFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.apache.logging.log4j.LogManager;</span><br><span class="line"><span class="keyword">import</span> org.apache.logging.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.bulk.BulkItemResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.bulk.BulkRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.bulk.BulkResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.delete.DeleteRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.get.GetRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.get.GetResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.index.IndexRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.update.UpdateRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RequestOptions;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestHighLevelClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.Strings;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.xcontent.XContentType;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.fetch.subphase.FetchSourceContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 针对ES中索引数据的操作</span></span><br><span class="line"><span class="comment"> * 增删改查</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EsDataOp</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Logger logger = LogManager.getLogger(EsDataOp<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//获取RestClient连接</span></span><br><span class="line">        RestHighLevelClient client = <span class="keyword">new</span> RestHighLevelClient(</span><br><span class="line">                RestClient.builder(</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata01"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata02"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata03"</span>, <span class="number">9200</span>, <span class="string">"http"</span>)));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建索引</span></span><br><span class="line">        <span class="comment">//addIndexByJson(client);</span></span><br><span class="line">        <span class="comment">//addIndexByMap(client);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//查询索引</span></span><br><span class="line">        <span class="comment">//getIndex(client);</span></span><br><span class="line">        <span class="comment">//getIndexByFiled(client);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//更新索引</span></span><br><span class="line">        <span class="comment">//注意：可以使用创建索引直接完整更新已存在的数据</span></span><br><span class="line">        <span class="comment">//updateIndexByPart(client);//局部更新</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//删除索引</span></span><br><span class="line">        <span class="comment">//deleteIndex(client);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//Bulk批量操作</span></span><br><span class="line">        <span class="comment">//bulkIndex(client);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        client.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bulkIndex</span><span class="params">(RestHighLevelClient client)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        BulkRequest request = <span class="keyword">new</span> BulkRequest();</span><br><span class="line">        request.add(<span class="keyword">new</span> IndexRequest(<span class="string">"emp"</span>).id(<span class="string">"20"</span>)</span><br><span class="line">                .source(XContentType.JSON,<span class="string">"field1"</span>, <span class="string">"value1"</span>,<span class="string">"field2"</span>,<span class="string">"value2"</span>));</span><br><span class="line">        request.add(<span class="keyword">new</span> DeleteRequest(<span class="string">"emp"</span>, <span class="string">"10"</span>));<span class="comment">//id为10的数据不存在，但是执行删除是不会报错的</span></span><br><span class="line">        request.add(<span class="keyword">new</span> UpdateRequest(<span class="string">"emp"</span>, <span class="string">"11"</span>)</span><br><span class="line">                .doc(XContentType.JSON,<span class="string">"age"</span>, <span class="number">19</span>));</span><br><span class="line">        request.add(<span class="keyword">new</span> UpdateRequest(<span class="string">"emp"</span>, <span class="string">"12"</span>)<span class="comment">//id为12的数据不存在，这一条命令在执行的时候会失败</span></span><br><span class="line">                .doc(XContentType.JSON,<span class="string">"age"</span>, <span class="number">19</span>));</span><br><span class="line">        <span class="comment">//执行</span></span><br><span class="line">        BulkResponse bulkResponse = client.bulk(request, RequestOptions.DEFAULT);</span><br><span class="line">        <span class="comment">//如果Bulk中的个别语句出错不会导致整个Bulk执行失败，所以可以在这里判断一下是否有返回执行失败的信息</span></span><br><span class="line">        <span class="keyword">for</span> (BulkItemResponse bulkItemResponse : bulkResponse) &#123;</span><br><span class="line">            <span class="keyword">if</span> (bulkItemResponse.isFailed()) &#123;</span><br><span class="line">                BulkItemResponse.Failure failure = bulkItemResponse.getFailure();</span><br><span class="line">                logger.error(<span class="string">"Bulk中出现了异常："</span>+failure);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">deleteIndex</span><span class="params">(RestHighLevelClient client)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        DeleteRequest request = <span class="keyword">new</span> DeleteRequest(<span class="string">"emp"</span>, <span class="string">"10"</span>);</span><br><span class="line">        <span class="comment">//执行</span></span><br><span class="line">        client.delete(request, RequestOptions.DEFAULT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">updateIndexByPart</span><span class="params">(RestHighLevelClient client)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        UpdateRequest request = <span class="keyword">new</span> UpdateRequest(<span class="string">"emp"</span>, <span class="string">"10"</span>);</span><br><span class="line">        String jsonString = <span class="string">"&#123;\"age\":23&#125;"</span>;</span><br><span class="line">        request.doc(jsonString, XContentType.JSON);</span><br><span class="line">        <span class="comment">//执行</span></span><br><span class="line">        client.update(request, RequestOptions.DEFAULT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getIndexByFiled</span><span class="params">(RestHighLevelClient client)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        GetRequest request = <span class="keyword">new</span> GetRequest(<span class="string">"emp"</span>, <span class="string">"10"</span>);</span><br><span class="line">        <span class="comment">//只查询部分字段</span></span><br><span class="line">        String[] includes = <span class="keyword">new</span> String[]&#123;<span class="string">"name"</span>&#125;;<span class="comment">//指定包含哪些字段</span></span><br><span class="line">        String[] excludes = Strings.EMPTY_ARRAY;<span class="comment">//指定多滤掉哪些字段</span></span><br><span class="line">        FetchSourceContext fetchSourceContext = <span class="keyword">new</span> FetchSourceContext(<span class="keyword">true</span>, includes, excludes);</span><br><span class="line">        request.fetchSourceContext(fetchSourceContext);</span><br><span class="line">        <span class="comment">//执行</span></span><br><span class="line">        GetResponse response = client.get(request, RequestOptions.DEFAULT);</span><br><span class="line">        <span class="comment">//通过response获取index、id、文档详细内容（source）</span></span><br><span class="line">        String index = response.getIndex();</span><br><span class="line">        String id = response.getId();</span><br><span class="line">        <span class="keyword">if</span>(response.isExists())&#123;<span class="comment">//如果没有查询到文档数据，则isExists返回false</span></span><br><span class="line">            <span class="comment">//获取json字符串格式的文档结果</span></span><br><span class="line">            String sourceAsString = response.getSourceAsString();</span><br><span class="line">            System.out.println(sourceAsString);</span><br><span class="line">            <span class="comment">//获取map格式的文档结果</span></span><br><span class="line">            Map&lt;String, Object&gt; sourceAsMap = response.getSourceAsMap();</span><br><span class="line">            System.out.println(sourceAsMap);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            logger.warn(<span class="string">"没有查询到索引库&#123;&#125;中id为&#123;&#125;的文档!"</span>,index,id);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getIndex</span><span class="params">(RestHighLevelClient client)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        GetRequest request = <span class="keyword">new</span> GetRequest(<span class="string">"emp"</span>, <span class="string">"10"</span>);</span><br><span class="line">        <span class="comment">//执行</span></span><br><span class="line">        GetResponse response = client.get(request, RequestOptions.DEFAULT);</span><br><span class="line">        <span class="comment">//通过response获取index、id、文档详细内容（source）</span></span><br><span class="line">        String index = response.getIndex();</span><br><span class="line">        String id = response.getId();</span><br><span class="line">        <span class="keyword">if</span>(response.isExists())&#123;<span class="comment">//如果没有查询到文档数据，则isExists返回false</span></span><br><span class="line">            <span class="comment">//获取json字符串格式的文档结果</span></span><br><span class="line">            String sourceAsString = response.getSourceAsString();</span><br><span class="line">            System.out.println(sourceAsString);</span><br><span class="line">            <span class="comment">//获取map格式的文档结果</span></span><br><span class="line">            Map&lt;String, Object&gt; sourceAsMap = response.getSourceAsMap();</span><br><span class="line">            System.out.println(sourceAsMap);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            logger.warn(<span class="string">"没有查询到索引库&#123;&#125;中id为&#123;&#125;的文档!"</span>,index,id);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addIndexByMap</span><span class="params">(RestHighLevelClient client)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        IndexRequest request = <span class="keyword">new</span> IndexRequest(<span class="string">"emp"</span>);</span><br><span class="line">        request.id(<span class="string">"11"</span>);</span><br><span class="line">        HashMap&lt;String, Object&gt; jsonMap = <span class="keyword">new</span> HashMap&lt;String, Object&gt;();</span><br><span class="line">        jsonMap.put(<span class="string">"name"</span>, <span class="string">"tom"</span>);</span><br><span class="line">        jsonMap.put(<span class="string">"age"</span>, <span class="number">17</span>);</span><br><span class="line">        request.source(jsonMap);</span><br><span class="line">        <span class="comment">//执行</span></span><br><span class="line">        client.index(request, RequestOptions.DEFAULT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">addIndexByJson</span><span class="params">(RestHighLevelClient client)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        IndexRequest request = <span class="keyword">new</span> IndexRequest(<span class="string">"emp"</span>);</span><br><span class="line">        request.id(<span class="string">"10"</span>);</span><br><span class="line">        String jsonString = <span class="string">"&#123;"</span> +</span><br><span class="line">                <span class="string">"\"name\":\"jessic\","</span> +</span><br><span class="line">                <span class="string">"\"age\":20"</span> +</span><br><span class="line">                <span class="string">"&#125;"</span>;</span><br><span class="line">        request.source(jsonString, XContentType.JSON);</span><br><span class="line">        <span class="comment">//执行</span></span><br><span class="line">        client.index(request, RequestOptions.DEFAULT);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-全文检索引擎Elasticsearch-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8EElasticsearch-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E5%BC%95%E6%93%8EElasticsearch-2.html</id>
    <published>2023-06-02T10:03:09.000Z</published>
    <updated>2023-06-11T09:19:40.209Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="全文检索引擎Elasticsearch-2"><a href="#全文检索引擎Elasticsearch-2" class="headerlink" title="全文检索引擎Elasticsearch-2"></a>全文检索引擎Elasticsearch-2</h1><h2 id="3-Elasticsearch分词详解"><a href="#3-Elasticsearch分词详解" class="headerlink" title="3 Elasticsearch分词详解"></a>3 Elasticsearch分词详解</h2><h3 id="ES分词介绍"><a href="#ES分词介绍" class="headerlink" title="ES分词介绍"></a>ES分词介绍</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ES中在添加数据，也就是创建索引的时候，会先对数据进行分词。</span><br><span class="line">在查询索引数据的时候，也会先根据查询的关键字进行分词。</span><br><span class="line">所以在ES中分词这个过程是非常重要的，涉及到查询的效率和准确度。</span><br><span class="line"></span><br><span class="line">假设有一条数据，数据中有一个字段是titile，这个字段的值为LexCorp BFG-9000。</span><br><span class="line">我们想要把这条数据在ES中创建索引，方便后期检索。</span><br><span class="line"></span><br><span class="line">创建索引和查询索引的大致流程是这样的：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111457924.png" alt="image-20230611145714539"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">图中左侧是创建索引的过程：</span><br><span class="line">首先对数据进行空白字符分割，将LexCorp BFG-9000切分为LexCorp和BFG-9000。</span><br><span class="line">然后进行单词切割，将LexCorp切分为Lex和Corp，BFG-9000切分为BFG和9000。</span><br><span class="line">最后执行小写转换操作，将英文单词全部转换为小写。</span><br><span class="line"></span><br><span class="line">图中右侧是查询索引的过程：</span><br><span class="line">后期想要查询LexCorp BFG-9000这条数据，但是具体的内容记不清了，大致想起来了一些关键词Lex corp bfg9000。</span><br><span class="line">接下来就根据这些关键词进行查询，</span><br><span class="line">首先还是对数据进行空白符分割，将Lex corp bfg9000切分为Lex、corp 和bfg9000。</span><br><span class="line">然后进行单词切割，Lex和corp不变，将bfg9000切分为bfg和9000。</span><br><span class="line">最后执行小写转换操作，将英文单词全部转换为小写。</span><br><span class="line">这样其实在检索的时候就可以忽略英文大小写了，因为前面在创建索引的时候也会对英文进行小写转换。</span><br><span class="line"></span><br><span class="line">到这可以发现，使用Lex corp bfg9000是可以查找到LexCorp BFG-9000这条数据的，因为在经过空白符分割、单词切割、小写转换之后，这两条数据是一样的，其实只要能有一个单词是匹配的，就可以把这条数据查找出来。</span><br><span class="line"></span><br><span class="line">了解了这个流程之后，我们以后在搜索引擎里面搜索一些内容的时候其实就知道要怎么快速高效的检索内容了，只需要输入一些关键词，中间最好用空格隔开，针对英文字符不用纠结大小写了。</span><br><span class="line"></span><br><span class="line">这些数据在ES中分词之后，其实在底层会产生倒排索引，注意了，倒排索引是ES能够提供快速检索能力的核心，下面来看一下这个倒排索引</span><br></pre></td></tr></table></figure><h3 id="倒排索引介绍"><a href="#倒排索引介绍" class="headerlink" title="倒排索引介绍"></a>倒排索引介绍</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">假设有一批数据，数据中有两个字段，文档编号和文档内容。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111501138.png" alt="image-20230611150131888"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对这一批数据，在ES中创建索引之后，最终产生的倒排索引内容大致是这样的：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111501077.png" alt="image-20230611150153996"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line"></span><br><span class="line">单词ID：记录每个单词的单词编号。</span><br><span class="line">单词：对应的单词。</span><br><span class="line">文档频率：代表文档集合中有多少个文档包含某个单词。</span><br><span class="line">倒排列表：包含单词ID及其它必要信息。</span><br><span class="line">DocId：单词出现的文档id。</span><br><span class="line">TF：单词在某个文档中出现的次数。</span><br><span class="line">POS：单词在文档中出现的位置。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">以单词 加盟 为例，其单词编号为6，文档频率为3，代表整个文档集合中有3个文档包含这个单词，对应的倒排列表为&#123;(2;1;&lt;4&gt;),(3;1;&lt;7&gt;),(5;1;&lt;5&gt;)&#125;，含义是在文档2，3，5中出现过这个单词，在每个文档中都只出现过1次，单词 加盟 在第一个文档的POS（位置）是4，即文档的第四个单词是 加盟 ，其它的类似。</span><br><span class="line">这个倒排索引已经是一个非常完备的索引系统，实际搜索系统的索引结构基本如此。</span><br></pre></td></tr></table></figure><h3 id="分词器的作用"><a href="#分词器的作用" class="headerlink" title="分词器的作用"></a>分词器的作用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">前面分析了ES在创建索引和查询索引的时候都需要进行分词，分词需要用到分词器。下面来具体分析一下分词器的作用：</span><br><span class="line"></span><br><span class="line">分词器的作用是把一段文本中的词按照一定规则进行切分。</span><br><span class="line"></span><br><span class="line">分词器对应的是Analyzer类，这是一个抽象类，切分词的具体规则是由子类实现的。</span><br><span class="line">也就是说不同的分词器分词的规则是不同的！</span><br><span class="line"></span><br><span class="line">所以对于不同的语言，要用不同的分词器。</span><br><span class="line">在创建索引时会用到分词器，在搜索时也会用到分词器，这两个地方要使用同一个分词器，否则可能会搜索不出结果。</span><br></pre></td></tr></table></figure><h4 id="分词器的工作流程"><a href="#分词器的工作流程" class="headerlink" title="分词器的工作流程"></a>分词器的工作流程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">分词器的工作流程一般是这样的：</span><br><span class="line"></span><br><span class="line">1.切分关键词，把关键的、核心的单词切出来。</span><br><span class="line">2.去除停用词。</span><br><span class="line">3.对于英文单词，把所有字母转为小写（搜索时不区分大小写）</span><br><span class="line"></span><br><span class="line">针对停用词下面来详细分析一下：</span><br></pre></td></tr></table></figure><h5 id="停用词"><a href="#停用词" class="headerlink" title="停用词"></a>停用词</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">有些词在文本中出现的频率非常高，但是对文本所携带的信息基本不产生影响。</span><br><span class="line">例如：</span><br><span class="line">英文停用词：a、an、the、of等</span><br><span class="line">中文停用词：的、了、着、是、标点符号等</span><br><span class="line"></span><br><span class="line">文本经过分词之后，停用词通常被过滤掉，不会被进行索引。</span><br><span class="line">在检索的时候，用户的查询中如果含有停用词，检索系统也会将其过滤掉（因为用户输入的查询字符串也要进行分词处理）。</span><br><span class="line">排除停用词可以加快建立索引的速度，减小索引库文件的大小，并且还可以提高查询的准确度。</span><br><span class="line">如果不去除停用词，可能会存在这个情况：</span><br><span class="line">假设有一批文章数据，基本上每篇文章里面都有 的 这个词，那我在检索的时候只要输入了的这个词，那么所有文章都认为是满足条件的数据，但是这样是没有意义的。</span><br><span class="line"></span><br><span class="line">常见的英文停用词汇总：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line">a</span><br><span class="line">about</span><br><span class="line">above</span><br><span class="line">after</span><br><span class="line">again</span><br><span class="line">against</span><br><span class="line">all</span><br><span class="line">am</span><br><span class="line">an</span><br><span class="line">and</span><br><span class="line">any</span><br><span class="line">are</span><br><span class="line">aren&#39;t</span><br><span class="line">as</span><br><span class="line">at</span><br><span class="line">be</span><br><span class="line">because</span><br><span class="line">been</span><br><span class="line">before</span><br><span class="line">being</span><br><span class="line">below</span><br><span class="line">between</span><br><span class="line">both</span><br><span class="line">but</span><br><span class="line">by</span><br><span class="line">can&#39;t</span><br><span class="line">cannot</span><br><span class="line">could</span><br><span class="line">couldn&#39;t</span><br><span class="line">did</span><br><span class="line">didn&#39;t</span><br><span class="line">do</span><br><span class="line">does</span><br><span class="line">doesn&#39;t</span><br><span class="line">doing</span><br><span class="line">don&#39;t</span><br><span class="line">down</span><br><span class="line">during</span><br><span class="line">each</span><br><span class="line">few</span><br><span class="line">for</span><br><span class="line">from</span><br><span class="line">further</span><br><span class="line">had</span><br><span class="line">hadn&#39;t</span><br><span class="line">has</span><br><span class="line">hasn&#39;t</span><br><span class="line">have</span><br><span class="line">haven&#39;t</span><br><span class="line">having</span><br><span class="line">he</span><br><span class="line">he&#39;d</span><br><span class="line">he&#39;ll</span><br><span class="line">he&#39;s</span><br><span class="line">her</span><br><span class="line">here</span><br><span class="line">here&#39;s</span><br><span class="line">hers</span><br><span class="line">herself</span><br><span class="line">him</span><br><span class="line">himself</span><br><span class="line">his</span><br><span class="line">how</span><br><span class="line">how&#39;s</span><br><span class="line">i</span><br><span class="line">i&#39;d</span><br><span class="line">i&#39;ll</span><br><span class="line">i&#39;m</span><br><span class="line">i&#39;ve</span><br><span class="line">if</span><br><span class="line">in</span><br><span class="line">into</span><br><span class="line">is</span><br><span class="line">isn&#39;t</span><br><span class="line">it</span><br><span class="line">it&#39;s</span><br><span class="line">its</span><br><span class="line">itself</span><br><span class="line">let&#39;s</span><br><span class="line">me</span><br><span class="line">more</span><br><span class="line">most</span><br><span class="line">mustn&#39;t</span><br><span class="line">my</span><br><span class="line">myself</span><br><span class="line">no</span><br><span class="line">nor</span><br><span class="line">not</span><br><span class="line">of</span><br><span class="line">off</span><br><span class="line">on</span><br><span class="line">once</span><br><span class="line">only</span><br><span class="line">or</span><br><span class="line">other</span><br><span class="line">ought</span><br><span class="line">our</span><br><span class="line">ours</span><br><span class="line">ourselves</span><br><span class="line">out</span><br><span class="line">over</span><br><span class="line">own</span><br><span class="line">same</span><br><span class="line">shan&#39;t</span><br><span class="line">she</span><br><span class="line">she&#39;d</span><br><span class="line">she&#39;ll</span><br><span class="line">she&#39;s</span><br><span class="line">should</span><br><span class="line">shouldn&#39;t</span><br><span class="line">so</span><br><span class="line">some</span><br><span class="line">such</span><br><span class="line">than</span><br><span class="line">that</span><br><span class="line">that&#39;s</span><br><span class="line">the</span><br><span class="line">their</span><br><span class="line">theirs</span><br><span class="line">them</span><br><span class="line">themselves</span><br><span class="line">then</span><br><span class="line">there</span><br><span class="line">there&#39;s</span><br><span class="line">these</span><br><span class="line">they</span><br><span class="line">they&#39;d</span><br><span class="line">they&#39;ll</span><br><span class="line">they&#39;re</span><br><span class="line">they&#39;ve</span><br><span class="line">this</span><br><span class="line">those</span><br><span class="line">through</span><br><span class="line">to</span><br><span class="line">too</span><br><span class="line">under</span><br><span class="line">until</span><br><span class="line">up</span><br><span class="line">very</span><br><span class="line">was</span><br><span class="line">wasn&#39;t</span><br><span class="line">we</span><br><span class="line">we&#39;d</span><br><span class="line">we&#39;ll</span><br><span class="line">we&#39;re</span><br><span class="line">we&#39;ve</span><br><span class="line">were</span><br><span class="line">weren&#39;t</span><br><span class="line">what</span><br><span class="line">what&#39;s</span><br><span class="line">when</span><br><span class="line">when&#39;s</span><br><span class="line">where</span><br><span class="line">where&#39;s</span><br><span class="line">which</span><br><span class="line">while</span><br><span class="line">who</span><br><span class="line">who&#39;s</span><br><span class="line">whom</span><br><span class="line">why</span><br><span class="line">why&#39;s</span><br><span class="line">with</span><br><span class="line">won&#39;t</span><br><span class="line">would</span><br><span class="line">wouldn&#39;t</span><br><span class="line">you</span><br><span class="line">you&#39;d</span><br><span class="line">you&#39;ll</span><br><span class="line">you&#39;re</span><br><span class="line">you&#39;ve</span><br><span class="line">your</span><br><span class="line">yours</span><br><span class="line">yourself</span><br><span class="line">yourselves</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line">常见的中文停用词汇总：</span><br><span class="line"></span><br><span class="line">的</span><br><span class="line">一</span><br><span class="line">不</span><br><span class="line">在</span><br><span class="line">人</span><br><span class="line">有</span><br><span class="line">是</span><br><span class="line">为</span><br><span class="line">以</span><br><span class="line">于</span><br><span class="line">上</span><br><span class="line">他</span><br><span class="line">而</span><br><span class="line">后</span><br><span class="line">之</span><br><span class="line">来</span><br><span class="line">及</span><br><span class="line">了</span><br><span class="line">因</span><br><span class="line">下</span><br><span class="line">可</span><br><span class="line">到</span><br><span class="line">由</span><br><span class="line">这</span><br><span class="line">与</span><br><span class="line">也</span><br><span class="line">此</span><br><span class="line">但</span><br><span class="line">并</span><br><span class="line">个</span><br><span class="line">其</span><br><span class="line">已</span><br><span class="line">无</span><br><span class="line">小</span><br><span class="line">我</span><br><span class="line">们</span><br><span class="line">起</span><br><span class="line">最</span><br><span class="line">再</span><br><span class="line">今</span><br><span class="line">去</span><br><span class="line">好</span><br><span class="line">只</span><br><span class="line">又</span><br><span class="line">或</span><br><span class="line">很</span><br><span class="line">亦</span><br><span class="line">某</span><br><span class="line">把</span><br><span class="line">那</span><br><span class="line">你</span><br><span class="line">乃</span><br><span class="line">它</span><br><span class="line">吧</span><br><span class="line">被</span><br><span class="line">比</span><br><span class="line">别</span><br><span class="line">趁</span><br><span class="line">当</span><br><span class="line">从</span><br><span class="line">到</span><br><span class="line">得</span><br><span class="line">打</span><br><span class="line">凡</span><br><span class="line">儿</span><br><span class="line">尔</span><br><span class="line">该</span><br><span class="line">各</span><br><span class="line">给</span><br><span class="line">跟</span><br><span class="line">和</span><br><span class="line">何</span><br><span class="line">还</span><br><span class="line">即</span><br><span class="line">几</span><br><span class="line">既</span><br><span class="line">看</span><br><span class="line">据</span><br><span class="line">距</span><br><span class="line">靠</span><br><span class="line">啦</span><br><span class="line">了</span><br><span class="line">另</span><br><span class="line">么</span><br><span class="line">每</span><br><span class="line">们</span><br><span class="line">嘛</span><br><span class="line">拿</span><br><span class="line">哪</span><br><span class="line">那</span><br><span class="line">您</span><br><span class="line">凭</span><br><span class="line">且</span><br><span class="line">却</span><br><span class="line">让</span><br><span class="line">仍</span><br><span class="line">啥</span><br><span class="line">如</span><br><span class="line">若</span><br><span class="line">使</span><br><span class="line">谁</span><br><span class="line">虽</span><br><span class="line">随</span><br><span class="line">同</span><br><span class="line">所</span><br><span class="line">她</span><br><span class="line">哇</span><br><span class="line">嗡</span><br><span class="line">往</span><br><span class="line">哪</span><br><span class="line">些</span><br><span class="line">向</span><br><span class="line">沿</span><br><span class="line">哟</span><br><span class="line">用</span><br><span class="line">于</span><br><span class="line">咱</span><br><span class="line">则</span><br><span class="line">怎</span><br><span class="line">曾</span><br><span class="line">至</span><br><span class="line">致</span><br><span class="line">着</span><br><span class="line">诸</span><br><span class="line">自</span><br></pre></td></tr></table></figure><h5 id="中文分词方式"><a href="#中文分词方式" class="headerlink" title="中文分词方式"></a>中文分词方式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对中文而言，在分词的时候有多种分词规则：</span><br><span class="line">常见的有单字分词、二分法分词、词库分词等</span><br><span class="line">单字分词：&quot;我&quot;、&quot;们&quot;、&quot;是&quot;、&quot;中&quot;、&quot;国&quot;、&quot;人&quot;</span><br><span class="line">二分法分词：&quot;我们&quot;、&quot;们是&quot;、&quot;是中&quot;、&quot;中国&quot;、&quot;国人&quot;。</span><br><span class="line">词库分词：按照某种算法构造词，然后去匹配已建好的词库集合，如果匹配到就切分出来成为词语。</span><br><span class="line"></span><br><span class="line">从这里面可以看出来，其实最理想的中文分词方式是词库分词。</span><br></pre></td></tr></table></figure><h5 id="常见的中文分词器"><a href="#常见的中文分词器" class="headerlink" title="常见的中文分词器"></a>常见的中文分词器</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对前面分析的几种中文分词方式，对应的有一些已经实现好的中分分词器。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111510363.png" alt="image-20230611151020118"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在词库分词方式领域里面，最经典的就是IK分词器，你懂得！</span><br></pre></td></tr></table></figure><h4 id="ES中文分词插件-es-ik"><a href="#ES中文分词插件-es-ik" class="headerlink" title="ES中文分词插件(es-ik)"></a>ES中文分词插件(es-ik)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在中文数据检索场景中，为了提供更好的检索效果，需要在ES中集成中文分词器，因为ES默认是按照英文的分词规则进行分词的，基本上可以认为是单字分词，对中文分词效果不理想。</span><br><span class="line">ES之前是没有提供中文分词器的，现在官方也提供了一些，但是在中文分词领域，IK分词器是不可撼动的，所以在这里我们主要讲一下如何在ES中集成IK这个中文分词器。</span><br><span class="line">首先下载es-ik插件，需要到github上下载。</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;medcl&#x2F;elasticsearch-analysis-ik</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111515484.png" alt="image-20230611151532296"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111515497.png" alt="image-20230611151550453"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">最终的下载地址为：</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;medcl&#x2F;elasticsearch-analysis-ik&#x2F;releases&#x2F;download&#x2F;v7.13.4&#x2F;elasticsearch-analysis-ik-7.13.4.zip</span><br><span class="line"></span><br><span class="line">注意：在ES中安装IK插件的时候，需要在ES集群的所有节点中都安装。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1：将下载好的elasticsearch-analysis-ik-7.13.4.zip上传到bigdata01的&#x2F;data&#x2F;soft&#x2F; elasticsearch-7.13.4目录中。</span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# ll elasticsearch-analysis-ik-7.13.4.zip </span><br><span class="line">-rw-r--r--. 1 root root 4504502 Sep  3  2021 elasticsearch-analysis-ik-7.13.4.zip</span><br><span class="line"></span><br><span class="line">2：将elasticsearch-analysis-ik-7.13.4.zip远程拷贝到bigdata02和bigdata03上。</span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# scp -rq elasticsearch-analysis-ik-7.13.4.zip  bigdata02:&#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4</span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# scp -rq elasticsearch-analysis-ik-7.13.4.zip  bigdata03:&#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">3：在bigdata01节点离线安装IK插件。</span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# bin&#x2F;elasticsearch-plugin install file:&#x2F;&#x2F;&#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4&#x2F;elasticsearch-analysis-ik-7.13.4.zip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：在安装的过程中会有警告信息提示需要输入y确认继续向下执行。</span><br><span class="line"></span><br><span class="line">最后看到如下内容就表示安装成功了。</span><br><span class="line">-&gt; Installed analysis-ik</span><br><span class="line">-&gt; Please restart Elasticsearch to activate any plugins installed</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">config目录下面的analysis-ik里面存储的是ik的配置文件信息。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# cd config&#x2F;</span><br><span class="line">[root@bigdata01 config]# ll analysis-ik&#x2F;</span><br><span class="line">total 8260</span><br><span class="line">-rwxrwxrwx. 1 root root 5225922 Feb 27 20:57 extra_main.dic</span><br><span class="line">-rwxrwxrwx. 1 root root   63188 Feb 27 20:57 extra_single_word.dic</span><br><span class="line">-rwxrwxrwx. 1 root root   63188 Feb 27 20:57 extra_single_word_full.dic</span><br><span class="line">-rwxrwxrwx. 1 root root   10855 Feb 27 20:57 extra_single_word_low_freq.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     156 Feb 27 20:57 extra_stopword.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     625 Feb 27 20:57 IKAnalyzer.cfg.xml</span><br><span class="line">-rwxrwxrwx. 1 root root 3058510 Feb 27 20:57 main.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     123 Feb 27 20:57 preposition.dic</span><br><span class="line">-rwxrwxrwx. 1 root root    1824 Feb 27 20:57 quantifier.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     164 Feb 27 20:57 stopword.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     192 Feb 27 20:57 suffix.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     752 Feb 27 20:57 surname.dic</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plugins目录下面的analysis-ik里面存储的是ik的核心jar包。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# cd plugins&#x2F;</span><br><span class="line">[root@bigdata01 plugins]# ll analysis-ik&#x2F;</span><br><span class="line">total 1428</span><br><span class="line">-rwxrwxrwx. 1 root root 263965 Feb 27 20:56 commons-codec-1.9.jar</span><br><span class="line">-rwxrwxrwx. 1 root root  61829 Feb 27 20:56 commons-logging-1.2.jar</span><br><span class="line">-rwxrwxrwx. 1 root root  54626 Feb 27 20:56 elasticsearch-analysis-ik-7.13.4.jar</span><br><span class="line">-rwxrwxrwx. 1 root root 736658 Feb 27 20:56 httpclient-4.5.2.jar</span><br><span class="line">-rwxrwxrwx. 1 root root 326724 Feb 27 20:56 httpcore-4.4.4.jar</span><br><span class="line">-rwxrwxrwx. 1 root root   1807 Feb 27 20:56 plugin-descriptor.properties</span><br><span class="line">-rwxrwxrwx. 1 root root    125 Feb 27 20:56 plugin-security.policy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">4：在bigdata02节点离线安装IK插件。</span><br><span class="line"></span><br><span class="line">5：在bigdata03节点离线安装IK插件。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">6：如果集群正在运行，则需要停止集群。</span><br><span class="line">在bigdata01上停止。</span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# jps</span><br><span class="line">1680 Elasticsearch</span><br><span class="line">2047 Jps</span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# kill 1680</span><br><span class="line"></span><br><span class="line">在bigdata02上停止。</span><br><span class="line"></span><br><span class="line">在bigdata03上停止。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">7：修改elasticsearch-7.13.4的plugins目录下analysis-ik子目录的权限。</span><br><span class="line">直接修改elasticsearch-7.13.4目录的权限即可。</span><br><span class="line">在bigdata01上执行。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 elasticsearch-7.13.4]# cd ..</span><br><span class="line">[root@bigdata01 soft]# chmod -R 777 elasticsearch-7.13.4</span><br><span class="line"></span><br><span class="line">在bigdata02上执行。</span><br><span class="line">在bigdata03上执行。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">8：重新启动ES集群。</span><br><span class="line">在bigdata01上执行。</span><br><span class="line">[root@bigdata01 soft]# su es</span><br><span class="line">[es@bigdata01 soft]$ cd &#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4</span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ bin&#x2F;elasticsearch -d</span><br><span class="line"></span><br><span class="line">在bigdata02上执行。</span><br><span class="line">在bigdata03上执行。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">9：验证IK的分词效果。</span><br><span class="line">首先使用默认分词器测试中文分词效果。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST  &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_analyze?pretty&#39; -d &#39;&#123;&quot;text&quot;:&quot;我们是中国人&quot;&#125;&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tokens&quot; : [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;我&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 1,</span><br><span class="line">      &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line">      &quot;position&quot; : 0</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;们&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 1,</span><br><span class="line">      &quot;end_offset&quot; : 2,</span><br><span class="line">      &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line">      &quot;position&quot; : 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;是&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 2,</span><br><span class="line">      &quot;end_offset&quot; : 3,</span><br><span class="line">      &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line">      &quot;position&quot; : 2</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;中&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 3,</span><br><span class="line">      &quot;end_offset&quot; : 4,</span><br><span class="line">      &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line">      &quot;position&quot; : 3</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;国&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 4,</span><br><span class="line">      &quot;end_offset&quot; : 5,</span><br><span class="line">      &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line">      &quot;position&quot; : 4</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;人&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 5,</span><br><span class="line">      &quot;end_offset&quot; : 6,</span><br><span class="line">      &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line">      &quot;position&quot; : 5</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">然后使用IK分词器测试中文分词效果。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST  &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_analyze?pretty&#39; -d &#39;&#123;&quot;text&quot;:&quot;我们是中国人&quot;,&quot;tokenizer&quot;:&quot;ik_max_word&quot;&#125;&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tokens&quot; : [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;我们&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 2,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 0</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;是&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 2,</span><br><span class="line">      &quot;end_offset&quot; : 3,</span><br><span class="line">      &quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line">      &quot;position&quot; : 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;中国人&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 3,</span><br><span class="line">      &quot;end_offset&quot; : 6,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 2</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;中国&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 3,</span><br><span class="line">      &quot;end_offset&quot; : 5,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 3</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;国人&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 4,</span><br><span class="line">      &quot;end_offset&quot; : 6,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 4</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在这里我们发现分出来的单词里面有一个 是，这个单词其实可以认为是一个停用词，在分词的时候是不需要切分出来的。</span><br><span class="line">在这被切分出来了，那也就意味着在进行停用词过滤的时候没有过滤掉。</span><br><span class="line"></span><br><span class="line">针对ik这个词库而言，它的停用词词库里面都有哪些单词呢？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 elasticsearch-7.13.4]# cd config&#x2F;analysis-ik&#x2F;</span><br><span class="line">[root@bigdata01 analysis-ik]# ll</span><br><span class="line">total 8260</span><br><span class="line">-rwxrwxrwx. 1 root root 5225922 Feb 27 20:57 extra_main.dic</span><br><span class="line">-rwxrwxrwx. 1 root root   63188 Feb 27 20:57 extra_single_word.dic</span><br><span class="line">-rwxrwxrwx. 1 root root   63188 Feb 27 20:57 extra_single_word_full.dic</span><br><span class="line">-rwxrwxrwx. 1 root root   10855 Feb 27 20:57 extra_single_word_low_freq.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     156 Feb 27 20:57 extra_stopword.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     625 Feb 27 20:57 IKAnalyzer.cfg.xml</span><br><span class="line">-rwxrwxrwx. 1 root root 3058510 Feb 27 20:57 main.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     123 Feb 27 20:57 preposition.dic</span><br><span class="line">-rwxrwxrwx. 1 root root    1824 Feb 27 20:57 quantifier.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     164 Feb 27 20:57 stopword.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     192 Feb 27 20:57 suffix.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     752 Feb 27 20:57 surname.dic</span><br><span class="line">[root@bigdata01 analysis-ik]# more stopword.dic </span><br><span class="line">a</span><br><span class="line">an</span><br><span class="line">and</span><br><span class="line">are</span><br><span class="line">as</span><br><span class="line">at</span><br><span class="line">be</span><br><span class="line">but</span><br><span class="line">by</span><br><span class="line">for</span><br><span class="line">if</span><br><span class="line">in</span><br><span class="line">into</span><br><span class="line">is</span><br><span class="line">it</span><br><span class="line">no</span><br><span class="line">not</span><br><span class="line">of</span><br><span class="line">on</span><br><span class="line">or</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ik的停用词词库是stopword.dic这个文件，这个文件里面目前都是一些英文停用词。</span><br><span class="line">我们可以手工在这个文件中把中文停用词添加进去，先添加 是 这个停用词。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 analysis-ik]# vi stopword.dic </span><br><span class="line">.....</span><br><span class="line">是</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">然后把这个文件的改动同步到集群中的所有节点上。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 analysis-ik]# scp -rq stopword.dic bigdata02:&#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4&#x2F;config&#x2F;analysis-ik&#x2F;</span><br><span class="line">[root@bigdata01 analysis-ik]# scp -rq stopword.dic bigdata03:&#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4&#x2F;config&#x2F;analysis-ik&#x2F;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">重启集群让配置生效。</span><br><span class="line"></span><br><span class="line">再使用IK分词器测试一下中文分词效果。</span><br><span class="line"></span><br><span class="line">[root@bigdata01 analysis-ik]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST  &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;_analyze?pretty&#39; -d &#39;&#123;&quot;text&quot;:&quot;我们是中国人&quot;,&quot;tokenizer&quot;:&quot;ik_max_word&quot;&#125;&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tokens&quot; : [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;我们&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 2,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 0</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;中国人&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 3,</span><br><span class="line">      &quot;end_offset&quot; : 6,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;中国&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 3,</span><br><span class="line">      &quot;end_offset&quot; : 5,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 2</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;国人&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 4,</span><br><span class="line">      &quot;end_offset&quot; : 6,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 3</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时再查看会发现没有&quot;是&quot; 这个单词了，相当于在过滤停用词的时候把它过滤掉了。</span><br></pre></td></tr></table></figure><h3 id="es-ik添加自定义词库"><a href="#es-ik添加自定义词库" class="headerlink" title="es-ik添加自定义词库"></a>es-ik添加自定义词库</h3><h4 id="自定义词库"><a href="#自定义词库" class="headerlink" title="自定义词库"></a>自定义词库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">针对一些特殊的词语在分词的时候也需要能够识别。</span><br><span class="line">例如：公司产品的名称或者网络上新流行的词语</span><br><span class="line">假设我们公司开发了一款新产品，命名为：数据大脑，我们希望ES在分词的时候能够把这个产品名称直接识别成一个词语。</span><br><span class="line">现在使用ik分词器测试一下分词效果：</span><br><span class="line"></span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST  &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;_analyze?pretty&#39; -d &#39;&#123;&quot;text&quot;:&quot;数据大脑&quot;,&quot;tokenizer&quot;:&quot;ik_max_word&quot;&#125;&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tokens&quot; : [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;数据&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 2,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 0</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;大脑&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 2,</span><br><span class="line">      &quot;end_offset&quot; : 4,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 1</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">结果发现ik分词器会把数据大脑分为 数据 和 大脑这两个单词。</span><br><span class="line">因为这个词语是我们自己造出来的，并不是通用的词语，所以ik分词器识别不出来也属于正常。</span><br><span class="line">想要让IK分词器识别出来，就需要自定义词库了，也就是把我们自己造的词语添加到词库里面，这样在分词的时候就可以识别到了。</span><br><span class="line">下面演示一下如何在IK中自定义词库：</span><br><span class="line">1：首先在ik插件对应的配置文件目录下创建一个自定义词库文件my.dic</span><br><span class="line">首先在bigdata01节点上操作。</span><br><span class="line">切换到es用户，进入到ik插件对应的配置文件目录</span><br><span class="line"></span><br><span class="line">[root@bigdata01 ~]# su es</span><br><span class="line">[es@bigdata01 root]$ cd &#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4</span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ cd config</span><br><span class="line">[es@bigdata01 config]$ cd analysis-ik</span><br><span class="line">[es@bigdata01 analysis-ik]$ ll</span><br><span class="line">total 8260</span><br><span class="line">-rwxrwxrwx. 1 root root 5225922 Feb 27 20:57 extra_main.dic</span><br><span class="line">-rwxrwxrwx. 1 root root   63188 Feb 27 20:57 extra_single_word.dic</span><br><span class="line">-rwxrwxrwx. 1 root root   63188 Feb 27 20:57 extra_single_word_full.dic</span><br><span class="line">-rwxrwxrwx. 1 root root   10855 Feb 27 20:57 extra_single_word_low_freq.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     156 Feb 27 20:57 extra_stopword.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     625 Feb 27 20:57 IKAnalyzer.cfg.xml</span><br><span class="line">-rwxrwxrwx. 1 root root 3058510 Feb 27 20:57 main.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     123 Feb 27 20:57 preposition.dic</span><br><span class="line">-rwxrwxrwx. 1 root root    1824 Feb 27 20:57 quantifier.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     171 Feb 27 21:42 stopword.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     192 Feb 27 20:57 suffix.dic</span><br><span class="line">-rwxrwxrwx. 1 root root     752 Feb 27 20:57 surname.dic</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">创建自定义词库文件my.dic</span><br><span class="line">直接在文件中添加词语即可，每一个词语一行。</span><br><span class="line"></span><br><span class="line">[es@bigdata01 analysis-ik]$ vi my.dic</span><br><span class="line">数据大脑</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：这个my.dic词库文件可以在Linux中直接使用vi命令创建，或者在Windows中创建之后上传到这里。</span><br><span class="line"></span><br><span class="line">-如果是在Linux中直接使用vi命令创建，可以直接使用。</span><br><span class="line">-如果是在Windows中创建的，需要注意文件的编码必须是UTF-8 without BOM 格式【UTF-8 无 BOM格式】</span><br><span class="line"></span><br><span class="line">以Notepad++为例：新版本的Notepad++里面的文件编码有这么几种，需要选择【使用UTF-8编码】，这个就是UTF-8 without BOM 格式。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111535279.png" alt="image-20230611153519003"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">2：修改ik的IKAnalyzer.cfg.xml配置文件</span><br><span class="line">进入到ik插件对应的配置文件目录中，修改IKAnalyzer.cfg.xml配置文件</span><br><span class="line"></span><br><span class="line">[es@bigdata01 analysis-ik]$ vi IKAnalyzer.cfg.xml </span><br><span class="line"></span><br><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;!DOCTYPE properties SYSTEM &quot;http:&#x2F;&#x2F;java.sun.com&#x2F;dtd&#x2F;properties.dtd&quot;&gt;</span><br><span class="line">&lt;properties&gt;</span><br><span class="line">        &lt;comment&gt;IK Analyzer 扩展配置&lt;&#x2F;comment&gt;</span><br><span class="line">        &lt;!--用户可以在这里配置自己的扩展字典 --&gt;</span><br><span class="line">        &lt;entry key&#x3D;&quot;ext_dict&quot;&gt;my.dic&lt;&#x2F;entry&gt;</span><br><span class="line">         &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt;</span><br><span class="line">        &lt;entry key&#x3D;&quot;ext_stopwords&quot;&gt;&lt;&#x2F;entry&gt;</span><br><span class="line">        &lt;!--用户可以在这里配置远程扩展字典 --&gt;</span><br><span class="line">        &lt;!-- &lt;entry key&#x3D;&quot;remote_ext_dict&quot;&gt;words_location&lt;&#x2F;entry&gt; --&gt;</span><br><span class="line">        &lt;!--用户可以在这里配置远程扩展停止词字典--&gt;</span><br><span class="line">        &lt;!-- &lt;entry key&#x3D;&quot;remote_ext_stopwords&quot;&gt;words_location&lt;&#x2F;entry&gt;--&gt;</span><br><span class="line">&lt;&#x2F;properties&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：需要把my.dic词库文件添加到key&#x3D;&quot;ext_dict&quot;这个entry中，切记不要随意新增entry，随意新增的entry是不被IK识别的，并且entry的名称也不能乱改，否则也不会识别。</span><br><span class="line"></span><br><span class="line">如果需要指定多个自定义词库文件的话需要使用分号;隔开。</span><br><span class="line">例如：&lt;entry key&#x3D;&quot;ext_dict&quot;&gt;my.dic;your.dic&lt;&#x2F;entry&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3：将修改好的IK配置文件复制到集群中的所有节点中</span><br><span class="line"></span><br><span class="line">注意：如果是多个节点的ES集群，一定要把配置远程拷贝到其他节点。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">先从bigdata01上将my.dic拷贝到bigdata02和bigdata03</span><br><span class="line"></span><br><span class="line">[es@bigdata01 analysis-ik]$ scp -rq my.dic bigdata02:&#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4&#x2F;config&#x2F;analysis-ik&#x2F;</span><br><span class="line">The authenticity of host &#39;bigdata02 (192.168.182.101)&#39; can&#39;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:SnzVynyweeRcPIorakoDQRxFhugZp6PNIPV3agX&#x2F;bZM.</span><br><span class="line">ECDSA key fingerprint is MD5:f6:1a:48:78:64:77:89:52:c4:ad:63:82:a5:d5:57:92.</span><br><span class="line">Are you sure you want to continue connecting (yes&#x2F;no)? yes</span><br><span class="line">es@bigdata02&#39;s password: </span><br><span class="line">[es@bigdata01 analysis-ik]$ scp -rq my.dic bigdata03:&#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4&#x2F;config&#x2F;analysis-ik&#x2F;</span><br><span class="line">The authenticity of host &#39;bigdata03 (192.168.182.102)&#39; can&#39;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:SnzVynyweeRcPIorakoDQRxFhugZp6PNIPV3agX&#x2F;bZM.</span><br><span class="line">ECDSA key fingerprint is MD5:f6:1a:48:78:64:77:89:52:c4:ad:63:82:a5:d5:57:92.</span><br><span class="line">Are you sure you want to continue connecting (yes&#x2F;no)? yes</span><br><span class="line">es@bigdata03&#39;s password:</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：因为现在使用的是普通用户es，所以在使用scp的时候需要指定目标机器的用户名（如果是root可以省略不写），并且还需要手工输入密码，因为之前是基于root用户做的免密码登录。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再从bigdata01上将IKAnalyzer.cfg.xml拷贝到bigdata02和bigdata03</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：如果后期想增加自定义停用词库，也需要按照这个思路进行添加，只不过停用词库需要配置到 key&#x3D;&quot;ext_stopwords&quot;这个entry中。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">4：重启ES验证一下自定义词库的分词效果</span><br><span class="line"></span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST  &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;_analyze?pretty&#39; -d &#39;&#123;&quot;text&quot;:&quot;数据大脑&quot;,&quot;tokenizer&quot;:&quot;ik_max_word&quot;&#125;&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tokens&quot; : [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;数据大脑&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 4,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 0</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;数据&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 2,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;大脑&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 2,</span><br><span class="line">      &quot;end_offset&quot; : 4,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 2</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">现在发现数据大脑这个词语可以被识别出来了，说明自定义词库生效了。</span><br></pre></td></tr></table></figure><h4 id="热更新词库"><a href="#热更新词库" class="headerlink" title="热更新词库"></a>热更新词库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">针对前面分析的自定义词库，后期只要词库内容发生了变动，就需要重启ES才能生效，在实际工作中，频繁重启ES集群不是一个好办法</span><br><span class="line">所以ES提供了热更新词库的解决方案，在不重启ES集群的情况下识别新增的词语，这样就很方便了，也不会对线上业务产生影响。</span><br><span class="line">下面来演示一下热更新词库的使用：</span><br><span class="line">1：在bigdata04上部署HTTP服务</span><br><span class="line">在这使用tomcat作为Web容器，先下载一个tomcat 8.x版本。</span><br><span class="line">tomcat 8.0.52版本下载地址：</span><br><span class="line">https:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;tomcat&#x2F;tomcat-8&#x2F;v8.0.52&#x2F;bin&#x2F;apache-tomcat-8.0.52.tar.gz</span><br><span class="line">上传到bigdata04上的&#x2F;data&#x2F;soft目录里面，并且解压</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 soft]# ll apache-tomcat-8.0.52.tar.gz </span><br><span class="line">-rw-r--r--. 1 root root 9435483 Sep 22  2021 apache-tomcat-8.0.52.tar.gz</span><br><span class="line">[root@bigdata04 soft]# tar -zxvf apache-tomcat-8.0.52.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tomcat的ROOT项目中创建一个自定义词库文件hot.dic，在文件中输入一行内容：测试</span><br><span class="line">[root@bigdata04 soft]# cd apache-tomcat-8.0.52</span><br><span class="line">[root@bigdata04 apache-tomcat-8.0.52]# cd webapps&#x2F;ROOT&#x2F;</span><br><span class="line">[root@bigdata04 ROOT]# vi hot.dic</span><br><span class="line">测试</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">启动Tomcat</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ROOT]# cd &#x2F;data&#x2F;soft&#x2F;apache-tomcat-8.0.52</span><br><span class="line">[root@bigdata04 apache-tomcat-8.0.52]# bin&#x2F;startup.sh </span><br><span class="line">Using CATALINA_BASE:   &#x2F;data&#x2F;soft&#x2F;apache-tomcat-8.0.52</span><br><span class="line">Using CATALINA_HOME:   &#x2F;data&#x2F;soft&#x2F;apache-tomcat-8.0.52</span><br><span class="line">Using CATALINA_TMPDIR: &#x2F;data&#x2F;soft&#x2F;apache-tomcat-8.0.52&#x2F;temp</span><br><span class="line">Using JRE_HOME:        &#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">Using CLASSPATH:       &#x2F;data&#x2F;soft&#x2F;apache-tomcat-8.0.52&#x2F;bin&#x2F;bootstrap.jar:&#x2F;data&#x2F;soft&#x2F;apache-tomcat-8.0.52&#x2F;bin&#x2F;tomcat-juli.jar</span><br><span class="line">Tomcat started.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">验证一下hot.dic文件是否可以通过浏览器访问：</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111553223.png" alt="image-20230611155315334"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意：页面会显示乱码，这是正常的，不用处理即可。</span><br><span class="line"></span><br><span class="line">2：修改ES集群中ik插件的IKAnalyzer.cfg.xml配置文件</span><br><span class="line">在bigdata01上修改。</span><br><span class="line">在key&#x3D;&quot;remote_ext_dict&quot;这个entry中添加hot.dic的远程访问链接</span><br><span class="line">http:&#x2F;&#x2F;bigdata04:8080&#x2F;hot.dic</span><br><span class="line"></span><br><span class="line">注意：一定要记得去掉key&#x3D;&quot;remote_ext_dict&quot;这个entry外面的注释，否则添加的内容是不生效的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[es@bigdata01 analysis-ik]$ vi IKAnalyzer.cfg.xml </span><br><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;!DOCTYPE properties SYSTEM &quot;http:&#x2F;&#x2F;java.sun.com&#x2F;dtd&#x2F;properties.dtd&quot;&gt;</span><br><span class="line">&lt;properties&gt;</span><br><span class="line">        &lt;comment&gt;IK Analyzer 扩展配置&lt;&#x2F;comment&gt;</span><br><span class="line">        &lt;!--用户可以在这里配置自己的扩展字典 --&gt;</span><br><span class="line">        &lt;entry key&#x3D;&quot;ext_dict&quot;&gt;my.dic&lt;&#x2F;entry&gt;</span><br><span class="line">         &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt;</span><br><span class="line">        &lt;entry key&#x3D;&quot;ext_stopwords&quot;&gt;&lt;&#x2F;entry&gt;</span><br><span class="line">        &lt;!--用户可以在这里配置远程扩展字典 --&gt;</span><br><span class="line">        &lt;entry key&#x3D;&quot;remote_ext_dict&quot;&gt;http:&#x2F;&#x2F;bigdata04:8080&#x2F;hot.dic&lt;&#x2F;entry&gt; </span><br><span class="line">        &lt;!--用户可以在这里配置远程扩展停止词字典--&gt;</span><br><span class="line">        &lt;!-- &lt;entry key&#x3D;&quot;remote_ext_stopwords&quot;&gt;words_location&lt;&#x2F;entry&gt;--&gt;</span><br><span class="line">&lt;&#x2F;properties&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3：将修改好的IK配置文件复制到集群中的所有节点中</span><br><span class="line"></span><br><span class="line">4：重启ES集群验证效果。</span><br><span class="line">因为修改了配置，所以需要重启集群。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">验证：</span><br><span class="line">对北京雾霾这个词语进行分词</span><br><span class="line"></span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST  &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;_analyze?pretty&#39; -d &#39;&#123;&quot;text&quot;:&quot;北京雾霾&quot;,&quot;tokenizer&quot;:&quot;ik_max_word&quot;&#125;&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tokens&quot; : [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;北京&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 2,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 0</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;雾&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 2,</span><br><span class="line">      &quot;end_offset&quot; : 3,</span><br><span class="line">      &quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line">      &quot;position&quot; : 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;霾&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 3,</span><br><span class="line">      &quot;end_offset&quot; : 4,</span><br><span class="line">      &quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line">      &quot;position&quot; : 2</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">正常情况下 北京雾霾 会被分被拆分为多个词语，但是在这我希望ES能够把 北京雾霾 认为是一个完整的词语，又不希望重启ES。</span><br><span class="line">这样就可以修改前面配置的hot.dic文件，在里面增加一个词语：北京雾霾</span><br><span class="line">在bigdata04里面操作，此时可以在Linux中直接编辑文件。</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-tomcat-8.0.52]# cd webapps&#x2F;ROOT&#x2F;</span><br><span class="line">[root@bigdata04 ROOT]# vi hot.dic </span><br><span class="line">测试</span><br><span class="line">北京雾霾</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">文件保存之后，在bigdata01上查看ES的日志会看到如下日志信息：</span><br><span class="line">[2027-03-09T18:43:12,700][INFO ][o.w.a.d.Dictionary       ] [bigdata01] start to reload ik dict.</span><br><span class="line">[2027-03-09T18:43:12,701][INFO ][o.w.a.d.Dictionary       ] [bigdata01] try load config from &#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4&#x2F;config&#x2F;analysis-ik&#x2F;IKAnalyzer.cfg.xml</span><br><span class="line">[2027-03-09T18:43:12,929][INFO ][o.w.a.d.Dictionary       ] [bigdata01] [Dict Loading] &#x2F;data&#x2F;soft&#x2F;elasticsearch-7.13.4&#x2F;config&#x2F;analysis-ik&#x2F;my.dic</span><br><span class="line">[2027-03-09T18:43:12,929][INFO ][o.w.a.d.Dictionary       ] [bigdata01] [Dict Loading] http:&#x2F;&#x2F;bigdata04:8080&#x2F;hot.dic</span><br><span class="line">[2027-03-09T18:43:12,934][INFO ][o.w.a.d.Dictionary       ] [bigdata01] ﻿测试</span><br><span class="line">[2027-03-09T18:43:12,935][INFO ][o.w.a.d.Dictionary       ] [bigdata01] 北京雾霾</span><br><span class="line">[2027-03-09T18:43:12,935][INFO ][o.w.a.d.Dictionary       ] [bigdata01] reload ik dict finished.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">再对北京雾霾这个词语进行分词</span><br><span class="line"></span><br><span class="line">[es@bigdata01 elasticsearch-7.13.4]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST  &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;test&#x2F;_analyze?pretty&#39; -d &#39;&#123;&quot;text&quot;:&quot;北京雾霾&quot;,&quot;tokenizer&quot;:&quot;ik_max_word&quot;&#125;&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;tokens&quot; : [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;北京雾霾&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 4,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 0</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;北京&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 2,</span><br><span class="line">      &quot;type&quot; : &quot;CN_WORD&quot;,</span><br><span class="line">      &quot;position&quot; : 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;雾&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 2,</span><br><span class="line">      &quot;end_offset&quot; : 3,</span><br><span class="line">      &quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line">      &quot;position&quot; : 2</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;霾&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 3,</span><br><span class="line">      &quot;end_offset&quot; : 4,</span><br><span class="line">      &quot;type&quot; : &quot;CN_CHAR&quot;,</span><br><span class="line">      &quot;position&quot; : 3</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">此时，发现北京雾霾这个词语就可以完整被切分出来了，到这为止，我们就成功实现了热更新自定义词库的功能。</span><br><span class="line"></span><br><span class="line">注意：默认情况下，最多一分钟之内就可以识别到新增的词语。</span><br><span class="line">通过查看es-ik插件的源码可以发现</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;medcl&#x2F;elasticsearch-analysis-ik&#x2F;blob&#x2F;master&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;wltea&#x2F;analyzer&#x2F;dic&#x2F;Monitor.java</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111559457.png" alt="image-20230611155917849"></p><h2 id="4-Elasticsearch查询详解"><a href="#4-Elasticsearch查询详解" class="headerlink" title="4 Elasticsearch查询详解"></a>4 Elasticsearch查询详解</h2><h3 id="ES-Search查询"><a href="#ES-Search查询" class="headerlink" title="ES Search查询"></a>ES Search查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在ES中查询单条数据可以使用Get，想要查询一批满足条件的数据的话，就需要使用Search了。</span><br><span class="line">下面来看一个案例，查询索引库中的所有数据，代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.es;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.search.SearchRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.search.SearchResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RequestOptions;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestHighLevelClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.SearchHit;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.SearchHits;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Search详解</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EsSearchOp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//获取RestClient连接</span></span><br><span class="line">        RestHighLevelClient client = <span class="keyword">new</span> RestHighLevelClient(</span><br><span class="line">                RestClient.builder(</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata01"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata02"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata03"</span>, <span class="number">9200</span>, <span class="string">"http"</span>)));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SearchRequest searchRequest = <span class="keyword">new</span> SearchRequest();</span><br><span class="line">        <span class="comment">//指定索引库，支持指定一个或者多个，也支持通配符，例如：user*</span></span><br><span class="line">        searchRequest.indices(<span class="string">"user"</span>);</span><br><span class="line">        <span class="comment">//执行查询操作</span></span><br><span class="line">        SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取查询返回的结果</span></span><br><span class="line">        SearchHits hits = searchResponse.getHits();</span><br><span class="line">        <span class="comment">//获取数据总量</span></span><br><span class="line">        <span class="keyword">long</span> numHits = hits.getTotalHits().value;</span><br><span class="line">        System.out.println(<span class="string">"数据总数："</span>+numHits);</span><br><span class="line">        <span class="comment">//获取具体内容</span></span><br><span class="line">        SearchHit[] searchHits = hits.getHits();</span><br><span class="line">        <span class="comment">//迭代解析具体内容</span></span><br><span class="line">        <span class="keyword">for</span> (SearchHit hit : searchHits) &#123;</span><br><span class="line">            String sourceAsString = hit.getSourceAsString();</span><br><span class="line">            System.out.println(sourceAsString);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        client.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在执行代码之前先初始化数据：</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;1&#39; -d &#39;&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:20&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;2&#39; -d &#39;&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:15&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;3&#39; -d &#39;&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:17&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;4&#39; -d &#39;&#123;&quot;name&quot;:&quot;jess&quot;,&quot;age&quot;:19&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;5&#39; -d &#39;&#123;&quot;name&quot;:&quot;mick&quot;,&quot;age&quot;:23&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;6&#39; -d &#39;&#123;&quot;name&quot;:&quot;lili&quot;,&quot;age&quot;:12&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;7&#39; -d &#39;&#123;&quot;name&quot;:&quot;john&quot;,&quot;age&quot;:28&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;8&#39; -d &#39;&#123;&quot;name&quot;:&quot;jojo&quot;,&quot;age&quot;:30&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;9&#39; -d &#39;&#123;&quot;name&quot;:&quot;bubu&quot;,&quot;age&quot;:16&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;10&#39; -d &#39;&#123;&quot;name&quot;:&quot;pig&quot;,&quot;age&quot;:21&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;11&#39; -d &#39;&#123;&quot;name&quot;:&quot;mary&quot;,&quot;age&quot;:19&#125;&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">在IDEA中执行代码，可以看到下面结果：</span><br><span class="line">数据总数：11</span><br><span class="line">&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:20&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:15&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:17&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;jess&quot;,&quot;age&quot;:19&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;mick&quot;,&quot;age&quot;:23&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;lili&quot;,&quot;age&quot;:12&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;john&quot;,&quot;age&quot;:28&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;jojo&quot;,&quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;bubu&quot;,&quot;age&quot;:16&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;pig&quot;,&quot;age&quot;:21&#125;</span><br><span class="line"></span><br><span class="line">显示数据总数有11条，但是下面的明细内容只有10条，这是因为ES默认只会返回10条数据，如果默认返回所有满足条件的数据，对ES的压力就比较大了。</span><br></pre></td></tr></table></figure><h3 id="searchType详解"><a href="#searchType详解" class="headerlink" title="searchType详解"></a>searchType详解</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ES在查询数据的时候可以指定searchType，也就是搜索类型</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;指定searchType</span><br><span class="line">searchRequest.searchType(SearchType.QUERY_THEN_FETCH);</span><br><span class="line"></span><br><span class="line">searchType之前是可以指定为下面这4种：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111606352.png" alt="image-20230611160600236"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其中QUERY AND FETCH和DFS QUERY AND FETCH这两种searchType现在已经不支持了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">这4种搜索类型到底有什么区别，下面我们来详细分析一下：</span><br><span class="line"></span><br><span class="line">在具体分析这4种搜索类型的区别之前，我们先分析一下分布式搜索的背景：</span><br><span class="line">ES天生就是为分布式而生的，但分布式有分布式的缺点，比如要搜索某个单词，但是数据却分别在5个分片（Shard)上面，这5个分片可能在5台主机上面。因为全文搜索天生就要排序（按照匹配度进行排名）,但数据却在5个分片上，如何得到最后正确的排序呢？ES是这样做的，大概分两步。</span><br><span class="line">第1步：ES客户端将会同时向5个分片发起搜索请求。</span><br><span class="line">第2步：这5个分片基于本分片的内容独立完成搜索，然后将符合条件的结果全部返回。</span><br><span class="line">大致流程如下图所示：</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111613627.png" alt="image-20230611161344874" style="zoom: 50%;"><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111614039.png" alt="image-20230611161408321" style="zoom:50%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">然而这其中有两个问题。</span><br><span class="line">第一：数量问题。比如，用户需要搜索&quot;衣服&quot;，要求返回符合条件的前10条。但在5个分片中，可能都存储着衣服相关的数据。所以ES会向这5个分片都发出查询请求，并且要求每个分片都返回符合条件的10条记录。这种情况，ES中5个分片最多会收到10*5&#x3D;50条记录，这样返回给用户的结果数量会多于用户请求的数量。</span><br><span class="line">第二：排名问题。上面说的搜索，每个分片计算符合条件的前10条数据都是基于自己分片的数据进行打分计算的。计算分值使用的词频和文档频率等信息都是基于自己分片的数据进行的，而ES进行整体排名是基于每个分片计算后的分值进行排序的(相当于打分依据就不一样，最终对这些数据统一排名的时候就不准确了)，这就可能会导致排名不准确的问题。如果我们想更精确的控制排序，应该先将计算排序和排名相关的信息（词频和文档频率等打分依据）从5个分片收集上来，进行统一计算，然后使用整体的词频和文档频率为每个分片中的数据进行打分，这样打分依据就一样了。</span><br><span class="line"></span><br><span class="line">再举个例子解释一下【排名问题】：</span><br><span class="line">假设某学校有一班和二班两个班级。</span><br><span class="line">期末考试之后，学校要给全校前十名学员发奖金。</span><br><span class="line">但是一班和二班考试的时候使用的不是一套试卷。</span><br><span class="line">一班：使用的是A卷【A卷偏容易】</span><br><span class="line">二班：使用的是B卷【B卷偏难】</span><br><span class="line">结果就是一班的最高分是100分，最低分是80分。</span><br><span class="line">二班的最高分是70分，最低分是30分。</span><br><span class="line"></span><br><span class="line">这样全校前十名就都是一班的学员了。这显然是不合理的。</span><br><span class="line">因为一班和二班的试卷难易程度不一样，也就是打分依据不一样，所以不能放在一块排名，这个就解释了刚才的排名问题。</span><br><span class="line">如果想要保证排名准确的话，需要保证一班和二班使用的试卷内容一样。</span><br><span class="line">可以这样做，把A卷和B卷的内容组合到一块，作为C卷。</span><br><span class="line">一班和二班考试都使用C卷，这样他们的打分依据就一样了，最终再根据所有学员的成绩排名求前十名就准确合理了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这两个问题，ES也没有什么较好的解决方法，最终把选择的权利交给用户，方法就是在搜索的时候指定searchType。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">1.QUERY AND FETCH(淘汰)</span><br><span class="line">向索引的所有分片都发出查询请求，各分片返回的时候把元素文档（document）和计算后的排名信息一起返回。</span><br><span class="line">这种搜索方式是最快的。因为相比下面的几种搜索方式，这种查询方法只需要去分片查询一次。但是各个分片返回的结果的数量之和可能是用户要求的数据量的N倍。</span><br><span class="line">优点：</span><br><span class="line">只需要查询一次</span><br><span class="line">缺点：</span><br><span class="line">返回的数据量不准确，可能返回(N*分片数量)的数据</span><br><span class="line">并且数据排名也不准确</span><br><span class="line"></span><br><span class="line">2.QUERY THEN FETCH（ES默认的搜索方式）</span><br><span class="line">如果你搜索时，没有指定搜索方式，就是使用的这种搜索方式。这种搜索方式，大概分两个步骤，</span><br><span class="line">第一步，先向所有的分片发出请求，各分片只返回文档id(注意，不包括文档document)和排名相关的信息(也就是文档对应的分值)，然后按照各分片返回的文档的分数进行重新排序和排名，取前size个文档。</span><br><span class="line">第二步，根据文档id去相关的分片取文档。这种方式返回的文档数量与用户要求的数量是相等的。</span><br><span class="line">优点：</span><br><span class="line">返回的数据量是准确的</span><br><span class="line">缺点：</span><br><span class="line">性能一般，</span><br><span class="line">并且数据排名不准确</span><br><span class="line"></span><br><span class="line">3.DFS QUERY AND FETCH(淘汰)</span><br><span class="line">这种方式比第一种方式多了一个DFS步骤，有这一步，可以更精确控制搜索打分和排名。</span><br><span class="line">也就是在进行查询之前，先对所有分片发送请求，把所有分片中的词频和文档频率等打分依据全部汇总到一块，再执行后面的操作、</span><br><span class="line">优点：</span><br><span class="line">数据排名准确</span><br><span class="line">缺点：</span><br><span class="line">性能一般</span><br><span class="line">返回的数据量不准确，可能返回(N*分片数量)的数据</span><br><span class="line"></span><br><span class="line">4.DFS QUERY THEN FETCH</span><br><span class="line">比第2种方式多了一个DFS步骤。</span><br><span class="line">也就是在进行查询之前，先对所有分片发送请求，把所有分片中的词频和文档频率等打分依据全部汇总到一块，再执行后面的操作、</span><br><span class="line">优点：</span><br><span class="line">返回的数据量是准确的</span><br><span class="line">数据排名准确</span><br><span class="line">缺点：</span><br><span class="line">性能最差【这个最差只是表示在这四种查询方式中性能最慢，也不至于不能忍受，如果对查询性能要求不是非常高，而对查询准确度要求比较高的时候可以考虑这个】</span><br></pre></td></tr></table></figure><h3 id="DFS是一个什么样的过程？"><a href="#DFS是一个什么样的过程？" class="headerlink" title="DFS是一个什么样的过程？"></a>DFS是一个什么样的过程？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DFS其实就是在进行真正的查询之前，先把各个分片的词频率和文档频率收集一下，然后进行词搜索的时候，各分片依据全局的词频率和文档频率进行搜索和排名。显然如果使用DFS_QUERY_THEN_FETCH这种查询方式，效率是最低的，因为一个搜索，可能要请求3次分片。但使用DFS方法，搜索精度是最高的。</span><br><span class="line"></span><br><span class="line">总结一下，从性能考虑QUERY_AND_FETCH是最快的，DFS_QUERY_THEN_FETCH是最慢的。从搜索的准确度来说，DFS要比非DFS的准确度更高。</span><br><span class="line"></span><br><span class="line">目前官方舍弃了QUERY AND FETCH和DFS QUERY AND FETCH这两种类型，保留了QUERY THEN FETCH和DFS QUERY THEN FETCH，这两种都是可以保证数据量是准确的。如果对查询的精确度要求没那么高，就使用QUERY THEN FETCH，如果对查询数据的精确度要求非常高，就使用DFS QUERY THEN FETCH。</span><br></pre></td></tr></table></figure><h3 id="ES-查询扩展"><a href="#ES-查询扩展" class="headerlink" title="ES 查询扩展"></a>ES 查询扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在查询数据的时候可以在searchRequest中指定一些参数，实现过滤、分页、排序、高亮等功能</span><br></pre></td></tr></table></figure><h4 id="过滤"><a href="#过滤" class="headerlink" title="过滤"></a>过滤</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">首先看一下如何在查询的时候指定过滤条件</span><br><span class="line">核心代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//指定查询条件</span></span><br><span class="line">SearchSourceBuilder searchSourceBuilder = <span class="keyword">new</span> SearchSourceBuilder();</span><br><span class="line"><span class="comment">//查询所有，可以不指定，默认就是查询索引库中的所有数据</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.matchAllQuery());</span></span><br><span class="line"><span class="comment">//对指定字段的值进行过滤，注意：在查询数据的时候会对数据进行分词</span></span><br><span class="line"><span class="comment">//如果指定多个query，后面的query会覆盖前面的query</span></span><br><span class="line"><span class="comment">//针对字符串类型内容的查询，不支持通配符</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.matchQuery("name","tom"));</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.matchQuery("age","17"));//针对age的值，这里可以指定字符串或者数字都可以</span></span><br><span class="line"><span class="comment">//针对字符串类型内容的查询，支持通配符，但是性能较差，可以认为是全表扫描</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.wildcardQuery("name","t*"));</span></span><br><span class="line"><span class="comment">//区间查询，主要针对数据类型，可以使用from+to 或者gt,gte+lt,lte</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.rangeQuery("age").from(0).to(20));</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.rangeQuery("age").gte(0).lte(20));</span></span><br><span class="line"><span class="comment">//不限制边界，指定为null即可</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.rangeQuery("age").from(0).to(null));</span></span><br><span class="line"><span class="comment">//同时指定多个条件，条件之间的关系支持and(must)、or(should)</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.boolQuery().should(QueryBuilders.matchQuery("name","tom")).should(QueryBuilders.matchQuery("age",19)));</span></span><br><span class="line"><span class="comment">//多条件组合查询的时候，可以设置条件的权重值，将满足高权重值条件的数据排到结果列表的前面</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.boolQuery().should(QueryBuilders.matchQuery("name","tom").boost(1.0f)).should(QueryBuilders.matchQuery("age",19).boost(5.0f)));</span></span><br><span class="line"><span class="comment">//对多个指定字段的值进行过滤，注意：多个字段的数据类型必须一致，否则会报错，如果查询的字段不存在不会报错</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.multiMatchQuery("tom","name","tag"));</span></span><br><span class="line"><span class="comment">//这里通过queryStringQuery可以支持Lucene的原生查询语法，更加灵活，注意：AND、OR、TO之类的关键字必须大写</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.queryStringQuery("name:tom AND age:[15 TO 30]"));</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.boolQuery().must(QueryBuilders.matchQuery("name","tom")).must(QueryBuilders.rangeQuery("age").from(15).to(30)));</span></span><br><span class="line"><span class="comment">//queryStringQuery支持通配符，但是性能也是比较差</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.queryStringQuery("name:t*"));</span></span><br><span class="line"><span class="comment">//精确查询，查询的时候不分词，针对人名、手机号、主机名、邮箱号码等字段的查询时一般不需要分词</span></span><br><span class="line"><span class="comment">//初始化一条测试数据name=刘德华，默认情况下在建立索引的时候刘德华 会被切分为刘、德、华这三个词</span></span><br><span class="line"><span class="comment">//所以这里精确查询是查不出来的，使用matchQuery是可以查出来的</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.matchQuery("name","刘德华"));</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.termQuery("name","刘德华"));</span></span><br><span class="line"><span class="comment">//正常情况下想要使用termQuery实现精确查询的字段不能进行分词</span></span><br><span class="line"><span class="comment">//但是有时候会遇到某个字段已经分词建立索引了，后期还想要实现精确查询</span></span><br><span class="line"><span class="comment">//重新建立索引也不现实，怎么办呢？</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.queryStringQuery("name:\"刘德华\""));</span></span><br><span class="line"><span class="comment">//matchQuery默认会根据分词的结果进行 or 操作，满足任意一个词语的数据都会查询出来</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.matchQuery("name","刘德华"));</span></span><br><span class="line"><span class="comment">//如果想要对matchQuery的分词结果实现and操作，可以通过operator进行设置</span></span><br><span class="line"><span class="comment">//这种方式也可以解决某个字段已经分词建立索引了，后期还想要实现精确查询的问题（间接实现，其实是查询了满足刘、德、华这三个词语的内容）</span></span><br><span class="line"><span class="comment">//searchSourceBuilder.query(QueryBuilders.matchQuery("name","刘德华").operator(Operator.AND));</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">默认情况下ES会对刘德华这个词语进行分词，效果如下（使用的默认分词器）：</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST  &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;emp&#x2F;_analyze?pretty&#39; -d &#39;&#123;&quot;text&quot;:&quot;刘德华&quot;&#125;&#39;      </span><br><span class="line">&#123;</span><br><span class="line">  &quot;tokens&quot; : [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;刘&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 0,</span><br><span class="line">      &quot;end_offset&quot; : 1,</span><br><span class="line">      &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line">      &quot;position&quot; : 0</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;德&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 1,</span><br><span class="line">      &quot;end_offset&quot; : 2,</span><br><span class="line">      &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line">      &quot;position&quot; : 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;token&quot; : &quot;华&quot;,</span><br><span class="line">      &quot;start_offset&quot; : 2,</span><br><span class="line">      &quot;end_offset&quot; : 3,</span><br><span class="line">      &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,</span><br><span class="line">      &quot;position&quot; : 2</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">初始化数据：</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;12&#39; -d &#39;&#123;&quot;name&quot;:&quot;刘德华&quot;,&quot;age&quot;:60&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]$ curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;user&#x2F;_doc&#x2F;13&#39; -d &#39;&#123;&quot;name&quot;:&quot;刘老二&quot;,&quot;age&quot;:20&#125;&#39;</span><br></pre></td></tr></table></figure><h4 id="分页"><a href="#分页" class="headerlink" title="分页"></a>分页</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ES每次返回的数据默认最多是10条，可以认为是一页的数据，这个数据量是可以控制的</span><br><span class="line">核心代码如下：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;分页</span><br><span class="line">&#x2F;&#x2F;设置每页的起始位置，默认是0</span><br><span class="line">&#x2F;&#x2F;searchSourceBuilder.from(0);</span><br><span class="line">&#x2F;&#x2F;设置每页的数据量，默认是10</span><br><span class="line">&#x2F;&#x2F;searchSourceBuilder.size(10);</span><br></pre></td></tr></table></figure><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">在返回满足条件的结果之前，可以按照指定的要求对数据进行排序，默认是按照搜索条件的匹配度返回数据的。</span><br><span class="line">核心代码如下：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;排序</span><br><span class="line">&#x2F;&#x2F;按照age字段，倒序排序</span><br><span class="line">&#x2F;&#x2F;searchSourceBuilder.sort(&quot;age&quot;, SortOrder.DESC);</span><br><span class="line">&#x2F;&#x2F;注意：age字段是数字类型，不需要分词，name字段是字符串类型(Text)，默认会被分词，所以不支持排序和聚合操作</span><br><span class="line">&#x2F;&#x2F;如果想要根据这些会被分词的字段进行排序或者聚合，需要指定使用他们的keyword类型，这个类型表示不会对数据分词</span><br><span class="line">&#x2F;&#x2F;searchSourceBuilder.sort(&quot;name.keyword&quot;, SortOrder.DESC);</span><br><span class="line">&#x2F;&#x2F;keyword类型的特性其实也适用于精确查询的场景，可以在matchQuery中指定字段的keyword类型实现精确查询，不管在建立索引的时候有没有被分词都不影响使用</span><br><span class="line">&#x2F;&#x2F;searchSourceBuilder.query(QueryBuilders.matchQuery(&quot;name.keyword&quot;, &quot;刘德华&quot;));</span><br></pre></td></tr></table></figure><h4 id="高亮"><a href="#高亮" class="headerlink" title="高亮"></a>高亮</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">针对用户搜索时的关键词，如果匹配到了，最终在页面展现的时候可以标红高亮显示，看起来比较清晰。</span><br><span class="line">设置高亮的核心代码如下：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;高亮</span><br><span class="line">&#x2F;&#x2F;设置高亮字段</span><br><span class="line">HighlightBuilder highlightBuilder &#x3D; new HighlightBuilder()</span><br><span class="line">        .field(&quot;name&quot;);&#x2F;&#x2F;支持多个高亮字段，使用多个field方法指定即可</span><br><span class="line">&#x2F;&#x2F;设置高亮字段的前缀和后缀内容</span><br><span class="line">highlightBuilder.preTags(&quot;&lt;font color&#x3D;&#39;red&#39;&gt;&quot;);</span><br><span class="line">highlightBuilder.postTags(&quot;&lt;&#x2F;font&gt;&quot;);</span><br><span class="line">searchSourceBuilder.highlighter(highlightBuilder);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">解析高亮内容的核心代码如下：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;迭代解析具体内容</span><br><span class="line">for (SearchHit hit : searchHits) &#123;</span><br><span class="line">    &#x2F;*String sourceAsString &#x3D; hit.getSourceAsString();</span><br><span class="line">    System.out.println(sourceAsString);*&#x2F;</span><br><span class="line">    Map&lt;String, Object&gt; sourceAsMap &#x3D; hit.getSourceAsMap();</span><br><span class="line">    String name &#x3D; sourceAsMap.get(&quot;name&quot;).toString();</span><br><span class="line">    int age &#x3D; Integer.parseInt(sourceAsMap.get(&quot;age&quot;).toString());</span><br><span class="line">    &#x2F;&#x2F;获取高亮字段内容</span><br><span class="line">    Map&lt;String, HighlightField&gt; highlightFields &#x3D; hit.getHighlightFields();</span><br><span class="line">    &#x2F;&#x2F;获取name字段的高亮内容</span><br><span class="line">    HighlightField highlightField &#x3D; highlightFields.get(&quot;name&quot;);</span><br><span class="line">    if(highlightField!&#x3D;null)&#123;</span><br><span class="line">        Text[] fragments &#x3D; highlightField.getFragments();</span><br><span class="line">        name &#x3D; &quot;&quot;;</span><br><span class="line">        for (Text text : fragments) &#123;</span><br><span class="line">            name +&#x3D; text;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;获取最终的结果数据</span><br><span class="line">    System.out.println(name+&quot;---&quot;+age);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：必须要设置查询的字段，否则无法实现高亮。</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;高亮查询name字段</span><br><span class="line">searchSourceBuilder.query(QueryBuilders.matchQuery(&quot;name&quot;,&quot;tom&quot;));</span><br><span class="line">searchSourceBuilder.query(QueryBuilders.matchQuery(&quot;name&quot;,&quot;刘德华&quot;));</span><br></pre></td></tr></table></figure><h3 id="评分依据-了解"><a href="#评分依据-了解" class="headerlink" title="评分依据(了解)"></a>评分依据(了解)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ES在返回满足条件的数据的时候，按照搜索条件的匹配度返回数据的，匹配度最高的数据排在最前面，这个匹配度其实就是ES中返回结果中的score字段的值。</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;获取数据的匹配度分值，值越大说明和搜索的关键字匹配度越高</span><br><span class="line">float score &#x3D; hit.getScore();</span><br><span class="line">&#x2F;&#x2F;获取最终的结果数据</span><br><span class="line">System.out.println(name+&quot;---&quot;+age+&quot;---&quot;+score);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">此时，我们搜索name&#x3D;刘华 的数据</span><br><span class="line">searchSourceBuilder.query(QueryBuilders.matchQuery(&quot;name&quot;, &quot;刘华&quot;));</span><br><span class="line"></span><br><span class="line">结果如下：</span><br><span class="line">数据总数：2</span><br><span class="line">&lt;font color&#x3D;&#39;red&#39;&gt;刘&lt;&#x2F;font&gt;德&lt;font color&#x3D;&#39;red&#39;&gt;华&lt;&#x2F;font&gt;---60---2.591636</span><br><span class="line">&lt;font color&#x3D;&#39;red&#39;&gt;刘&lt;&#x2F;font&gt;老二---20---1.0036464</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">可以看到第一条数据的score分值为2.59</span><br><span class="line">第二条数据的score分值为1.00</span><br><span class="line"></span><br><span class="line">score分值具体是如何计算出来的呢？可以通过开启评分依据进行查看详细信息：</span><br><span class="line">首先开启评分依据：</span><br><span class="line">&#x2F;&#x2F;评分依据，true：开启，false：关闭</span><br><span class="line">searchSourceBuilder.explain(true);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">获取评分依据信息：</span><br><span class="line">&#x2F;&#x2F;获取Score的评分依据</span><br><span class="line">Explanation explanation &#x3D; hit.getExplanation();</span><br><span class="line">&#x2F;&#x2F;打印评分依据</span><br><span class="line">if(explanation!&#x3D;null)&#123;</span><br><span class="line">    System.out.println(explanation.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">再执行程序，就可以看到具体的评分依据信息了：</span><br><span class="line">数据总数：2</span><br><span class="line">&lt;font color&#x3D;&#39;red&#39;&gt;刘&lt;&#x2F;font&gt;德&lt;font color&#x3D;&#39;red&#39;&gt;华&lt;&#x2F;font&gt;---60---2.591636</span><br><span class="line">2.591636 &#x3D; sum of:</span><br><span class="line">  1.0036464 &#x3D; weight(name:刘 in 1) [PerFieldSimilarity], result of:</span><br><span class="line">    1.0036464 &#x3D; score(freq&#x3D;1.0), computed as boost * idf * tf from:</span><br><span class="line">      2.2 &#x3D; boost</span><br><span class="line">      1.4552872 &#x3D; idf, computed as log(1 + (N - n + 0.5) &#x2F; (n + 0.5)) from:</span><br><span class="line">        3.0 &#x3D; n, number of documents containing term</span><br><span class="line">        14.0 &#x3D; N, total number of documents with field</span><br><span class="line">      0.3134796 &#x3D; tf, computed as freq &#x2F; (freq + k1 * (1 - b + b * dl &#x2F; avgdl)) from:</span><br><span class="line">        1.0 &#x3D; freq, occurrences of term within document</span><br><span class="line">        1.2 &#x3D; k1, term saturation parameter</span><br><span class="line">        0.75 &#x3D; b, length normalization parameter</span><br><span class="line">        3.0 &#x3D; dl, length of field</span><br><span class="line">        1.4285715 &#x3D; avgdl, average length of field</span><br><span class="line">  1.5879896 &#x3D; weight(name:华 in 1) [PerFieldSimilarity], result of:</span><br><span class="line">    1.5879896 &#x3D; score(freq&#x3D;1.0), computed as boost * idf * tf from:</span><br><span class="line">      2.2 &#x3D; boost</span><br><span class="line">      2.3025851 &#x3D; idf, computed as log(1 + (N - n + 0.5) &#x2F; (n + 0.5)) from:</span><br><span class="line">        1.0 &#x3D; n, number of documents containing term</span><br><span class="line">        14.0 &#x3D; N, total number of documents with field</span><br><span class="line">      0.3134796 &#x3D; tf, computed as freq &#x2F; (freq + k1 * (1 - b + b * dl &#x2F; avgdl)) from:</span><br><span class="line">        1.0 &#x3D; freq, occurrences of term within document</span><br><span class="line">        1.2 &#x3D; k1, term saturation parameter</span><br><span class="line">        0.75 &#x3D; b, length normalization parameter</span><br><span class="line">        3.0 &#x3D; dl, length of field</span><br><span class="line">        1.4285715 &#x3D; avgdl, average length of field</span><br><span class="line"></span><br><span class="line">&lt;font color&#x3D;&#39;red&#39;&gt;刘&lt;&#x2F;font&gt;老二---20---1.0036464</span><br><span class="line">1.0036464 &#x3D; sum of:</span><br><span class="line">  1.0036464 &#x3D; weight(name:刘 in 2) [PerFieldSimilarity], result of:</span><br><span class="line">    1.0036464 &#x3D; score(freq&#x3D;1.0), computed as boost * idf * tf from:</span><br><span class="line">      2.2 &#x3D; boost</span><br><span class="line">      1.4552872 &#x3D; idf, computed as log(1 + (N - n + 0.5) &#x2F; (n + 0.5)) from:</span><br><span class="line">        3.0 &#x3D; n, number of documents containing term</span><br><span class="line">        14.0 &#x3D; N, total number of documents with field</span><br><span class="line">      0.3134796 &#x3D; tf, computed as freq &#x2F; (freq + k1 * (1 - b + b * dl &#x2F; avgdl)) from:</span><br><span class="line">        1.0 &#x3D; freq, occurrences of term within document</span><br><span class="line">        1.2 &#x3D; k1, term saturation parameter</span><br><span class="line">        0.75 &#x3D; b, length normalization parameter</span><br><span class="line">        3.0 &#x3D; dl, length of field</span><br><span class="line">        1.4285715 &#x3D; avgdl, average length of field</span><br></pre></td></tr></table></figure><h3 id="ES中分页的性能问题"><a href="#ES中分页的性能问题" class="headerlink" title="ES中分页的性能问题"></a>ES中分页的性能问题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在使用ES实现分页查询的时候，不要一次请求过多或者页码过大的结果，这样会对服务器造成很大的压力，因为它们会在返回前排序。</span><br><span class="line">ES是分布式搜索，所以ES客户端的一个查询请求会发送到索引对应的多个分片中，每个分片都会生成自己的排序结果，最后再进行集中排序，以确保最终结果的正确性。</span><br><span class="line"></span><br><span class="line">我们假设在搜索一个拥有5个主分片的索引，当我们请求第一页数据的时候，每个分片产生自己前10名，然后将它们返回给请求节点，然后这个请求节点会将收到的50条结果重新排序以产生最终的前10名。</span><br><span class="line"></span><br><span class="line">现在想象一下我们如果要获得第1,000页的数据，也就是第10,001到第10,010条数据，每一个分片都会先产生自己的前10,010名，然后请求节点统一处理这50,050条数据，最后再丢弃掉其中的50,040条！</span><br><span class="line">现在我们就明白了，在分布式系统中，大页码请求所消耗的系统资源是呈指数式增长的。这也是为什么网络搜索引擎一般不会提供超过1,000条搜索结果的原因。</span><br><span class="line">例如：百度上的效果。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111650965.png" alt="image-20230611165023568"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当然还有一点原因是后面的搜索结果基本上也不是我们想要的数据了，我们在使用搜索引擎的时候，一般只会看第1页和第2页的数据。</span><br></pre></td></tr></table></figure><h3 id="aggregations聚合统计"><a href="#aggregations聚合统计" class="headerlink" title="aggregations聚合统计"></a>aggregations聚合统计</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ES中可以实现基于字段进行分组聚合的统计</span><br><span class="line">聚合操作支持count()、sum()、avg()、max()、min()等</span><br><span class="line"></span><br><span class="line">下面来看两个案例</span><br></pre></td></tr></table></figure><h4 id="统计相同年龄的学员个数"><a href="#统计相同年龄的学员个数" class="headerlink" title="统计相同年龄的学员个数"></a>统计相同年龄的学员个数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需求：统计相同年龄的学员个数</span><br><span class="line">数据如下所示：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111657341.png" alt="image-20230611165752222"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">首先在ES中初始化这份数据：</span><br><span class="line"></span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;1&#39; -d&#39;&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:18&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;2&#39; -d&#39;&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:29&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;3&#39; -d&#39;&#123;&quot;name&quot;:&quot;jessica&quot;,&quot;age&quot;:18&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;4&#39; -d&#39;&#123;&quot;name&quot;:&quot;dave&quot;,&quot;age&quot;:19&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;5&#39; -d&#39;&#123;&quot;name&quot;:&quot;lilei&quot;,&quot;age&quot;:18&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;6&#39; -d&#39;&#123;&quot;name&quot;:&quot;lili&quot;,&quot;age&quot;:29&#125;&#39;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.es;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.search.SearchRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.search.SearchResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RequestOptions;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestHighLevelClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.AggregationBuilders;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.bucket.terms.Terms;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.builder.SearchSourceBuilder;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 聚合统计：统计相同年龄的学员个数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EsAggOp01</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//获取RestClient连接</span></span><br><span class="line">        RestHighLevelClient client = <span class="keyword">new</span> RestHighLevelClient(</span><br><span class="line">                RestClient.builder(</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata01"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata02"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata03"</span>, <span class="number">9200</span>, <span class="string">"http"</span>)));</span><br><span class="line">        SearchRequest searchRequest = <span class="keyword">new</span> SearchRequest();</span><br><span class="line">        searchRequest.indices(<span class="string">"stu"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定查询条件</span></span><br><span class="line">        SearchSourceBuilder searchSourceBuilder = <span class="keyword">new</span> SearchSourceBuilder();</span><br><span class="line">        <span class="comment">//指定分组信息，默认是执行count聚合</span></span><br><span class="line">        TermsAggregationBuilder aggregation = AggregationBuilders.terms(<span class="string">"age_term"</span>)</span><br><span class="line">                .field(<span class="string">"age"</span>);</span><br><span class="line">        searchSourceBuilder.aggregation(aggregation);</span><br><span class="line"></span><br><span class="line">        searchRequest.source(searchSourceBuilder);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行查询操作</span></span><br><span class="line">        SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取分组信息</span></span><br><span class="line">        Terms terms = searchResponse.getAggregations().get(<span class="string">"age_term"</span>);</span><br><span class="line">        List&lt;? extends Terms.Bucket&gt; buckets = terms.getBuckets();</span><br><span class="line">        <span class="keyword">for</span> (Terms.Bucket bucket: buckets) &#123;</span><br><span class="line">            System.out.println(bucket.getKey()+<span class="string">"---"</span>+bucket.getDocCount());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        client.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="统计每个学员的总成绩"><a href="#统计每个学员的总成绩" class="headerlink" title="统计每个学员的总成绩"></a>统计每个学员的总成绩</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需求：统计每个学员的总成绩</span><br><span class="line">数据如下所示：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306111659147.png" alt="image-20230611165858836"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;score&#x2F;_doc&#x2F;1&#39; -d&#39;&#123;&quot;name&quot;:&quot;tom&quot;,&quot;subject&quot;:&quot;chinese&quot;,&quot;score&quot;:59&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;score&#x2F;_doc&#x2F;2&#39; -d&#39;&#123;&quot;name&quot;:&quot;tom&quot;,&quot;subject&quot;:&quot;math&quot;,&quot;score&quot;:89&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;score&#x2F;_doc&#x2F;3&#39; -d&#39;&#123;&quot;name&quot;:&quot;jack&quot;,&quot;subject&quot;:&quot;chinese&quot;,&quot;score&quot;:78&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;score&#x2F;_doc&#x2F;4&#39; -d&#39;&#123;&quot;name&quot;:&quot;jack&quot;,&quot;subject&quot;:&quot;math&quot;,&quot;score&quot;:85&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;score&#x2F;_doc&#x2F;5&#39; -d&#39;&#123;&quot;name&quot;:&quot;jessica&quot;,&quot;subject&quot;:&quot;chinese&quot;,&quot;score&quot;:97&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;score&#x2F;_doc&#x2F;6&#39; -d&#39;&#123;&quot;name&quot;:&quot;jessica&quot;,&quot;subject&quot;:&quot;math&quot;,&quot;score&quot;:68&#125;&#39;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.es;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.http.HttpHost;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.search.SearchRequest;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.search.SearchResponse;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RequestOptions;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.client.RestHighLevelClient;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.Aggregation;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.AggregationBuilders;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.bucket.terms.Terms;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.aggregations.metrics.Sum;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.search.builder.SearchSourceBuilder;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 聚合统计：统计每个学员的总成绩</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EsAggOp02</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//获取RestClient连接</span></span><br><span class="line">        RestHighLevelClient client = <span class="keyword">new</span> RestHighLevelClient(</span><br><span class="line">                RestClient.builder(</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata01"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata02"</span>, <span class="number">9200</span>, <span class="string">"http"</span>),</span><br><span class="line">                        <span class="keyword">new</span> HttpHost(<span class="string">"bigdata03"</span>, <span class="number">9200</span>, <span class="string">"http"</span>)));</span><br><span class="line">        SearchRequest searchRequest = <span class="keyword">new</span> SearchRequest();</span><br><span class="line">        searchRequest.indices(<span class="string">"score"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定查询条件</span></span><br><span class="line">        SearchSourceBuilder searchSourceBuilder = <span class="keyword">new</span> SearchSourceBuilder();</span><br><span class="line">        <span class="comment">//指定分组和求sum</span></span><br><span class="line">        TermsAggregationBuilder aggregation = AggregationBuilders.terms(<span class="string">"name_term"</span>)</span><br><span class="line">                .field(<span class="string">"name.keyword"</span>)<span class="comment">//指定分组字段，如果是字符串(Text)类型，则需要指定使用keyword类型</span></span><br><span class="line">                .subAggregation(AggregationBuilders.sum(<span class="string">"sum_score"</span>).field(<span class="string">"score"</span>));<span class="comment">//指定求sum,也支持avg、min、max等操作</span></span><br><span class="line">        searchSourceBuilder.aggregation(aggregation);</span><br><span class="line"></span><br><span class="line">        searchRequest.source(searchSourceBuilder);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行查询操作</span></span><br><span class="line">        SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取分组信息</span></span><br><span class="line">        Terms terms = searchResponse.getAggregations().get(<span class="string">"name_term"</span>);</span><br><span class="line">        List&lt;? extends Terms.Bucket&gt; buckets = terms.getBuckets();</span><br><span class="line">        <span class="keyword">for</span> (Terms.Bucket bucket: buckets) &#123;</span><br><span class="line">            <span class="comment">//获取sum聚合的结果</span></span><br><span class="line">            Sum sum = bucket.getAggregations().get(<span class="string">"sum_score"</span>);</span><br><span class="line">            System.out.println(bucket.getKey()+<span class="string">"---"</span>+sum.getValue());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        client.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="aggregations获取所有分组数据"><a href="#aggregations获取所有分组数据" class="headerlink" title="aggregations获取所有分组数据"></a>aggregations获取所有分组数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">默认情况下，ES只会返回10个分组的数据，如果分组之后的结果超过了10组，如何解决？</span><br><span class="line"></span><br><span class="line">可以通过在聚合操作中使用size方法进行设置，获取指定个数的数据组或者获取所有的数据组。</span><br><span class="line">在案例1的基础上再初始化一批测试数据：</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;61&#39; -d&#39;&#123;&quot;name&quot;:&quot;s1&quot;,&quot;age&quot;:31&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;62&#39; -d&#39;&#123;&quot;name&quot;:&quot;s2&quot;,&quot;age&quot;:32&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;63&#39; -d&#39;&#123;&quot;name&quot;:&quot;s3&quot;,&quot;age&quot;:33&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;64&#39; -d&#39;&#123;&quot;name&quot;:&quot;s4&quot;,&quot;age&quot;:34&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;65&#39; -d&#39;&#123;&quot;name&quot;:&quot;s5&quot;,&quot;age&quot;:35&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;66&#39; -d&#39;&#123;&quot;name&quot;:&quot;s6&quot;,&quot;age&quot;:36&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;67&#39; -d&#39;&#123;&quot;name&quot;:&quot;s7&quot;,&quot;age&quot;:37&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;68&#39; -d&#39;&#123;&quot;name&quot;:&quot;s8&quot;,&quot;age&quot;:38&#125;&#39;</span><br><span class="line">[root@bigdata01 ~]# curl -H &quot;Content-Type: application&#x2F;json&quot; -XPOST &#39;http:&#x2F;&#x2F;bigdata01:9200&#x2F;stu&#x2F;_doc&#x2F;69&#39; -d&#39;&#123;&quot;name&quot;:&quot;s9&quot;,&quot;age&quot;:39&#125;&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">支持案例1的代码，查看返回的分组个数：</span><br><span class="line">18---3</span><br><span class="line">29---2</span><br><span class="line">19---1</span><br><span class="line">31---1</span><br><span class="line">32---1</span><br><span class="line">33---1</span><br><span class="line">34---1</span><br><span class="line">35---1</span><br><span class="line">36---1</span><br><span class="line">37---1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">发现结果中返回的分组个数是10个，没有全部都显示出来，这个其实和分页也没关系，尝试增加分页的代码发现也是无效的：</span><br><span class="line">&#x2F;&#x2F;增加分页参数，注意：分页参数针对分组数据是无效的。</span><br><span class="line">searchSourceBuilder.from(0).size(20);</span><br><span class="line"></span><br><span class="line">执行案例1的代码，结果发现还是10条数据。</span><br><span class="line">18---3</span><br><span class="line">29---2</span><br><span class="line">19---1</span><br><span class="line">31---1</span><br><span class="line">32---1</span><br><span class="line">33---1</span><br><span class="line">34---1</span><br><span class="line">35---1</span><br><span class="line">36---1</span><br><span class="line">37---1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">通过在聚合操作上使用size方法进行设置：</span><br><span class="line">TermsAggregationBuilder aggregation &#x3D; AggregationBuilders.terms(&quot;age_term&quot;)</span><br><span class="line">        .field(&quot;age&quot;)</span><br><span class="line">        .size(20);&#x2F;&#x2F;获取指定分组个数的数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">执行案例1的代码：</span><br><span class="line">18---3</span><br><span class="line">29---2</span><br><span class="line">19---1</span><br><span class="line">31---1</span><br><span class="line">32---1</span><br><span class="line">33---1</span><br><span class="line">34---1</span><br><span class="line">35---1</span><br><span class="line">36---1</span><br><span class="line">37---1</span><br><span class="line">38---1</span><br><span class="line">39---1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时可以获取到所有分组的数据，因为结果一共有12个分组，在代码中通过size设置最多可以获取到20个分组的数据。</span><br><span class="line"></span><br><span class="line">如果前期不确定到底有多少个分组的数据，还想获取到所有分组的数据，此时可以在size中设置一个Integer的最大值，这样基本上就没什么问题了，但是注意：如果最后的分组个数太多，会给ES造成比较大的压力，所以官方在这做了限制，让用户手工指定获取多少分组的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TermsAggregationBuilder aggregation &#x3D; AggregationBuilders.terms(&quot;age_term&quot;)</span><br><span class="line">        .field(&quot;age&quot;)</span><br><span class="line">        .size(Integer.MAX_VALUE);&#x2F;&#x2F;获取指定分组个数的数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在ES7.x版本之前，想要获取所有的分组数据，只需要在size中指定参数为0即可。现在ES7.x版本不支持这个数值了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周-Flink极速上手篇-Flink新版本1.12以上-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1-12%E4%BB%A5%E4%B8%8A-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1-12%E4%BB%A5%E4%B8%8A-3.html</id>
    <published>2023-06-02T09:40:07.000Z</published>
    <updated>2023-06-02T10:01:55.124Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="Flink新版本1-12以上-3"><a href="#Flink新版本1-12以上-3" class="headerlink" title="Flink新版本1.12以上-3"></a>Flink新版本1.12以上-3</h1><h2 id="Checkpoint与State剖析"><a href="#Checkpoint与State剖析" class="headerlink" title="Checkpoint与State剖析"></a>Checkpoint与State剖析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们已经掌握了Checkpoint和State使用，下面我们来从底层原理层面深度分析一下Checkpoint和State的细节流程。</span><br><span class="line"></span><br><span class="line">首先是checkpoint的生成过程</span><br></pre></td></tr></table></figure><h3 id="Checkpoint的生成过程"><a href="#Checkpoint的生成过程" class="headerlink" title="Checkpoint的生成过程"></a>Checkpoint的生成过程</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021740884.png" alt="image-20230602174046655"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">我们先整体看一下这个图：</span><br><span class="line"></span><br><span class="line">首先看图中左边的内容，这块内容表示是输入数据流中的数据以及对应的偏移量。</span><br><span class="line">其中里面的send、follow是具体的数据，下面的1、2、3、4、5、6是数据对应的偏移量。</span><br><span class="line">这个输入数据流表示是直播间内用户产生的实时行为数据，send表示送礼，follow表示关注</span><br><span class="line">在这里我们需要对这些用户行为数据实时统计，统计出每种行为出现的总次数。</span><br><span class="line"></span><br><span class="line">中间的[send ,4]表示目前消费者消费到的那条数据及对应的偏移量，这个信息会存储在基于内存的状态中。</span><br><span class="line"></span><br><span class="line">右边的[count(send), 3]、[count(follow) ,1],这些是实时汇总的结果，这些数据也会存储在基于内存的状态中。</span><br><span class="line"></span><br><span class="line">Flink触发执行Checkpoint之后会把内存中存储的状态数据写入到下面的持久化存储中。</span><br><span class="line"></span><br><span class="line">下面我们来详细看一下checkpoint的执行流程。</span><br><span class="line">1：当消费到[send，4]这条数据时，正好达到了checkpoint的执行时机，此时JobManager中的checkpoint coordinator会触发checkpoint开始执行。</span><br><span class="line">此时状态中存储的消费偏移量是4</span><br><span class="line"></span><br><span class="line">2-1：checkpoint真正开始执行的时候，他会先把状态中维护的消费偏移量写入到持久化存储中。</span><br><span class="line"></span><br><span class="line">2-2：写入结束后，DataSource组件会把状态的存储路径信息反馈给JobManager中的checkpoint coordinator。</span><br><span class="line"></span><br><span class="line">3-1、3-2、4-1、4-2：接着后面算子中的状态数据：[count(send), 3]、[count(follow) ,1]也会进行同样的步骤</span><br><span class="line"></span><br><span class="line">5：等所有的算子都完成了状态数据的持久化存储，也就是说 checkpoint coordinator 收集到了所有任务反馈给他的状态存储路径，这个时候就认为这一次的checkpoint真正执行成功了，最后他会向持久化存储中再备份一个 checkpoint metadata元数据文件，那么本次整个checkpoint流程就完成了。如果中间有任何一个阶段不成功，那么本次checkpoint就宣告失败。</span><br><span class="line"></span><br><span class="line">当达到下一次checkpoint执行时机的时候，会继续重复前面的执行流程。</span><br><span class="line"></span><br><span class="line">这就是checkpoint的生成过程。</span><br></pre></td></tr></table></figure><h3 id="Checkpoint的恢复过程"><a href="#Checkpoint的恢复过程" class="headerlink" title="Checkpoint的恢复过程"></a>Checkpoint的恢复过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们来分析一下Checkpoint的恢复过程。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021745635.png" alt="image-20230602174549554"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">继续接着前面的业务流程，前面我们在消费完第4条数据的时候触发了一次checkpoint。</span><br><span class="line">checkpoint执行结束后，紧接着消费者开始消费第5条数据，当把第5条数据follow消费出来之后，在计算的时候由于资源问题导致出现了故障，此时任务异常结束了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021746680.png" alt="image-20230602174608348"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">任务结束后，Flink尝试重启任务，并恢复数据到之前的状态。</span><br><span class="line">在最开始重启任务的时候，任务中基于内存的状态都是空的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021746670.png" alt="image-20230602174632536"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当任务重新启动之后，会根据指定的快照数据进行恢复，此时上一次在快照时保存的偏移量4、[count(send), 3]、[count(follow) ,1]这些数据对应的都恢复到了正确的位置。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021746394.png" alt="image-20230602174650649"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">恢复成功之后，任务会基于之前的偏移量4继续往后面消费，所以又把[follow，5]这条数据消费出来了。</span><br><span class="line">此时算子中计算的结果，count(follow)就变成了2。</span><br><span class="line">这就是正常的数据处理流程了。</span><br></pre></td></tr></table></figure><h3 id="Checkpoint-Barrier"><a href="#Checkpoint-Barrier" class="headerlink" title="Checkpoint Barrier"></a>Checkpoint Barrier</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">在Checkpoint执行过程中，还有一个重要的角色，Barrier。</span><br><span class="line">可以将Barrier认为是一个标记或者说是一条特殊的数据。</span><br><span class="line"></span><br><span class="line">Flink任务在每次触发Checkpoint的时候都会由JobManager的Checkpoint Coordinator向Source端插入一个Barrier。</span><br><span class="line"></span><br><span class="line">Barrier中包含有一个Checkpoint ID，用于标识它属于哪一个Checkpoint。</span><br><span class="line"></span><br><span class="line">Barrier中的Checkpoint ID是严格单调递增的(自增ID)。</span><br><span class="line">看下面这个图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021747611.png" alt="image-20230602174724039"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">图里面的A\B\C\D这些属于数据流中的正常数据。</span><br><span class="line">其中1和2是checkpoint ID，这是特殊的Barrier标记数据。</span><br><span class="line">每次触发checkpoint的时候，JobManager的Checkpoint Coordinator都会向数据流中插入一个Barrier。</span><br><span class="line"></span><br><span class="line">这个Barrier标记在源码中对应的类是CheckpointBarrier</span><br><span class="line">可以大致看一下他的代码：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class CheckpointBarrier extends RuntimeEvent &#123;</span><br><span class="line"></span><br><span class="line">    private final long id;</span><br><span class="line">    private final long timestamp;</span><br><span class="line">    private final CheckpointOptions checkpointOptions;</span><br><span class="line"></span><br><span class="line">    public CheckpointBarrier(long id, long timestamp, CheckpointOptions checkpointOptions) &#123;</span><br><span class="line">        this.id &#x3D; id;</span><br><span class="line">        this.timestamp &#x3D; timestamp;</span><br><span class="line">        this.checkpointOptions &#x3D; checkNotNull(checkpointOptions);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">从这可以看出来他里面除了包含一个自增的id字段，还有一个时间戳字段，以及保存checkpoint参数的字段。</span><br><span class="line"></span><br><span class="line">Barrier中的Checkpoint ID其实就是之前我们在任务界面中查看Checkpoint相关信息时显示的那个ID。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021748333.png" alt="image-20230602174822221"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Checkpoint在执行的时候，Source在接收到Barrier，会把Barrier广播发送到下游算子，当下游算子A接收到了所有输入数据流的Barrier时，意味着算子A已经处理完了截止到当前Checkpoint的数据。</span><br><span class="line"></span><br><span class="line">然后算子A就可以执行Checkpoint，并将Barrier广播发送至下游算子。</span><br><span class="line"></span><br><span class="line">Barrier的最大作用是用于算子各个子任务之间对齐检查点，Barrier对齐之后才会保存状态数据，最终保持一致性语义。</span><br><span class="line"></span><br><span class="line">怎么理解呢？</span><br><span class="line"></span><br><span class="line">看下面这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021748577.png" alt="image-20230602174840536"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Source会把Barrier广播发送到下游算子A。</span><br><span class="line"></span><br><span class="line">Source的并行度为2，所以Source的每个子任务都会给下游的算子广播发送Barrier。</span><br><span class="line"></span><br><span class="line">算子A在这里也产生了2个子任务，对应的是算子A-1和算子A-2，每个子任务负责处理一部分数据</span><br><span class="line"></span><br><span class="line">在这里Source-1这个子任务会给下游的算子A-1和算子A-2分别发送Barrier。</span><br><span class="line"></span><br><span class="line">Source-2这个子任务也会给下游的算子A-1和算子A-2分别发送Barrier。</span><br><span class="line"></span><br><span class="line">当算子A-1收到Source-1和Source-2这两个子任务发给他的Barrier标记时，他才会真正执行checkpoint，因为这个时候才实现了Barrier对齐，也就意味着这个子任务把本次checkpoint之前需要处理的数据都处理完了，这个时候执行checkpoint操作，才能保证数据的一致性。</span><br><span class="line"></span><br><span class="line">算子A-2也是这样的。</span><br></pre></td></tr></table></figure><h4 id="Kafka-Flink-Kafka实现端到端一致性"><a href="#Kafka-Flink-Kafka实现端到端一致性" class="headerlink" title="Kafka+Flink+Kafka实现端到端一致性"></a>Kafka+Flink+Kafka实现端到端一致性</h4><h3 id="Flink-Kafka相关源码分析"><a href="#Flink-Kafka相关源码分析" class="headerlink" title="Flink-Kafka相关源码分析"></a>Flink-Kafka相关源码分析</h3><h2 id="Kafka连接器新API的使用"><a href="#Kafka连接器新API的使用" class="headerlink" title="Kafka连接器新API的使用"></a>Kafka连接器新API的使用</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>找工作知识复习</title>
    <link href="http://tianyong.fun/%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0.html"/>
    <id>http://tianyong.fun/%E6%89%BE%E5%B7%A5%E4%BD%9C%E7%9F%A5%E8%AF%86%E5%A4%8D%E4%B9%A0.html</id>
    <published>2023-05-25T08:59:41.000Z</published>
    <updated>2023-05-27T17:14:09.354Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="找工作知识复习"><a href="#找工作知识复习" class="headerlink" title="找工作知识复习"></a>找工作知识复习</h1><h2 id="直播平台三度关系推荐"><a href="#直播平台三度关系推荐" class="headerlink" title="直播平台三度关系推荐"></a>直播平台三度关系推荐</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.技术选型：</span><br><span class="line">数据采集聚合</span><br><span class="line">数据分发</span><br><span class="line">数据存储</span><br><span class="line">数据计算</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2.整体架构</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3.Neo4j安装，概念，创建节点和关系，查找，更新，索引，批量导入</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4.数据来源分析，模拟产生</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5.zookeeper,kafka启动-&gt;创建topic-&gt;数据分发-&gt;数据落盘</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6.第二个任务：实时维护粉丝关注</span><br></pre></td></tr></table></figure><h2 id="Java常识"><a href="#Java常识" class="headerlink" title="Java常识"></a>Java常识</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">继承-&gt;extends-&gt;protected-&gt;super-&gt;构造函数第一行应该是父类的某个构造函数，否则默认父类无参数构造函数-&gt;限制继承final和sealed、permits-&gt;覆写和重载-&gt;多态-&gt;向上转型</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">抽象方法-&gt;抽象类-&gt;继承抽象类时必须覆写(否则还是抽象类)-&gt;可以有实例字段和方法</span><br><span class="line"></span><br><span class="line">接口-&gt;interface-&gt;implements-&gt;一个类可以继承多个接口-&gt;一个接口可以extends一个接口-&gt;不能有实例字段(可以有静态字段，且要有final修饰)和方法-&gt;可以有default方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">静态字段static-&gt;静态方法</span><br></pre></td></tr></table></figure><h2 id="IDEA常识"><a href="#IDEA常识" class="headerlink" title="IDEA常识"></a>IDEA常识</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">内存设置：</span><br><span class="line">一个全局内存设置，一个为单独程序设置运行时的内存</span><br></pre></td></tr></table></figure><h2 id="Maven"><a href="#Maven" class="headerlink" title="Maven"></a>Maven</h2><pre><code></code></pre><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v2.0-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-4.html</id>
    <published>2023-05-17T07:34:44.000Z</published>
    <updated>2023-05-17T07:39:42.425Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v2-0-4"><a href="#第十八周-直播平台三度关系推荐v2-0-4" class="headerlink" title="第十八周 直播平台三度关系推荐v2.0-4"></a>第十八周 直播平台三度关系推荐v2.0-4</h1><h2 id="数据加工总线之SparkSQL计算引擎开发"><a href="#数据加工总线之SparkSQL计算引擎开发" class="headerlink" title="数据加工总线之SparkSQL计算引擎开发"></a>数据加工总线之SparkSQL计算引擎开发</h2><h3 id="核心功能点梳理"><a href="#核心功能点梳理" class="headerlink" title="核心功能点梳理"></a>核心功能点梳理</h3><h3 id="开发基于SparkSQL的计算引擎"><a href="#开发基于SparkSQL的计算引擎" class="headerlink" title="开发基于SparkSQL的计算引擎"></a>开发基于SparkSQL的计算引擎</h3><h2 id="数据加工总线之FlinkSQL计算引擎开发"><a href="#数据加工总线之FlinkSQL计算引擎开发" class="headerlink" title="数据加工总线之FlinkSQL计算引擎开发"></a>数据加工总线之FlinkSQL计算引擎开发</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v2.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v2.0-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-3.html</id>
    <published>2023-05-03T09:35:36.000Z</published>
    <updated>2023-05-17T07:39:38.772Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v2-0-3"><a href="#第十八周-直播平台三度关系推荐v2-0-3" class="headerlink" title="第十八周 直播平台三度关系推荐v2.0-3"></a>第十八周 直播平台三度关系推荐v2.0-3</h1><h2 id="数据中台的前世今生"><a href="#数据中台的前世今生" class="headerlink" title="数据中台的前世今生"></a>数据中台的前世今生</h2><h3 id="什么是中台"><a href="#什么是中台" class="headerlink" title="什么是中台"></a>什么是中台</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">中台是2019年开始火起来的一个概念，它最早是由阿里在2015年提出的“大中台，小前台”战略中延伸出来的概念，灵感来源于一家芬兰的小公司Supercell——一家仅有300名员工，却接连推出爆款游戏，是全球最会赚钱的明星游戏公司。2015年年中,马云带领阿里巴巴集团高管,拜访了位于芬兰赫尔辛基的这家移动游戏公司，这家看似很小的公司，设置了一个强大的技术平台，来支持众多的小团队进行游戏研发。这样一来，他们就可以专心创新，不用担心基础却又至关重要的技术支撑问题。恰恰是这家小公司，开创了中台的“玩法”，并将其运用到了极致。</span><br><span class="line">下面我们举个例子，通过IT行业的发展来进一步理解什么是中台？为什么要出现中台？</span><br></pre></td></tr></table></figure><p>传统IT时代</p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162119743.png" alt="image-20230516211621053"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在传统IT时代，无论项目如何复杂，都可以分为 前台 和 后台 两部分，简单明了。</span><br><span class="line">每一个业务线负责维护自己的前台和后台</span><br><span class="line">这里的前台不仅仅包含前端页面，还包含提供的各种服务</span><br><span class="line">后台指的是底层的服务，例如我们提取的一些工具服务</span><br><span class="line">在当时，项目的发展相对稳定，并不需要像互联网时代那么快速的去迭代和试错，所以这种架构没有什么问题。</span><br></pre></td></tr></table></figure><h4 id="传统IT时代存在的问题"><a href="#传统IT时代存在的问题" class="headerlink" title="传统IT时代存在的问题"></a>传统IT时代存在的问题</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162119728.png" alt="image-20230516211851281"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">发展到现在这个时代，传统的前台+后台这种架构是存在一些问题的，每一个产品线之间都会有一些重复的内容，例如这里面的用户模块和支付模块，每一个产品线都需要，如果每一个产品线都是自己开发自己的，这样就会有三套用户模块和支付模块，对于集团公司而言，这就叫重复造轮子。如果后期又增加了新的产品线，还要重新再开发用户模块和支付模块。</span><br><span class="line"></span><br><span class="line">所以说为了提高开发效率，我们有必要抽取出一个中间组织，为所有的产品线提供一些公共资源，这个中间组织就是中台。</span><br><span class="line"></span><br><span class="line">下面来看一个引入了中台之后的案例。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162122689.png" alt="image-20230516212224583"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">本来是各个部门都建立了自己的数据采集，数仓，数据模型等内容，重复开发，浪费成本。各个部门的数据也没有打通，数据很难产生很大的价值。</span><br><span class="line">引入了中台之后，构建了统一的数据采集、统一的数据资产中心、统一的数据建模、分析与挖掘、统一的数据服务，最终向各部门统一提供数据支撑。</span><br></pre></td></tr></table></figure><h4 id="阿里”大中台小前台架构-”"><a href="#阿里”大中台小前台架构-”" class="headerlink" title="阿里”大中台小前台架构 ”"></a>阿里”大中台小前台架构 ”</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来这个是阿里的大中台 小前台架构</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162125389.png" alt="image-20230516212502449"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">阿里许多产品线的共通业务经过下沉，形成了中台的各种业务中心，为各大业务线提供支持。</span><br><span class="line">这样前台应用就会更加灵活，想要构建一个新的前台应用也是比较快速容易的。</span><br></pre></td></tr></table></figure><h4 id="中台架构主要解决的问题"><a href="#中台架构主要解决的问题" class="headerlink" title="中台架构主要解决的问题"></a>中台架构主要解决的问题</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面我们来总结一下中台这种架构主要解决的问题。</span><br><span class="line"></span><br><span class="line">信息获取成本高，之前是每一个产品线都需要单独维护自己的数据，成本比较高。</span><br><span class="line">服务具有不确定性，通过中台可以以不变应万变</span><br><span class="line">互联互通成本高，不同产品线的数据想要打通成本过高。</span><br><span class="line">低水平重复建设，不同产品线需要重复建设相同的模块。</span><br><span class="line">通过中台，可以很好的解决这些问题。</span><br></pre></td></tr></table></figure><h4 id="中台的延伸"><a href="#中台的延伸" class="headerlink" title="中台的延伸"></a>中台的延伸</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">中台是一个大而全的概念，基于中台延伸出了多个方向</span><br><span class="line">技术中台</span><br><span class="line">移动中台</span><br><span class="line">业务中台</span><br><span class="line">数据中台</span><br><span class="line">研发中台</span><br><span class="line">组织中台</span><br><span class="line">等等…</span><br><span class="line"></span><br><span class="line">在这里我们可以把中台理解为航空母舰，这些中台都是基于这个航空母舰延伸出来的</span><br><span class="line"></span><br><span class="line">技术中台提供了技术支撑能力，帮助我们解决了基础设施，分布式数据库等底层技术问题，为前台特种兵提供了精良的武器装备。</span><br><span class="line"></span><br><span class="line">移动中台提供了战场一线火力支援能力，帮助我们提供更加个性化的服务，增强用户体验，为战场提供了陆军支援能力，随机应变，所向披靡。</span><br><span class="line">注意：这里的移动中台并不是说这个中台会移动，这里的移动表示的是移动端的意思，就是手机端。</span><br><span class="line"></span><br><span class="line">业务中台提供重用服务，例如用户中心，订单中心之类的开箱即用可重用能力，为战场提供了强大的后台炮火支援能力，随叫随到，威力强大。</span><br><span class="line"></span><br><span class="line">数据中台提供了数据分析能力，帮助我们从数据中学习改进，调整方向，为战场提供了强大及时的雷达监测能力，帮助我们掌控战场。</span><br><span class="line"></span><br><span class="line">研发中台提供了技术实践支撑能力，帮助我们快速搭建项目，管理进度，测试，持续集成，持续交付，是前台特种兵的训练基地及快速送达战场的机动运输部队。</span><br><span class="line"></span><br><span class="line">组织中台为我们的项目提供投资管理、风险管理、资源调度等，是战场的指挥部，战争的大脑，指挥前线，调度后方。</span><br></pre></td></tr></table></figure><h4 id="阿里中台技术栈全景"><a href="#阿里中台技术栈全景" class="headerlink" title="阿里中台技术栈全景"></a>阿里中台技术栈全景</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下阿里的中台技术栈全景</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162133313.png" alt="image-20230516213309367"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">最下面是一些基础设施和基础中间件</span><br><span class="line">上层是业务中台和数据中台</span><br><span class="line">其中业务中台里面是以业务进行区分，抽取出来的一些公共组件，例如：会员中心，商品中心，交易中心、订单中心、支付中心、评价中心</span><br><span class="line">后期如果新增的产品线需要用到这些功能的时候可以从业务总台中直接开箱即用，提高效率。</span><br><span class="line">数据中台中包含大数据计算服务(包含离线和实时)、大数据开发套件(这里面包含的是一些小工具)、画像分析、数据可视化、数仓规则、数据服务等，可以实现数据的一站式接入和使用。</span><br><span class="line">移动中台包含了很多移动端的公共组件和功能。</span><br><span class="line">基于这些中台就可以快速为上层这些应用提供各种支持了。</span><br></pre></td></tr></table></figure><h3 id="什么是数据中台"><a href="#什么是数据中台" class="headerlink" title="什么是数据中台"></a>什么是数据中台</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">前面我们讲了什么是中台，中台其实是一个统称，基于中台也延伸出了很多分支。</span><br><span class="line">每一个分支深究起来都有很多内容，不过目前来说，在这些中台的分支里面，数据中台是最为火热的，因为数据是可以直接为企业决策提供支持，可以直接产生价值的。</span><br><span class="line"></span><br><span class="line">下面我们就来具体分析一下什么是数据中台</span><br><span class="line">针对数据中台的定义业内目前有很多种说法，没有官方的定义，不同的人有不同的理解。</span><br><span class="line"></span><br><span class="line">通俗来讲数据中台是指利用大数据技术，对海量数据统一进行采集、计算、存储，并且对外提供数据服务。</span><br><span class="line">数据中台的主要作用在于将企业内部所有数据统一处理形成标准化数据，挖掘出对企业最有价值的数据，构建企业数据资产库，对内对外提供一致的，高可用的大数据服务。</span><br><span class="line"></span><br><span class="line">正式一点来说，可以这样理解</span><br><span class="line">数据中台是一套可持续 ”让企业的数据用起来 ” 的机制</span><br><span class="line">通过数据中台把数据变为一种服务能力，既能提升决策水平，又能直接支撑企业业务</span><br><span class="line">数据中台不仅仅是技术，也不仅仅是产品，而是一套完整的让数据用起来的机制。</span><br><span class="line">数据中台不是单纯的技术叠加，不是一个技术化的大数据平台，二者有本质区别。</span><br><span class="line">大数据平台更关心技术层面的事情，包括研发效率，平台的大数据处理能力，针对的往往是技术人员</span><br><span class="line">而数据中台的核心是数据服务能力，数据中台不仅面向技术人员，更需要面向多个部门的业务人员。</span><br></pre></td></tr></table></figure><h3 id="数据中台的演进过程"><a href="#数据中台的演进过程" class="headerlink" title="数据中台的演进过程"></a>数据中台的演进过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">数据中台并不是直接就有的，也是根据时代的发展，企业的需求，一步一步演进出来的。</span><br><span class="line">下面我们就来看一下数据中台的演进过程。</span><br><span class="line"></span><br><span class="line">1：最开始是 数据库阶段，主要是OLTP（联机事务处理）的需求；</span><br><span class="line">以淘宝为例，最开始淘宝还只是一个简单的网站，淘宝的整个结构就是前端的一些页面，加上后端的数据库，只是个简单的OLTP系统，主要就是交易的事务处理。</span><br><span class="line"></span><br><span class="line">这个阶段，互联网黄页才刚刚出现，数据来源大部分还是传统商业的ERP&#x2F;CRM的结构化数据，数据量并不大，也就是GB的级别。简单的数据库就能满足需求。</span><br><span class="line"></span><br><span class="line">随着淘宝用户超过100万，分析需求的比重就越来越大。淘宝需要知道它的交易来自于哪些地区，来自于哪些人，谁在买淘宝的东西等等，于是，就进入了数据处理的第二个阶段：数据仓库阶段。</span><br><span class="line"></span><br><span class="line">2：数据仓库阶段，OLAP（联机分析处理）成为主要需求；</span><br><span class="line">OLTP和OLAP对数据存储和计算的需求是不一样的，OLTP处理的是结构化的交易数据，而OLAP对应的是互联网数据，而互联网里面数据量最大的是日志，90%以上的数据都是用户点击之类的非结构化的日志数据，而且数据量已经达到了TB的级别。</span><br><span class="line"></span><br><span class="line">针对分析需求，就诞生了数据仓库，数据仓库主要解决大量数据的存储和计算需求，也就是把非结构化的数据转化成结构化数据，存储下来。</span><br><span class="line"></span><br><span class="line">这个阶段，数据仓库支持的主要就是BI和报表需求。</span><br><span class="line"></span><br><span class="line">随着数据量越来越大，从TB进入了PB级别，原来的技术架构越来越不能支持海量数据处理，这时候就进入了第三个阶段：数据平台阶段。</span><br><span class="line"></span><br><span class="line">3：数据平台阶段，主要解决BI和报表需求的技术问题；</span><br><span class="line">这个阶段解决的还是BI和报表需求，但是主要是在解决底层的技术问题，也就是数据库架构设计的问题。</span><br><span class="line"></span><br><span class="line">这在数据库技术领域被概括为「Shared Everything、Shared Nothing、或Shared Disk」，说的就是数据库架构设计本身的不同技术思路之争。</span><br><span class="line"></span><br><span class="line">Shared Everything一般是针对单个主机，完全透明共享CPU&#x2F;MEMORY&#x2F;IO，并行处理能力是最差的，典型的代表SQLServer。</span><br><span class="line"></span><br><span class="line">Shared Disk的代表是Oracle RAC，用户访问RAC就像访问一个数据库，但是这背后是一个集群，RAC来保证这个集群的数据一致性。</span><br><span class="line"></span><br><span class="line">问题在于Oracle RAC(实时应用集群)是基于IOE架构的（使用IBM的小型机、Oracle数据库、EMC存储设备）。在海量数据处理上，IOE架构有天然的限制，不适合未来的发展。</span><br><span class="line"></span><br><span class="line">Shared Nothing的代表就是Hadoop。Hadoop的并行处理和扩展能力更好。</span><br><span class="line"></span><br><span class="line">Hadoop的好处是如果要增加数据处理的能力和容量，只需要增加服务器就好，成本不高，在海量数据处理和大规模并行处理上有很大优势。</span><br><span class="line"></span><br><span class="line">综上所述，第三阶段就是，建立Shared Nothing的海量数据处理平台来解决数据存储成本增长过快的问题。</span><br><span class="line"></span><br><span class="line">4：数据中台阶段，通过系统来对接OLTP（事务处理）和OLAP（报表分析）的需求，强调数据业务化的能力。</span><br><span class="line">这个阶段的特征是数据量呈现指数级增长，从PB迈向了EB级别，未来会到什么量级，谁也说不清楚。</span><br><span class="line"></span><br><span class="line">主要是因为，2015年之后，IOT（物联网）发展起来，带动了视频、图像、声音数据的增长，未来90%的数据可能都来自于视频、图像、声音这些非结构化数据，这些数据需要视觉计算技术、图像解析引擎+视频解析引擎+音频解析引擎来转换成结构化数据。5G技术的发展，可能会进一步放大视频、图像、声音数据的重要性。</span><br><span class="line"></span><br><span class="line">线下要想和线上一样，通过数据来改善业务，就要和线上一样能做到行为可监测，数据可收集，这是前提。线下最大量的就是视频、图像、声音数据，而这些数据靠人来手工收集，肯定是不靠谱的，依靠IOT（物联网）技术和算法的进步，最终会通过智能端来自动化获取数据。</span><br><span class="line"></span><br><span class="line">要使用这些数据，光有视觉算法和智能端也不行，要有云来存储和处理这些数据，以及打通其它领域的数据。</span><br><span class="line"></span><br><span class="line">目前的数据中台，最底层的数据平台还是偏技术的，是中台技术方案的其中一个组件，主要解决数据存储和计算的问题；在往上面就是一层数据服务层，数据服务层通过服务化API能够把数据和前台的业务层对接；数据中台里面都是系统去做对接，通过智能算法，能把前台的分析需求和交易需求去做对接，最终赋能业务。</span><br></pre></td></tr></table></figure><h3 id="数据中台-VS-数据仓库"><a href="#数据中台-VS-数据仓库" class="headerlink" title="数据中台 VS 数据仓库"></a>数据中台 VS 数据仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">数据仓库主要支持管理决策和业务分析</span><br><span class="line">数据中台是将数据服务化之后提供给业务系统，目的是将数据能力渗透到各个业务环节，不限于决策分析类场景</span><br><span class="line">数据中台建设包含数据体系建设，也就是数据中台包含数据仓库的完整内容</span><br><span class="line">所以说数据仓库阶段的成果是可以转化到数据中台阶段的，并不会全部推倒重做。</span><br></pre></td></tr></table></figure><h3 id="数据中台需要具备的四大能力"><a href="#数据中台需要具备的四大能力" class="headerlink" title="数据中台需要具备的四大能力"></a>数据中台需要具备的四大能力</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">根据我们前面对数据中台的分析，总结起来，数据中台需要具备以下能力：</span><br><span class="line">1：数据汇聚整合</span><br><span class="line">随着业务的发展，企业内部往往有多个信息部门和数据中心，大量系统、功能和应用重复建设，存在巨大的数据资源、计算资源和人力资源的浪费，同时组织壁垒也会导致数据孤岛的出现，使得内外部数据难以全局规划，数据中台需要对数据进行整合和完善。</span><br><span class="line"></span><br><span class="line">2：数据提纯加工</span><br><span class="line">数据就像石油，需要经过提纯加工才能使用，这个过程就是数据资产化。</span><br><span class="line">数据中台必须联通全域数据，通过统一的数据标准和质量体系，建设提纯加工后的标准数据资产体系，以满足企业业务对数据的需求。</span><br><span class="line"></span><br><span class="line">3：数据服务可视化</span><br><span class="line">为了尽快让数据用起来，数据中台必须提供快捷，快速的数据服务能力，让相关人员能够迅速开发数据应用，支持数据资产场景化能力的快速输出，以响应客户的动态需求。</span><br><span class="line"></span><br><span class="line">4：数据价值变现</span><br><span class="line">数据中台通过打通企业数据，提供以前单个部门无法提供的数据服务能力，以实现数据的更大价值变现。</span><br></pre></td></tr></table></figure><h2 id="数据中台架构"><a href="#数据中台架构" class="headerlink" title="数据中台架构"></a>数据中台架构</h2><h3 id="数据中台总体架构图"><a href="#数据中台总体架构图" class="headerlink" title="数据中台总体架构图"></a>数据中台总体架构图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面我们通过理论层面对数据中台有了一定的了解，下面我们通过架构层面来详细看一下数据中台的设计</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171109254.png" alt="image-20230517110950057"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">数据中台是位于底层存储计算平台与上层的数据应用之间的一整套体系。</span><br><span class="line">数据中台屏蔽掉底层存储平台的计算技术复杂性，降低对技术人才的需求，让数据的使用成本更低。</span><br><span class="line">通过数据中台的数据汇聚、数据开发模块建立企业数据资产。</span><br><span class="line">通过数据体系对数据进行分层存储</span><br><span class="line">通过资产管理、数据服务，把数据资产变为数据服务能力，服务于企业业务。</span><br><span class="line">数据安全管理、数据运营体系，保障数据中台可以长期健康、持续运转。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">数据汇聚</span><br><span class="line">数据汇聚是数据中台数据接入的入口，数据中台本身不产生数据，所有的数据来自于业务系统，数据库、日志、文件等，这些数据分散在不同的网络环境和存储平台中，难以利用，很难产生业务价值，所以需要统一汇聚。</span><br><span class="line"></span><br><span class="line">数据开发</span><br><span class="line">数据开发是一整套数据加工以及处理的工具，因为通过数据汇聚模块汇聚到中台的数据没有经过处理，基本是按照数据的原始状态堆砌在一起的，这样业务是很难直接使用的。所以需要通过数据开发模块实现对数据的加工处理，形成有价值的数据，提供给业务部门使用。</span><br><span class="line"></span><br><span class="line">数据体系</span><br><span class="line">通过数据汇聚、数据开发，中台就具备了构建数仓平台的基本能力，这一块其实就是将采集过来的各种数据按照数仓的标准进行建设。</span><br><span class="line"></span><br><span class="line">数据资产管理</span><br><span class="line">通过数仓建立起来的数据资产比较偏向于技术，业务人员比较难理解，资产管理是以业务人员更好理解的方式，把数据资产展现给企业的业务人员。</span><br><span class="line"></span><br><span class="line">数据服务体系</span><br><span class="line">数据服务体系就是把数据变为一种服务能力，通过数据服务让数据参与到业务，激活整个数据中台，数据服务体系是数据中台存在的价值所在。</span><br><span class="line"></span><br><span class="line">数据运营体系</span><br><span class="line">是数据中台得以健康、持续运转的基础</span><br><span class="line"></span><br><span class="line">数据安全管理</span><br><span class="line">是为了保证数据中台中的数据安全。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是一个典型的数据中台总体架构设计。</span><br></pre></td></tr></table></figure><h3 id="数据中台-四字箴言"><a href="#数据中台-四字箴言" class="headerlink" title="数据中台 四字箴言"></a>数据中台 四字箴言</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果大家之前没有工作过的话，可能对数据中台还是不好理解，所以在这我将数据中台的功能总结为四个字：采、存、通、用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">下面我们来详细分析一下这四字箴言</span><br><span class="line"></span><br><span class="line">采</span><br><span class="line">采：表示采集的意思，就是采集企业中的所有数据。</span><br><span class="line">随着互联网、移动互联网、物联网等技术的兴起，企业的业务形态开始多元化，数据的产生形式也是多样化的，对应的就需要有多种采集形式</span><br><span class="line"></span><br><span class="line">埋点采集、硬件采集、爬虫采集、数据库采集、日志采集</span><br><span class="line"></span><br><span class="line">埋点采集：一般是采集用户行为信息，例如用户在平台上的浏览、点击、停留等行为</span><br><span class="line">硬件采集：指的是物联网数据采集，例如通过无人机传感器来采集空气质量指标</span><br><span class="line">爬虫采集：指的是采集互联网上的公开数据，例如：电商平台竞品价格采集</span><br><span class="line">数据库采集：一般是采集企业内的业务数据，例如：用户交易数据、用户个人信息数据等</span><br><span class="line">日志采集：一般是采集软件运行时产生的日志</span><br><span class="line"></span><br><span class="line">这些是常见的采集形式</span><br><span class="line"></span><br><span class="line">从数据组织形式可以分为：结构化数据、半结构化数据、非结构化数据</span><br><span class="line">结构化数据：数据规则、完整、能够通过二维逻辑来表现的数据，严格遵守数据格式与长度规范，常见的有数据库中的数据、excel中的数据</span><br><span class="line">半结构化数据：数据规则、完整，同样严格遵守数据格式与长度规范，但无法通过二维关系来表现，常见的有JSON、XML等格式的数据</span><br><span class="line">非结构化数据：数据结构不规则或不完整，不方便用二维逻辑表来表现，需要经过复杂的逻辑处理才能提取其中的信息内容，常见的有word文档、图片、视频、音频等数据</span><br><span class="line"></span><br><span class="line">从数据的时效性上来划分，可以分为：离线数据、实时数据</span><br><span class="line">离线数据：主要用于大批量数据的周期性迁移，对时效性要求不高，一般采用分布式批量数据同步的形式，通过连接读取数据，读取数据过程中可以有全量、增量的方式，经过统一处理后写入到目标存储。</span><br><span class="line">实时数据：主要面向低延时的数据应用场景，一般通过实时监控的方式实现，例如通过读取数据库的binlog日志来实现数据库的实时数据采集。</span><br><span class="line"></span><br><span class="line">前面我们针对数据的采集形式、数据的组织形式、数据的时效性进行了分析，那这些数据在采集的时候具体应该使用什么类型的工具呢？</span><br><span class="line"></span><br><span class="line">常见的采集工具有：Flume、FileBeat、Logstash、Sqoop、Canal、DataX等</span><br><span class="line">其中Flume、FileBeat、Logstash适合采集日志数据，这三个组件的特性在前面项目课程中已经详细分析过了，在这不再赘述。</span><br><span class="line">sqoop是在结构化数据和HDFS之间进行批量数据迁移的工具，适合批量采集数据库中的数据，它的主要优势是，在特定场景下，数据交换过程会有很大的性能提升。主要缺点是处理过程定制程度较高，需要在脚本中调整配置参数实现，在用户的一些自定义逻辑和数据同步链路监控方面比较薄弱。</span><br><span class="line">DataX是阿里开源的一套数据采集工具，提供数据采集全链路的流量监控，将作业本身的状态，数据流量，数据速度，执行速度等信息进行展示，提供脏数据探测功能，支持传输过程中对传输报错进行策略化处理。由于它是基于进程内读写直连的方式，高并发数据采集场景下对机器内存要求比较高。不过DataX不支持非结构化数据的采集。</span><br><span class="line"></span><br><span class="line">这些单个工具都无法很好的满足企业复杂的数据采集场景，所以我们需要对已有的采集工具进行二次开发，以可视化配置的方式提供给用户，屏蔽底层工具的复杂性，要支持常见的数据源采集：关系型数据库、NoSQL数据库、MQ、文件系统等，并且支持增量同步、全量同步等方式。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">存</span><br><span class="line">将数据采集过来之后，就需要考虑数据存储了。</span><br><span class="line">在这里我们可以将数据分为两种：静态数据和动态数据</span><br><span class="line">其中静态数据：是以 HDFS 、S3等分布式文件系统作为存储引擎，适用于高吞吐量的离线大数据分析场景。这类存储的局限性是数据无法进行随机的读写。</span><br><span class="line">动态数据：是以 HBase、Cassandra等NoSQL数据库作为存储引擎，适用于大数据随机读写的场景。这类存储的局限性是批量读取吞吐量远不如HDFS，不适合用于批量数据分析的场景。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">通</span><br><span class="line">表示是对数据进行加工计算，构建企业级数据仓库，打通企业中的全域数据。</span><br><span class="line">针对数据的加工计算，可以分为两大块，离线计算和实时计算</span><br><span class="line">离线计算中的代表框架为：MapReduce、Hive、和Spark</span><br><span class="line">实时计算中的代表框架为：Storm、SparkStreaming和Flink，针对实时计算，现在主要是以Flink为主了。</span><br><span class="line">针对这些计算框架，如果每一个计算任务都需要开发代码的话，对使用人员就不友好了，特别是针对一些业务人员，他们不会写代码，只会写SQL，所以这时候我们就需要开发一套基于SQL的一站式开发平台，底层引擎使用Spark和Flink，支持离线数据计算和实时数据计算。</span><br><span class="line">让用户彻底规避掉繁重的底层代码开发工作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">用</span><br><span class="line">企业全域数据采集、存储，打通之后，就涉及到如何去用了。</span><br><span class="line">这里的”用” 包含很多层面。</span><br><span class="line">首先是包括数据资产管理，也可以称之为数据治理，其中包含数据元标准管理，数据标签管理，数据模型管理、元数据管理、数据质量管理等，保证数据中台里面数据的合理化和规范化，充分发挥数据的价值。</span><br><span class="line">对于数据的拥有者和管理者来说，通过对数据的合理管理和有效应用，能盘活并充分释放数据的巨大价值，但如果不能对数据进行有效管理，数据就用不起来，或者即使用起来也用不好，在这种情况下，堆积如山的无序数据给企业带来的是高昂的成本。</span><br><span class="line"></span><br><span class="line">在使用数据的时候还需要做好数据安全管理，随着大数据技术和应用的快速发展，数据所承载的多维度业务价值已被越来越多的挖掘和应用变现，随之而来的是数据安全和隐私已经成为世界性的关注点，上升到国家战略层面，最近闹得沸沸扬扬的特朗普要禁用国外版的抖音(TikTok)事件，特朗普的理由就是TikTok平台的数据对他们产生了威胁。</span><br><span class="line">所以说数据安全很有必要，整体的数据安全管理体系通过分层建设、分级防护，创造面向数据的安全管理体系系统框架，形成完整的数据安全管理体系。</span><br><span class="line">数据中台的建设，应该始终把数据安全管理放在最重要的位置上，通过设计完备的数据安全管理体系，多方面，多层次保障数据安全。</span><br><span class="line"></span><br><span class="line">最终我们需要把安全、有价值的数据快速方便的提供给上层应用，此时需要通过数据服务对外开放，也就是API接口的形式。</span><br><span class="line">举个例子，水是生命之源，是人们赖以生存和发展的重要物质资源，在日常生活中，可以通过不同的方式使用水，这也给我们的生活带来了巨大便利。</span><br><span class="line">在数据世界中，数据资产就好比日常生活中生命所需的水资源，无处不在且不可或缺。但是如果没有相应的水加工厂，运输管道，人们只能到水库打水喝，这明显会极大影响人们正常的生活和工作。因此，将数据封装成数据服务，以接口形式提供给上层应用，才能极大释放、提升数据资产的价值。</span><br><span class="line"></span><br><span class="line">最后总结一下，数据中台其实可以这样理解，采集企业全域数据，存储起来，通过加工计算打通数据之间的关系，最后以API接口的形式对外提供数据服务。这就是数据中台要做的事情。</span><br></pre></td></tr></table></figure><h2 id="什么样的企业适合建设数据中台"><a href="#什么样的企业适合建设数据中台" class="headerlink" title="什么样的企业适合建设数据中台"></a>什么样的企业适合建设数据中台</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前面我们分析了什么是数据中台，数据中台的好处，以及数据中台的架构，是不是所有的企业都需要构建数据中台呢？</span><br><span class="line">不是的，下面就来看一下到底什么样的企业适合建设数据中台</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171128064.png" alt="image-20230517112856557"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">看这个案例：</span><br><span class="line">某企业下面有多个科研实体，每个科研实体下面有多个研究中心</span><br><span class="line">类似于一个集团公司，下面有多个子公司，每个子公司里面还有多个产品线。</span><br><span class="line">这种类型的企业业务复杂，有丰富的数据维度和多个业务场景，比较适合建设数据中台。</span><br><span class="line">初创型企业没有必要搭建数据中台，首先要解决的是生存问题，甚至于连数据仓库都没必要搭建，需要等企业走上正轨进入快速发展期的时候才需要构建数据仓库、数据中台。</span><br></pre></td></tr></table></figure><h3 id="数据应用成熟度的四个阶段"><a href="#数据应用成熟度的四个阶段" class="headerlink" title="数据应用成熟度的四个阶段"></a>数据应用成熟度的四个阶段</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">当然了，评价一个企业是否适合建设数据中台，也是有一些量化指标的，可以根据企业中的数据应用成熟度来进行判断，我们可以把企业中数据应用成熟度分为四个阶段</span><br><span class="line"></span><br><span class="line">统计分析</span><br><span class="line">决策支持</span><br><span class="line">数据驱动</span><br><span class="line">运营优化</span><br><span class="line"></span><br><span class="line">针对不同的阶段，从企业战略定位、企业数据形态、数据应用场景、数据应用工具、企业组织架构等多个方面，不同特征维度进行参考判定，也就构成了数据应用成熟度评估模型。</span><br><span class="line">依据这四个阶段的划分标准，企业可以进行数据应用成熟度的自我评测。</span><br><span class="line">数据应用成熟度越高，则代表数据对业务的支撑能力越强，数据应用成熟度越低，则意味着业务对数据的依赖程度越低。</span><br><span class="line"></span><br><span class="line">来看一下具体的评估模型。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171132712.png" alt="image-20230517113246298"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">第一阶段：统计分析阶段</span><br><span class="line">统计分析阶段主要有以下特征</span><br><span class="line">1：在企业战略方面，该阶段的企业战略定位纯粹以业务为驱动，主要以满足企业业务需求，实现业务流程的流程化、自动化为导向。</span><br><span class="line">2：在企业数据形态方面，该阶段的企业可能有少量的业务数据积累，但没有以数据为导向积累数据，数据主要以业务系统依托的关系型数据库进行存储，数据无组织，各业务数据分散存储和管理，数据维度单一，无数据质量管控。</span><br><span class="line">3：在数据应用场景方面，该阶段的数据应用场景只针对业务系统中的关键数据和指标，进行简单的、单一维度的统计分析和管理，辅助业务总结，每次基于业务目标的数据统计都需要定制化开发，如周报、月报等。</span><br><span class="line">4：在数据应用工具方面，该阶段业务报表主要以系统内嵌报表以及Excel报表为主，模式相对单一。</span><br><span class="line">5：在企业组织架构发面，该阶段企业无专门的数据相关部门，主要以IT部门的数据库运维管理和业务部门的数据分析师为主，需要数据报表时，一般用系统中定制的统计报表或者由特定业务部门提供Excel报表。</span><br><span class="line"></span><br><span class="line">如果对数据的应用仅停留在单系统、单维度的统计分析上，只用于对历史业务开展情况进行简单分析，数据并没有发挥出应有的价值，数据只是辅助企业了解业务运转的情况。我们希望能通过数据为业务决策提供支撑，因此就出现了第二阶段</span><br><span class="line"></span><br><span class="line">第二阶段：决策支持阶段</span><br><span class="line">决策支持阶段主要有以下特征</span><br><span class="line">1：在企业战略方面，该阶段，企业开始具备通过数据支撑经营决策的思路，并在考虑通过数据可视化的方式实现数据与业务的融合，以解决业务问题和支撑管理决策。</span><br><span class="line">2：在企业数据形态方面，企业开始注重业务过程中的数据积累，开始对各业务环节的数据进行汇聚、管理、数据维度逐渐丰富。以面向业务主体的指标体系为形式进行数据组织，开始注重数据质量的管控，实施数据质量控制。</span><br><span class="line">3：在数据应用场景方面，该阶段的数据应用场景开始基于数据仓库进行各业务主题的数据收集、管理、分析，为企业管理人员提供决策支持。</span><br><span class="line">4：在数据应用工具方面，开始针对数据收集和管理 建立数据仓库，数据开发工具和专业可视化工具，进行系统化数据收集、管理和分析。</span><br><span class="line">5：在企业组织架构发面，开始出现数据分析师的岗位，可能会设立专门的数据挖掘或商业智能部门来支撑企业进行数据化决策。</span><br><span class="line"></span><br><span class="line">无论是在统计分析阶段还是决策支撑阶段，业务的运转和数据之间依然是相互隔离的。企业对数据的应用都还停留在对部分维度的业务数据进行分析得到结果后，再由人工对业务进行不同程度的干预，最终实现业务优化。而我们希望能够让数据直接驱动业务变得更精准，更有效。最典型的应用场景就是类似于头条、抖音里面的个性化推荐功能，通过数据直接驱动业务的优化。所以就出现了第三阶段</span><br><span class="line"></span><br><span class="line">第三阶段：数据驱动阶段</span><br><span class="line"></span><br><span class="line">数据驱动阶段主要有以下特征</span><br><span class="line">1：在企业战略方面，企业开始将数据作为重要资产和生产资料，通过大数据技术对企业相关数据进行汇聚、打通和分析挖掘，为业务应用提供数据服务，通过数据驱动业务发展。</span><br><span class="line">2：在企业数据形态方面，业务数据积累具备一定规模，对结构化数据、非结构化数据进行处理与应用，根据需求进行数据清洗加工和标准化处理。</span><br><span class="line">3：在数据应用场景方面，该阶段的数据应用场景主要以满足业务需求为主，主要是用数据提升现有业务能力，进行智能化升级。</span><br><span class="line">4：在数据应用工具方面，在该阶段，企业开始通过大数据生态体系中的批计算、流计算等大数据处理技术进行数据汇聚和开发，并最终为现有的业务场景赋能，以驱动业务升级。</span><br><span class="line">5：在企业组织架构发面，在该阶段，企业开始正式设立独立的大数据部门。</span><br><span class="line"></span><br><span class="line">数据驱动阶段，数据其实已经与业务紧密结合，数据在业务运转过程中直接产生价值，但是，由于数据应用都是独立建设的，没有从全局考虑，企业在数据应用的过程中，经常会遇到标准口径不一致，内容重复建设，各业务数据无法融合产生更大的价值，企业数据价值无法被业务快速应用等问题，因此，出现了第四阶段</span><br><span class="line"></span><br><span class="line">第四阶段：运营优化阶段</span><br><span class="line">运营优化阶段主要有以下特征</span><br><span class="line">1：在企业战略方面，该阶段，企业开始建设数据中台，数据中台定位是为企业未来5~10年发展提供数据能力支撑，在DT时代对企业进行智能化升级。</span><br><span class="line">2：在企业数据形态方面，在该阶段，企业数据伴随数据驱动的业务快速发展，数据量快速增长，通过建立企业体系化，标准化的数据采集、存储、实现企业数据的全面资产化。</span><br><span class="line">3：在数据应用场景方面，在该阶段，数据应用通过统一的数据资产体系，提供统一、标准化的数据服务能力，为企业各类快速变化的业务应用提供数据服务支撑。</span><br><span class="line">4：在数据应用工具方面，建立一套体系化的数据汇聚、加工、管理、服务及应用体系，逐渐实现大数据能力工具化，工具平台化，平台智能化。</span><br><span class="line">5：在企业组织架构方面，在该阶段，企业组织架构中开始在管理层设置数据管理委员会来负责数据机制的建设和管理，将数据变为企业的一种独特资产。同时也会成立专门的资产运营部门，保障数据资产应用的合理性和效率，将更多的数据服务消费者引入到数据平台之中。</span><br><span class="line"></span><br><span class="line">这就是数据应用成熟度的四个阶段，目前中大型企业大部分处于从决策支持阶段转向数据驱动阶段，一些一线大型互联网企业正在处于从数据驱动阶段转向运营优化阶段。</span><br><span class="line">目前数据中台正处于快速发展阶段，成熟的大型公司都在开始着手构建数据中台。</span><br></pre></td></tr></table></figure><h3 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">下面有几个小案例，我们来分析一下</span><br><span class="line"></span><br><span class="line">企业A：类似于”万能钥匙 ”之类的工具类APP，随着DAU的增加，需要给用户提供个性化推荐内容</span><br><span class="line">目前比较合适的是启动一个内容推荐类的算法项目，在可预见的将来，看不到更多的数据场景，所以不适合建设数据中台。</span><br><span class="line"></span><br><span class="line">企业B：类似于”百果园 ”之类的连锁店，门店数量比较多，需要用大数据来精细化运营用户和商品</span><br><span class="line">因为具备了一定的门店规模和数据规模，可以实现一些个性化营销推送，商品猜你喜欢等功能，比较适合建设数据中台。</span><br><span class="line"></span><br><span class="line">企业C：类似于”华为 ”这样的多业态集团公司，旗下有多个业务板块，各个业务板块都有自己的数仓和报表</span><br><span class="line"></span><br><span class="line">这种属于多业态集团公司，是最适合建设数据中台的。</span><br></pre></td></tr></table></figure><h2 id="数据中台企业级解决方案"><a href="#数据中台企业级解决方案" class="headerlink" title="数据中台企业级解决方案"></a>数据中台企业级解决方案</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面我们对数据中台的理论进行分析，下面我们来看一下，数据中台在一些大型企业中的落地方案</span><br></pre></td></tr></table></figure><h3 id="阿里数据中台"><a href="#阿里数据中台" class="headerlink" title="阿里数据中台"></a>阿里数据中台</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在国内，”中台”的概念是阿里带头喊出来的，所以我们先来看一下阿里的数据中台方案</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171137113.png" alt="image-20230517113733709"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">最底层是计算和存储平台</span><br><span class="line">往上面是垂直数据中心，负责全域数据采集与引入，以需求为驱动，以数据多样性的全域思想为指导，采集与引入全业务、多终端、多形态的数据；</span><br><span class="line">再往上面是公共数据中心，按照基础层、公共中间层、应用层的数据分层架构模式，通过数据指标结构化、规范化的方式实现指标口径统一</span><br><span class="line">再往上面是萃取数据中心，形成以业务核心对象为中心的连接和标签体系，深度萃取数据价值</span><br><span class="line">再往上面是统一主题式服务，通过构建服务元数据中心和数据服务查询引擎，面向业务统一数据出口与数据查询逻辑，屏蔽多数据源与多物理表；</span><br><span class="line">左侧是数据资产管理：通过资产分析、应用、优化、运营等方面实现降低数据管理成本、追踪数据价值。</span><br><span class="line"></span><br><span class="line">最上层的是不同的产品线应用，通过下面的数据中台提供一站式数据服务支撑。</span><br><span class="line"></span><br><span class="line">阿里数据中台有三大核心：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171138752.png" alt="image-20230517113808804"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">OneData（统一数据）：定义数据标准与建模标准，对离线数据、实时数据建立数据资产体系</span><br><span class="line">OneEntity（统一实体）：主数据的管理，实现全域产品体系主数据融合</span><br><span class="line">OneService（统一服务）：统一对外提供API与SDK服务</span><br><span class="line">这就是阿里数据中台的三大核心。其实就是将全域数据统一标准，然后打通，最终统一对外提供数据服务。</span><br></pre></td></tr></table></figure><h3 id="菜鸟数据中台"><a href="#菜鸟数据中台" class="headerlink" title="菜鸟数据中台"></a>菜鸟数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171138932.png" alt="image-20230517113839690"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">整体技术架构，分三层，底层是基础设施，基础平台，中间是中台，上面是前台。</span><br><span class="line">有些同学可能会有困惑“数据中台和大数据平台”的区别是什么？</span><br><span class="line">图中的基础平台就是我们常说的大数据平台，包含了数据的采集、计算、加工等。</span><br><span class="line">数据中台是构建在整个大数据平台之上的，它是围绕数据运营、分析、应用等场景去做的一套解决方案。</span><br><span class="line"></span><br><span class="line">数据中台分成两块，一个是数据层，一个是服务层。数据层就是我们前面说的“数仓“，这里边包含菜鸟的所有数据，沉淀的数据资产。</span><br><span class="line">再往上是服务层，这里划分成了几个套件，每个套件都是围绕数据使用的一个场景做的解决方案 。</span><br><span class="line"></span><br><span class="line">右侧的东西是数据管理套件，从数据的加工生产到使用，它从全链路的视角把数据给管理起来。</span><br><span class="line"></span><br><span class="line">最上层是前台业务。</span><br></pre></td></tr></table></figure><h3 id="滴滴数据中台"><a href="#滴滴数据中台" class="headerlink" title="滴滴数据中台"></a>滴滴数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171146305.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">最底层是数据架构：数据架构体系包含了当前大数据领域主流的技术</span><br><span class="line"></span><br><span class="line">再往上面是数据研发，数据中间件，实现数据的采集，计算和数据质量监控。</span><br><span class="line"></span><br><span class="line">再往上面是数据资产体系，构建了数据字典和数据图谱，然后通过数据赋能，提供各种自助查询和可视化分析。</span><br><span class="line"></span><br><span class="line">最上层是数据服务层，将数据服务化提供给各个产品使用。</span><br></pre></td></tr></table></figure><h3 id="苏宁数据中台"><a href="#苏宁数据中台" class="headerlink" title="苏宁数据中台"></a>苏宁数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171145463.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">最底层是大数据计算存储引擎</span><br><span class="line">上层是数据开发套件，负责加工计算数据，</span><br><span class="line">然后是数据仓库主题域，构建多维度的数据主题，</span><br><span class="line">接下来是数据治理套件，管理数据模型，保证数据质量。</span><br><span class="line"></span><br><span class="line">再往上层是数据应用引擎，这里面包含了可视化引擎，数据服务引擎，数据分析引擎和画像引擎，通过这几个引擎对外提供数据服务。</span><br><span class="line"></span><br><span class="line">最上层是数据应用层，主要是使用数据的。</span><br></pre></td></tr></table></figure><h3 id="华为云数据中台"><a href="#华为云数据中台" class="headerlink" title="华为云数据中台"></a>华为云数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171145118.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">华为云数据中台在这里可以划分为三块</span><br><span class="line"></span><br><span class="line">第一块是数据建设：其实就是把全域数据采集过来，对数据加工计算，基于各种维度构建数据主题。</span><br><span class="line">第二块是平台建设：这里面抽取出来了一些公共的功能组件，并且提供了数据服务。</span><br><span class="line"></span><br><span class="line">第三块是中台消费场景，这里面会有多种场景依靠数据中台提供数据支撑。</span><br></pre></td></tr></table></figure><h3 id="浙江移动数据中台"><a href="#浙江移动数据中台" class="headerlink" title="浙江移动数据中台"></a>浙江移动数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171144694.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">浙江移动打造的数据中台，是为了实现跨域数据整合并沉淀公共的数据能力，同时提供丰富的数据模型，标准化的数据服务，个性化的开发平台与工具，满足一线数据开放和智慧运营的要求。</span><br><span class="line">这个数据中台架构中主要包含了三块内容</span><br><span class="line">数据模型、数据服务和数据开发。</span><br><span class="line"></span><br><span class="line">数据模型：负责实现数据与数据模型打通。</span><br><span class="line">数据服务：负责封装标准数据服务，对外提供数据查询服务</span><br><span class="line">数据开发：针对各种个性化数据应用开发需求提供技术支持</span><br></pre></td></tr></table></figure><h3 id="某大数据服务商数据中台"><a href="#某大数据服务商数据中台" class="headerlink" title="某大数据服务商数据中台"></a>某大数据服务商数据中台</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171142475.png" alt="image-20230517114238211"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">底层是基础设施和计算层</span><br><span class="line">往上面是数据开发治理模块，里面涉及离线数据开发、实时数据开发和算法开发。</span><br><span class="line">再往上层是数据服务层，这里面会对数据进行体系化，最终对外提供服务。</span><br><span class="line">最上层是业务应用层，这里会通过数据服务提供数据支撑。</span><br></pre></td></tr></table></figure><h3 id="某企业数据大脑"><a href="#某企业数据大脑" class="headerlink" title="某企业数据大脑"></a>某企业数据大脑</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171141192.png" alt="image-20230517114128072"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这个是某企业的数据大脑总体设计，里面包含了数据中台。</span><br><span class="line">数据大脑主要是为了解决大数据系统建设中数据存储、连通、使用的共性问题，形成业务数据中台，包括数据资源的统一规划，数据整体建模和资产管理，数据标签化计算，形成不同行业的数据体系。</span><br><span class="line">将行业知识库和领域数据相结合，开发各类计算组件，构建统一数据加工平台，对数据进行加工整理支撑行业应用，形成相关领域行业的数据大脑。</span><br><span class="line"></span><br><span class="line">这里面的数据中台主要包含以下内容：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171141182.png" alt="image-20230517114145986"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">到这为止，我们分析了多个企业的数据中台，虽然这些企业的数据中台架构没有完全一样的，但是总结下来我们会发现，他们里面都会有一些共同的核心内容。</span><br><span class="line">数据采集存储、数据加工计算，整合打通各个维度数据，最后对外提供数据服务。</span><br><span class="line">其实精简之后就是我在前面给大家总结的数据中台四字箴言，采存通用。</span><br><span class="line"></span><br><span class="line">希望通过我们前面的学习能够然大家对数据中台有一个整体的认识。</span><br></pre></td></tr></table></figure><h2 id="数据中台之数据加工总线"><a href="#数据中台之数据加工总线" class="headerlink" title="数据中台之数据加工总线"></a>数据中台之数据加工总线</h2><h3 id="目前大数据领域实时计算的现状"><a href="#目前大数据领域实时计算的现状" class="headerlink" title="目前大数据领域实时计算的现状"></a>目前大数据领域实时计算的现状</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">随着大数据行业的整体发展，企业对实时计算的需求越来越多，特别是在构建实时数仓的时候，需要接入很多实时数据源，并且数仓还是分层的，针对每一层的数据都需要进行实时计算，此时就需要开发很多实时计算程序，实时计算程序的复用性很低，针对每一种类型的数据都需要开发对应的实时计算程序，开发成本高，并且对程序员也不友好，需要专门的大数据开发工程师，所以我们希望在实时计算领域能够提供类似HiveSQL的功能，直接写SQL就能实现实时计算任务，不需要每次都写一堆的代码，提高工作效率，尽可能让会只会SQL的普通开发人员也能轻松的开发实时计算任务。</span><br><span class="line"></span><br><span class="line">为了解决这个痛点，于是，我们研发了数据加工总线平台，也可以称之为数据实时流转平台。</span><br></pre></td></tr></table></figure><h3 id="什么是数据加工总线"><a href="#什么是数据加工总线" class="headerlink" title="什么是数据加工总线"></a>什么是数据加工总线</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">为了使实时数据的处理能够更加高效、简单，所以我们研发了一站式实时数据开发平台。只需要在页面选择数据源、目的地以及对应的SQL计算逻辑，就可以轻松实现海量实时数据计算任务的开发。</span><br><span class="line"></span><br><span class="line">这个平台主要的功能就是支持SQL实现实时数据计算任务的开发。</span><br><span class="line"></span><br><span class="line">我们期望达到的目标，通过这套平台，可以实现用SQL解决80%以上的实时数据计算需求。</span><br></pre></td></tr></table></figure><h3 id="数据加工总线原型图总览"><a href="#数据加工总线原型图总览" class="headerlink" title="数据加工总线原型图总览"></a>数据加工总线原型图总览</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于数据加工总线涉及前端和后端，在企业中前端代码有专门的同事负责开发，我们大数据部门只需要负责后台功能开发即可，所以在课程中不涉及前端页面代码，在这里通过原型图来演示一下数据加工总线具体的使用流程，加深大家的理解。</span><br><span class="line">注意：原型图只能在这里给大家演示一下，不能发出去，希望大家理解。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171152004.jpg" alt="图片描述"></p><h3 id="数据加工总线架构图V1-0"><a href="#数据加工总线架构图V1-0" class="headerlink" title="数据加工总线架构图V1.0"></a>数据加工总线架构图V1.0</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下数据加工总线的后台架构图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171154966.jpg" alt="图片描述"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">数据源和目的地都是Kafka，因为目前在大数据领域，实时数据一般都是用的Kafka。</span><br><span class="line">中间就是我们需要开发的核心计算引擎，基于SparkSQL封装的实时SQL计算引擎。</span><br><span class="line">为什么在这要选择使用SparkSQL？</span><br><span class="line">因为SparkSQL在我们公司已经广泛应用了很长时间了，并且Spark框架本身也迭代了很多版本了，比较稳定，整个生态圈也比较完善。所以前期在技术选型的时候优先考虑的是底层计算引擎的稳定性。</span><br><span class="line">还有就是秒级别的延时是可以满足业务需求的，所以当时SparkSQL+SparkStreaming是最好的方案。</span><br><span class="line">Flink当时版本还不是很稳定，FlinkSQL也是刚出现没多久，所以没有直接使用Flink。</span><br><span class="line">当时我们也考虑了，等第一个版本稳定了以后，后期再把FlinkSQL也增加进来，提供两套底层计算引擎，可以根据需求进行动态切换。</span><br><span class="line"></span><br><span class="line">针对这里面的数据字段和数据模型的概念做一下解释</span><br><span class="line">由于我们需要在SparkSQL中基于kafka的数据进行建表，kafka中的数据我们使用的是json格式的，json格式的数据只有字段名称，缺少字段类型信息，官方一点来说其实就是缺少元数据信息，所以需要针对kafka中的数据定义元数据，这样才能在SparkSQL中建表。</span><br><span class="line"></span><br><span class="line">注意：元数据的定义是在数据治理子系统中维护的。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v2.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v2.0-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-2.html</id>
    <published>2023-05-03T09:32:38.000Z</published>
    <updated>2023-05-20T12:50:47.753Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v2-0-2"><a href="#第十八周-直播平台三度关系推荐v2-0-2" class="headerlink" title="第十八周 直播平台三度关系推荐v2.0-2"></a>第十八周 直播平台三度关系推荐v2.0-2</h1><h2 id="每周一计算最近一周内主活主播的三度关系列表-任务六"><a href="#每周一计算最近一周内主活主播的三度关系列表-任务六" class="headerlink" title="每周一计算最近一周内主活主播的三度关系列表(任务六)"></a>每周一计算最近一周内主活主播的三度关系列表(任务六)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现每周一计算最近一周内主活主播的三度关系列表</span><br><span class="line">创建子module项目：get_recommend_list</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="子项目pom"><a href="#子项目pom" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;get_recommend_list&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line"></span><br><span class="line">注意：此时需要通过flink读取neo4j中的数据，但是针对DataSet是不支持addSource方法的，但是它里面有一个createInput，可以接收一个自定义的InputFormat，所以我就需要定义一个Neo4jInputFormat了</span><br></pre></td></tr></table></figure><h3 id="Neo4jInputFormat-scala"><a href="#Neo4jInputFormat-scala" class="headerlink" title="Neo4jInputFormat.scala"></a>Neo4jInputFormat.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">创建类：Neo4jInputFormat</span><br><span class="line">代码如下：</span><br><span class="line"></span><br><span class="line">注意：此代码中的输入组件只能使用单并行度执行，如果使用多并行度查询可能会出现重复数据</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.io.statistics.<span class="type">BaseStatistics</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.io.&#123;<span class="type">DefaultInputSplitAssigner</span>, <span class="type">NonParallelInput</span>, <span class="type">RichInputFormat</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.io.&#123;<span class="type">GenericInputSplit</span>, <span class="type">InputSplit</span>, <span class="type">InputSplitAssigner</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">Driver</span>, <span class="type">GraphDatabase</span>, <span class="type">Result</span>, <span class="type">Session</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从Neo4j中查询满足条件的主播</span></span><br><span class="line"><span class="comment"> * 一周内活跃过，并且主播等级大于4</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neo4jInputFormat</span> <span class="keyword">extends</span> <span class="title">RichInputFormat</span>[<span class="type">String</span>,<span class="type">InputSplit</span>] <span class="keyword">with</span> <span class="title">NonParallelInput</span></span>&#123; <span class="comment">// 这里是多继承</span></span><br><span class="line">  <span class="comment">//注意：with NonParallelInput 表示此组件不支持多并行度</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//保存neo4j相关的配置参数</span></span><br><span class="line">  <span class="keyword">var</span> param: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>] = <span class="type">Map</span>()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> driver: <span class="type">Driver</span> = _</span><br><span class="line">  <span class="keyword">var</span> session: <span class="type">Session</span> = _</span><br><span class="line">  <span class="keyword">var</span> result: <span class="type">Result</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 构造函数</span></span><br><span class="line"><span class="comment">   * 接收neo4j相关的配置参数</span></span><br><span class="line"><span class="comment">   * @param param</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(param: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>])&#123;</span><br><span class="line">    <span class="keyword">this</span>()</span><br><span class="line">    <span class="keyword">this</span>.param = param</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 配置此输入格式</span></span><br><span class="line"><span class="comment">   * @param parameters</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">configure</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 获取输入数据的基本统计信息</span></span><br><span class="line"><span class="comment">   * @param cachedStatistics</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getStatistics</span></span>(cachedStatistics: <span class="type">BaseStatistics</span>): <span class="type">BaseStatistics</span> = &#123;</span><br><span class="line">    cachedStatistics</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 对输入数据切分split</span></span><br><span class="line"><span class="comment">   * @param minNumSplits</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createInputSplits</span></span>(minNumSplits: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">InputSplit</span>] = &#123;</span><br><span class="line">    <span class="type">Array</span>(<span class="keyword">new</span> <span class="type">GenericInputSplit</span>(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 获取切分的split</span></span><br><span class="line"><span class="comment">   * @param inputSplits</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getInputSplitAssigner</span></span>(inputSplits: <span class="type">Array</span>[<span class="type">InputSplit</span>]): <span class="type">InputSplitAssigner</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DefaultInputSplitAssigner</span>(inputSplits)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 初始化方法：只执行一次</span></span><br><span class="line"><span class="comment">   * 获取neo4j连接，开启会话</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">openInputFormat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//初始化Neo4j连接</span></span><br><span class="line">    <span class="keyword">this</span>.driver = <span class="type">GraphDatabase</span>.driver(param(<span class="string">"boltUrl"</span>), <span class="type">AuthTokens</span>.basic(param(<span class="string">"userName"</span>), param(<span class="string">"passWord"</span>)))</span><br><span class="line">    <span class="comment">//开启会话</span></span><br><span class="line">    <span class="keyword">this</span>.session = driver.session()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 关闭Neo4j连接</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">closeInputFormat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(driver!=<span class="literal">null</span>)&#123;</span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 此方法也是只执行一次</span></span><br><span class="line"><span class="comment">   * @param split</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(split: <span class="type">InputSplit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.result = session.run(<span class="string">"match (a:User) where a.timestamp &gt;="</span>+param(<span class="string">"timestamp"</span>)+<span class="string">" and a.level &gt;= "</span>+param(<span class="string">"level"</span>)+<span class="string">" return a.uid"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 如果数据读取完毕号以后，需要返回true</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reachedEnd</span></span>(): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    !result.hasNext</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 读取结果数据，一次读取一条</span></span><br><span class="line"><span class="comment">   * @param reuse</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">nextRecord</span></span>(reuse: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> record = result.next()</span><br><span class="line">    <span class="keyword">val</span> uid = record.get(<span class="number">0</span>).asString()</span><br><span class="line">    uid</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 关闭会话</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(session!=<span class="literal">null</span>)&#123;</span><br><span class="line">      session.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="GetRecommendList-scala"><a href="#GetRecommendList-scala" class="headerlink" title="GetRecommendList.scala"></a>GetRecommendList.scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.&#123;<span class="type">ArrayBuffer</span>, <span class="type">ListBuffer</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务6：</span></span><br><span class="line"><span class="comment"> * 每周一计算最近一周内主活主播的三度关系列表</span></span><br><span class="line"><span class="comment"> * 注意：</span></span><br><span class="line"><span class="comment"> * 1：待推荐主播最近一周内活跃过</span></span><br><span class="line"><span class="comment"> * 2：待推荐主播等级&gt;4</span></span><br><span class="line"><span class="comment"> * 3：待推荐主播最近1个月视频评级满足3B+或2A+(flag=1)</span></span><br><span class="line"><span class="comment"> * 4：待推荐主播的粉丝列表关注重合度&gt;2</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GetRecommendListScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"GetRecommendListScala"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">var</span> timestamp = <span class="number">0</span>L <span class="comment">//过滤最近一周内是否活跃过</span></span><br><span class="line">    <span class="keyword">var</span> dumplicateNum = <span class="number">2</span> <span class="comment">//粉丝列表关注重合度</span></span><br><span class="line">    <span class="keyword">var</span> level = <span class="number">4</span> <span class="comment">//主播等级</span></span><br><span class="line">    <span class="keyword">var</span> outputPath = <span class="string">"hdfs://bigdata01:9000/data/recommend_data/20260201"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      appName = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">      timestamp = args(<span class="number">4</span>).toLong</span><br><span class="line">      dumplicateNum = args(<span class="number">5</span>).toInt</span><br><span class="line">      level = args(<span class="number">6</span>).toInt</span><br><span class="line">      outputPath = args(<span class="number">7</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span>  org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> param = <span class="type">Map</span>(<span class="string">"boltUrl"</span>-&gt;boltUrl,<span class="string">"userName"</span>-&gt;userName,<span class="string">"passWord"</span>-&gt;passWord,<span class="string">"timestamp"</span>-&gt;timestamp.toString,<span class="string">"level"</span>-&gt;level.toString)</span><br><span class="line">    <span class="comment">//获取一周内主活的主播 并且主播等级大于4的数据</span></span><br><span class="line">    <span class="keyword">val</span> uidSet = env.createInput(<span class="keyword">new</span> <span class="type">Neo4jInputFormat</span>(param))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一次处理一批</span></span><br><span class="line">    <span class="comment">//过滤出粉丝关注重合度&gt;2的数据，并且对关注重合度倒序排序</span></span><br><span class="line">    <span class="comment">//最终的数据格式是：主播id,待推荐的主播id</span></span><br><span class="line">    <span class="keyword">val</span> mapSet = uidSet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      <span class="comment">//保存计算出来的结果</span></span><br><span class="line">      <span class="keyword">val</span> resultArr = <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">      it.foreach(uid=&gt;&#123;</span><br><span class="line">        <span class="comment">//计算一个用户的三度关系(主播的三度关系)</span></span><br><span class="line">        <span class="comment">//注意：数据量打了之后，这个计算操作是非常耗时</span></span><br><span class="line">        <span class="keyword">val</span> result = session.run(<span class="string">"match (a:User &#123;uid:'"</span>+uid+<span class="string">"'&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt; (c:User) return a.uid as auid,c.uid as cuid,count(c.uid) as sum order by sum desc limit 30"</span>)</span><br><span class="line">        <span class="comment">//对b、c的主活时间进行过滤，以及对c的level和flag值进行过滤</span></span><br><span class="line">        <span class="comment">/*val result = session.run("match (a:User &#123;uid:'"+uid+"'&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt; (c:User)" +</span></span><br><span class="line"><span class="comment">          " where b.timestamp &gt;= "+timestamp+" and c.timestamp &gt;= "+timestamp+" and c.level &gt;= "+level +" and c.flag =1 " +</span></span><br><span class="line"><span class="comment">          " return a.uid as auid,c.uid as cuid,count(c.uid) as sum order by sum desc limit 30")*/</span></span><br><span class="line">        <span class="keyword">while</span>(result.hasNext)&#123;</span><br><span class="line">          <span class="keyword">val</span> record = result.next()</span><br><span class="line">          <span class="keyword">val</span> sum = record.get(<span class="string">"sum"</span>).asInt()</span><br><span class="line">          <span class="keyword">if</span>(sum &gt; dumplicateNum)&#123;</span><br><span class="line">            resultArr += record.get(<span class="string">"auid"</span>).asString()+<span class="string">"\t"</span>+record.get(<span class="string">"cuid"</span>).asString()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      resultArr.iterator</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把数据转成tupl2的形式</span></span><br><span class="line">    <span class="keyword">val</span> tup2Set = mapSet.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> splits = line.split(<span class="string">"\t"</span>)</span><br><span class="line">      (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//根据主播id进行分组，可以获取到这个主播的待推荐列表</span></span><br><span class="line">    <span class="keyword">val</span> reduceSet = tup2Set.groupBy(_._1).reduceGroup(it =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> list = it.toList</span><br><span class="line">      <span class="keyword">val</span> tmpList = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">      <span class="keyword">for</span> (l &lt;- list) &#123;</span><br><span class="line">        tmpList += l._2</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//把结果组装成这种形式 1001   1002,1003,1004</span></span><br><span class="line">      (list.head._1, tmpList.toList.mkString(<span class="string">","</span>))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//注意：writeAsCsv只能保存tuple类型的数据</span></span><br><span class="line">    <span class="comment">//writerAsText可以支持任何类型，如果是对象，会调用对象的toString方法写入到文件中</span></span><br><span class="line">    reduceSet.writeAsCsv(outputPath,<span class="string">"\n"</span>,<span class="string">"\t"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行任务</span></span><br><span class="line">    env.execute(appName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">其实我们也可以直接在这里将结果输入写入到redis中，不过为了整体看起来更加规范，在这就先把数据临时写到hdfs上面。</span><br><span class="line"></span><br><span class="line">在本地执行代码，然后到hdfs上面确认结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161459949.png" alt="image-20230516145733981"></p><h3 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来对程序编译打包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br></pre></td></tr></table></figure><h3 id="startGetRecommendList-sh"><a href="#startGetRecommendList-sh" class="headerlink" title="startGetRecommendList.sh"></a>startGetRecommendList.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取上周一的时间</span></span><br><span class="line">dt=`date -d "7 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">    dt=`date -d "7 days ago $1" +"%Y%m%d"`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="GetRecommendListScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"><span class="meta">#</span><span class="bash">获取上周一的时间戳(单位：毫秒)</span></span><br><span class="line">timestamp=`date --date="$&#123;dt&#125;" +%s`000</span><br><span class="line"><span class="meta">#</span><span class="bash">粉丝列表关注重合度</span></span><br><span class="line">dumplicateNum=2</span><br><span class="line"><span class="meta">#</span><span class="bash">主播等级</span></span><br><span class="line">level=4</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">输出结果数据路径</span></span><br><span class="line">outputPath="hdfs://bigdata01:9000/data/recommend_data/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.GetRecommendListScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/get_recommend_list-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;appName&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125; $&#123;timestamp&#125; $&#123;dumplicateNum&#125; $&#123;level&#125; $&#123;outputPath&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $8&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startGetRecommendList.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161504206.png" alt="image-20230516150429277"></p><h2 id="三度关系列表数据导出到Redis-任务七"><a href="#三度关系列表数据导出到Redis-任务七" class="headerlink" title="三度关系列表数据导出到Redis(任务七)"></a>三度关系列表数据导出到Redis(任务七)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现将三度关系列表数据导出到Redis</span><br><span class="line"></span><br><span class="line">注意：此任务每周执行一次，在任务6执行完毕以后执行这个。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">创建子module项目：export_data</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="pom-xml"><a href="#pom-xml" class="headerlink" title="pom.xml"></a>pom.xml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;export_data&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;redis.clients&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;jedis&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line">创建类：ExportDataScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><h3 id="ExportDataScala"><a href="#ExportDataScala" class="headerlink" title="ExportDataScala"></a>ExportDataScala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.<span class="type">Jedis</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务7：</span></span><br><span class="line"><span class="comment"> * 将三度列表关系数据导出到Redis</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ExportDataScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/recommend_data/20260125"</span></span><br><span class="line">    <span class="keyword">var</span> redisHost = <span class="string">"bigdata04"</span></span><br><span class="line">    <span class="keyword">var</span> redisPort = <span class="number">6379</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      redisHost = args(<span class="number">1</span>)</span><br><span class="line">      redisPort = args(<span class="number">2</span>).toInt</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//读取hdfs中的数据</span></span><br><span class="line">    <span class="keyword">val</span> text = env.readTextFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    text.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取jedis连接</span></span><br><span class="line">      <span class="keyword">val</span> jedis = <span class="keyword">new</span> <span class="type">Jedis</span>(redisHost, redisPort)</span><br><span class="line">      <span class="comment">//开启管道(提高性能，不开启也没事)</span></span><br><span class="line">      <span class="keyword">val</span> pipeline = jedis.pipelined()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//获取uid</span></span><br><span class="line">        <span class="keyword">val</span> uid = fields(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//获取待推荐主播列表</span></span><br><span class="line">        <span class="keyword">val</span> recommend_uids = fields(<span class="number">1</span>).split(<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//注意：在这里给key起一个有意义的名字，l表示list类型、rec是recommend的简写(简写是因为key要放内存)</span></span><br><span class="line">        <span class="keyword">val</span> key = <span class="string">"l_rec_"</span>+uid</span><br><span class="line"></span><br><span class="line">        <span class="comment">//先删除(保证每周更新一次),pipeline中的删除操作在scala语言下使用有问题</span></span><br><span class="line">        jedis.del(key)</span><br><span class="line">        <span class="keyword">for</span>(r_uid &lt;- recommend_uids)&#123;</span><br><span class="line">          pipeline.rpush(key,r_uid)</span><br><span class="line">          <span class="comment">//给key设置一个有效时间，30天，如果30天数据没有更新，则删除此key</span></span><br><span class="line">          pipeline.expire(key,<span class="number">30</span>*<span class="number">24</span>*<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//提交管道中的命令</span></span><br><span class="line">      pipeline.sync()</span><br><span class="line">      <span class="comment">//关闭jedis连接</span></span><br><span class="line">      jedis.close()</span><br><span class="line">      <span class="string">""</span></span><br><span class="line">    &#125;).print()</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在执行代码之前，需要先把redis服务启动起来</span><br></pre></td></tr></table></figure><h3 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在本地执行代码，到redis中验证效果。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161647908.png" alt="image-20230516164750844"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161638708.png" alt="image-20230516163851685"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161649565.png" alt="image-20230516164936168"></p><h3 id="打包-1"><a href="#打包-1" class="headerlink" title="打包"></a>打包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来对程序编译打包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br></pre></td></tr></table></figure><h3 id="startExportData-sh"><a href="#startExportData-sh" class="headerlink" title="startExportData.sh"></a>startExportData.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取上周一的时间</span></span><br><span class="line">dt=`date -d "7 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">    dt=`date -d "7 days ago $1" +"%Y%m%d"`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/recommend_data/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="ExportDataScala"`date +%s`</span><br><span class="line">redisHost="bigdata04"</span><br><span class="line">redisPort=6379</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.ExportDataScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/export_data-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;filePath&#125; $&#123;redisHost&#125; $&#123;redisPort&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $8&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="提交任务-1"><a href="#提交任务-1" class="headerlink" title="提交任务"></a>提交任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">向集群提交任务，先把redis中之前生成的数据删一下</span><br><span class="line"></span><br><span class="line">sh -x startExportData.sh 20260201</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">任务成功执行，验证redis中的结果也是正确的</span><br></pre></td></tr></table></figure><h2 id="数据接口定义及开发-java-web了解即可"><a href="#数据接口定义及开发-java-web了解即可" class="headerlink" title="数据接口定义及开发(java web了解即可)"></a>数据接口定义及开发(java web了解即可)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前面我们把结果数据计算好了，那接下来我们需要开发数据接口，对外提供数据。</span><br><span class="line">首先定义接口文档</span><br></pre></td></tr></table></figure><h3 id="数据接口文档定义"><a href="#数据接口文档定义" class="headerlink" title="数据接口文档定义"></a>数据接口文档定义</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">为了方便跨部门数据使用，我们需要定义接口文档，便于其他部门的同事使用我们的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161701517.png" alt="image-20230516170118438"></p><h3 id="数据接口代码开发"><a href="#数据接口代码开发" class="headerlink" title="数据接口代码开发"></a>数据接口代码开发</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">开发数据接口需要用到javaweb项目，在这给大家演示一下如何基于spring-boot搭建一个javaweb项目</span><br><span class="line">创建子module项目：data_server</span><br><span class="line">在pom.xml中添加依赖</span><br><span class="line">首先添加spring-boot的依赖，还有fastjson依赖，因为我们后面在传输数据的时候需要使用json格式</span><br></pre></td></tr></table></figure><h4 id="pom-xml-1"><a href="#pom-xml-1" class="headerlink" title="pom.xml"></a>pom.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;data_server&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;redis.clients&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;jedis&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.3.2&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spring-boot-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.1.1.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;mainClass&gt;com.imooc.Application&lt;&#x2F;mainClass&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;repackage&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在项目的resource目录中添加这两个文件</span><br><span class="line"></span><br><span class="line">application.properties</span><br><span class="line">logback.xml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">创建包：com.imooc</span><br><span class="line">然后把下面这几个文件夹及文件拷贝到com.imooc包里面</span><br><span class="line">controller</span><br><span class="line">Application.java</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161716013.png" alt="image-20230516171622873"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">直接在Application类中右键执行，就可以启动这个javaweb项目，项目内部已经集成了tomcta容器，监听的端口是8085</span><br><span class="line">验证项目是否可以正常访问。</span><br><span class="line">在浏览器中访问</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161726656.png" alt="image-20230516172612542"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">能看到结果数据说明此项目的基础框架是ok的，接下来我们就来开发一个接口</span><br><span class="line">由于在这我们需要操作redis，所以需要到pom.xml中增加jedis的依赖，以及把我们之前开发的RedisUtils工具类也拷贝过来</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在com.imooc下创建utils目录，把RedisUtils拷贝到里面</span><br><span class="line"></span><br><span class="line">接下来到DataController类中增加一个方法：getRecommendList</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><h4 id="DataController-java"><a href="#DataController-java" class="headerlink" title="DataController.java"></a>DataController.java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> com.imooc.utils.RedisUtils;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.*;</span><br><span class="line"><span class="keyword">import</span> redis.clients.jedis.Jedis;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 数据接口V1.0</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RestController</span><span class="comment">//控制器类</span></span><br><span class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/v1"</span>)<span class="comment">//映射路径</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataController</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(DataController<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 测试接口</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@RequestMapping</span>(value=<span class="string">"/t1"</span>,method = RequestMethod.GET)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">test</span><span class="params">(@RequestParam(<span class="string">"name"</span>)</span> String name) </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"hello,"</span>+name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据主播uid查询三度关系推荐列表数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 返回数据格式：</span></span><br><span class="line"><span class="comment">     * &#123;"flag":"success/error","msg":"错误信息","rec_uids":["1005","1004"]&#125;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> uid</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@RequestMapping</span>(value=<span class="string">"/get_recommend_list"</span>,method = RequestMethod.GET)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> JSONObject <span class="title">getRecommendList</span><span class="params">(@RequestParam(<span class="string">"uid"</span>)</span> String uid) </span>&#123;</span><br><span class="line">        JSONObject resobj = <span class="keyword">new</span> JSONObject();</span><br><span class="line">        String flag = <span class="string">"success"</span>;</span><br><span class="line">        String msg = <span class="string">"ok"</span>;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            Jedis jedis = RedisUtils.getJedis();</span><br><span class="line">            <span class="comment">//获取待推荐列表数据</span></span><br><span class="line">            List&lt;String&gt; uidList = jedis.lrange(<span class="string">"l_rec_"</span> + uid, <span class="number">0</span>, -<span class="number">1</span>);</span><br><span class="line">            String[] uidArr = uidList.toArray(<span class="keyword">new</span> String[<span class="number">0</span>]);</span><br><span class="line">            resobj.put(<span class="string">"rec_uids"</span>,uidArr);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            flag = <span class="string">"error"</span>;</span><br><span class="line">            msg = e.getMessage();</span><br><span class="line">            logger.error(msg);</span><br><span class="line">        &#125;</span><br><span class="line">        resobj.put(<span class="string">"flag"</span>,flag);</span><br><span class="line">        resobj.put(<span class="string">"msg"</span>,msg);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> resobj;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Application-java"><a href="#Application-java" class="headerlink" title="Application.java"></a>Application.java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spring boot 入口启动程序</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SpringBootApplication</span> <span class="comment">//定义springboot入口程序</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SpringApplication.run(Application<span class="class">.<span class="keyword">class</span>,<span class="title">args</span>)</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">重新启动data_server项目</span><br><span class="line">然后在浏览器中访问刚才开发的接口，能看到正常输出结果则说明此接口是正常的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161730743.png" alt="image-20230516173008401"></p><h3 id="打包-2"><a href="#打包-2" class="headerlink" title="打包"></a>打包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup java -jar data_server-1.0-SNAPSHOT.jar &amp;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个命令相当于模拟浏览器的请求</span><br><span class="line">curl -XGET &#39;http:&#x2F;&#x2F;bigdata04:8085&#x2F;v1&#x2F;get_recommend_list?uid&#x3D;1000&#39;</span><br><span class="line">&#123;&quot;msg&quot;:&quot;ok&quot;,&quot;flag&quot;:&quot;success&quot;,&quot;rec_uids&quot;:[&quot;1005&quot;,&quot;1004&quot;]&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305161754559.png" alt="image-20230516175456652"></p><h2 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略略略</span><br></pre></td></tr></table></figure><h2 id="项目扩展优化"><a href="#项目扩展优化" class="headerlink" title="项目扩展优化"></a>项目扩展优化</h2><h3 id="如何保证在Neo4j中维护平台全量粉丝关注数据"><a href="#如何保证在Neo4j中维护平台全量粉丝关注数据" class="headerlink" title="如何保证在Neo4j中维护平台全量粉丝关注数据"></a>如何保证在Neo4j中维护平台全量粉丝关注数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对粉丝关注数据我们有两份</span><br><span class="line">第一份是历史粉丝关注数据</span><br><span class="line">第二份是实时粉丝关注数据</span><br><span class="line"></span><br><span class="line">如何通过这两份数据实现维护平台全量粉丝关注数据呢？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">背景是这样的</span><br><span class="line">历史粉丝数据是由服务端每天晚上0点的时候定时同步到mysql数据库中的，因为之前平台是把粉丝的关注数据，存储到了redis中，每天晚上定时向mysql中同步一次。</span><br><span class="line">实时粉丝数据在准备做这个项目之前通过日志采集工具把这些数据采集到kafka里面了</span><br><span class="line"></span><br><span class="line">基于此，假设我们是在2026年2月1日那天上午10点开始将mysql中的历史数据导出来，然后批量导入到neo4j中，mysql中的粉丝数据其实是截止到2026年2月1日0点的。</span><br><span class="line">这个导入过程当时耗时将近2天。</span><br><span class="line">也就是在2026年2月3日上午10点左右导入完毕的，此时neo4j中的粉丝关注数据是截止到2026年2月1日0点的。</span><br><span class="line"></span><br><span class="line">接下来我们需要通过kafka来将这两天内的粉丝关注数据读取出来，补充到neo4j中，如何实现呢？</span><br><span class="line">因为我们的kafka当时是保存3天的数据，所以说这里面保存的还有2026-01-31 10点左右开始的数据，所以说当时我们开发好SparkStreaming程序之后，使用一个新的消费者groupid，然后将auto.offset.reset设置为earlist，读取最早的数据，这样就可以将这个topic中前3天的数据都读出来，然后在neo4j中进行维护，这样其实会重复执行2026-01-31 10点到2月1 日0点之间的数据，但是对最终的结果是没有影响的。</span><br><span class="line">这样就可以实现在neo4j中全量维护粉丝关注数据了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305162119832.png" alt="image-20230516210302781"></p><h3 id="如何解决数据乱序导致的粉丝关注关系不准确"><a href="#如何解决数据乱序导致的粉丝关注关系不准确" class="headerlink" title="如何解决数据乱序导致的粉丝关注关系不准确"></a>如何解决数据乱序导致的粉丝关注关系不准确</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">通过在sparkStreaming内部对读取到的一小批数据基于时间进行排序，按照时间顺序执行粉丝关注相关操作，这样可以从一定程度上解决数据乱序的问题</span><br><span class="line">在v2.0中，我们使用了Flink计算引擎，此时可以使用watermark+eventtime来解决数据乱序的问题。</span><br></pre></td></tr></table></figure><h3 id="如何优化三度关系推荐列表数据计算程序"><a href="#如何优化三度关系推荐列表数据计算程序" class="headerlink" title="如何优化三度关系推荐列表数据计算程序"></a>如何优化三度关系推荐列表数据计算程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">针对三度关系推荐列表数据计算程序：GetRecommendListScala</span><br><span class="line">这个任务在执行的时候需要执行20个小时左右，因为这里面会先查询出来满足条件的主播，然后挨个计算这些主播的三度关系数据，这里面需要和neo4j进行交互，主要慢在了neo4j这里，因为三度关系查询是比较复杂的，所以会比较耗时。</span><br><span class="line">这个任务在执行的时候我们会发现它有时候无缘无故的提示task丢失，进而导致任务失败，还得重新计算，代价太大，所以这样不太靠谱。</span><br><span class="line">后来发现是由于spark离线任务执行时间过长的时候会出现这种task丢失的问题。</span><br><span class="line">所以后来我们对这个程序又做了优化。</span><br><span class="line">针对第一步计算出来的主播列表，分成20份保存到hdfs上面</span><br></pre></td></tr></table></figure><h3 id="项目数据规模"><a href="#项目数据规模" class="headerlink" title="项目数据规模"></a>项目数据规模</h3><h3 id="集群资源规模-HDP集群"><a href="#集群资源规模-HDP集群" class="headerlink" title="集群资源规模(HDP集群)"></a>集群资源规模(HDP集群)</h3><h3 id="集群数据规模"><a href="#集群数据规模" class="headerlink" title="集群数据规模"></a>集群数据规模</h3><h3 id="Neo4j性能指标"><a href="#Neo4j性能指标" class="headerlink" title="Neo4j性能指标"></a>Neo4j性能指标</h3><h3 id="Neo4j核心参数修改"><a href="#Neo4j核心参数修改" class="headerlink" title="Neo4j核心参数修改"></a>Neo4j核心参数修改</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v2.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v2.0-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0-1.html</id>
    <published>2023-04-24T07:37:38.000Z</published>
    <updated>2023-05-21T10:40:33.211Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v2-0-1"><a href="#第十八周-直播平台三度关系推荐v2-0-1" class="headerlink" title="第十八周 直播平台三度关系推荐v2.0-1"></a>第十八周 直播平台三度关系推荐v2.0-1</h1><h2 id="V1-0架构存在的问题"><a href="#V1-0架构存在的问题" class="headerlink" title="V1.0架构存在的问题"></a>V1.0架构存在的问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">V1.0这个架构里面其实存在三个主要的问题</span><br><span class="line"></span><br><span class="line">SparkStreaming程序的实时性不够</span><br><span class="line">其实说实话，针对目前的粉丝实时关注数据，使用SparkStreaming程序来维护问题也不大，但是我们程序猿是要有追求的，既然有更好的方案，那我们肯定不能使用差的，所以这块我们我们需要使用Flink来实现，它可以提供真正意义上的实时。</span><br><span class="line"></span><br><span class="line">三度关系推荐数据适合存储在缓存系统中(Redis)</span><br><span class="line">咱们前面把最终计算好的三度关系数据保存在了MySQL中。</span><br><span class="line">其实这种数据是比较适合存储到一些基于内存的缓存系统中的，对查询的性能要求比较高，并且这些数据也是有时效性的，需要定时更新和删除老数据，所以说此时，使用redis是比较合适的，redis的查询性能比较高，并且redis中可以给key设置一个生存时间，可以实现定时删除过期数据的效果。</span><br><span class="line">咱们这份数据是有2个字段，第一列是主播uid，第二列是待推荐的主播uid</span><br><span class="line">存储到redis里面的话value使用list类型即可，在list类型里面存储待推荐的主播列表</span><br><span class="line"></span><br><span class="line">为了规范数据使用，建议开发数据接口</span><br><span class="line">咱们前面把存储系统直接暴露给其他业务部门，是不太安全的，并且他们使用起来也不方便，在实际工作中，各个业务部门之间进行数据交互，正规流程都是提供接口。</span><br><span class="line">还有一个原因是这样的，我们在开发一个功能的时候，假设需要前端和后端同时开发，这个时候我们就需要提前把数据接口定义好，先提供假数据，这样前端和后端可以同时进行开发，前端在开发页面功能的时候就可以使用我们提供的数据接口了，当我们把后端功能搞定以后，修改数据接口的底层逻辑代码，接入真实的数据即可，这个时候对前端而言是没有任何影响的，就算后期我们修改表结构了，对前端也没有什么影响，只要接口没有变就行，这样也可以实现前端和后端的解耦。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020904879.png" alt="image-20230502090405428"></p><h2 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来再来回顾一下技术选型，看看在V2.0中有哪些变化</span><br><span class="line">还是这4大块</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020905225.png" alt="image-20230502090508445"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在数据采集这块，去掉了Sqoop，因为在这里我们需要将三度关系数据导出到redis中，sqoop是不支持的，所以我们需要开发flink程序实现数据导出功能。</span><br><span class="line"></span><br><span class="line">在数据存储这块，我们把MySQL改为了Redis</span><br><span class="line"></span><br><span class="line">在数据计算这块，我们把Spark计算引擎改为了Flink，在V2.0中，我们将之前使用Spark开发的代码都使用Flink重新实现一遍。</span><br><span class="line"></span><br><span class="line">数据展现模块没有变化。</span><br></pre></td></tr></table></figure><h2 id="整体架构设计"><a href="#整体架构设计" class="headerlink" title="整体架构设计"></a>整体架构设计</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020909294.png" alt="image-20230502090921788"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这是最新的架构图，在这里面，替换了Spark计算引擎和MySQL数据库，引入了Flink和Redis</span><br><span class="line">以及在这里引入了数据接口模块，通过接口对外提供数据。</span><br><span class="line">其它的地方没有变化。</span><br><span class="line"></span><br><span class="line">注意：其实针对离线计算使用Spark或者Flink没有多大区别，不过我们还是希望一个项目中的计算框架相对来说是统一的，这样好管理，也好维护，所以在V2.0架构中，不管是离线计算还是实时计算，都使用Flink实现。</span><br></pre></td></tr></table></figure><h2 id="数据采集模块开发"><a href="#数据采集模块开发" class="headerlink" title="数据采集模块开发"></a>数据采集模块开发</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据采集模块没有变化，所以在这就不再分析了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020911095.png" alt="image-20230502091109759"></p><h2 id="数据计算核心指标详细分析"><a href="#数据计算核心指标详细分析" class="headerlink" title="数据计算核心指标详细分析"></a>数据计算核心指标详细分析</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020913388.png" alt="image-20230502091324304"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这里面的数据源和计算指标都没有变化</span><br><span class="line">一共还是这7个步骤</span><br></pre></td></tr></table></figure><h2 id="历史粉丝关注数据初始化-任务一"><a href="#历史粉丝关注数据初始化-任务一" class="headerlink" title="历史粉丝关注数据初始化(任务一)"></a>历史粉丝关注数据初始化(任务一)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一步：历史粉丝关注数据初始化</span><br><span class="line">这块流程是没有变化的，使用load csv将我们之前导出的历史粉丝关注数据进行初始化即可。</span><br><span class="line">把neo4j中之前的数据清空一下，直接删除neo4j下面的data目录即可，然后启动neo4j</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">通过浏览器访问neo4j，重新设置密码</span><br><span class="line"></span><br><span class="line">然后我们使用neo4j的shell命令行执行下面命令。</span><br><span class="line"></span><br><span class="line">连接neo4j</span><br><span class="line">bin&#x2F;cypher-shell -a bolt:&#x2F;&#x2F;bigdata04:7687 -u neo4j -p admin</span><br><span class="line"></span><br><span class="line">建立索引</span><br><span class="line">CREATE CONSTRAINT ON (user:User) ASSERT user.uid IS UNIQUE;</span><br><span class="line"></span><br><span class="line">批量导入数据</span><br><span class="line">USING PERIODIC COMMIT 1000</span><br><span class="line">       LOAD CSV WITH HEADERS FROM &#39;file:&#x2F;&#x2F;&#x2F;follower_00.log&#39; AS line FIELDTERMINATOR &#39;\t&#39;</span><br><span class="line">       MERGE (viewer:User &#123; uid: toString(line.fuid)&#125;)</span><br><span class="line">       MERGE (anchor:User &#123; uid: toString(line.uid)&#125;)</span><br><span class="line">       MERGE (viewer)-[:follow]-&gt;(anchor);</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020919105.png" alt="image-20230502091913756"></p><h2 id="实时维护粉丝关注数据-任务二"><a href="#实时维护粉丝关注数据-任务二" class="headerlink" title="实时维护粉丝关注数据(任务二)"></a>实时维护粉丝关注数据(任务二)</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305020920206.png" alt="image-20230502092054017"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实时维护Neo4j中粉丝关注数据</span><br><span class="line">先创建maven项目db_video_recommend_v2</span><br><span class="line">在pom.xml中添加项目需要用到的所有依赖</span><br><span class="line"></span><br><span class="line">再创建子module项目：real_time_follow</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="父项目pom"><a href="#父项目pom" class="headerlink" title="父项目pom"></a>父项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;packaging&gt;pom&lt;&#x2F;packaging&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;modules&gt;</span><br><span class="line">        &lt;module&gt;real_time_follow&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_user_level&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_user_active&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_video_info&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;get_recommend_list&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;export_data&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;data_server&lt;&#x2F;module&gt;</span><br><span class="line">    &lt;&#x2F;modules&gt;</span><br><span class="line">    &lt;dependencyManagement&gt;</span><br><span class="line">        &lt;dependencies&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.1.1.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.httpcomponents&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;httpclient&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;4.5.12&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;commons-dbutils&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;commons-dbutils&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;8.0.20&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.2.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.2.68&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- flink相关依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-streaming-java_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;flink-connector-kafka_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- log4j的依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- neo4j相关依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;4.1.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- jedis依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;redis.clients&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;jedis&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.9.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!--spring-boot依赖--&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.1.1.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;&#x2F;dependencyManagement&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="子项目pom"><a href="#子项目pom" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;real_time_follow&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">    &lt;!-- log4j的依赖 --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;!-- flink的依赖 --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-streaming-java_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-connector-kafka_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;!-- neo4j的依赖 --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        </span><br><span class="line">        &lt;!-- fastjson的依赖 --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="RealTimeFollowScala-scala"><a href="#RealTimeFollowScala-scala" class="headerlink" title="RealTimeFollowScala.scala"></a>RealTimeFollowScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line">创建类：RealTimeFollowScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务2：</span></span><br><span class="line"><span class="comment"> * 实时维护粉丝关注数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTimeFollowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"RealTimeFollowScala"</span></span><br><span class="line">    <span class="keyword">var</span> kafkaBrokers = <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span></span><br><span class="line">    <span class="keyword">var</span> groupId = <span class="string">"con_f_1"</span></span><br><span class="line">    <span class="keyword">var</span> topic = <span class="string">"user_follow"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      appName = args(<span class="number">0</span>)</span><br><span class="line">      kafkaBrokers = args(<span class="number">1</span>)</span><br><span class="line">      groupId = args(<span class="number">2</span>)</span><br><span class="line">      topic = args(<span class="number">3</span>)</span><br><span class="line">      boltUrl = args(<span class="number">4</span>)</span><br><span class="line">      userName = args(<span class="number">5</span>)</span><br><span class="line">      passWord = args(<span class="number">6</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,kafkaBrokers)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,groupId)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//解析json数据中的核心字段</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">      <span class="keyword">val</span> desc = jsonObj.getString(<span class="string">"desc"</span>)</span><br><span class="line">      <span class="keyword">val</span> followerUid = jsonObj.getString(<span class="string">"followeruid"</span>)</span><br><span class="line">      <span class="keyword">val</span> followUid = jsonObj.getString(<span class="string">"followuid"</span>)</span><br><span class="line">      (desc, followerUid, followUid)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用Neo4jSink维护粉丝关注数据</span></span><br><span class="line">    <span class="keyword">val</span> param = <span class="type">Map</span>(<span class="string">"boltUrl"</span>-&gt;boltUrl,<span class="string">"userName"</span>-&gt;userName,<span class="string">"passWord"</span>-&gt;passWord)</span><br><span class="line">    tupStream.addSink(<span class="keyword">new</span> <span class="type">Neo4jSink</span>(param))</span><br><span class="line"></span><br><span class="line">    env.execute(appName)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">注意：由于flink中的实时计算是来一条数据计算一次，在StreamAPI中没有mapPartition方法，不支持一批一批的处理，如果每处理一条数据就获取一次Neo4j数据库连接，这样效率就太差了，所以我们需要实现一个自定义的sink组件，在sink组件内部有一个初始化函数可以获取一次连接，多次使用，这样就不需要频繁创建neo4j数据库连接了。</span><br><span class="line"></span><br><span class="line">实现自定义的sink需要实现SinkFunction接口或者继承RichSinkFunction</span><br><span class="line">具体实现逻辑可以参考已有connector中针对sink组件的实现</span><br><span class="line">例如：RedisSink</span><br><span class="line">源码在这里：</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;bahir-flink&#x2F;blob&#x2F;master&#x2F;flink-connector-redis&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;flink&#x2F;streaming&#x2F;connectors&#x2F;redis&#x2F;RedisSink.java</span><br><span class="line"></span><br><span class="line">这里面一共有三个主要的函数：</span><br><span class="line">1：open，是一个初始化方法，在Sink组件初始化的时候执行一次，适合在里面初始化一些资源连接</span><br><span class="line">2：invoke，会被频繁调用，sink接收到一条数据这个方法就会执行一次，具体的业务逻辑在这里实现</span><br><span class="line">3：close，当任务停止的时候，会先调用sink组件中的close方法，适合在里面做一些关闭资源的操作</span><br></pre></td></tr></table></figure><h3 id="Neo4jSink-scala"><a href="#Neo4jSink-scala" class="headerlink" title="Neo4jSink.scala"></a>Neo4jSink.scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.&#123;<span class="type">RichSinkFunction</span>, <span class="type">SinkFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">Driver</span>, <span class="type">GraphDatabase</span>, <span class="type">Transaction</span>, <span class="type">TransactionWork</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 维护粉丝数据在Neo4j中的关注关系</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neo4jSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[<span class="type">Tuple3</span>[<span class="type">String</span>,<span class="type">String</span>,<span class="type">String</span>]]</span>&#123;</span><br><span class="line">  <span class="comment">//保存neo4j相关的配置参数</span></span><br><span class="line">  <span class="keyword">var</span> param: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>] = <span class="type">Map</span>()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> driver: <span class="type">Driver</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 构造函数</span></span><br><span class="line"><span class="comment">   * 接收neo4j相关的配置参数</span></span><br><span class="line"><span class="comment">   * @param param</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(param: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>])&#123;</span><br><span class="line">    <span class="keyword">this</span>()</span><br><span class="line">    <span class="keyword">this</span>.param = param</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 初始化方法，只执行一次</span></span><br><span class="line"><span class="comment">   * 适合初始化资源连接</span></span><br><span class="line"><span class="comment">   * @param parameters</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.driver = <span class="type">GraphDatabase</span>.driver(param(<span class="string">"boltUrl"</span>), <span class="type">AuthTokens</span>.basic(param(<span class="string">"userName"</span>), param(<span class="string">"passWord"</span>)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 核心代码，来一条数据，此方法会执行一次</span></span><br><span class="line"><span class="comment">   * @param value</span></span><br><span class="line"><span class="comment">   * @param context</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(value: (<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>), context: <span class="type">SinkFunction</span>.<span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//开启会话</span></span><br><span class="line">    <span class="keyword">val</span> session = driver.session()</span><br><span class="line">    <span class="keyword">val</span> followType = value._1</span><br><span class="line">    <span class="keyword">val</span> followerUid = value._2</span><br><span class="line">    <span class="keyword">val</span> followUid = value._3</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(<span class="string">"follow"</span>.equals(followType))&#123;</span><br><span class="line">      <span class="comment">//添加关注：因为涉及多条命令，所以需要使用事务</span></span><br><span class="line">      session.writeTransaction(<span class="keyword">new</span> <span class="type">TransactionWork</span>[<span class="type">Unit</span>]()&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(tx: <span class="type">Transaction</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">try</span>&#123;</span><br><span class="line">            tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followerUid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">            tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followUid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">            tx.run(<span class="string">"match (a:User &#123;uid:'"</span>+followerUid+<span class="string">"'&#125;),(b:User &#123;uid:'"</span>+followUid+<span class="string">"'&#125;) merge (a) -[:follow]-&gt; (b)"</span>)</span><br><span class="line">            tx.commit()</span><br><span class="line">          &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; tx.rollback()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      <span class="comment">//取消关注</span></span><br><span class="line">      session.run(<span class="string">"match (:User &#123;uid:'"</span>+followerUid+<span class="string">"'&#125;) -[r:follow]-&gt; (:User &#123;uid:'"</span>+followUid+<span class="string">"'&#125;) delete r"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//关闭会话</span></span><br><span class="line">    session.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 任务停止的时候会先调用此方法</span></span><br><span class="line"><span class="comment">   * 适合关闭资源连接</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//关闭连接</span></span><br><span class="line">    <span class="keyword">if</span>(driver!=<span class="literal">null</span>)&#123;</span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：需要确保zookeeper、kafka服务是正常运行的。</span><br><span class="line"></span><br><span class="line">接下来需要产生测试数据，我们可以继续使用之前generate_data项目中的GenerateRealTimeFollowData产生数据，这种流程我们前面在v1.0中已经使用过了。</span><br><span class="line">下面给大家演示一种方便测试的方法</span><br><span class="line">其实我们可以通过kafka的基于console的生产者直接向user_follow这个topic生产数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关注数据</span><br><span class="line">&#123;&quot;followeruid&quot;:&quot;2004&quot;,&quot;followuid&quot;:&quot;2008&quot;,&quot;timestamp&quot;:1598771070069,&quot;type&quot;:&quot;user_follow&quot;,&quot;desc&quot;:&quot;follow&quot;&#125;</span><br><span class="line">取消关注数据</span><br><span class="line">&#123;&quot;followeruid&quot;:&quot;2004&quot;,&quot;followuid&quot;:&quot;2008&quot;,&quot;timestamp&quot;:1598771070069,&quot;type&quot;:&quot;user_follow&quot;,&quot;desc&quot;:&quot;unfollow&quot;&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305021217745.png" alt="image-20230502121745119"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305021218562.png" alt="image-20230502121838255"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">到neo4j中确认效果发现确实新增了一个关注关系</span><br><span class="line">再模拟产生一条粉丝取消关注的数据</span><br><span class="line"></span><br><span class="line">到neo4j中确认效果发现刚才新增的关注关系没有了。</span><br><span class="line">这样就说明我们自己定义的Neo4jSink是可以正常工作的。</span><br><span class="line"></span><br><span class="line">注意：在实际工作中，有时候为了方便测试代码是否可以正常运行，很多时候也会采用这种基于控制台的生产者直接模拟产生数据，这样不会经过中间商，没有差价！</span><br><span class="line">如果使用整个数据采集全链路流程的话，可能会由于中间某个环节出问题导致的最终看不到效果，此时我们还得排查到底是哪里出了问题，这样就乱套了，本来是要验证代码逻辑的，结果又要去排查其它地方的问题了。</span><br><span class="line">所以说针对流程比较复杂的，我们在测试的时候一块一块进行测试，先验证代码逻辑没问题，最后再跑一个全流程确认一下最终效果。</span><br></pre></td></tr></table></figure><h3 id="集群执行"><a href="#集群执行" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们希望把代码提交到集群上运行</span><br><span class="line">需要先调整代码，把参数提取出来</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">对项目打jar包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br><span class="line">这里面的scala版本指定的是2.12版本</span><br><span class="line">注意：flink官方建议把所有依赖都打进一个jar包，所以我们在这就把依赖打进一个jar包里面。</span><br><span class="line">在flink1.11的时候新增了一个特性，可以支持动态指定依赖的jar包，但是我测试了还是有bug，所以在这我们就只能把依赖都打进jar包里面，其实我内心是拒绝的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：针对log4j,flink相关的依赖在打包的时候不需要打进去，所以需要添加provided属性</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时我们就需要使用这个带有jar-with-dependencies的jar包了</span><br><span class="line">real_time_follow-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><h3 id="startRealTimeFollow-sh"><a href="#startRealTimeFollow-sh" class="headerlink" title="startRealTimeFollow.sh"></a>startRealTimeFollow.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发任务提交脚本</span><br><span class="line">startRealTimeFollow.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="RealTimeFollowScala"</span><br><span class="line">kafkaBrokers="bigdata01:9092,bigdata02:9092,bigdata03:9092"</span><br><span class="line">groupId="con_f_1"</span><br><span class="line">topic="user_follow"</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.RealTimeFollowScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/real_time_follow-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;appName&#125; $&#123;kafkaBrokers&#125; $&#123;groupId&#125; $&#123;topic&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在执行之前需要配置flink的环境变量，FLINK_HOME</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305021821488.png" alt="image-20230502182120542"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">向集群提交任务</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305021821591.png" alt="image-20230502182148965"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">通过kafka的console控制台生产者，模拟产生数据，到neo4j中确认效果，发现是没有问题的。</span><br><span class="line"></span><br><span class="line">注意：此时是存在数据乱序的问题的，前面在讲Flink的时候我们详细讲解过Flink中的乱序处理方案，在这里给大家留一个作业，对这个代码进行改造，解决数据乱序问题。</span><br></pre></td></tr></table></figure><h2 id="每天定时更新主播等级-任务三"><a href="#每天定时更新主播等级-任务三" class="headerlink" title="每天定时更新主播等级(任务三)"></a>每天定时更新主播等级(任务三)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现每天定时更新主播等级</span><br><span class="line">创建子module项目：update_user_level</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="子项目pom-1"><a href="#子项目pom-1" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_level&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br></pre></td></tr></table></figure><h3 id="UpdateUserLevelScala-scala"><a href="#UpdateUserLevelScala-scala" class="headerlink" title="UpdateUserLevelScala.scala"></a>UpdateUserLevelScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">创建类：UpdateUserLevelScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务3：</span></span><br><span class="line"><span class="comment"> * 每天定时更新主播等级</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateUserLevelScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/cl_level_user/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//读取hdfs中的数据</span></span><br><span class="line">    <span class="keyword">val</span> text = env.readTextFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//校验数据准确性</span></span><br><span class="line">    <span class="keyword">val</span> filterSet = text.filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length == <span class="number">8</span> &amp;&amp; !fields(<span class="number">0</span>).equals(<span class="string">"id"</span>)) &#123;</span><br><span class="line">        <span class="literal">true</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    filterSet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//添加等级</span></span><br><span class="line">        session.run(<span class="string">"merge (u:User &#123;uid:'"</span>+fields(<span class="number">1</span>).trim+<span class="string">"'&#125;) set u.level = "</span>+fields(<span class="number">3</span>).trim)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span> <span class="comment">// spark的foreachpartition不需要返回数据，flink dataset的mappartition需要返回数据</span></span><br><span class="line">    &#125;).print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-1"><a href="#本地执行-1" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用之前生成的这份数据</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;cl_level_user&#x2F;20260201</span><br><span class="line">在本地执行代码，到neo4j中确认节点中是否新增了level属性，如果有，就说明程序执行成功了。</span><br></pre></td></tr></table></figure><h3 id="集群执行-1"><a href="#集群执行-1" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来对程序编译打包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br></pre></td></tr></table></figure><h3 id="startUpdateUserLevel-sh"><a href="#startUpdateUserLevel-sh" class="headerlink" title="startUpdateUserLevel.sh"></a>startUpdateUserLevel.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/cl_level_user/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="UpdateUserLevelScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.UpdateUserLevelScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/update_user_level-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $8&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="每天定时更新用户活跃时间-任务四"><a href="#每天定时更新用户活跃时间-任务四" class="headerlink" title="每天定时更新用户活跃时间(任务四)"></a>每天定时更新用户活跃时间(任务四)</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031521064.png" alt="image-20230503152132924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现每天定时更新用户活跃时间</span><br><span class="line">创建子module项目：update_user_active</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="子项目pom-2"><a href="#子项目pom-2" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_active&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateUserActiveScala-scala"><a href="#UpdateUserActiveScala-scala" class="headerlink" title="UpdateUserActiveScala.scala"></a>UpdateUserActiveScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line">创建类：UpdateUserActiveScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务4：</span></span><br><span class="line"><span class="comment"> * 每天定时更新用户活跃时间</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateUserActiveScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/user_active/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//读取hdfs中的数据</span></span><br><span class="line">    <span class="keyword">val</span> text = env.readTextFile(filePath)</span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    text.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">        <span class="keyword">val</span> uid = jsonObj.getString(<span class="string">"uid"</span>)</span><br><span class="line">        <span class="keyword">val</span> timeStamp = jsonObj.getString(<span class="string">"UnixtimeStamp"</span>)</span><br><span class="line">        <span class="comment">//添加用户活跃时间</span></span><br><span class="line">        session.run(<span class="string">"merge (u:User &#123;uid:'"</span>+uid+<span class="string">"'&#125;) set u.timestamp = "</span>+timeStamp)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span> <span class="comment">// 和前面一个一样的道理</span></span><br><span class="line">    &#125;).print() <span class="comment">// 为了任务能够执行</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-2"><a href="#本地执行-2" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用之前生成的这份数据</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;user_active&#x2F;20260201</span><br><span class="line">在本地执行代码，到neo4j中确认节点中是否新增了timestamp属性，如果有，就说明程序执行成功了。</span><br></pre></td></tr></table></figure><h3 id="集群执行-2"><a href="#集群执行-2" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来对程序编译打包</span><br><span class="line">在pom.xml中添加编译打包配置</span><br></pre></td></tr></table></figure><h3 id="startUpdateUserActive-sh"><a href="#startUpdateUserActive-sh" class="headerlink" title="startUpdateUserActive.sh"></a>startUpdateUserActive.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发任务脚本</span><br><span class="line">startUpdateUserActive.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/user_active/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="UpdateUserActiveScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.UpdateUserActiveScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/update_user_active-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $8&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="每周一计算最近一个月主播视频评级-任务五"><a href="#每周一计算最近一个月主播视频评级-任务五" class="headerlink" title="每周一计算最近一个月主播视频评级(任务五)"></a>每周一计算最近一个月主播视频评级(任务五)</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031537915.png" alt="image-20230503153659474"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">使用Flink程序实现每周一计算最近一个月主播视频评级</span><br><span class="line">创建子module项目：update_video_info</span><br><span class="line">在项目中创建scala目录，引入scala2.12版本的SDK</span><br><span class="line">创建package：com.imooc.flink</span><br><span class="line">在pom.xml中添加依赖</span><br></pre></td></tr></table></figure><h3 id="子项目pom-3"><a href="#子项目pom-3" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend_v2&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_video_info&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- 编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                    &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateVideoInfoScala-scala"><a href="#UpdateVideoInfoScala-scala" class="headerlink" title="UpdateVideoInfoScala.scala"></a>UpdateVideoInfoScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在resources目录中添加log4j.properties配置文件</span><br><span class="line">创建类：UpdateVideoInfoScala</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.<span class="type">Order</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务5：</span></span><br><span class="line"><span class="comment"> * 每周一计算最近一个月主播视频评级</span></span><br><span class="line"><span class="comment"> * 把最近几次视频评级在3B+或2A+的主播，在neo4j中设置flag=1</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意：在执行程序之前，需要先把flag=1的重置为0</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateVideoInfoScala</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> logger = <span class="type">LoggerFactory</span>.getLogger(<span class="string">"UpdateVideoInfoScala"</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/video_info/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在Driver端执行此代码，将flag=1的值重置为0</span></span><br><span class="line">    <span class="comment">//获取neo4j的连接</span></span><br><span class="line">    <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">    <span class="comment">//开启一个会话</span></span><br><span class="line">    <span class="keyword">val</span> session = driver.session()</span><br><span class="line">    session.run(<span class="string">"match (a:User) where a.flag=1 set a.flag = 0"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//关闭会话</span></span><br><span class="line">    session.close()</span><br><span class="line">    <span class="comment">//关闭连接</span></span><br><span class="line">    driver.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//读取hdfs中的数据</span></span><br><span class="line">    <span class="keyword">val</span> text = env.readTextFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//解析数据中的uid、rating、timestamp</span></span><br><span class="line">    <span class="keyword">val</span> tup3Set = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">        <span class="keyword">val</span> uid = jsonObj.getString(<span class="string">"uid"</span>)</span><br><span class="line">        <span class="keyword">val</span> rating = jsonObj.getString(<span class="string">"rating"</span>)</span><br><span class="line">        <span class="keyword">val</span> timestamp: <span class="type">Long</span> = jsonObj.getLong(<span class="string">"timestamp"</span>) <span class="comment">//这里要显示指定类型，不然会报错</span></span><br><span class="line">        (uid, rating, timestamp)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; logger.error(<span class="string">"json数据解析失败："</span> + line)</span><br><span class="line">          (<span class="string">"0"</span>, <span class="string">"0"</span>, <span class="number">0</span>L)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤异常数据</span></span><br><span class="line">    <span class="keyword">val</span> filterSet = tup3Set.filter(_._2 != <span class="string">"0"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取用户最近3场直播(视频)的评级信息</span></span><br><span class="line">    <span class="keyword">val</span> top3Set = filterSet.groupBy(<span class="number">0</span>)</span><br><span class="line">      .sortGroup(<span class="number">2</span>, <span class="type">Order</span>.<span class="type">DESCENDING</span>)</span><br><span class="line">      .reduceGroup(it =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> list = it.toList</span><br><span class="line">        <span class="comment">//(2002,A,1769913940002)(2002,A,1769913940001)(2002,A,1769913940000)</span></span><br><span class="line">        <span class="comment">//uid,rating,timestamp \t uid,rating,timestamp \t uid,rating,timestamp</span></span><br><span class="line">        <span class="keyword">val</span> top3 = list.take(<span class="number">3</span>).mkString(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//(2002,(2002,A,1769913940002)(2002,A,1769913940001)(2002,A,1769913940000))</span></span><br><span class="line">        (list.head._1, top3)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤出来满足3场B+的数据</span></span><br><span class="line">    <span class="keyword">val</span> top3BSet = top3Set.filter(tup =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> flag = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">val</span> fields = tup._2.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length == <span class="number">3</span>) &#123;</span><br><span class="line">        <span class="comment">//3场B+，表示里面没有出现C和D</span></span><br><span class="line">        <span class="keyword">val</span> tmp_str = fields(<span class="number">0</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">1</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">2</span>).split(<span class="string">","</span>)(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (!tmp_str.contains(<span class="string">"C"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"D"</span>)) &#123;</span><br><span class="line">          flag = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把满足3场B+的数据更新到neo4j中，设置flag=1</span></span><br><span class="line">    top3BSet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(tup=&gt;&#123;</span><br><span class="line">        session.run(<span class="string">"match (a:User &#123;uid:'"</span>+tup._1+<span class="string">"'&#125;) where a.level &gt;=15 set a.flag = 1"</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤出来满足2场A+的数据</span></span><br><span class="line">    <span class="keyword">val</span>   = top3Set.filter(tup =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> flag = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">val</span> fields = tup._2.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="comment">//2场A+，获取最近两场直播评级，里面不能出现B、C、D</span></span><br><span class="line">        <span class="keyword">val</span> tmp_str = fields(<span class="number">0</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">1</span>).split(<span class="string">","</span>)(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (!tmp_str.contains(<span class="string">"B"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"C"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"D"</span>)) &#123;</span><br><span class="line">          flag = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把满足2场A+的数据更新到neo4j中，设置flag=1</span></span><br><span class="line">    top2ASet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(tup=&gt;&#123;</span><br><span class="line">        session.run(<span class="string">"match (a:User &#123;uid:'"</span>+tup._1+<span class="string">"'&#125;) where a.level &gt;=4 set a.flag = 1"</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span> <span class="comment">// 这里的道理和前面一样</span></span><br><span class="line">    &#125;).print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-3"><a href="#本地执行-3" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">使用之前生成的这份数据</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;video_info&#x2F;20260201</span><br><span class="line">在本地执行代码，到neo4j中确认节点中是否有flag属性的值。</span><br></pre></td></tr></table></figure><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031614511.png" alt="image-20230503161453968"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">代码中对uid进行分区那里没有显示指定类型，会报错。也是常见错误</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031610325.png" alt="image-20230503161026920"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上面代码mappartion中的数据库链接部分，没有创建session，但并没有报错，用的是外部driver端的session；数据库链接并不支持序列化，不支持从driver到节点的传输。这种异常很常见。</span><br></pre></td></tr></table></figure><h3 id="多个输入目录问题"><a href="#多个输入目录问题" class="headerlink" title="多个输入目录问题"></a>多个输入目录问题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：flink目前不支持直接读取多个hdfs目录，在spark中，我们可以将多个hdfs目录使用逗号拼接成一个输入路径，flink目前不支持这种用法。</span><br></pre></td></tr></table></figure><h4 id="UpdateVideoInfoMoreFileScala-scala"><a href="#UpdateVideoInfoMoreFileScala-scala" class="headerlink" title="UpdateVideoInfoMoreFileScala.scala"></a>UpdateVideoInfoMoreFileScala.scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那如何实现读取最近一个月的数据呢？</span><br><span class="line">我们可以使用flink中的union算子间接实现读取多个hdfs目录的效果</span><br><span class="line">复制一份代码，改名字为：UpdateVideoInfoMoreFileScala</span><br><span class="line"></span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.<span class="type">Order</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务5：</span></span><br><span class="line"><span class="comment"> * 每周一计算最近一个月主播视频评级</span></span><br><span class="line"><span class="comment"> * 把最近几次视频评级在3B+或2A+的主播，在neo4j中设置flag=1</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 注意：在执行程序之前，需要先把flag=1的重置为0</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateVideoInfoMoreFileScala</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> logger = <span class="type">LoggerFactory</span>.getLogger(<span class="string">"UpdateVideoInfoScala"</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/video_info/20260201,hdfs://bigdata01:9000/data/video_info/20260217"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> userName = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> passWord = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length&gt;<span class="number">0</span>)&#123;</span><br><span class="line">      filePath = args(<span class="number">0</span>)</span><br><span class="line">      boltUrl = args(<span class="number">1</span>)</span><br><span class="line">      userName = args(<span class="number">2</span>)</span><br><span class="line">      passWord = args(<span class="number">3</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*//在Driver端执行此代码，将flag=1的值重置为0</span></span><br><span class="line"><span class="comment">    //获取neo4j的连接</span></span><br><span class="line"><span class="comment">    val driver = GraphDatabase.driver(boltUrl, AuthTokens.basic(userName, passWord))</span></span><br><span class="line"><span class="comment">    //开启一个会话</span></span><br><span class="line"><span class="comment">    val session = driver.session()</span></span><br><span class="line"><span class="comment">    session.run("match (a:User) where a.flag=1 set a.flag = 0")</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    //关闭会话</span></span><br><span class="line"><span class="comment">    session.close()</span></span><br><span class="line"><span class="comment">    //关闭连接</span></span><br><span class="line"><span class="comment">    driver.close()*/</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//添加隐式转换代码</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用union实现读取多个hdfs目录中的数据</span></span><br><span class="line">    <span class="keyword">val</span> files = filePath.split(<span class="string">","</span>)</span><br><span class="line">    <span class="keyword">var</span> allText: <span class="type">DataSet</span>[<span class="type">String</span>] = env.fromElements(<span class="string">"123"</span>)</span><br><span class="line">    <span class="keyword">for</span>(file &lt;- files)&#123;</span><br><span class="line">      allText = allText.union(env.readTextFile(file)) <span class="comment">// 这里要赋值才符合逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">"原始数据条数："</span>+allText.count())</span><br><span class="line"></span><br><span class="line">    <span class="comment">//解析数据中的uid、rating、timestamp</span></span><br><span class="line">    <span class="keyword">val</span> tup3Set = allText.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">        <span class="keyword">val</span> uid = jsonObj.getString(<span class="string">"uid"</span>)</span><br><span class="line">        <span class="keyword">val</span> rating = jsonObj.getString(<span class="string">"rating"</span>)</span><br><span class="line">        <span class="keyword">val</span> timestamp: <span class="type">Long</span> = jsonObj.getLong(<span class="string">"timestamp"</span>)</span><br><span class="line">        (uid, rating, timestamp)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; logger.error(<span class="string">"json数据解析失败："</span> + line)</span><br><span class="line">          (<span class="string">"0"</span>, <span class="string">"0"</span>, <span class="number">0</span>L)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤异常数据</span></span><br><span class="line">    <span class="keyword">val</span> filterSet = tup3Set.filter(_._2 != <span class="string">"0"</span>)</span><br><span class="line">    println(<span class="string">"过滤后的数据："</span>+filterSet.count())</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取用户最近3场直播(视频)的评级信息</span></span><br><span class="line">    <span class="keyword">val</span> top3Set = filterSet.groupBy(<span class="number">0</span>)</span><br><span class="line">      .sortGroup(<span class="number">2</span>, <span class="type">Order</span>.<span class="type">DESCENDING</span>)</span><br><span class="line">      .reduceGroup(it =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> list = it.toList</span><br><span class="line">        <span class="comment">//(2002,A,1769913940002)(2002,A,1769913940001)(2002,A,1769913940000)</span></span><br><span class="line">        <span class="comment">//uid,rating,timestamp \t uid,rating,timestamp \t uid,rating,timestamp</span></span><br><span class="line">        <span class="keyword">val</span> top3 = list.take(<span class="number">3</span>).mkString(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//(2002,(2002,A,1769913940002)(2002,A,1769913940001)(2002,A,1769913940000))</span></span><br><span class="line">        (list.head._1, top3)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤出来满足3场B+的数据</span></span><br><span class="line">    <span class="keyword">val</span> top3BSet = top3Set.filter(tup =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> flag = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">val</span> fields = tup._2.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length == <span class="number">3</span>) &#123;</span><br><span class="line">        <span class="comment">//3场B+，表示里面没有出现C和D</span></span><br><span class="line">        <span class="keyword">val</span> tmp_str = fields(<span class="number">0</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">1</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">2</span>).split(<span class="string">","</span>)(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (!tmp_str.contains(<span class="string">"C"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"D"</span>)) &#123;</span><br><span class="line">          flag = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把满足3场B+的数据更新到neo4j中，设置flag=1</span></span><br><span class="line">    top3BSet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(tup=&gt;&#123;</span><br><span class="line">        session.run(<span class="string">"match (a:User &#123;uid:'"</span>+tup._1+<span class="string">"'&#125;) where a.level &gt;=15 set a.flag = 1"</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//过滤出来满足2场A+的数据</span></span><br><span class="line">    <span class="keyword">val</span> top2ASet = top3Set.filter(tup =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> flag = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">val</span> fields = tup._2.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="keyword">if</span> (fields.length &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="comment">//2场A+，获取最近两场直播评级，里面不能出现B、C、D</span></span><br><span class="line">        <span class="keyword">val</span> tmp_str = fields(<span class="number">0</span>).split(<span class="string">","</span>)(<span class="number">1</span>) + <span class="string">","</span> + fields(<span class="number">1</span>).split(<span class="string">","</span>)(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (!tmp_str.contains(<span class="string">"B"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"C"</span>) &amp;&amp; !tmp_str.contains(<span class="string">"D"</span>)) &#123;</span><br><span class="line">          flag = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把满足2场A+的数据更新到neo4j中，设置flag=1</span></span><br><span class="line">    top2ASet.mapPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(userName, passWord))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(tup=&gt;&#123;</span><br><span class="line">        session.run(<span class="string">"match (a:User &#123;uid:'"</span>+tup._1+<span class="string">"'&#125;) where a.level &gt;=4 set a.flag = 1"</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      <span class="string">""</span></span><br><span class="line">    &#125;).print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="集群执行-3"><a href="#集群执行-3" class="headerlink" title="集群执行"></a>集群执行</h3><h3 id="startUpdateVideoInfo-sh"><a href="#startUpdateVideoInfo-sh" class="headerlink" title="startUpdateVideoInfo.sh"></a>startUpdateVideoInfo.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发任务脚本</span><br><span class="line">startUpdateVideoInfo.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">获取最近一个月的文件目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash">filepath=<span class="string">""</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">for</span>((i=1;i&lt;=30;i++))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">do</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    filepath+=<span class="string">"hdfs://bigdata01:9000/data/video_info/"</span>`date -d <span class="string">"<span class="variable">$i</span> days ago"</span> +<span class="string">"%Y%m%d"</span>`,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">done</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：在使用的时候需要将最后面的逗号去掉 <span class="variable">$&#123;filePath:0:-1&#125;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/video_info/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">appName="UpdateVideoInfoScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">userName="neo4j"</span><br><span class="line">passWord="admin"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">注意：需要将flink脚本路径配置到linux的环境变量中</span></span><br><span class="line">flink run \</span><br><span class="line">-m $&#123;masterUrl&#125; \</span><br><span class="line">-ynm $&#123;appName&#125; \</span><br><span class="line">-yqu default \</span><br><span class="line">-yjm 1024 \</span><br><span class="line">-ytm 1024 \</span><br><span class="line">-ys 1 \</span><br><span class="line">-p 5 \</span><br><span class="line">-c com.imooc.flink.UpdateVideoInfoScala \</span><br><span class="line">/data/soft/video_recommend_v2/jobs/update_video_info-1.0-SNAPSHOT-jar-with-dependencies.jar $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;userName&#125; $&#123;passWord&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;tmp=$8;getline;print tmp","$8&#125;'` // 这个程序提交到时候会有两个application，以前的判断方式不适合了</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED,SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时其实会发现产生了两个Flink任务。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305031731219.png" alt="image-20230503173125131"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v2.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v2-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周-直播平台三度关系推荐v1-0-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-3.html</id>
    <published>2023-04-24T07:37:33.000Z</published>
    <updated>2023-05-26T10:02:54.316Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v1-0-3"><a href="#第十八周-直播平台三度关系推荐v1-0-3" class="headerlink" title="第十八周 直播平台三度关系推荐v1.0-3"></a>第十八周 直播平台三度关系推荐v1.0-3</h1><h2 id="数据计算之实时维护粉丝关注-第二个任务"><a href="#数据计算之实时维护粉丝关注-第二个任务" class="headerlink" title="数据计算之实时维护粉丝关注(第二个任务)"></a>数据计算之实时维护粉丝关注(第二个任务)</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261212936.png" alt="image-20230426121205538"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下数据计算中的第二步，实时维护粉丝关注数据。我们的实时粉丝关注数据呢，来源于服务端日志，因为当用户在直播平台中对主播进行关注和取消关注的时候呢，会调用服务端接口。所以说服务端会记录这些操作日志。具体的数据格式呢，是这样。这是一个json格式，fuid就代表了粉丝。uid代表的是主播。好，这个timestamp，它表示这个具体你这个关注行为，或者你取消关注行为，它产生的时间。这个type呢，表示这个数据是什么类型的数据，它是粉丝关注相关的数据。那具体这条数据是关注还是取消关注，我们要根据这个desc这个参数来定。它里面这个值如果是follow就表示是关注，如果是UN follow，就表示取消关注。所以说后期我们在解析这个数据的时候呢，其实核心字段就是三个followeruid，还有这个followuid，还有这个desc。那接下来我们就需要使用sparkstreamming实时维护neo4j粉丝关注的相关数据。</span><br></pre></td></tr></table></figure><h3 id="父项目pom"><a href="#父项目pom" class="headerlink" title="父项目pom"></a>父项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;packaging&gt;pom&lt;&#x2F;packaging&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;modules&gt;</span><br><span class="line">        &lt;module&gt;generate_data&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;data_collect&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;server_inter&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;real_time_follow&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_user_level&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_user_active&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;update_video_info&lt;&#x2F;module&gt;</span><br><span class="line">        &lt;module&gt;get_recommend_list&lt;&#x2F;module&gt;</span><br><span class="line">    &lt;&#x2F;modules&gt;</span><br><span class="line">    &lt;dependencyManagement&gt;</span><br><span class="line">        &lt;dependencies&gt;</span><br><span class="line">            &lt;!-- log4j的依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.2.68&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.1.1.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.httpcomponents&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;httpclient&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;4.5.12&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;commons-dbutils&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;commons-dbutils&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;1.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;8.0.20&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.2.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- spark相关依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spark-streaming_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;!-- neo4j相关依赖 --&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;4.1.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">            &lt;dependency&gt;</span><br><span class="line">                &lt;groupId&gt;neo4j-contrib&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;neo4j-spark-connector&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.5-M1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;&#x2F;dependencyManagement&gt;</span><br><span class="line">    &lt;repositories&gt;</span><br><span class="line">        &lt;!-- list of other repositories --&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;SparkPackagesRepo&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;url&gt;http:&#x2F;&#x2F;dl.bintray.com&#x2F;spark-packages&#x2F;maven&lt;&#x2F;url&gt;</span><br><span class="line">        &lt;&#x2F;repository&gt;</span><br><span class="line">    &lt;&#x2F;repositories&gt;</span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以及添加一个repository，因为neo4j-spark-connector这个依赖在maven中央仓库中是没有的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意我在这个父项目创建module子项目，副项目这个pom文件里面啊，提前把相关的依赖啊都给它放进，我们可以看一下，主要是下面这些东西。这是Spark相关的依赖，Spark core、spark sql、 Spark streaming。还有这个spark streaming kafka对吧，因为你要读取kafka嘛。以及下面呢，是你后这相关的一些依赖。现在我提前把它拿过来，后面我们再具体用到的时候呢，我再去分析。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261215313.png" alt="image-20230426121552111"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261216414.png" alt="image-20230426121614568"></p><h3 id="子项目pom"><a href="#子项目pom" class="headerlink" title="子项目pom"></a>子项目pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;real_time_follow&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那针对我们这个实施项目，它里面都需要什么依赖呢？注意它里面呢，你首先需要这个spark streaming以及spark streaming kafka吧。以及用neo4j那个依赖。还有一个，因为我们的原始数据是json的，我们要解析json，所以说呢，还要用那个fastjson这个包。那所以说我们看一下啊。neo4j-java-driver就类似于我们要操作mysql一样，我们要添加mysql对应的一个connector。啊，这个是类似的啊。</span><br><span class="line"></span><br><span class="line">在这呢，你可以把这个版本号给它删掉就行。因为在这啊，我们是在这个父项目里面统一来维护管理这些依赖，其实主要是管理这些版本。因为这个最外层呢，你看它套了一个dependency manager里面呢是一个dependency。所以说你看和这个比的话，它外面是多了一个这个depend manager。所以说呢，在这统一管理这些依赖的版本，你后期这里面这些子项目，你需要用到哪些依赖，你把这些依赖拿过来就可以使用了，这样的话你不需要指定版本，它呢会读取你那个父项目里面pom里面指定的这个版本。如果说你有个性化的需求，你说你那个公共的版本不满足我的要求，我想使用某一个特殊的版本，那么你可以在这来指定你的版本就可以了。这样的话只针对你这个子项目有效。那把这几个依赖加过来就可以了，现在我们来看看这个啊，前面这几个就没什么好说的了，主要看看这个neo4j-java-driver。就类似我们操作mysql一样，我们需要找到neo4j它的一个驱动jar包给大家分析一下我是在哪找到的？官网，这个其实啊，我们直接到这个maven仓库里面去搜也行，你到仓库里面去搜这个new瑞杠Java杠driver也是可以找到的，是一样的。这是这个流程。</span><br><span class="line"></span><br><span class="line">好，那接下来呢，我们把这个项目它的一个技术环境，再给它配一下邮件。摩托塞定是。这样里面注意咱那个library，我们把这个scala2.11这个SDK给它加进来。因为针对Spark而言，我们用的是2.11。你在这呢，我们再建一个录入叫SC。把它设置为S。OK，注意前面我们在开发代码的时候，我们先用代码来开发。等最后的时候呢，我会把那个java代码具体的实现呢，也给大家提供过去。我们课上讲的时候以这个scala代码为主。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那接下来建一个包。com。点五克点Spark。在这里面，我们创建一个object。叫real time。follow。那在这我们先写点注释，这个呢是任务二了，就第二个任务。实施维护粉丝关注数据。你妈吗？对，在这我们一个streaming。context。它里面呢，需要接触一个SPA以及一个时间second。second。我们把这个周期呢设为五秒行吧。上面呢，我们来创建，你有一个Spark，这个都是固定的写法。said master。LOGO2。因为你是一个实时处理程序。set APP name。就是这个。提取的变量。好。好，这个呢，也提取一个变量有SSC吧。这个呢表示创建。context。</span><br><span class="line"></span><br><span class="line">那接下来呢，我们需要获取。消费卡夫卡的数据流。这个呢，也是一个固定的写法。搞不搞？第二数一。我们使用那个direct。泛型呢，都是spring这个我们前面讲过啊嗯。把这个SSC传过去。后面呢，穿这个location。street。这个后面呢，直接那个五月份。好，它下面还有一个参数叫consumer。一个订阅功能啊。这块泛型呢，也都是three。sweets。注意这里面我们需要给它指定两个参数，一个呢是topics对吧，指定topic。还有一个呢，是卡夫卡的一个参数。需要这两个参数。那我们这上面需要重建一下。嗯。在这呢，指定要读取的topic的名称。主要是两个。那我们先创建第一个。他其实就是这个。我们直接使用个map。object。直接在这块来处理啊。首先在里面使用卡夫卡的一个扑克地址。RI。101冒号9092。逗号。0203把这个改一下，0203。好，这样第一个参数就搞定了。我们还需要指定一下这个K的一个序列化类型。前面先空着，我先把后面那个写，写完之后呢，复制一下。嗯。glass of string。这样就可以了。那前面的话呢，其实就是K点把这个拿过来。he ser。小写。这个呢是value的序列化类型，嗯。把这个复制一下，再改一下。好，这样就可以了，嗯。那下面来指定这个消费者组ID。嗯嗯。给我点ID。con吧，行吧。这个就是你来自己来指定。下面是一个消费策略。就你第一次消费的时候如何来消费？auto ofetet。就是读取最新的数据。这个是自动提交offset。嗯嗯。enable。there auto。这个是醋啊。现在啊，你最好给他强制指定类型，要不然用的时候呢，可能会有问题。加点allow their words。对吧，这个就可以了，那既然我们指定我们需要消费的这个topic。它是一个，它可以接收多个。注意，接着我们应该消费那个user。对吧，这里面是实时的粉丝关注相关的数据。OK，那这块呢就可以了，我们就可以获取到这个消费卡不卡的一个数据流了。这个是一个卡夫卡。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>, <span class="type">Transaction</span>, <span class="type">TransactionWork</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务2：</span></span><br><span class="line"><span class="comment"> * 实时维护粉丝关注数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTimeFollowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"RealTimeFollowScala"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定Kafka的配置信息</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Object</span>](</span><br><span class="line">      <span class="comment">//kafka的broker地址信息</span></span><br><span class="line">      <span class="string">"bootstrap.servers"</span>-&gt;<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>,</span><br><span class="line">      <span class="comment">//key的序列化类型</span></span><br><span class="line">      <span class="string">"key.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//value的序列化类型</span></span><br><span class="line">      <span class="string">"value.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//消费者组id</span></span><br><span class="line">      <span class="string">"group.id"</span>-&gt;<span class="string">"con_1"</span>,</span><br><span class="line">      <span class="comment">//消费策略</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span>-&gt;<span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">//自动提交offset</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span>-&gt;(<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//指定要读取的topic的名称</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"user_follow"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">    <span class="keyword">val</span> kafkaDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    <span class="comment">//首先将kafkaDStream转换为rdd，然后就可以调用rdd中的foreachPartition了</span></span><br><span class="line">    kafkaDStream.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">      <span class="comment">//一次处理一个分区的数据</span></span><br><span class="line">      rdd.foreachPartition(it=&gt;&#123;</span><br><span class="line">        <span class="comment">//获取neo4j连接</span></span><br><span class="line">        <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(<span class="string">"bolt://bigdata04:7687"</span>, <span class="type">AuthTokens</span>.basic(<span class="string">"neo4j"</span>, <span class="string">"admin"</span>))</span><br><span class="line">        <span class="comment">//开启一个会话</span></span><br><span class="line">        <span class="keyword">val</span> session = driver.session()</span><br><span class="line">        it.foreach(record=&gt;&#123;</span><br><span class="line">          <span class="comment">//获取粉丝关注相关数据</span></span><br><span class="line">          <span class="comment">//&#123;"followeruid":"1001","followuid":"1002","timestamp":1798198304,"type":"user_follow","desc":"follow"&#125;</span></span><br><span class="line">          <span class="keyword">val</span> line = record.value()</span><br><span class="line">          <span class="comment">//解析数据</span></span><br><span class="line">          <span class="keyword">val</span> userFollowObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">          <span class="comment">//获取数据类型，关注 or  取消关注</span></span><br><span class="line">          <span class="keyword">val</span> followType = userFollowObj.getString(<span class="string">"desc"</span>)</span><br><span class="line">          <span class="comment">//获取followeruid</span></span><br><span class="line">          <span class="keyword">val</span> followeruid = userFollowObj.getString(<span class="string">"followeruid"</span>)</span><br><span class="line">          <span class="comment">//获取followuid</span></span><br><span class="line">          <span class="keyword">val</span> followuid = userFollowObj.getString(<span class="string">"followuid"</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span>(<span class="string">"follow"</span>.equals(followType))&#123;</span><br><span class="line">            <span class="comment">//添加关注：因为涉及多条命令，所以需要使用事务</span></span><br><span class="line">            session.writeTransaction(<span class="keyword">new</span> <span class="type">TransactionWork</span>[<span class="type">Unit</span>] ()&#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(tx: <span class="type">Transaction</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">                <span class="keyword">try</span>&#123;</span><br><span class="line">                  tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">                  tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">                  tx.run(<span class="string">"match(a:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;),(b:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;) merge (a) -[:follow]-&gt; (b)"</span>)</span><br><span class="line">                  <span class="comment">//提交事务</span></span><br><span class="line">                  tx.commit()</span><br><span class="line">                &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">                  <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; tx.rollback()</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">//取消关注</span></span><br><span class="line">            session.run(<span class="string">"match(:User &#123;uid: '"</span>+followeruid+<span class="string">"'&#125;) -[r:follow]-&gt; (:User &#123;uid: '"</span>+followuid+<span class="string">"'&#125;) delete r"</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//关闭会话</span></span><br><span class="line">        session.close()</span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        driver.close()</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="RealTimeFollowScala-scala"><a href="#RealTimeFollowScala-scala" class="headerlink" title="RealTimeFollowScala.scala"></a>RealTimeFollowScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那下面我们就开始处理试卷。那你这边注意，你首先将这个卡不卡呀，转一下。转换为RDD。然后呢，就可以。就要用RDD中的这个for each part。好，第二和一起，阿D先把它转换成阿里D。这样这里面传过来其实就是阿里列。转成R之后，我们就可以调用它里面这个for each了，那这样的话就可以一次处理一批的数据。比如说一次处理一个分区的数。为什么要这样做呢？因为我们在这里面需要去连neo4j。我们要把这个具体用户实时的这个关注，以及取消关注这些关系呢，给他维护到neo4j里面，所以说在这呢，相当于我们需要获取用户这些链接，然后呢去操作它。</span><br><span class="line"></span><br><span class="line">那这样的话，你最好是一批获取一次链接，这样效率比较高对吧。我每一条数据过来都获取链接，这样效率比较低。因为本身你这个实时其实就是一小批一小批的数据，那针对这一小批里面的数据，你可以再按分区对吧，每个分区获取一次链接。啊，for。这是一个AI，嗯。好，那在这里面。我们就可以调用那个it点，不好意思就可以迭代它里面的每一条数据了，对吧。这是一个了吧。那现在注意，我们需要先获取u或这个链接。怎么获取呢？因为我们提前已经把那个六后力杠Java杠driver那个依赖已经引进去了，所以说你在这儿可以直接用。哇。第二。我们使用这个。这个前面呢，是那个具体的URLURL其实就是那个开头的那个。这个零四冒号7687。后面的话我们需要指定下这个用户名和密码，就这个奥。basic。嗯，指定一下用户名。你徒弟还有密码。嗯。好，这样的话，下面我们就获取到这个用户的一个链接了。那下面的话，我们需要使用这个链接呢，开启一个会话。开启绘画之后才可以去用啊。这个。那这样的话，在里面就可以使用这个session。那你在最后啊，用完之后啊，需要把它们两个给它关闭掉，先把这个写一下，关闭会话。session。close。嗯。关闭连接。there are the clothes。好，那下面呢，我们来把里面这个代码给它完善一下，这里面是核心的代码。那在这里面，我们那就可以获取这个粉丝关注相关的数据。它其实呢，就是那个阶层串出了一块阶层数据。在这了。men。这样的话就可以获取到这行数据了。把这行计算数据获取到，获取到之后注意下面呢，我们来解析数据。用杰森。yeah。us objects。嗯，把它传过去。这个呢，其实就是user。follow。好，那接下来我们就从它里面去获取一些数据。获取数据类型。到底是？关注哦，取消关注。你又使用那个user点，咱们刚才分析了，它里面有一个DC这个词对吧。follow types。那接下来我们还需要获取这个follow u ID。嗯。thatw。嗯。W。what ID？接下来获取了一个follow u ID。follow u ID。嗯嗯。好，这三个核心的这个字段呢，我们都获取到。那获取到之后，下面的话，我们其实就可以根据这个photo这个类型来执行具体关注或者是取消关注这个操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">那我们接着来判断一下。如果呀，这个。follow the equal。这个full time。如果说你这个值等于follow，那相当于就是关注。else。就是取消关注了。添加关注。这是取消关注。注意你说这种写法。和这样写，你说我把它放到前面。这里面呢，写那个follow有什么区别吗？注意你这种写法，如果说。他呢，是空是闹，你这样调用会报了一个空，人异常啊。但我把它放到前面，那肯定不会报空人了啊，这肯定是有值的。你就算你那个pro等于no，这个顶多是不相等，它也不会出现这个控制异常啊，这个需要注意一下。下面呢，我们来执行添加关注操作。这需要注意。因为呢，它这里面啊，会涉及多条命令。所以说呢，我们需要使用事物。类似于我们先需要添加两个节点，然后呢，再给这两个基点绑定一个关注关系。这样的话就是三条命令。这三条命令在执行的时候，要么都成功，要么都失败。好，那所以说在这我们需要这样来做，通过session。第二。right transaction。开启一个事物。你要传一个，你有一个栓死action。walk。注意这块这个泛型啊，表示你这里面这个代码啊，最终的返回的数据类型在这我们不需要返回东西，所以说呢。把要空就行了。实现里面微信的方法。就这个。好，那这里面就需要实现我们具体的逻辑了。注意。它这里面相当有一个数啊transaction，所以说我们直接调TS点。你看它里面传的是一个query string，其实就是具体的一个执行的一个语句。墨。user。ID。这是一个字符串，所以说要加个单引号啊。那我们在这获取这个UID。那下面还有一个。这是follow u ID对吧。先把这两个节点给它创建了。那下面我们要给它们两个绑定关系。注意你在这绑定关系的话，这个相当于它是分开执行的啊，只不过说他们在一个事物里面，所以你在这啊，先添加这两个下面再来查。match。把你刚才添加这两个节点查出来，再给它们两个绑定关系。user。ad。这是这个UID，我们先把这个查出来，然后呢，再把第二个。给它起个名字叫B。user。这是UID。ID对吧，这样的话，你看其实我们就把这两个节点个查出来了。你前面是添加在这，把他们两个都查出来，这查第一个，这是查第二个。并且呢，给每一个都起了一个别名，它叫a，它叫B，对吧。那后面呢，就可以使用这个。墨。a。好。这样就可以了。在这x.com提交15。那注意，如果说他在执行的时候失败了，这样的话，我们需要让这个事物回滚。catch。直接捕获这个最大的异常。只要抛一场，我们就调用它的一个back。这样就可以了。所以这里面啊，执行的还是咱们前面讲那个step语句对吧。OK，这是添加关注。那取消关注就简单了呀。取消关注，其实呢，一条命令就可以了。相当于呢，我使用match把它查出来，然后后边的话使用一个DD的，那这样的话，其实呢，你就不需要开启这种事物了，因为你相当于取消关注，只有一条命令，就不需要开启这个事故了啊，然后自动提交就行。所以说我们直接使用这个session点。软。match。user。UID。说了UID。然后面他呢。follow。这个哥们在。UID。follow u ID。注意这时候呢，给这个关系呢，起一个名字叫啊。所以说在最后面直接把这个关系给它删掉就行。这个意思啊。这个呢，就是取消关注，取消他们两个之间的佛的关系。这样的话其实就可以了啊。我们这里面的一个核心代码就搞定了。最后是一个启动任务。start。应该在这里面。这个是等待任务停止。嗯。嗯。好，这样就可以了。这样的话，我们这个代码啊，其实就开发好了。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">那接下来我们需要确保卡夫卡服务cub的采集程序、数据分发程序以及服务端接口服务正常运行。那这样的话，我们就可以在本地去运行这个Spark人命程序了。好在这样我们把它启动起来。先在本地去执行一下。等这个程序启动之后呢，我们再去调那个接口，模拟产生这个用户实时的关注和取消关注数据。你先确认你这个人命程序是正常的</span><br><span class="line"></span><br><span class="line">看到没有，它这个程序不停就说明了它是OK的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">好，那接下来呢，我们就来模拟产生一些数据。找到这个generate date这个项目对吧。我们调里面这个generate real time执行这个，它每执行一次都会产生一条这个实时的一个粉丝关注或者取消关注的数据。执行一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261635084.png" alt="image-20230426163505984"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是一个follow。你看这个2009FOLLOW了1005，那我们来看一下。在这刷新一下，或者你重新点那个follow，或者你点那个刷新都可以。看到没有2009FOLLOW了1005没问题吧。我们之前是没有这个的啊，之前它是没有这个follow关系的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261636657.png" alt="image-20230426163612378"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">OK，所以说呢，我们这个代码呢，正常执行了。</span><br><span class="line"></span><br><span class="line">但是注意了，目前这个程序啊，其实是存在一些问题的，因为数据通过filebeat的采集，再到kafka，最终我们消费的数据顺序和数据产生的顺序可能就不一致了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261641417.png" alt="image-20230426164158476"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">举个例子：</span><br><span class="line">用户A先关注了用户B</span><br><span class="line">用户A很快又取关了用户B</span><br><span class="line">这样就会产生两条日志数据，这两条数据经过采集分发之后，最后我们消费过来的数据顺序很有可能是这样的</span><br><span class="line">用户A取关用户B</span><br><span class="line">用户A关注了用户B</span><br><span class="line">这种最终的结果就是用户A关注了用户B，那这样就不准确了，虽然这种情况是小概率事件，但是也是存在的，在SparkStreaming中如何解决呢？</span><br><span class="line">由于SparkStreaming是一小批一小批处理的，所以我们可以针对每次获取的这一小批数据根据数据产生的时间戳进行排序，从小到大，然后按照这个顺序去操作这些数据，这样其实就能在很大程度上避免我们刚才分析的这种问题了，但是这样并没有完全解决掉这个问题，如果两条数据分到了两批数据里面，还是会存在这个问题的，不过这种情况出现的概率就很低了，我们暂时就忽略不计了。</span><br><span class="line">在这我把这个思路分析好了，给大家留一个作业，大家下去之后自己尝试动手实现一下这个功能</span><br><span class="line"></span><br><span class="line">使用sparkstreaming解决数据乱序的问题。因为我们的原始数据里面，里面有个FUID，还有UID，还有一个时间戳，对吧，那现在在sparkstreaming里面，你获取到这一批数据之后，你对这一批数据按照time stamp进行排序，从小到大。排完序之后呢，再去执行这些操作。这样就可以了。大家呢，先自己动手做一下啊。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那接下来呢，我们来对这个代码啊，再给它完善一下。可以让它呢，同时支持在本地运行和集群运行。把这个给它停掉。</span><br><span class="line"></span><br><span class="line">这里面啊，可能会发生变化的这些信息呢，全部都给它提取出来。让它支持动态传递啊，这样的话，我们这个代码呢，更会更加通用。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>, <span class="type">Transaction</span>, <span class="type">TransactionWork</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务2：</span></span><br><span class="line"><span class="comment"> * 实时维护粉丝关注数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTimeFollowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> masterUrl = <span class="string">"local[2]"</span></span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"RealTimeFollowScala"</span></span><br><span class="line">    <span class="keyword">var</span> seconds = <span class="number">5</span></span><br><span class="line">    <span class="keyword">var</span> kafkaBrokers = <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span></span><br><span class="line">    <span class="keyword">var</span> groupId = <span class="string">"con_1"</span></span><br><span class="line">    <span class="keyword">var</span> topic = <span class="string">"user_follow"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> username = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> password = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      masterUrl = args(<span class="number">0</span>)</span><br><span class="line">      appName = args(<span class="number">1</span>)</span><br><span class="line">      seconds = args(<span class="number">2</span>).toInt</span><br><span class="line">      kafkaBrokers = args(<span class="number">3</span>)</span><br><span class="line">      groupId = args(<span class="number">4</span>)</span><br><span class="line">      topic = args(<span class="number">5</span>)</span><br><span class="line">      boltUrl = args(<span class="number">6</span>)</span><br><span class="line">      username = args(<span class="number">7</span>)</span><br><span class="line">      password = args(<span class="number">8</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(masterUrl).setAppName(appName)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(seconds))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka的配置信息</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Object</span>](</span><br><span class="line">      <span class="comment">//kafka的broker地址信息</span></span><br><span class="line">      <span class="string">"bootstrap.servers"</span>-&gt;kafkaBrokers,</span><br><span class="line">      <span class="comment">//key的序列化类型</span></span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//value的序列化类型</span></span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//消费者组id</span></span><br><span class="line">      <span class="string">"group.id"</span> -&gt; groupId,</span><br><span class="line">      <span class="comment">//消费策略</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">//自动提交offset</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定要读取的topic的名称</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(topic)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">    <span class="keyword">val</span> kafkaDstream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    <span class="comment">//首先将kafkaDstream转换为rdd，然后就可以调用rdd中的foreachPartition了</span></span><br><span class="line">    kafkaDstream.foreachRDD(rdd=&gt;&#123;</span><br><span class="line">      <span class="comment">//一次处理一个分区的数据</span></span><br><span class="line">      rdd.foreachPartition(it=&gt;&#123;</span><br><span class="line">        <span class="comment">//获取neo4j的连接</span></span><br><span class="line">        <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(username, password))</span><br><span class="line">        <span class="comment">//开启一个会话</span></span><br><span class="line">        <span class="keyword">val</span> session = driver.session()</span><br><span class="line">        it.foreach(record=&gt;&#123;</span><br><span class="line">          <span class="comment">//获取粉丝关注相关数据</span></span><br><span class="line">          <span class="keyword">val</span> line = record.value()</span><br><span class="line">          <span class="comment">//解析数据</span></span><br><span class="line">          <span class="keyword">val</span> userFollowObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">          <span class="comment">//获取数据类型，关注 or  取消关注</span></span><br><span class="line">          <span class="keyword">val</span> followType = userFollowObj.getString(<span class="string">"desc"</span>)</span><br><span class="line">          <span class="comment">//获取followeruid</span></span><br><span class="line">          <span class="keyword">val</span> followeruid = userFollowObj.getString(<span class="string">"followeruid"</span>)</span><br><span class="line">          <span class="comment">//获取followuid</span></span><br><span class="line">          <span class="keyword">val</span> followuid = userFollowObj.getString(<span class="string">"followuid"</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span>(<span class="string">"follow"</span>.equals(followType))&#123; <span class="comment">// 顺序前后不要改</span></span><br><span class="line">            <span class="comment">//添加关注：因为涉及多条命令，所以需要使用事务</span></span><br><span class="line">            session.writeTransaction(<span class="keyword">new</span> <span class="type">TransactionWork</span>[<span class="type">Unit</span>] ()&#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(tx: <span class="type">Transaction</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">                <span class="keyword">try</span>&#123;</span><br><span class="line">                  tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">                  tx.run(<span class="string">"merge (:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;)"</span>)</span><br><span class="line">                  tx.run(<span class="string">"match(a:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;),(b:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;) merge (a) -[:follow]-&gt; (b)"</span>)</span><br><span class="line">                  <span class="comment">//提交事务</span></span><br><span class="line">                  tx.commit()</span><br><span class="line">                &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">                  <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; tx.rollback()</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">//取消关注</span></span><br><span class="line">            session.run(<span class="string">"match (:User &#123;uid:'"</span>+followeruid+<span class="string">"'&#125;) -[r:follow]-&gt; (:User &#123;uid:'"</span>+followuid+<span class="string">"'&#125;) delete r"</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">//关闭会话</span></span><br><span class="line">        session.close()</span><br><span class="line">        <span class="comment">//关闭连接</span></span><br><span class="line">        driver.close()</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：建议这个项目中的所有依赖包全部在spark-submit脚本后面的–jars中指定(neo4j-java-driver、fastjson、spark-streaming-kafka这三个需要手动指定，其它的spark安装包已有)，这样最终生成的任_务jar就比较小了，提交任务的时候速度会比较快。所以这里面的jar-with-dependencies插件就可以不使用了，因为我们打jar包的时候不需要把依赖打进去，这个时候也不需要在依赖中添加provided参数了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261654714.png" alt="image-20230426165401187"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261704743.png" alt="image-20230426170407568"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">为了方便的提交任务，我们再开发一个任务提交脚本，</span><br><span class="line">在项目中创建一个bin目录，把脚本放到这个bin目录一样，这样便于管理维护</span><br><span class="line">startRealTimeFollow.sh</span><br></pre></td></tr></table></figure><h3 id="startRealTimeFollow-sh"><a href="#startRealTimeFollow-sh" class="headerlink" title="startRealTimeFollow.sh"></a>startRealTimeFollow.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">masterUrl&#x3D;&quot;yarn-cluster&quot;</span><br><span class="line">master&#x3D;&#96;echo $&#123;masterUrl&#125; | awk -F&#39;-&#39; &#39;&#123;print $1&#125;&#39;&#96;</span><br><span class="line">deployMode&#x3D;&#96;echo $&#123;masterUrl&#125; | awk -F&#39;-&#39; &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line"></span><br><span class="line">appName&#x3D;&quot;RealTimeFollowScala&quot;</span><br><span class="line">seconds&#x3D;5</span><br><span class="line">kafkaBrokers&#x3D;&quot;bigdata01:9092,bigdata02:9092,bigdata03:9092&quot;</span><br><span class="line">groupId&#x3D;&quot;con_1&quot;</span><br><span class="line">topic&#x3D;&quot;user_follow&quot;</span><br><span class="line">boltUrl&#x3D;&quot;bolt:&#x2F;&#x2F;bigdata04:7687&quot;</span><br><span class="line">username&#x3D;&quot;neo4j&quot;</span><br><span class="line">password&#x3D;&quot;admin&quot;</span><br><span class="line"></span><br><span class="line">yarnCommonLib&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;yarnCommonLib&quot;</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.RealTimeFollowScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;&#x2F;fastjson-1.2.68.jar,$&#123;yarnCommonLib&#125;&#x2F;spark-streaming-kafka-0-10_2.11-2.4.3.jar,$&#123;yarnCommonLib&#125;&#x2F;neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;&#x2F;kafka-clients-2.4.1.jar,$&#123;yarnCommonLib&#125;&#x2F;reactive-streams-1.0.3.jar \</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;jobs&#x2F;real_time_follow-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;seconds&#125; $&#123;kafkaBrokers&#125; $&#123;groupId&#125; $&#123;topic&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">jar包路径可以是本地，也可以是hdfs上，建议hdfs</span><br><span class="line"></span><br><span class="line">在本地如何找这些jar包?在.m2(是maven本地目录)目录下去找，然后再上传</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">针对这个参数：yarnCommonLib&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;yarnCommonLib&quot;</span><br><span class="line">使用公共的依赖包目录，使用起来方便，管理维护起来也方便，并且还可以提高任务执行效率，因为当我们向集群提交的时候，任务需要的依赖jar包是会自动上传到hdfs的一个临时目录的，如果我们提前把jar包上传到hdfs上面，就不会再重新上传了</span><br><span class="line"></span><br><span class="line">针对–jars后面指定的依赖jar包，需要额外再指定kafka-clients-2.4.1.jar和reactive-streams-1.0.3.jar，一个是kafka的，一个是neo4j需要依赖的，否则提交任务到集群执行是会报错的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261823893.png" alt="image-20230426182327467"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261841795.png" alt="image-20230426184121107"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261842193.png" alt="image-20230426184238164"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261843621.png" alt="image-20230426184330270"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">重新生成一条实时关注数据，查看结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261844934.png" alt="image-20230426184425485"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">停止sparkStreaming任务</span><br><span class="line"></span><br><span class="line">这是正常的一帆风顺的流程。</span><br><span class="line"></span><br><span class="line">那我们如果是第一次这样做，肯定会遇到各种各样的问题，在这里来给大家复现一下。</span><br><span class="line">按照正常的思路，这个项目依赖的jar包最开始我们肯定只会使用这三个</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">masterUrl&#x3D;&quot;yarn-cluster&quot;</span><br><span class="line">master&#x3D;&#96;echo $&#123;masterUrl&#125; | awk -F&#39;-&#39; &#39;&#123;print $1&#125;&#39;&#96;</span><br><span class="line">deployMode&#x3D;&#96;echo $&#123;masterUrl&#125; | awk -F&#39;-&#39; &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line"></span><br><span class="line">appName&#x3D;&quot;RealTimeFollowScala&quot;</span><br><span class="line">seconds&#x3D;5</span><br><span class="line">kafkaBrokers&#x3D;&quot;bigdata01:9092,bigdata02:9092,bigdata03:9092&quot;</span><br><span class="line">groupId&#x3D;&quot;con_1&quot;</span><br><span class="line">topic&#x3D;&quot;user_follow&quot;</span><br><span class="line">boltUrl&#x3D;&quot;bolt:&#x2F;&#x2F;bigdata04:7687&quot;</span><br><span class="line">username&#x3D;&quot;neo4j&quot;</span><br><span class="line">password&#x3D;&quot;admin&quot;</span><br><span class="line"></span><br><span class="line">yarnCommonLib&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;yarnCommonLib&quot;</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.RealTimeFollowScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;&#x2F;fastjson-1.2.68.jar,$&#123;yarnCommonLib&#125;&#x2F;spark-streaming-kafka-0-10_2.11-2.4.3.jar,$&#123;yarnCommonLib&#125;&#x2F;neo4j-java-driver-4.1.1.jar \</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;jobs&#x2F;real_time_follow-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;seconds&#125; $&#123;kafkaBrokers&#125; $&#123;groupId&#125; $&#123;topic&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startRealTimeFollow.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时你会发现任务执行失败了</span><br><span class="line">报错信息如下，在控制台就可以看到具体的报错信息：</span><br><span class="line">这个报错信息表示是缺少kafka的一些依赖，org&#x2F;apache&#x2F;kafka&#x2F;common&#x2F;serialization&#x2F;StringDeserializer，通过这里面的包名可以看出来，这个是kafka-clients的依赖，如果直接看不出来，那就到网上搜一下这个报错信息看看有没有收获java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;kafka&#x2F;common&#x2F;serialization&#x2F;StringDeserializer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">所以我们在脚本中再添加这个kafka-client的依赖</span><br><span class="line"></span><br><span class="line">再提交</span><br><span class="line"></span><br><span class="line">发现任务还是执行失败，在控制台可以看到报错信息如下</span><br><span class="line"></span><br><span class="line">这里提示neo4j中的org.neo4j.driver.Config初始化失败，但是neo4j的依赖我们也添加进去了，为什么还会报这个错呢？</span><br><span class="line"></span><br><span class="line">如果这里看不到有用的错误信息，我们可以尝试到YARN中查看</span><br><span class="line">在YARN的8088界面中我们进入到spark的任务界面(注意：需要确保hadoop的日志聚合功能开启，以及Spark的historyServer进程也是开启的。)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262122548.png" alt="image-20230426212208228"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262122638.png" alt="image-20230426212224386"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262122756.png" alt="image-20230426212235821"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">其实根源是在这，主要是缺少这个类org.reactivestreams.Publisher，最终导致的org.neo4j.driver.Config无法初始化。</span><br><span class="line"></span><br><span class="line">那这个类org.reactivestreams.Publisher是从哪来的呢？</span><br><span class="line">它是neo4j需要使用的一个依赖里面的类</span><br><span class="line">到哪找呢？</span><br><span class="line">查看neo4j-java-driver(子pom里ctrl+点击neo4j-java-driver)这个依赖的依赖，会发现它里面确实有一个依赖的名称是org.reactivestreams，所以说我们还需要把这个依赖的jar包找到引进去。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">到此为止，把这个jar包再加进去就可以正常执行了，这就是我们排查问题的一个思路和流程，这个思路大家一定要学以致用。</span><br><span class="line"></span><br><span class="line">这个任务开发到这就结束了</span><br></pre></td></tr></table></figure><h2 id="数据计算之每天定时更新主播等级-第三个任务"><a href="#数据计算之每天定时更新主播等级-第三个任务" class="headerlink" title="数据计算之每天定时更新主播等级(第三个任务)"></a>数据计算之每天定时更新主播等级(第三个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">主播等级数据来源于服务端数据库(定时增量导入到HDFS中)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262130828.png" alt="image-20230426213050640"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：表中有两个等级字段，一个是用户等级，一个是主播等级</span><br><span class="line"></span><br><span class="line">在这我们需要使用主播等级</span><br><span class="line">针对这份数据，最核心的两个字段是第2列和第4列</span><br><span class="line">第2列是用户uid</span><br><span class="line">第4列是主播等级anchor_level</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个任务需要做的是把每天主播等级发生了变化的数据更新到neo4j中，在neo4j中也维护一份主播的等级</span><br><span class="line">创建一个子module：update_user_level</span><br><span class="line">创建scala目录，添加scala2.11的sdk</span><br><span class="line">引入依赖</span><br></pre></td></tr></table></figure><h3 id="所需依赖"><a href="#所需依赖" class="headerlink" title="所需依赖"></a>所需依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateUserLevelScala-scala"><a href="#UpdateUserLevelScala-scala" class="headerlink" title="UpdateUserLevelScala.scala"></a>UpdateUserLevelScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在update_user_level下面的scala里面创建包：com.imooc.spark</span><br><span class="line">创建类：UpdateUserLevelScala</span><br><span class="line">代码如下</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务3：</span></span><br><span class="line"><span class="comment"> * 每天定时更新主播等级</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateUserLevelScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> masterUrl = <span class="string">"local"</span></span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"UpdateUserLevelScala"</span></span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/cl_level_user/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> username = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> password = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      masterUrl = args(<span class="number">0</span>)</span><br><span class="line">      appName = args(<span class="number">1</span>)</span><br><span class="line">      filePath = args(<span class="number">2</span>)</span><br><span class="line">      boltUrl = args(<span class="number">3</span>)</span><br><span class="line">      username = args(<span class="number">4</span>)</span><br><span class="line">      password = args(<span class="number">5</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(masterUrl)</span><br><span class="line">      .setAppName(appName)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取用户等级数据</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD = sc.textFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//校验数据准确性</span></span><br><span class="line">    <span class="keyword">val</span> filterRDD = linesRDD.filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">      <span class="comment">//判断每一行的列数是否正确，以及这一行是不是表头</span></span><br><span class="line">      <span class="keyword">if</span> (fields.length == <span class="number">8</span> &amp;&amp; !fields(<span class="number">0</span>).equals(<span class="string">"id"</span>)) &#123;</span><br><span class="line">        <span class="literal">true</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">   </span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    filterRDD.foreachPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(username, password))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> fields = line.split(<span class="string">"\t"</span>)</span><br><span class="line">        <span class="comment">//添加等级</span></span><br><span class="line">        session.run(<span class="string">"merge(u:User &#123;uid:'"</span>+fields(<span class="number">1</span>).trim+<span class="string">"'&#125;) set u.level = "</span>+fields(<span class="number">3</span>).trim) <span class="comment">//.trim防止空格被引入</span></span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在本地执行代码</span><br><span class="line">效果如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262214894.png" alt="image-20230426221422858"></p><h3 id="startUpdateUserLevel-sh"><a href="#startUpdateUserLevel-sh" class="headerlink" title="startUpdateUserLevel.sh"></a>startUpdateUserLevel.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发任务执行脚本</span><br><span class="line">startUpdateUserLevel.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/cl_level_user/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">master=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $1&#125;'`</span><br><span class="line">deployMode=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $2&#125;'`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">appName="UpdateUserLevelScala"`date +%s` // 加的这个是为了后面过滤任务列表，查看是否执行成功</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">username="neo4j"</span><br><span class="line">password="admin"</span><br><span class="line"></span><br><span class="line">yarnCommonLib="hdfs://bigdata01:9000/yarnCommonLib"</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.UpdateUserLevelScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;/neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;/reactive-streams-1.0.3.jar \</span><br><span class="line">/data/soft/video_recommend/jobs/update_user_level-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $7&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262237405.png" alt="image-20230426223703754"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262238354.png" alt="image-20230426223841949"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262239377.png" alt="image-20230426223922101"></p><h3 id="打包配置"><a href="#打包配置" class="headerlink" title="打包配置"></a>打包配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure><h3 id="子项目完整pom"><a href="#子项目完整pom" class="headerlink" title="子项目完整pom"></a>子项目完整pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_level&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262246028.png" alt="image-20230426224642629"></p><h3 id="集群执行"><a href="#集群执行" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startUpdateUserLevel.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262254705.png" alt="image-20230426225404311"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262254651.png" alt="image-20230426225455351"></p><h2 id="数据计算之每天定时更新用户活跃时间-第四个任务"><a href="#数据计算之每天定时更新用户活跃时间-第四个任务" class="headerlink" title="数据计算之每天定时更新用户活跃时间(第四个任务)"></a>数据计算之每天定时更新用户活跃时间(第四个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据来源于客户端上报，每天只要打开过APP就会上报数据</span><br><span class="line">数据格式：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262258359.png" alt="image-20230426225850610"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">之前我们通过埋点模拟上报数据，通过flume落盘到hdfs上面，这样在hdfs上面产生的目录会使用当天日期，为了保证我这里使用的目录和大家都保持一致，所以在这我就生成一个固定的日期目录</span><br><span class="line">使用代码GenerateUserActiveDataV2，在代码中指定日期2026-02-01，这样会把模拟生成的用户活跃数据直接上传到hdfs上面，因为之前的数据采集流程我们已经详细分析过了，所以在这就直接把数据上传到hdfs上面了。</span><br></pre></td></tr></table></figure><h3 id="生成数据"><a href="#生成数据" class="headerlink" title="生成数据"></a>生成数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行代码：GenerateUserActiveDataV2，将会把数据上传到hdfs的这个目录下</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;user_active&#x2F;20260201&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262316695.png" alt="image-20230426231654458"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262323730.png" alt="image-20230426232332517"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这个任务需要做的是把每天主动活跃的用户更新到neo4j中，在neo4j中维护一份用户的最新活跃时间</span><br><span class="line">创建子module项目：update_user_active</span><br><span class="line">创建scala目录，引入scala2.11版本的sdk</span><br><span class="line">在scala目录中创建包：com.imooc.spark</span><br><span class="line">引入依赖</span><br></pre></td></tr></table></figure><h3 id="所需依赖-1"><a href="#所需依赖-1" class="headerlink" title="所需依赖"></a>所需依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateUserActiveScala-scala"><a href="#UpdateUserActiveScala-scala" class="headerlink" title="UpdateUserActiveScala.scala"></a>UpdateUserActiveScala.scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务4：</span></span><br><span class="line"><span class="comment"> * 每天定时更新用户活跃时间</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UpdateUserActiveScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> masterUrl = <span class="string">"local"</span></span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"UpdateUserActiveScala"</span></span><br><span class="line">    <span class="keyword">var</span> filePath = <span class="string">"hdfs://bigdata01:9000/data/user_active/20260201"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> username = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> password = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      masterUrl = args(<span class="number">0</span>)</span><br><span class="line">      appName = args(<span class="number">1</span>)</span><br><span class="line">      filePath = args(<span class="number">2</span>)</span><br><span class="line">      boltUrl = args(<span class="number">3</span>)</span><br><span class="line">      username = args(<span class="number">4</span>)</span><br><span class="line">      password = args(<span class="number">5</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setMaster(masterUrl)</span><br><span class="line">      .setAppName(appName)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取用户活跃数据</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD = sc.textFile(filePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据 </span></span><br><span class="line">    linesRDD.foreachPartition(it=&gt;&#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(username, password))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      it.foreach(line=&gt;&#123;</span><br><span class="line">        <span class="keyword">val</span> jsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">        <span class="keyword">val</span> uid = jsonObj.getString(<span class="string">"uid"</span>)</span><br><span class="line">        <span class="keyword">val</span> timeStamp = jsonObj.getString(<span class="string">"UnixtimeStamp"</span>)</span><br><span class="line">        <span class="comment">//添加用户活跃时间</span></span><br><span class="line">        session.run(<span class="string">"merge(u:User &#123;uid:'"</span>+uid+<span class="string">"'&#125;) set u.timestamp = "</span>+timeStamp)</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-1"><a href="#本地执行-1" class="headerlink" title="本地执行"></a>本地执行</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262336732.png" alt="image-20230426233615284"></p><h3 id="startUpdateUserActive-sh"><a href="#startUpdateUserActive-sh" class="headerlink" title="startUpdateUserActive.sh"></a>startUpdateUserActive.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/user_active/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">master=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $1&#125;'`</span><br><span class="line">deployMode=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $2&#125;'`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">appName="UpdateUserActiveScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">username="neo4j"</span><br><span class="line">password="admin"</span><br><span class="line"></span><br><span class="line">yarnCommonLib="hdfs://bigdata01:9000/yarnCommonLib"</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.UpdateUserActiveScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;/fastjson-1.2.68.jar,,$&#123;yarnCommonLib&#125;/neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;/reactive-streams-1.0.3.jar \</span><br><span class="line">/data/soft/video_recommend/jobs/update_user_active-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $7&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="打包配置-1"><a href="#打包配置-1" class="headerlink" title="打包配置"></a>打包配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure><h3 id="子项目完整pom-1"><a href="#子项目完整pom-1" class="headerlink" title="子项目完整pom"></a>子项目完整pom</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_active&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="集群执行-1"><a href="#集群执行-1" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startUpdateUserActive.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262344942.png" alt="image-20230426234450357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262345661.png" alt="image-20230426234521405"></p><h2 id="数据计算之每周一计算最近一个月主播视频评级-1-第五个任务"><a href="#数据计算之每周一计算最近一个月主播视频评级-1-第五个任务" class="headerlink" title="数据计算之每周一计算最近一个月主播视频评级-1(第五个任务)"></a>数据计算之每周一计算最近一个月主播视频评级-1(第五个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">视频数据来源于服务端，当主播开播结束后会产生一条视频数据</span><br><span class="line">数据格式：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262349598.png" alt="image-20230426234917792"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">之前我们通过埋点模拟上报数据，通过flume落盘到hdfs上面，这样在hdfs上面产生的目录会使用当天日期，为了保证我这里使用的目录和大家都保持一致，所以在这我就生成一个固定的日期目录</span><br><span class="line">使用代码GenerateVideoInfoDataV2，在代码中指定日期2026-02-01，这样会把模拟生成的用户活跃数据直接上传到hdfs上面，因为之前的数据采集流程我们已经详细分析过了，所以在这就直接把数据上传到hdfs上面了。</span><br><span class="line"></span><br><span class="line">执行代码：GenerateVideoInfoDataV2，将会把数据上传到hdfs的这个目录下</span><br><span class="line">hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;video_info&#x2F;20260201&#x2F;</span><br></pre></td></tr></table></figure><h3 id="生成数据-1"><a href="#生成数据-1" class="headerlink" title="生成数据"></a>生成数据</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304262359274.png" alt="image-20230426235956140"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这个任务需要做的就是统计最近一个月内主播的视频评级信息</span><br><span class="line">在这我们先初始化一天的数据即可，计算一天和计算一个月的数据，计算逻辑是一样的，只有spark任务的输入路径不一样</span><br><span class="line">如果是一个月的数据，假设这一个月有30天，则需要把这30天对应的30个目录使用逗号分隔，拼接成一个字符串，作为Spark任务的输入即可。</span><br><span class="line"></span><br><span class="line">为什么这个任务要每周计算一次，而不是每天计算一次呢？</span><br><span class="line">因为很多主播不会每天都开播，所以我们每天都计算意义不大，均衡考虑之后按照每周计算一次这个频率。</span><br><span class="line"></span><br><span class="line">创建子module项目：update_video_info</span><br><span class="line">创建scala目录，引入scala2.11版本的sdk</span><br><span class="line">在scala目录中创建包：com.imooc.spark</span><br><span class="line">引入依赖</span><br></pre></td></tr></table></figure><h3 id="所需依赖-2"><a href="#所需依赖-2" class="headerlink" title="所需依赖"></a>所需依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="UpdateVideoInfoScala-scala"><a href="#UpdateVideoInfoScala-scala" class="headerlink" title="UpdateVideoInfoScala.scala"></a>UpdateVideoInfoScala.scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.spark</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.neo4j.driver.&#123;AuthTokens, GraphDatabase&#125;</span><br><span class="line">import org.slf4j.LoggerFactory</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 任务5：</span><br><span class="line"> * 每周一计算最近一个月主播视频评级</span><br><span class="line"> * 把最近几次视频评级在3B+或2A+的主播，在neo4j中设置flag&#x3D;1</span><br><span class="line"> *</span><br><span class="line"> * 注意：在执行程序之前需要先把flag&#x3D;1的重置为0</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object UpdateVideoInfoScala &#123;</span><br><span class="line">  val logger &#x3D; LoggerFactory.getLogger(&quot;UpdateVideoInfoScala&quot;)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    var masterUrl &#x3D; &quot;local&quot;</span><br><span class="line">    var appName &#x3D; &quot;UpdateVideoInfoScala&quot;</span><br><span class="line">    var filePath &#x3D; &quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;video_info&#x2F;20260201&quot;</span><br><span class="line">    var boltUrl &#x3D; &quot;bolt:&#x2F;&#x2F;bigdata04:7687&quot;</span><br><span class="line">    var username &#x3D; &quot;neo4j&quot;</span><br><span class="line">    var password &#x3D; &quot;admin&quot;</span><br><span class="line">    if(args.length &gt; 0)&#123;</span><br><span class="line">      masterUrl &#x3D; args(0)</span><br><span class="line">      appName &#x3D; args(1)</span><br><span class="line">      filePath &#x3D; args(2)</span><br><span class="line">      boltUrl &#x3D; args(3)</span><br><span class="line">      username &#x3D; args(4)</span><br><span class="line">      password &#x3D; args(5)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;在Driver端执行此代码，将flag&#x3D;1的重置为0</span><br><span class="line">    &#x2F;&#x2F;获取neo4j的连接</span><br><span class="line">    val driver &#x3D; GraphDatabase.driver(boltUrl, AuthTokens.basic(username, password))</span><br><span class="line">    &#x2F;&#x2F;开启一个会话</span><br><span class="line">    val session &#x3D; driver.session()</span><br><span class="line">    session.run(&quot;match(a:User) where a.flag &#x3D;1 set a.flag &#x3D; 0&quot;)</span><br><span class="line">    &#x2F;&#x2F;关闭会话</span><br><span class="line">    session.close()</span><br><span class="line">    &#x2F;&#x2F;关闭连接</span><br><span class="line">    driver.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;获取SparkContext</span><br><span class="line">    val conf &#x3D; new SparkConf()</span><br><span class="line">      .setMaster(masterUrl)</span><br><span class="line">      .setAppName(appName)</span><br><span class="line">    val sc &#x3D; new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;读取视频评级数据</span><br><span class="line">    val linesRDD &#x3D; sc.textFile(filePath)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;解析数据中的uid，rating，timestamp</span><br><span class="line">    val tup3RDD &#x3D; linesRDD.map(line &#x3D;&gt; &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        val jsonObj &#x3D; JSON.parseObject(line)</span><br><span class="line">        val uid &#x3D; jsonObj.getString(&quot;uid&quot;)</span><br><span class="line">        val rating &#x3D; jsonObj.getString(&quot;rating&quot;)</span><br><span class="line">        val timestamp: Long &#x3D; jsonObj.getLong(&quot;timestamp&quot;)</span><br><span class="line">        (uid, rating, timestamp)</span><br><span class="line">      &#125; catch &#123;</span><br><span class="line">        case ex: Exception &#x3D;&gt; logger.error(&quot;json数据解析失败：&quot; + line)</span><br><span class="line">          (&quot;0&quot;, &quot;0&quot;, 0L)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;过滤异常数据</span><br><span class="line">    val filterRDD &#x3D; tup3RDD.filter(_._2 !&#x3D; &quot;0&quot;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;获取用户最近3场直播(视频)的评级信息</span><br><span class="line">    val top3RDD &#x3D; filterRDD.groupBy(_._1).map(group &#x3D;&gt; &#123;</span><br><span class="line">      &#x2F;&#x2F;获取最近3次开播的数据，使用制表符拼接成一个字符串</span><br><span class="line">      &#x2F;&#x2F;uid,rating,timestamp \t uid,rating,timestamp \t uid,rating,timestamp </span><br><span class="line">      val top3 &#x3D; group._2.toList.sortBy(_._3).reverse.take(3).mkString(&quot;\t&quot;)</span><br><span class="line">      (group._1, top3)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;过滤出来满足3场B+的数据</span><br><span class="line">    val top3BRDD &#x3D; top3RDD.filter(tup &#x3D;&gt; &#123;</span><br><span class="line">      var flag &#x3D; false</span><br><span class="line">      val fields &#x3D; tup._2.split(&quot;\t&quot;)</span><br><span class="line">      if (fields.length &#x3D;&#x3D; 3) &#123;</span><br><span class="line">        &#x2F;&#x2F;3场B+，表示里面没有出现C和D</span><br><span class="line">        val tmp_str &#x3D; fields(0).split(&quot;,&quot;)(1) + &quot;,&quot; + fields(1).split(&quot;,&quot;)(1) + &quot;,&quot; + fields(2).split(&quot;,&quot;)(1)</span><br><span class="line">        if (!tmp_str.contains(&quot;C&quot;) &amp;&amp; !tmp_str.contains(&quot;D&quot;)) &#123;</span><br><span class="line">          flag &#x3D; true</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;把满足3场B+的数据更新到neo4j中，增加一个字段flag，flag&#x3D;1表示是视频评级满足条件的主播，允许推荐给用户</span><br><span class="line">    &#x2F;&#x2F;注意：针对3场B+的数据还需要额外再限制一下主播等级，主播等级需要&gt;&#x3D;15，这样可以保证筛选出来的主播尽可能是一些优质主播</span><br><span class="line">    top3BRDD.foreachPartition(it&#x3D;&gt;&#123;</span><br><span class="line">      &#x2F;&#x2F;获取neo4j的连接</span><br><span class="line">      val driver &#x3D; GraphDatabase.driver(boltUrl, AuthTokens.basic(username, password))</span><br><span class="line">      &#x2F;&#x2F;开启一个会话</span><br><span class="line">      val session &#x3D; driver.session()</span><br><span class="line">      it.foreach(tup&#x3D;&gt;&#123;</span><br><span class="line">        session.run(&quot;match (a:User &#123;uid:&#39;&quot;+tup._1+&quot;&#39;&#125;) where a.level &gt;&#x3D;15 set a.flag &#x3D; 1&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line">      &#x2F;&#x2F;关闭会话</span><br><span class="line">      session.close()</span><br><span class="line">      &#x2F;&#x2F;关闭连接</span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;过滤出来满足2场A+的数据</span><br><span class="line">    val top2ARDD &#x3D; top3RDD.filter(tup &#x3D;&gt; &#123;</span><br><span class="line">      var flag &#x3D; false</span><br><span class="line">      val fields &#x3D; tup._2.split(&quot;\t&quot;)</span><br><span class="line">      if (fields.length &gt;&#x3D; 2) &#123;</span><br><span class="line">        &#x2F;&#x2F;2场A+，获取最近两场直播评级，里面不能出现B、C、D</span><br><span class="line">        val tmp_str &#x3D; fields(0).split(&quot;,&quot;)(1) + &quot;,&quot; + fields(1).split(&quot;,&quot;)(1)</span><br><span class="line">        if (!tmp_str.contains(&quot;B&quot;) &amp;&amp; !tmp_str.contains(&quot;C&quot;) &amp;&amp; !tmp_str.contains(&quot;D&quot;)) &#123;</span><br><span class="line">          flag &#x3D; true</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;把满足2场A+的数据更新到neo4j中，设置flag&#x3D;1</span><br><span class="line">    &#x2F;&#x2F;注意：针对2场A+的数据还需要额外限制一下主播等级，主播等级需要&gt;&#x3D;4 这样可以保证筛选出来的主播尽可能是一些优质主播</span><br><span class="line">    top2ARDD.foreachPartition(it&#x3D;&gt;&#123;</span><br><span class="line">      &#x2F;&#x2F;获取neo4j的连接</span><br><span class="line">      val driver &#x3D; GraphDatabase.driver(boltUrl, AuthTokens.basic(username, password))</span><br><span class="line">      &#x2F;&#x2F;开启一个会话</span><br><span class="line">      val session &#x3D; driver.session()</span><br><span class="line">      it.foreach(tup&#x3D;&gt;&#123;</span><br><span class="line">        session.run(&quot;match (a:User &#123;uid:&#39;&quot;+tup._1+&quot;&#39;&#125;) where a.level &gt;&#x3D;4 set a.flag &#x3D; 1&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line">      &#x2F;&#x2F;关闭会话</span><br><span class="line">      session.close()</span><br><span class="line">      &#x2F;&#x2F;关闭连接</span><br><span class="line">      driver.close()</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-2"><a href="#本地执行-2" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在本地执行代码</span><br><span class="line">然后到neo4j的web界面查看结果，发现只有uid为1005的数据对应的flag不等于1(没有flag属性)</span><br><span class="line">这样是正确的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304271046115.png" alt="image-20230427104641625"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304271048485.png" alt="image-20230427104824499"></p><h3 id="startUpdateVideoInfo-sh"><a href="#startUpdateVideoInfo-sh" class="headerlink" title="startUpdateVideoInfo.sh"></a>startUpdateVideoInfo.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面开发任务执行脚本</span><br><span class="line">注意：这个脚本中需要实现获取最近一个月的数据目录</span><br><span class="line">startUpdateVideoInfo.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取最近一个月的文件目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash">filepath=<span class="string">""</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">for</span>((i=1;i&lt;=30;i++))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">do</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    filepath+=<span class="string">"hdfs://bigdata01:9000/data/video_info/"</span>`date -d <span class="string">"<span class="variable">$i</span> days ago"</span> +<span class="string">"%Y%m%d"</span>`, // 这里的，号很经典</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="keyword">done</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取昨天时间</span></span><br><span class="line">dt=`date -d "1 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">HDFS输入数据路径</span></span><br><span class="line">filePath="hdfs://bigdata01:9000/data/video_info/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">master=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $1&#125;'`</span><br><span class="line">deployMode=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $2&#125;'`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">appName="UpdateVideoInfoScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">username="neo4j"</span><br><span class="line">password="admin"</span><br><span class="line"></span><br><span class="line">yarnCommonLib="hdfs://bigdata01:9000/yarnCommonLib"</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.UpdateVideoInfoScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;/fastjson-1.2.68.jar,$&#123;yarnCommonLib&#125;/neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;/reactive-streams-1.0.3.jar \</span><br><span class="line">/data/soft/video_recommend/jobs/update_video_info-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;filePath&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $7&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="子项目完整依赖"><a href="#子项目完整依赖" class="headerlink" title="子项目完整依赖"></a>子项目完整依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;update_user_active&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="集群执行-2"><a href="#集群执行-2" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startUpdateVideoInfo.sh 20260201</span><br></pre></td></tr></table></figure><h2 id="数据计算之每周一计算三度关系推荐列数据-第六个任务"><a href="#数据计算之每周一计算三度关系推荐列数据-第六个任务" class="headerlink" title="数据计算之每周一计算三度关系推荐列数据(第六个任务)"></a>数据计算之每周一计算三度关系推荐列数据(第六个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">前面我们在neo4j中维护了粉丝和主播的一些信息，在这里我们就需要基于neo4j中的数据统计主播的三度关系推荐列表了</span><br><span class="line"></span><br><span class="line">这个任务在这也是每周计算一次，我们测试过，每天都计算的话，最终的结果变化是不大的，所以就没必要每天计算了。</span><br><span class="line"></span><br><span class="line">创建子module项目：get_recommend_list</span><br><span class="line">创建scala目录，引入scala2.11版本的sdk</span><br><span class="line">在scala目录中创建包：com.imooc.spark</span><br><span class="line">引入依赖，这里面需要额外用到spark-sql和neo4j-spark-connector这两个依赖</span><br></pre></td></tr></table></figure><h3 id="生成数据-2"><a href="#生成数据-2" class="headerlink" title="生成数据"></a>生成数据</h3><h3 id="所需依赖-3"><a href="#所需依赖-3" class="headerlink" title="所需依赖"></a>所需依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;neo4j-contrib&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-spark-connector&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在使用spark读取neo4j中数据的时候，可以使用一个插件，在官网中可以找到</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281349757.png" alt="image-20230428134919089"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281349353.png" alt="image-20230428134931154"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281350275.png" alt="image-20230428135056752"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个版本是基于neo4j 4.0，我们现在使用的neo4j是3.5的，这种一般是向下兼容的，所以操作neo4j 3.5也是可以的，后面写的spark是2.4.5，这个也是可以的，我们使用的spark是2.4.3的，最后一位版本号不一致没问题。</span><br><span class="line">目前最新版本是基于scala2.12版本编译的，我们在spark项目中使用的scala版本是2.11，所以使用2.4.5-M1这个版本。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在使用这个依赖的时候，还需要配置它对应的repository，因为这个依赖没有在maven仓库中，把这些配置添加到父项目的pom.xml文件中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281352725.png" alt="image-20230428135228605"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281352639.png" alt="image-20230428135241954"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">咱们前面使用的neo4j-java-driver相当于是使用原生代码操作neo4j，而现在使用neo4j-spark-connector相当于把neo4j封装到spark中了，使用起来比较方便。</span><br><span class="line"></span><br><span class="line">在使用neo4j-spark-connector的时候，选择哪个版本呢？</span><br><span class="line">点这里进去看一下</span><br></pre></td></tr></table></figure><h3 id="GetRecommendListScala-scala"><a href="#GetRecommendListScala-scala" class="headerlink" title="GetRecommendListScala.scala"></a>GetRecommendListScala.scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.driver.&#123;<span class="type">AuthTokens</span>, <span class="type">GraphDatabase</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.neo4j.spark.<span class="type">Neo4j</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 任务6：</span></span><br><span class="line"><span class="comment"> * 每周一计算最近一周内主活主播的三度关系列表</span></span><br><span class="line"><span class="comment"> * 注意：</span></span><br><span class="line"><span class="comment"> * 1：待推荐主播最近一周内活跃过</span></span><br><span class="line"><span class="comment"> * 2：待推荐主播等级&gt;4</span></span><br><span class="line"><span class="comment"> * 3：待推荐主播最近1个月视频评级满足3B+或2A+(flag=1)</span></span><br><span class="line"><span class="comment"> * 4：待推荐主播的粉丝列表关注重合度&gt;2</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GetRecommendListScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> masterUrl = <span class="string">"local"</span></span><br><span class="line">    <span class="keyword">var</span> appName = <span class="string">"GetRecommendListScala"</span></span><br><span class="line">    <span class="keyword">var</span> boltUrl = <span class="string">"bolt://bigdata04:7687"</span></span><br><span class="line">    <span class="keyword">var</span> username = <span class="string">"neo4j"</span></span><br><span class="line">    <span class="keyword">var</span> password = <span class="string">"admin"</span></span><br><span class="line">    <span class="keyword">var</span> timestamp = <span class="number">0</span>L <span class="comment">//过滤最近一周内是否活跃过</span></span><br><span class="line">    <span class="keyword">var</span> duplicateNum = <span class="number">2</span> <span class="comment">//粉丝列表关注重合度</span></span><br><span class="line">    <span class="keyword">var</span> level = <span class="number">4</span> <span class="comment">//主播等级</span></span><br><span class="line">    <span class="keyword">var</span> outputPath = <span class="string">"hdfs://bigdata01:9000/data/recommend_data/20260201"</span></span><br><span class="line">    <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      masterUrl = args(<span class="number">0</span>)</span><br><span class="line">      appName = args(<span class="number">1</span>)</span><br><span class="line">      boltUrl = args(<span class="number">2</span>)</span><br><span class="line">      username = args(<span class="number">3</span>)</span><br><span class="line">      password = args(<span class="number">4</span>)</span><br><span class="line">      timestamp = args(<span class="number">5</span>).toLong</span><br><span class="line">      duplicateNum = args(<span class="number">6</span>).toInt</span><br><span class="line">      level = args(<span class="number">7</span>).toInt</span><br><span class="line">      outputPath = args(<span class="number">8</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(appName)</span><br><span class="line">      .setMaster(masterUrl)</span><br><span class="line">      .set(<span class="string">"spark.driver.allowMultipleContexts"</span>,<span class="string">"true"</span>)<span class="comment">//允许创建多个context</span></span><br><span class="line">      .set(<span class="string">"spark.neo4j.url"</span>,boltUrl)<span class="comment">//bolt的地址</span></span><br><span class="line">      .set(<span class="string">"spark.neo4j.user"</span>,username)<span class="comment">//neo4j用户名</span></span><br><span class="line">      .set(<span class="string">"spark.neo4j.password"</span>,password)<span class="comment">//neo4j密码</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取一周内主活的主播 并且主播等级大于4的数据</span></span><br><span class="line">    <span class="keyword">var</span> params = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Long</span>]()</span><br><span class="line">    params += (<span class="string">"timestamp"</span>-&gt;timestamp)</span><br><span class="line">    params += (<span class="string">"level"</span>-&gt;level)</span><br><span class="line">    <span class="keyword">val</span> neo4j: <span class="type">Neo4j</span> = <span class="type">Neo4j</span>(sc).cypher(<span class="string">"match (a:User) where a.timestamp &gt;= &#123;timestamp&#125; and a.level &gt;= &#123;level&#125; return a.uid"</span>).params(params)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将从neo4j中查询出来的数据转换为rowRDD</span></span><br><span class="line">    <span class="comment">//val rowRDD = neo4j.loadRowRdd</span></span><br><span class="line">    <span class="comment">//repartition 这里的repartition是为了把数据分为7份，这样下面的mapPartitions在执行的时候就有7个线程</span></span><br><span class="line">    <span class="comment">//这7个线程并行查询neo4j数据库</span></span><br><span class="line">    <span class="keyword">val</span> rowRDD = neo4j.loadRowRdd.repartition(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//一次处理一批</span></span><br><span class="line">    <span class="comment">//过滤出粉丝关注重合度&gt;2的数据，并且对关注重合度倒序排列</span></span><br><span class="line">    <span class="comment">//最终的数据格式是：主播id,待推荐的主播id</span></span><br><span class="line">    <span class="keyword">val</span> mapRDD = rowRDD.mapPartitions(it =&gt; &#123;</span><br><span class="line">      <span class="comment">//获取neo4j的连接</span></span><br><span class="line">      <span class="keyword">val</span> driver = <span class="type">GraphDatabase</span>.driver(boltUrl, <span class="type">AuthTokens</span>.basic(username, password))</span><br><span class="line">      <span class="comment">//开启一个会话</span></span><br><span class="line">      <span class="keyword">val</span> session = driver.session()</span><br><span class="line">      <span class="comment">//保存计算出来的结果</span></span><br><span class="line">      <span class="keyword">val</span> resultArr = <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">      it.foreach(row =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> uid = row.getString(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//计算一个用户的三度关系(主播的二度关系)</span></span><br><span class="line">        <span class="comment">//注意：数据量大了之后，这个计算操作是非常耗时</span></span><br><span class="line">        <span class="comment">//val result = session.run("match (a:User &#123;uid:'" + uid + "'&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt; (c:User) return a.uid as auid,c.uid as cuid,count(c.uid) as sum order by sum desc limit 30")</span></span><br><span class="line">        <span class="comment">//对b、c的主活时间进行过滤，以及对c的level和flag值进行过滤</span></span><br><span class="line">        <span class="keyword">val</span> result = session.run(<span class="string">"match (a:User &#123;uid:'"</span> + uid + <span class="string">"'&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt; (c:User) "</span> +</span><br><span class="line">          <span class="string">"where b.timestamp &gt;= "</span> + timestamp + <span class="string">" and c.timestamp &gt;= "</span> + timestamp + <span class="string">" and c.level &gt;= "</span> + level + <span class="string">" and c.flag = 1 "</span> +</span><br><span class="line">          <span class="string">"return a.uid as auid,c.uid as cuid,count(c.uid) as sum order by sum desc limit 30"</span>)</span><br><span class="line">        <span class="keyword">while</span> (result.hasNext) &#123;</span><br><span class="line">          <span class="keyword">val</span> record = result.next()</span><br><span class="line">          <span class="keyword">val</span> sum = record.get(<span class="string">"sum"</span>).asInt()</span><br><span class="line">          <span class="keyword">if</span> (sum &gt; duplicateNum) &#123;</span><br><span class="line">            resultArr += record.get(<span class="string">"auid"</span>).asString() + <span class="string">"\t"</span> + record.get(<span class="string">"cuid"</span>).asString()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//关闭会话</span></span><br><span class="line">      session.close()</span><br><span class="line">      <span class="comment">//关闭连接</span></span><br><span class="line">      driver.close()</span><br><span class="line">      resultArr.iterator</span><br><span class="line">    &#125;).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>) <span class="comment">//把RDD数据缓存起来</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//把数据转成tuple2的形式</span></span><br><span class="line">    <span class="keyword">val</span> tup2RDD = mapRDD.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> splits = line.split(<span class="string">"\t"</span>)</span><br><span class="line">      (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//根据主播id进行分组，可以获取到这个主播的待推荐列表</span></span><br><span class="line">    <span class="keyword">val</span> reduceRDD = tup2RDD.reduceByKey((v1, v2) =&gt; &#123;</span><br><span class="line">      v1 + <span class="string">","</span> + v2</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//最终把结果组装成这种形式</span></span><br><span class="line">    <span class="comment">//1001 1002,1003,1004</span></span><br><span class="line">    reduceRDD.map(tup=&gt;&#123;</span><br><span class="line">      tup._1+<span class="string">"\t"</span>+tup._2</span><br><span class="line">    &#125;).repartition(<span class="number">1</span>).saveAsTextFile(outputPath)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="本地执行-3"><a href="#本地执行-3" class="headerlink" title="本地执行"></a>本地执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">先使用这一行代码，在计算三度关系数据的时候暂时先不进行条件过滤。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281537558.png" alt="image-20230428153711075"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281537439.png" alt="image-20230428153735751"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接着使用这一行代码，在计算三度关系数据的对数据进行条件过滤</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281539054.png" alt="image-20230428153929765"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281540381.png" alt="image-20230428154023306"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到neo4j中验证一下，确实是正确的，因为1005的flag不为1，被过滤掉了，所以我在关注1000这个主播的时候平台只需要给我推荐主1004这个主播即可。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这个代码在执行mapPartitions的时候，最好把rowRDD的分区重新设置一下，如果让程序自动分配的话可能不太合理，分多了分少了都不太好</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">由于我们在mapPartitions中需要操作neo4j，所以这个时候rowRDD分区的数量就可以等于（neo4j服务器的CPU数量-1），要给neo4j预留出来一个cpu来处理其它任务请求。</span><br><span class="line">我们当时的服务器是8个CPU，给neo4j预留出来一个，剩下还有7个，所以说，neo4j此时可以对外提供的最大并发处理能力是7，那我们就把rowRDD设置为7个分区，就会有7个线程并行处理数据，它们可以并行操作neo4j，这样效率最高。</span><br><span class="line">如果给rowRDD设置的分区太多，对应的就会有多个线程并行操作neo4j，会给neo4j造成很大的压力，相当于neo4j在满负荷的运行，这个时候我们另外一个实时维护neo4j中粉丝关注数据的程序执行起来就很慢了，以及其他人如果这个时候想查询neo4j，也会非常慢，所以这样就不太好了。</span><br><span class="line">如果给rowRDD设置的分区太少，对应产生的执行线程就比较少，此时neo4j会比较空闲，没有多大压力，但是我们这个三度关系的任务执行就非常慢了。</span><br><span class="line">综上所述，建议把rowRDD的分区数量设置为7，这样可以充分利用neo4j服务器的性能，也不至于把neo4j服务器拖垮。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281547995.png" alt="image-20230428154738077" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">还有一点可以优化的，增加RDD持久化，把RDD数据缓存起来，这样可以避免个别task失败导致的数据重算，因为这个计算还是比较消耗时间的，所以说尽可能保证计算出来的数据不丢失。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281553783.png" alt="image-20230428155333860"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">问题：大家有没有想过，我们是否可以直接在Neo4j(sc).cypher(…)中指定一条查询语句，直接把所有的三度关系全部查询出来？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这样理论上是可以的，但是在实际中，当neo4j中存储的节点数和关系数量达到千万级别之后，同时查询所有满足条件主播的三度关系推荐列表的时候会很慢，有时候会导致等了几十分钟也查询不出来数据，所以在这我们就把这个功能进行了拆解，先查看满足条件的主播uid列表，然后再一个一个计算这些主播的三度关系推荐列表，这样可以提高计算效率，并且不会出现查询不出来结果的情况。</span><br></pre></td></tr></table></figure><h3 id="startGetRecommendList-sh"><a href="#startGetRecommendList-sh" class="headerlink" title="startGetRecommendList.sh"></a>startGetRecommendList.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">默认获取上周一的时间</span></span><br><span class="line">dt=`date -d "7 days ago" +"%Y%m%d"`</span><br><span class="line">if [ "x$1" != "x" ]</span><br><span class="line">then</span><br><span class="line">    dt=`date -d "7 days ago $1" +"%Y%m%d"`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">masterUrl="yarn-cluster"</span><br><span class="line">master=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $1&#125;'`</span><br><span class="line">deployMode=`echo $&#123;masterUrl&#125; | awk -F'-' '&#123;print $2&#125;'`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">appName="GetRecommendListScala"`date +%s`</span><br><span class="line">boltUrl="bolt://bigdata04:7687"</span><br><span class="line">username="neo4j"</span><br><span class="line">password="admin"</span><br><span class="line"><span class="meta">#</span><span class="bash">获取上周一的时间戳(单位：毫秒)</span></span><br><span class="line">timestamp=`date --date="$&#123;dt&#125;" +%s`000</span><br><span class="line"><span class="meta">#</span><span class="bash">粉丝列表关注重合度</span></span><br><span class="line">duplicateNum=2</span><br><span class="line"><span class="meta">#</span><span class="bash">主播等级</span></span><br><span class="line">level=4</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">输出结果数据路径</span></span><br><span class="line">outputPath="hdfs://bigdata01:9000/data/recommend_data/$&#123;dt&#125;"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yarnCommonLib="hdfs://bigdata01:9000/yarnCommonLib"</span><br><span class="line"></span><br><span class="line">spark-submit --master $&#123;master&#125; \</span><br><span class="line">--name $&#123;appName&#125; \</span><br><span class="line">--deploy-mode $&#123;deployMode&#125; \</span><br><span class="line">--queue default \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--num-executors 2 \</span><br><span class="line">--class com.imooc.spark.GetRecommendListScala \</span><br><span class="line">--jars $&#123;yarnCommonLib&#125;/neo4j-java-driver-4.1.1.jar,$&#123;yarnCommonLib&#125;/reactive-streams-1.0.3.jar,$&#123;yarnCommonLib&#125;/neo4j-spark-connector-2.4.5-M1.jar \</span><br><span class="line">/data/soft/video_recommend/jobs/get_recommend_list-1.0-SNAPSHOT.jar $&#123;masterUrl&#125; $&#123;appName&#125; $&#123;boltUrl&#125; $&#123;username&#125; $&#123;password&#125; $&#123;timestamp&#125; $&#123;duplicateNum&#125; $&#123;level&#125; $&#123;outputPath&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">验证任务执行状态</span></span><br><span class="line">appStatus=`yarn application -appStates FINISHED -list | grep $&#123;appName&#125; | awk '&#123;print $7&#125;'`</span><br><span class="line">if [ "$&#123;appStatus&#125;" != "SUCCEEDED" ]</span><br><span class="line">then</span><br><span class="line">    echo "任务执行失败"</span><br><span class="line">    # 发送短信或者邮件</span><br><span class="line">else</span><br><span class="line">    echo "任务执行成功"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="子项目完整依赖-1"><a href="#子项目完整依赖-1" class="headerlink" title="子项目完整依赖"></a>子项目完整依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;artifactId&gt;db_video_recommend&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;artifactId&gt;get_recommend_list&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.neo4j.driver&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-java-driver&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;neo4j-contrib&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;neo4j-spark-connector&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- scala编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;scalaCompatVersion&gt;2.11&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                    &lt;scalaVersion&gt;2.11.12&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                            &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure><h3 id="集群执行-3"><a href="#集群执行-3" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x startGetRecommendList.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281605986.png" alt="image-20230428160537691"></p><h2 id="数据计算之三度关系数据导出到Mysql-第七个任务"><a href="#数据计算之三度关系数据导出到Mysql-第七个任务" class="headerlink" title="数据计算之三度关系数据导出到Mysql(第七个任务)"></a>数据计算之三度关系数据导出到Mysql(第七个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">接下来我们需要使用Sqoop将HDFS中计算好的三度关系推荐列表数据导出到MySQL中</span><br><span class="line">需要在mysql中创建一个表recommend_list</span><br><span class="line">这个表有两列：</span><br><span class="line">第一列为主播uid</span><br><span class="line">第二列为待推荐主播uid</span><br><span class="line"></span><br><span class="line">当用户关注某个主播的时候，会根据这个主播的uid到这个表里面进行查询，把待推荐主播uid获取到，在页面中进行展现，推荐给用户，这样就都可以实现三度关系推荐的效果了。</span><br><span class="line"></span><br><span class="line">建表语句如下：</span><br><span class="line">recommend_list.sql</span><br></pre></td></tr></table></figure><h3 id="recommend-list-sql"><a href="#recommend-list-sql" class="headerlink" title="recommend_list.sql"></a>recommend_list.sql</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> recommend_list;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> recommend_list(</span><br><span class="line">uid <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">recommend_uids <span class="built_in">varchar</span>(<span class="number">500</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">primary <span class="keyword">key</span> (uid)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="export-recommend-list-sh"><a href="#export-recommend-list-sh" class="headerlink" title="export_recommend_list.sh"></a>export_recommend_list.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来使用sqoop将hdfs中的输出导出到mysql中</span><br><span class="line">在导出的时候实现插入和更新功能，如果uid对应的数据已存在，则更新，如果不存在则插入</span><br><span class="line">开发一个脚本</span><br><span class="line">export_recommend_list.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#默认获取上周一的时间</span><br><span class="line">dt&#x3D;&#96;date -d &quot;7 days ago&quot; +&quot;%Y%m%d&quot;&#96;</span><br><span class="line">if [ &quot;x$1&quot; !&#x3D; &quot;x&quot; ]</span><br><span class="line">then</span><br><span class="line">    dt&#x3D;&#96;date -d &quot;7 days ago $1&quot; +&quot;%Y%m%d&quot;&#96;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;video?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table recommend_list \</span><br><span class="line">--export-dir hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;recommend_data&#x2F;$&#123;dt&#125; \</span><br><span class="line">--input-fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--update-key uid \</span><br><span class="line">--update-mode allowinsert</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -x exexport_recommend_list.sh 20260201</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304281750435.png" alt="image-20230428175018471"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其实在实际工作中我们需要做的到这就可以了，然后把这个数据库的名称、表名、表中的字段含义写一个文档同步给服务端即可，具体的数据交互是由服务端和客户端进行对接的。</span><br></pre></td></tr></table></figure><h2 id="数据展现"><a href="#数据展现" class="headerlink" title="数据展现"></a>数据展现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在后面的v2.0架构中，我们会开发一个接口，对外提供数据，因为直接把数据库暴露给其它用户不太安全，倒不是怕他们删库跑路，是担心他们误操作把某些数据删掉了。</span><br><span class="line">等到我们在v2.0中开发了数据接口之后，我们再通过本地启动项目进行效果演示。</span><br></pre></td></tr></table></figure><h2 id="项目代码双语支持"><a href="#项目代码双语支持" class="headerlink" title="项目代码双语支持"></a>项目代码双语支持</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">咱们前面在开发具体的数据计算代码的时候，使用的都是scala代码，为了兼顾Java开发人员，针对数据处理中的功能代码在这我也提供了Java代码支持</span><br><span class="line">在这里这些java代码我就不再手敲了，到时候直接把对应的java代码一起提交到git上面，大家如果有需要了可以去一下。</span><br><span class="line">其实我是不建议在工作中使用java去开发spark和flink的，在这只是为了给大家多一个选择而已，根据行业经验而言，scala语言是开发spark和flink最好的选择。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">在v1.0中，我们侧重于学习整个项目的业务、架构逻辑和实战代码开发。</span><br><span class="line">针对项目调忧、项目数据规模、集群规模、Neo4j性能指标等内容，在后面的V2.0中给大家安排上。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v1.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v1.0-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-2.html</id>
    <published>2023-04-24T07:37:28.000Z</published>
    <updated>2023-05-20T10:28:05.377Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v1-0-2"><a href="#第十八周-直播平台三度关系推荐v1-0-2" class="headerlink" title="第十八周 直播平台三度关系推荐v1.0-2"></a>第十八周 直播平台三度关系推荐v1.0-2</h1><h2 id="数据采集架构详细设计"><a href="#数据采集架构详细设计" class="headerlink" title="数据采集架构详细设计"></a>数据采集架构详细设计</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242348422.png" alt="image-20230424234832289"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们就从我们整体架构里面的第一个模块数据采集模块开始。注意，在实际过程中，数据采集模块不是只针对某一个项目而言的，而是一个公共的采集平台，所有项目依赖的数据全部都来源于数据采集模块，所以在设计采集模块的时候要考虑通用性。不能仅仅是为了这一个项目而服务。</span><br><span class="line"></span><br><span class="line">咱们前面在分析整体架构的时候说过，filebeat采集的数据到达kafka以后，会通过flume再做一下分发，为什么要有这个分发这个过程呢？这个分发过程实现了什么功能呢？我们来看一下这张图。这个图里面呢，针对数据采集模块做了详细的分析，把数据采集模块呢，又划分了三层，数据采集聚合层，数据分发省数据落盘层。</span><br><span class="line"></span><br><span class="line">在这个数据采集聚合层，我们为了保证采集程序的通用性，不至于每次新增一个业务指标的数据，就去重新增加一个采集进程，或者修改采集程序的配置文件。所以呢，我们定义了一个规则。所有的日志数据全部保存在服务器的一个特定的目录下面。我会让filebeat的监控这个目录下面的所有文件。如果后期有新增业务日志，那么就会在这个目录下新增一种日志文件，filebeat就可以自动识别。但是这个时候会有一个问题。filebeat的输出只有一个。多种类型的日志数据会被filebeat采集到同一个topic中。如果各种类型的日志数据全部混到一块儿，会导致后期处理数据的时候比较麻烦。本来呢，我只想计算一种数据，但是这个时候我就需要读取这个大的topic。它里面呢，包含了很多种数据类型。这里面的一个数据量也很大，那我计算的时候呢，我就需要把它里面所有数据全部都读出来，然后再过滤。这样在计算的时候就会影响计算效率，也间接的浪费了计算资源。所以针对这个问题，我们又定义了一个规则，所有的日志数据全部使用json格式，并且呢，在json中增加一个type字段，标识数据的类型，这样每一条数据都有自己的类型标识，然后汇聚到kafka中的一个大的topic中。为了后面使用方便，我们就需要把这个大的topic中的数据啊，根据业务类型进行拆分，把不同类型的数据啊分发到不同的topic中。那这块的话，其实就是我们这个数据分发层要干的事情，相当于filebeat呢，它呢会把这个数据啊，全部都采集到kafka里面这个大topic里面。所有业务类型的日全部都采集到这一个topic里面。</span><br><span class="line"></span><br><span class="line">然后呢，我们接着就可以使用这个flume，对kafka中这个大topic中的数据进行分发。利用flume中的拦截器解析数据中的那个type字段的值，把type字段的值作为输出topic的一个名称。这样就可以把相同类型的数据分发到同一个topic中了。当然了，这些topic呢，我们需要提前创建。如果想要提高这个数据分发的能力，我们还可以在这儿启动多个flume进程。只需要保证多个flume中指定相同的group.id就可以了。这样就可以并行执行这个数据分发操作。那把这个数据分发到对应的topic里面以后呀，后面的实施计算程序就可以直接消费这个topic进行计算了。不需要再去读取那个大的topic，这样就可以提高计算性能。并且呢，我们还可以把需要备份的topic里面的数据啊，使用flume进行落盘，把它保存到Hdfs里面。这个呢，就是数据采集架构的详细设计。</span><br></pre></td></tr></table></figure><h2 id="数据来源分析"><a href="#数据来源分析" class="headerlink" title="数据来源分析"></a>数据来源分析</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181718754.png" alt="image-20230425104401098"></p><h3 id="服务端日志数据"><a href="#服务端日志数据" class="headerlink" title="服务端日志数据"></a>服务端日志数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们来分析一下，针对这个项目，我们需要采集哪些业务类型的数据，以及这些数据来源于什么地方。首先是服务端日志数据。什么是服务端日志呢？可以这样理解。就是我们在APP中点击一些按钮的时候，例如我们要关注一个主播，这个时候当我们点击这个关注按钮之后。APP呢，它会去请求对应的接口。接口中的代码逻辑就是将我们关注的数据保存到数据库里面。同时，这个接口也会记录一份日志。因为这个接口呢，是为APP提供后台服务的，所以它记录的日志我们称之为服务端日志。接着我们来简单画一个图来看一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181712187.png" alt="image-20230518171257153"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个呢，是隔壁老王。那是我们的一个用户。画的形象一点。这是我们的一个APP。我们这个APP里面呢，它有一个关注功能。这时候呢，你看老王啊，他会点击这个关注功能。要关注某一个主播。当他点击这个关注功能之后，这个APP里面这个关注功能，它对应的它会调用一个接口。那我们这块呢，有一个后台服务器。这个服务器里面呢，部署的有一个接口服务，就是这种HTTP接口。那对应的这个关注功能，其实呢，还会调那个接口。所以你在这呢，点击APP里面这个关注按钮。它底层啊，其实会调这个接口。这个接口呢，其实呢，它会操作一个数据库。相遇老王在这儿呢，关注了一个主播，最终啊调这个接口。接口呢去操作这个数据库，最终把老王关注了，谁把这个数据呢存到数据库里面。就是这个逻辑啊。那这个时候注意我们在这个接入服务里面呢，因为它俩现在也是一个外部项目，所以什么它里面可以记录日志，那这里面记录的日志呢。我们就把它称为是服务端日志。</span><br><span class="line"></span><br><span class="line">那在我们这个项目里面，针对服务端日志。主要包含实时粉丝、观众数据以及视频数据。这个实时粉丝关注数据啊，是因为用户呢，在点击关注以及取消关注的时候，都需要调用服务端接口。所以这个数据呢，会在服务端通过日志记录。还有就是这个视频数据。下面这个视频啊，其实就是直播。当主播关闭直播的时候，会调用服务端接口上报本次直播的相关指标数据。其实服务端记录的还有很多其他类型的数据，只不过说呢，我们这个项目目前呢，只需要这两种数据。</span><br></pre></td></tr></table></figure><h3 id="服务端数据库数据"><a href="#服务端数据库数据" class="headerlink" title="服务端数据库数据"></a>服务端数据库数据</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181716837.png" alt="image-20230518171613701"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下服务端数据库中的数据。注意服务端数据库中的数据啊，其实啊，就是我们刚才图里面这个数据库里面的数据。来看一下。就这块儿。就相当于啊，我们App某一些功能呢，它会调用这个后台的接口，然后调用这个接口之后呢，最终啊，其实操作的是这个数据库。我们所说的这个服务端数据库的数据，其实说的就是这块它里面的数据。那在这个项目里面，我们主要获取历史粉丝关注数据，以及呢主播等级数据。这里面我们需要历史粉丝关注数据，因为我们在做这个项目的时候，我们的直播平台已经运营了两三年了。所以说，我们需要把历史粉丝关注数据初始化到图数据库中。这些历史数据服务端存储在数据库中，所以说我们需要从数据库里面去取。还有就是这个主播的等级数据。其实这个数据呢，在服务端日志中也有，但是我们考虑到服务端数据库中的数据是最准确的。特别是针对用户相关的数据，最好是以服务端数据库中的为准。所以说呢，我们就从那个服务端数据库中，每天凌晨啊，定时把昨天等级发生了变化的这个主播等级数据呢，导入到hdfs，方便我们后面的离线计算时。</span><br></pre></td></tr></table></figure><h3 id="客户端日志数据"><a href="#客户端日志数据" class="headerlink" title="客户端日志数据"></a>客户端日志数据</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181715348.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">最后呢，我们来看一下客户端日志数据。刚才我们分析了服务端日志。</span><br><span class="line"></span><br><span class="line">那什么是客户端日志呢？其实啊，就是用户在APP客户端操作的时候。直接通过埋点上报的日志数据，这种数据称之为客户端日志数据。我们在这再画一下这个图。如果说呀，你在这做了一些操作，你说呢，我不调用这个后台这个接口。我呢通过买点直接上报用户这个行为说你关注了谁对吧，直接通过买点上报。这个时候的话，在这呢，我们会有一个。这个日志接口服务器。他这个呢，就是直接通过这个埋点上报。那最终啊，可以把这个用户的行为数据啊，直接上报到这个日志接口服务器里面。然后就没有，然后。他不会去操作数据库啊。里面啊，一般上报的都是一些用户的行为。咱们刚才你看服务端日志呢，现在这啊，我们是要调这个接口，最终呢，是要操作数据库的，要把我们这个关注行为。就是谁关注，谁要把这个数据给它保存到数据库里面。</span><br><span class="line"></span><br><span class="line">而这种呢，通过埋点上报呢，其实呢，他就不会和那个数据库打交道。所以说这种话一般称之为是客服端日志。是通过客户端直接上报的，不需要和这个服务端这个接口去交互的。这里记录的日志，我们称之为后端日志。</span><br><span class="line"></span><br><span class="line">那这个服务端日志和客端日志有什么区别吗？为什么再分成两种日志呢？以及说为什么有的地方我们使用客户端日志，为什么有的地方我们使用服务端日志呢。针对我们这个APP里面啊，它这个关注功能服务端记录的这个日志啊，会更加准确。因为服务端接口里面，它会涉及到对于数据库的一个操作里面会有事务。只有这条数据真正保存成功的时候，才会记录日志，如果说你在操作数据库保存失败了。现在你会回滚，这时候就不需要去记录这个操作日志了。你顶多记录一条失败的日志。但是客户端日志，只要用户在APP里面点击了一次关注功能，他就会上报一次日志。最终啊，这个日志接收服务器啊，就会接收到这个日志，并且呢，把它记录下来。他可能呢，会由于网络等原因啊，导致最终关注失败，但是呢，你这条日志。你是发过来了，并且呢，这块啊也记录下来。所以相对来说，服务端数据的准确性是比这个客户端日志这个准确性高的。</span><br><span class="line"></span><br><span class="line">如果说一份数据在服务端日志中和客户端日志中同时都有。那我们肯定要优先选择服务端中的日志数据。一般啊，我们在客户端通过埋点上报的数据啊，都是一些用户行为数据，这些数据啊，就算有一些误差也没有多大影响。它不涉及到一些和数据库交互的一些操作。那在我们这个项目中，针对客户端日志，我们只要获取用户活跃数据。活跃呢表示啊，只要用户每天打开APP就认为用户活跃，用户的这些行为数据呢，会在客户端通过埋点上报管。</span><br><span class="line"></span><br><span class="line">那这些呢，就是我们项目中需要的一些基础数据，主要是这五份儿数据。那刚才我们分析到服务端日志，服务端数据库数据，以及客户端日志，那这个这个图里面啊，其实这样这个呢，就是服务端日志数据。这块呢，就是服务端数据库里面的数据。那这块呢，其实就是客户端的一些日志数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181725293.png" alt="image-20230518172542961"></p><h2 id="模拟产生数据"><a href="#模拟产生数据" class="headerlink" title="模拟产生数据"></a>模拟产生数据</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181741539.png" alt="image-20230518174114092"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251045006.png" alt="image-20230425104534521"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">前面我们分析了具体需要什么数据，以及数据从哪里来，下面我们来看一下如何通过代码模拟产生这些数据。先看这个图。我们通过执行generate date这个项目中的这五个入口类，可以模拟产生这五种数据。就是咱们前面分析的那五种数据。那在这里注意一下，因为我们不能直接使用企业中的真实数据啊，所以在这里我会根据企业中真实数据的格式去模拟生成。最终的效果是没有区别的。</span><br><span class="line"></span><br><span class="line">我们来看一下这些对应的代码。生成服务端数据和客户端数据代码如下：</span><br><span class="line"></span><br><span class="line">【服务端日志】实时粉丝关注数据: GenerateRealTimeFollowData</span><br><span class="line">【服务端日志】视频数据：GenerateVideoInfoData</span><br><span class="line">【服务端数据库】历史粉丝关注数据：GenerateHistoryFollowData</span><br><span class="line">【服务端数据库】主播等级数据：GenerateUserLevelData</span><br><span class="line">【客户端日志】用户活跃数据：GenerateUserActiveData</span><br><span class="line">在执行这些代码的时候还是需要使用之前在微信公众号中获取的校验码</span><br><span class="line"></span><br><span class="line">注意：在执行这些代码之前，我们需要先把基础环境搞定了</span><br><span class="line">这些代码在执行的时候会调用服务端接口和客户端日志接收服务，以及还会向MySQL中写入数据</span><br><span class="line">所以需要把这两个服务部署起来，以及在MySQL中初始化数据库和对应的表。</span><br><span class="line"></span><br><span class="line">所以说我们需要提前把这个两个接口服务，以及mysql中数据库和对的表都给它初始化好，都给它部署起来。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">第一步：</span><br><span class="line">对data_collect项目编译打包</span><br><span class="line"></span><br><span class="line">部署data_collect</span><br><span class="line">将生成的jar包上传到bigdata04机器的&#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;data_collect目录下，如果目录不存在则创建</span><br><span class="line"></span><br><span class="line">[root@bigdata04 soft]# mkdir -p &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;data_collect</span><br><span class="line"># 中间省略了上传jar包的过程</span><br><span class="line">[root@bigdata04 soft]# cd video_recommend&#x2F;data_collect&#x2F;</span><br><span class="line">[root@bigdata04 data_collect]# ll</span><br><span class="line">total 16932</span><br><span class="line">-rw-r--r--. 1 root root 17336051 Aug 29  2020 data_collect-1.0-SNAPSHOT.jar</span><br><span class="line">[root@bigdata04 data_collect]# nohup java -jar data_collect-1.0-SNAPSHOT.jar &amp;</span><br><span class="line"></span><br><span class="line">测试服务是否正常</span><br><span class="line">注意：这个服务监听的端口是8080</span><br><span class="line">[root@bigdata04 data_collect]# jps -ml</span><br><span class="line">1598 data_collect-1.0-SNAPSHOT.jar</span><br><span class="line">[root@bigdata04 data_collect]# curl -XGET &#39;http:&#x2F;&#x2F;localhost:8080&#x2F;v1&#x2F;t1?name&#x3D;test&#39;</span><br><span class="line">&#123;&quot;status&quot;:200,&quot;msg&quot;:&quot;success&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">第二步：</span><br><span class="line">对server_inter项目编译打包</span><br><span class="line"></span><br><span class="line">部署server_inter</span><br><span class="line">将生成的jar包上传到bigdata04机器的&#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;server_inter目录下，如果目录不存在则创建</span><br><span class="line"></span><br><span class="line">[root@bigdata04 data_collect]# mkdir -p &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;server_inter</span><br><span class="line"># 中间省略了上传jar包的过程</span><br><span class="line">[root@bigdata04 data_collect]# cd ..&#x2F;server_inter&#x2F;</span><br><span class="line">[root@bigdata04 server_inter]# ll</span><br><span class="line">total 16932</span><br><span class="line">-rw-r--r--. 1 root root 17336083 Aug 29  2020 server_inter-1.0-SNAPSHOT.jar</span><br><span class="line">[root@bigdata04 server_inter]# nohup java -jar server_inter-1.0-SNAPSHOT.jar &amp;</span><br><span class="line"></span><br><span class="line">测试服务是否正常</span><br><span class="line"></span><br><span class="line">注意：这个服务监听的端口是8081</span><br><span class="line">[root@bigdata04 server_inter]# jps -ml</span><br><span class="line">1673 server_inter-1.0-SNAPSHOT.jar</span><br><span class="line">1598 data_collect-1.0-SNAPSHOT.jar</span><br><span class="line">[root@bigdata04 server_inter]# curl -XGET &#39;http:&#x2F;&#x2F;localhost:8081&#x2F;s1&#x2F;t1?name&#x3D;test&#39;</span><br><span class="line">&#123;&quot;status&quot;:200,&quot;msg&quot;:&quot;success&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第三步：</span><br><span class="line">初始化数据库脚本</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181742580.png" alt="image-20230518174258258"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">接下来执行代码，开始模拟产生数据</span><br><span class="line">1：【服务端日志】实时粉丝关注数据: GenerateRealTimeFollowData</span><br><span class="line">注意：修改服务端接口地址，我是在bigdata04机器上部署的</span><br><span class="line">代码执行之后，可以看到服务端记录的日志数据</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;log&#x2F;</span><br><span class="line">[root@bigdata04 log]# more server_inter.log </span><br><span class="line">&#123;&quot;followuid&quot;:&quot;2002&quot;,&quot;followeruid&quot;:&quot;2001&quot;,&quot;type&quot;:&quot;user_follow&quot;,&quot;times</span><br><span class="line">tamp&quot;:1598684303487,&quot;desc&quot;:&quot;unfollow&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2：【服务端日志】视频数据：GenerateVideoInfoData</span><br><span class="line">注意：修改服务端接口地址，我是在bigdata04机器上部署的</span><br><span class="line">代码执行之后，可以看到服务端记录的日志数据</span><br><span class="line">[root@bigdata04 log]# tail -1 server_inter.log         </span><br><span class="line">&#123;&quot;area&quot;:&quot;A_US&quot;,&quot;watchnumpv&quot;:364,&quot;follower&quot;:364,&quot;hosts&quot;:364,&quot;watchnumuv&quot;:364,&quot;gifter&quot;:364,&quot;nofollower&quot;:364,&quot;length&quot;:464,&quot;rating&quot;:&quot;A&quot;,&quot;smlook&quot;:364,&quot;type&quot;:&quot;video_info&quot;,&quot;gold&quot;:364,&quot;uid&quot;:&quot;2009&quot;,&quot;nickname&quot;:&quot;jack38&quot;,&quot;looktime&quot;:364,&quot;id&quot;:&quot;1769913940296&quot;,&quot;exp&quot;:364,&quot;timestamp&quot;:1769913940000&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3：【服务端数据库】历史粉丝关注数据：GenerateHistoryFollowData</span><br><span class="line">注意：执行这个代码的时候需要注意修改项目中的db.properties中数据库的地址信息</span><br><span class="line">代码执行之后，可以到数据库中看到数据</span><br><span class="line">查看数据库中的follower_00这个表，发现里面有数据了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4：【服务端数据库】主播等级数据：GenerateUserLevelData</span><br><span class="line">注意：执行这个代码的时候需要注意修改项目中的db.properties中数据库的地址信息</span><br><span class="line">代码执行之后，可以到数据库中看到数据</span><br><span class="line">查看数据库中的cl_level_user这个表，发现里面有数据了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">5：【客户端日志】用户活跃数据：GenerateUserActiveData</span><br><span class="line">注意：修改客户端日志接收服务器地址，我是在bigdata04机器上部署的</span><br><span class="line">代码执行之后，可以看到客户端埋点上报的日志数据</span><br><span class="line">[root@bigdata04 log]# head -1 data_collect.log </span><br><span class="line">&#123;&quot;uid&quot;:&quot;1000&quot;,&quot;ver&quot;:&quot;3.6.41&quot;,&quot;countryCode&quot;:&quot;VN&quot;,&quot;ip&quot;:&quot;171.247.0.154&quot;,&quot;UnixtimeStamp&quot;:&quot;1598587868773&quot;,&quot;mcc&quot;:&quot;452&quot;,&quot;type&quot;:&quot;user_active&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">至此，项目中需要的数据都可以正常产生了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来第一步我们先对这个data collect这个项目给他呢编译打包。嗯。CDDB。video。recommend。C的这个data collect。肯定package。港地skip test。好，编译成功。那把它上传到我们这个服务器上面，注意在这呢，我统一都上传到我们这个BD的零四这台机器上面。那接着呢，我统一再建一个目录。每个点啊。V6。recommend。所以说呢，针对这个项目相关的一些架包啊，那些东西全部都放到这个目录里面，这样方便管理维护啊。那我们进到这个目录下面，注意现在里面呢，我在创建一个目录。叫data collect。那这里面呢，就放我们这个data这个接口，这个夹包。打开这个图形化界面。把我们刚才生成那个架包给他传上去。it soft。在这。把这个家包传上来。好，那下面呢，我们把它启动起来。Java杠架。后面呢，直接指定那个另一个加包就行了，注意我们需要把它放到后台运行，所以说呢，前面加个no ho。后面呢，加个and符，这样就可以了。GPS一下。看到没有，这个架已经启动了，你可以这样GPS杠ML。这样看起来更加清晰，对吧，就这个data collect。那下面呢，我们来测试一下这个服务啊是否正常，因为它是一个接口服务。这里面呢，我们其实开放的有一个测试的一个接口。看到没有？你直接调用这个就行，它是一个测试结果，当然你在这需要传一个参数啊，它需要提出一个参数name。他会把它打印出来，这样可以确认一下你这个服务是否正常啊。这个怎么验证呢？它是一个HTTP请求，你可以选择在我们的浏览器里面去操作，或者说呢，我们直接在这个控制台里面也是可以的，通过curl这个命令杠X。盖着操作啊。HTTP冒号双斜线logo。冒号，注意它这个端口呢，是8080。我在这指定了。看到没有8080啊。VT对吧。name。等于test吧。你看状态是200是OK的啊，说明我那个服务呢是正常启动了。那接下来第二步，我们把这个server这个项目呢，给它编译打包。有这个项目。他是负责写收我们那个服务日志的。嗯。clean港。开始。好，编译成功。把它传上来。注意，那我们在这需要创建一个目录啊。你可以在这直接对吧，右键这样创建也是可以的啊都可以。serve。因此。嗯。好，上传成功。那这个呢，我们要把它启动一下。no Hu Java刚加。安福。嗯。先确认这个服务是不是在啊在吗。好，这个呢，我们也来验证一下。大家注意它这个接口名称呢，就不是这个，你可以到这儿来确认一下。这是S1T1对吧，以及呢，它的端口注意。因为我们是在同一台服务器上启动的，所以说呢，这两个项目啊，它监听的端口肯定是不能重复的啊，这个我给它改成80812。四上这个是8081。后面的是S1T1对吧。嗯。好，OK啊，还是这个size。好，那这样的话，这两个接口服务呢，就搞定了，这个以这个都部署好了。那接着第三步，我们需要把这个MY面这个数据库啊，还有表啊，给它触发好，这个呢，给大家提供了有这个触发脚本也比较方便啊。现在这右键。进行一个so文件。直接把这个脚本。指另外就行了。就是in my circle tables啊。这个脚本。好，执行成功。在这刷新一下。好，它呢，会产生这么些表啊。OK，这个后面我们具体用的时候再来具体查看。那接下来呢，我们就需要去执行这些代码了。这样的话呢，就可以模拟产生数据了。首先呢，我们先产生这个实时粉丝关注数据。就这个服务端呢。啊，服务端日志。模拟生成实时粉丝关注和取消关注数据。注意了，大家在下面呀，在执行我这个代码的时候，需要注意，你们需要改点东西。注意在上面，你看这个是调用我线上这个接口来获取这个模拟数据，获取到之后呢，注意。下面呢，会调用我们本地刚才部署的那个接口服务。因为这个呢是服务端日志，所以说它会调那个服务端接口。我呢是在BD的零四这台机箱部署的。对吧，这个服装接口，它这个对应的端口号是8081。对吧。后面这些东西呢，都不用改，你们下去改的话，主要改一下这个对吧，你在哪一台机器上去部署的这个什么server这个接入服务，那你在这就把那个IP或者主机名写到这就可以了。所以接着的话，就可以模拟产生这个服务端的这个调用请求，最终呢，去调用这个服务端这个接口，让这个接口呢记录日志。那你说这个接口，它具体把日志记录到哪个目录下面呢？注意，在这也能看懂啊。在resources这个目录下面有一个log back点查明。看到没有，我把这个日呢记到这个Di这个母下面了。然后他这块呢，具体的一个日志文件名称是server下换in加log。那对应的这个对collect看了也有。它日志呢，也是放到这个电log这个目录下面。然后它的日文名称呢，叫data collect。所以说这个到时候可以通过这个文件名就知道到底是哪个接口服务记录的日志啊，也好分析。那我们这个代码呢，都是OK的，以及这些接口这些东西呢，我们也不需要改，对吧，也都是OK的，下面呢，我们来执行一下。注意这个代码呢，它是实时产生数据，现在呢，你在这执行一次，它会产生一条数据，就是模拟那个实时产生啊。看到没有接口调用成功，注意这块是我在这记录的日志啊。借口立项成功，它最终产生的日是这个。好，那我们来看一下，我们具体去调这个服务端接口，这个服务端接口有没有把这个日给记录下来了，我们来确认一下。在这重新克隆一个绘画。对，log。到这个木下面。看到没有？这个里面其实已经有值了，对吧，我们可以在这摸看一下啊。没问题吧，他已经记录下来了，说明我们这个流程目前是通的啊。这个呢，就是服务端的实时粉丝关注数据。那接下来往下面看。这个generate video for。这个也是服务端日志对吧，是模拟生成视频相关的数据。就是你这个主播开播结束之后呢，其实它就会调用服务端接口。把当前这个直播的相关的一些信息上报过去啊。那注意你这个在用的时候也是一样的流程，对吧，这个上面还是就我那个接口。那下面的话呢，你需要在这对应改一下对吧。好，那我在这来执行一下。注意这个呢，一次性我可以采用多条数据啊。你看接口交换成功是OK的。那下面注意我们来确认一下数据。因为这时候它里面产生数据比较多，我就使用T杠一吧，查看里面最新的一条数据。对吧。也怪了。OK，所以说这个呢，是服务端的这两种类型的数据都采集过来了。往下面看。那接着呢，我们来看一下这个服务端数据库里面的数据。首先呢，是这个。就是这个历史，粉丝关注数据。注意。这个电板执行之后呢，它会把这个数据啊，初始化到我们那个数据库里面啊。注意，你在执行这个代码之前，你需要先执行我这个脚本。进行这个数据库和表的一个初始化。初始化之后的话就可以直接执行了，这里面别的不需要改动了，注意你要改点东西。看到没有，要改一下数据库啊。你的一个数据库在哪，把那个IP地址啊，以数据库啊，一个名称啊，对吧，用户名啊，密码啊对吧，这些东西都改一下就可以了。来，我们来执行一下。从那个里面你可以看出来，他最终把数据啊，写到这个FOLLOW00这个表里面。好，我们到这来看一下。好，数据呢，都写进来了。这些测数据啊。这个呢，其实就是历史的一个粉丝关注出去了。那接下来注意我们来看这个主播等级的。也是这个服务端数据库里面数据模拟生成主播等级数据啊。它也是操作数据库。那这里面的话，他会把数据写到这个level user这个表里面来执行一下。OK，我们来确认一下。好，数据都写进了。那这样的话，相当于服务端数据库里面这两份数据呢，我们也都给他触发好了。那其实还剩一份数据，就是这个客端数据，就是那个用户活跃的。gena use active。对吧，客端是模拟生成用户活跃数据。这个呢，还是通过调用我这个接口获取数据。那下面注意这时候呢，它会调用我们那个客户端的那个日志收集服务器，对吧，其实就是一个client。他们最终会调用他，把数据上报给他。所以你的用处呢，也要改成这个，看一下你这个data client，你部署到哪个机器，把这个改一下就可以了。来执行一下。好，这个data可爱呢，他接收到用户上报的这些数据之后呢，他呢也会把它记录日志记录到本地。嗯。嗯。在这他又起到这个did collect点里面。我们取头一条还杠一。看到没有，就这个这个类型是user active，看到没有，都是接格式的啊。那到此为止呢，项目中需要的数据呢，我们都可以正常产生了。</span><br></pre></td></tr></table></figure><h3 id="客服端日志接收服务"><a href="#客服端日志接收服务" class="headerlink" title="客服端日志接收服务"></a>客服端日志接收服务</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251102183.png" alt="image-20230425110236188"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251055893.png" alt="image-20230425105526920"></p><h3 id="服务端http接口"><a href="#服务端http接口" class="headerlink" title="服务端http接口"></a>服务端http接口</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251101505.png" alt="image-20230425110135498"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251101273.png" alt="image-20230425110105056"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251104599.png" alt="image-20230425110444470"></p><h3 id="实时粉丝关注"><a href="#实时粉丝关注" class="headerlink" title="实时粉丝关注"></a>实时粉丝关注</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251136630.png" alt="image-20230425111454589"></p><h3 id="视频数据"><a href="#视频数据" class="headerlink" title="视频数据"></a>视频数据</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251136684.png" alt="image-20230425111654573"></p><h3 id="历史粉丝关注"><a href="#历史粉丝关注" class="headerlink" title="历史粉丝关注"></a>历史粉丝关注</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251136744.png" alt="image-20230425112531665"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251125314.png" alt="image-20230425112548087"></p><h3 id="历史主播等级"><a href="#历史主播等级" class="headerlink" title="历史主播等级"></a>历史主播等级</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251130414.png" alt="image-20230425113019519"></p><h3 id="客户端用户活跃日志"><a href="#客户端用户活跃日志" class="headerlink" title="客户端用户活跃日志"></a>客户端用户活跃日志</h3><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251133562.png" alt="image-20230425113331494" style="zoom:67%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251133869.png" alt="image-20230425113349659"></p><h2 id="数据采集聚合"><a href="#数据采集聚合" class="headerlink" title="数据采集聚合"></a>数据采集聚合</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251159150.png" alt="image-20230425115912855"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">咱们前面把数据产生好了，下面就可以进行数据采集了，首先呢是使用这个filebeat呢，将所有日志数据采集到kafka的一个topic中中，把这个日志数据采集到kafka里面。日志数据有三份，服务端有两份，客户端日志有一份。注意，在这具体执行之前，我们需要先把这个zookeeper以及kafka这两个服务给它启动起来。</span><br><span class="line"></span><br><span class="line">启动zk</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[root@bigdata02 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[root@bigdata03 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">启动kafka</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">好，那接下来呢，我们需要在kafka中呢，去创建一些topic。我们主要创建哪些topic呢？就根据我们前面啊，在那个架构图里面分析的，首先第一个是一个大的topic，它里面呢，包含所有的这种日数据。all_type_data_r2p40，因为它是一个大topic里面数据量比较大，所以说呢，我们给它多分一些分区，后期的话呢，就可以提高你一个消费能力了。这个里面呢，存储所有采集过来的日志数据。</span><br><span class="line">我们后面其实还有一个数据分发层，分发层的话，相当于把这里面的数据啊，这个它分发出来。我们服务端有两种数据，一个呢是这个实时的一个粉丝的关注和取消关注数据。</span><br><span class="line">这个它具体那个类型的名称呢，叫user_follow。这个topic这个名称后面为什么不加这种后缀了呀。这是因为啊，这个数据它相当于是我们的原始json数据里面的某一个自动的值，就那个type自动的值，这个日志啊，是之前记录下来的，所以后期我们再用的话，就把它直接拿过来用就行了，就不要再去改它这个值了，改起来就比较麻烦了。并且呢，你这个对应起来也不太好对应，所以说呢，我们就使用那个原始那个数据里面那个type值作为这个topic贝这个名称。</span><br><span class="line"></span><br><span class="line">那下面还有一个video_info。这个里面存储视频信息</span><br><span class="line"></span><br><span class="line">还有一个客户端的。user_active就是用户的活跃数据。存储客户端上报的用户活跃数据。</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 40 --replication-factor 2 --topic all_type_data_r2p40</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic user_follow</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic video_info</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic user_active</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic default_r2p5</span><br><span class="line"></span><br><span class="line">注意后面还有一个default_r2p5，所以这个topic是干什么呢？注意现在啊，我们要把这个里面的数据给它分发到这几个topic里面。那在分发的时候有可能有一些数据啊，它里面没有type字段呀，或者说那个type字段的值不是这三个里面的。也就是说那个数据异常，这样的话，你需要把那个异常数据放到这个topic。这里面的存储无法具体分配到对应topic的异常数据。</span><br><span class="line"></span><br><span class="line">所以说呢，我们下面就需要去创建这些topic。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251200726.png" alt="image-20230425120032836"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们呢，就可以使用filebeat的去采集数据了，注意现在filebeat呢，我们还没有安装呢。需要安装到哪些机器上面呢？注意你要看你之前那个客户端的那个接口，以及服务端的接口，你都部署到哪些服务器上面，我在这呢，都给它部署到这个bigdata04上面了。因为filebeat呢，要采集data_collect这个接口记录的日志，以及server_inter这个接口记录的日志，所以说呢，你后期这两个接口你部署到哪台机器上面，那么你在对应的机器上就需要部署这个filebeat去采集数据。</span><br><span class="line"></span><br><span class="line">解压一下</span><br><span class="line">注意这个filebeat就是一个采集工具。所以说呢，使用起来很简单，我们只需要在里面指定输入和输出就可以。我们先修改它的配置文件，在这个目录下面有一个这个filebeat.yml这个配置文件</span><br><span class="line"></span><br><span class="line">注意它是一个YML格式的，不是那种XML这种格式的啊，这是种新型的那种配置文件格式。</span><br><span class="line"></span><br><span class="line">你可以在这配置它的输入。你看它这有一个什么呀，这个类型是日志的，但是目前这个呢没有开启。我们在这呢，想先测试一下。在测试的时候呢，我就在这个配置文件里面，把这个输入啊，给它指定成标准的一个输入，就是键盘输入，输出的话呢，就把它指定成控制台。这样分析起来好分析，也比较直观。那怎么加呢？注意其实我们在这里面需要加一个输入，就是配置一个输入。对吧，你就按照它这个格式去写就行了，冒号注意后面一个空格，注意这个空格是不能少的。找那个输出。没有output。输出注意你看它默认呢，现在开启了一个什么呀，这个输出呢，是把数据输出的这个elasticserch，那现在我们先不用这个，你把它注掉。就指定那个console，注意下面呢，我们来加一些参数，注意下面这个缩进，我能不能使用那个tab，不行，必须使用空格。你可以使用两个或者四个。pretty。注意这个表示什么意思啊，它表示啊，会把你这个输出的结果啊，给你格式化一下，要不然是非常乱的啊。大家后期啊，在改这个配置文件的时候一定要注意啊，首先是这个冒号后面这个空格不能少，还有呢，就是说你这个在做缩进的时候不能使用制表符，要使用空格，官方建议使用四个，你用两个啊三个啊都可以啊。那下面呢，我们需要把这个配置好的这个配置文件呢，给它上传上去行吗？保存一下。如果说你对这个文件熟悉了之后，你可以直接在这里面去改都是可以的。不熟悉的话，建议呢，你还是把这个配送件拿到本地去，改完之后呢再传上来。直接覆盖就行啊。那这样的话就可以啊，下面我们就可以启动filebeat</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251209159.png" alt="image-20230425120937212"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251211306.png" alt="image-20230425121132023"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251212200.png" alt="image-20230425121239236"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 filebeat-7.4.2-linux-x86_64]# .&#x2F;filebeat -c filebeat.yml</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251218213.png" alt="image-20230425121824126"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输出了一堆东西。虽然是一堆啊，还是有格式的，是一个阶层格式的，注意这个呢，就是因为我们在这加了这个属性，等于true，它呢会对你返回的数据呢，你做一个格式化。反馈的数据，你看就这个message里面内容，其他内容呢，都是它里面默认加的啊。接着你想停掉的话，直接CTRLC对吧。</span><br><span class="line"></span><br><span class="line">接下来我们需要继续修改配置文件，因为我们是希望filebeat的可以采集日志文件中的数据，将数据呢采集到kafka中。把刚才我们那个配置啊都给它删掉。然后呢，我们把这个基于日志的这个类型啊，给它开启了。这样的话它就可以读取文件了，你看path里面指定一些路径，注意你可以指定一个或者多个都可以。你指定一个的话，你可以这样，你指定一个目录下面可以使用那种通配符也是OK的啊。所以说的话，它可以监控多个目录里面的多个文件。这是输入就配置好了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251222076.png" alt="image-20230425122235895"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">接下来我们需要配置输出。这是一个输出组件，注意在这我们需要指定那个kafka的一个输出组件，那在这都需要指定什么配置呢？这个信息呢，到他那个官网上面是可以找到的啊，所以在边我就直接拿过来，你看output.kafka，把数据呢，输入到卡夫卡里面，直行卡卡的一个博物块地址对吧。123。还有这个topic，我们要把这个数据呢写到这里面。下面这几个配置你不用改就行了，这个呢表示啊，它往这个topic里面这些分区写数据的一个规则。这个表示卡里面那个艾机制是吧。这个呢是就是往里面写数据的时候，对这个数据15度压缩，这块呢，使用那个机会压缩可以提高性能。这个呢，表示呢，每一条数据最大的一个字节数量。好这样就可以了，你们下用的时候呢，有可能要改这以及这对吧，如果说你都和我的一样的话，其实都不用改。OK，这样的话就可以了，下面呢，把这个配置文件重新再传一下。</span><br><span class="line"></span><br><span class="line">这样就可以实现将服务端日志和客户端日志全部都采集到kafka中的all_type_data_r2p40这个topic中了。</span><br><span class="line"></span><br><span class="line">注意：filebeat需要部署在所有服务端接口机器和客户端日志收集机器上，</span><br><span class="line"></span><br><span class="line">在实际工作中，服务端接口机器会有多个，我们当时的服务端接口机器有100多台，客户端日志收集机器是有6台，在部署的时候是通过运维同学开发的部署工具批量部署的，要不然一个一个部署会疯的。</span><br><span class="line">在这由于服务端接口和客户单日志收集服务都在bigdata04上面，所以我就只需要在bigdata04上部署一套即可。</span><br><span class="line"></span><br><span class="line">在这里我们暂时先不启动filebeat进程，等我们把采集模块中所有的配置全部修改好了以后，从后面挨个开始启动进程，这样不会漏数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304251225756.png" alt="image-20230425122506901"></p><h2 id="数据分发"><a href="#数据分发" class="headerlink" title="数据分发"></a>数据分发</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下数据分发层，就是使用实现对采集到kafka指定topic里面的数据啊进行分发。咱们前面呢，把这个日数据采集过来啊，全部都放到kafka一个大的topic里面了，所以说下面呢，我们需要使用flume对这个大的topic里面数据啊进行分发，分发到一些小的topic里面，这样可以方便使用。注意，那这样的话，我们就需要在这个flume里面增加一配置文件，就是从那个大的topic里面消费数据，通过拦截器获取数据中的一个type自段的值作为输出kafka的一个topic的名称。这个配置文件呢，在这儿我已经写好了，我们来直接看一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#source+channle+sink的名字</span><br><span class="line">agent.sources &#x3D; kafkaSource</span><br><span class="line">agent.channels &#x3D; fileChannl</span><br><span class="line">agent.sinks &#x3D; kafkaSink</span><br><span class="line"></span><br><span class="line"># 指定source使用的channel</span><br><span class="line">agent.sources.kafkaSource.channels &#x3D; fileChannl</span><br><span class="line"># 指定sink需要使用的channel的</span><br><span class="line">agent.sinks.kafkaSink.channel &#x3D; fileChannl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-------- kafkaSource相关配置-----------------</span><br><span class="line">agent.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line">agent.sources.kafkaSource.batchSize &#x3D; 1000</span><br><span class="line">agent.sources.kafkaSource.batchDurationMillis &#x3D; 1000</span><br><span class="line">agent.sources.kafkaSource.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line">agent.sources.kafkaSource.kafka.topics &#x3D; all_type_data_r2p40</span><br><span class="line">agent.sources.kafkaSource.kafka.consumer.group.id &#x3D; flume_con_id_1</span><br><span class="line"></span><br><span class="line">#----------------- 拦截器 -------------------</span><br><span class="line"># 定义拦截器</span><br><span class="line">agent.sources.kafkaSource.interceptors &#x3D; i2 i1</span><br><span class="line"># 设置拦截器类型</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.type &#x3D; regex_extractor</span><br><span class="line"># 设置正则表达式，匹配指定的数据，这样设置会在数据的header中增加topic&#x3D;aaa</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.regex &#x3D; &quot;type&quot;:&quot;(\\w+)&quot;</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.serializers &#x3D; s1</span><br><span class="line">agent.sources.kafkaSource.interceptors.i1.serializers.s1.name &#x3D; topic</span><br><span class="line"># 避免遇到数据中没有type字段的，给这些数据赋一个默认topic【注意：这个拦截器必须设置】</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.type &#x3D; static</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.key &#x3D; topic</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.preserveExisting &#x3D; false</span><br><span class="line">agent.sources.kafkaSource.interceptors.i2.value &#x3D; default_r2p5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#------- fileChannel相关配置-------------------------</span><br><span class="line"># channel类型</span><br><span class="line">agent.channels.fileChannl.type &#x3D; file</span><br><span class="line">agent.channels.fileChannl.checkpointDir &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;all_type_data&#x2F;checkpoint</span><br><span class="line">agent.channels.kafka2HdfsShow.dataDirs &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;all_type_data&#x2F;data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#---------kafkaSink 相关配置------------------</span><br><span class="line">agent.sinks.kafkaSink.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent.sinks.kafkaSink.kafka.topic &#x3D; default</span><br><span class="line">agent.sinks.kafkaSink.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line">agent.sinks.kafkaSink.kafka.flumeBatchSize &#x3D; 1</span><br><span class="line">agent.sinks.kafkaSink.kafka.producer.acks &#x3D; 1</span><br><span class="line">agent.sinks.kafkaSink.kafka.producer.linger.ms &#x3D; 1</span><br><span class="line">agent.sinks.kafkaSink.kafka.producer.compression.type &#x3D; snappy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来就把它呢传上去就可以了，注意如果说我们需要在一台机器中启动多个flume进程的时候，最好在里面复制多个conf目录，因为如果在一个conf目录中启动多个agent的进程的话，多个agent的进程的日志信息会混到一块，后期排查问题会很麻烦啊，这个我们之前呢讲过了对吧啊，所以说呢，在这。进到这个。都门面。我们来复制一个。打靠谱。告不告？卡不卡？高不高相以我们从卡夫卡里面读出去，再把数据写到卡夫卡里面。进到里面。改一下他这个log的这个配置文件。把那个改一下，搞搞不搞搞卡不卡。好，这样的话就可以了。嗯。OK，那接下来我们在这里面来创建这个配置文件，就是刚才我们分析的这个。敢不敢杠后杠？敢不敢点。com？嗯。我们就组织一下。好，这样就可以了，注意这块搞定之后呢，我们就需要往下面走了，在这啊，我们也先不启动啊，那我们把这个数据落盘这块也搞定之后呢，从后往前启动。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252144970.png" alt="image-20230425214444152"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252145763.png" alt="image-20230425214458087"></p><h2 id="数据落盘"><a href="#数据落盘" class="headerlink" title="数据落盘"></a>数据落盘</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252156385.png" alt="image-20230425215638800"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下这个数据落盘层。我们需要使用采集指定topic各种数据进行落盘，便于离线计算。你看咱们前面呢，把这个数据呢，全部都采集到kafka这个大的topic里面，接下来做了一个分发，分发之后后面我们就要做这个落盘了。把我们需要落盘的数据呢，给它呢，落到这个HDFS上面，注意如果这里面这个大topic里面的所有类型的数据呢，我们都需要落盘的话，我们就可以直接读取这个大topic里面的所有数据，然后呢，使用这个拦截器。根据数据类型把它们呢保存到hdfs的多个目录中，这样呢比较方便，不过在我们这个项目中，这个大的topic里面一共有三种类型的数据，其中呢，有两种数据我们是需要进行落盘后期进行离线计算的。有一种数据呢是不需要的，只有实时计算上讲会用的，所以说我们可以使用两个flume agent来对我们需要的数据执行落盘操作。当然了，我们也可以啊，只使用一个flume agent的，那你这里面啊，也可以使用拦截器，只获取我们需要落盘的那两种数据，这样也是可以的，这个呢，给大家留一个作业，大家下去自己研究一下。使用flume拦截器如何呢？获取我们需要的两种数据，然后呢，分别把它呢落盘到hdfs的不同目录下面。</span><br><span class="line"></span><br><span class="line">那我在这呢，就使用两个agent来实现了，我们需要落盘的这两个topic呀，分别是这个user_active和这个video_info。所以说呢，在这我还需要复制两个目录。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252211650.png" alt="image-20230425221141206"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那我们进去去改一下那个logo的配置。那接下来我们还需要在这两个目录里面创建对应的一个配置文件，这两个配置文件啊，我也提前写好了，我们来看一下。</span><br><span class="line"></span><br><span class="line">因为这个东西啊，它就是一个体力活。咱们前面用过很多次了啊，没有什么技术含量。首先看这个user active，这都没什么好处了。S呢，是一个卡夫卡S，你读取这个user active。那个group使用这个be China。注意，下面是这个H里边的think。按T分目录存储就行了，放到对目录下面一个user。这就可以。那这个呢，是一个video。对吧，它读取的是一个video in这个topic。然后呢，就是说放到这个data木下面有个video。对吧，这是那个年月日。看天分，母乳好。那我在这呢，把这个复制一下。这是一个user active。脸卡不卡？到as。到。user。active。com。好，这个搞定。下一个先进到这个目录里面这个音符，把这个呢复制一下。VI、卡夫卡杠杠、video info。好。好，那这两个呢，我们都配置好了。所以接下来我们需要先确认一下这个含毒不集群啊，是否启动。对吧，我们之前已经起来了啊。好，那到此为止啊，这个采集相关的这个配置呢，我们都修改好了，下面呢，我们就要来启动一下。我们要从后往前启动，所以说呢，我们先启动这个数据落盘的这个辅助。我们来启动一下。诺哈普。b my name。agent康复指定配置文件跟目录。这个是com user active先求这个。进行配置文件刚刚。靠谱。在康复，然后呢。有点active下面有一个。啊，不搞搞HS user active com这个文件。agent。好，接下来呢，是第二个，我把前面这些能敷的呢复制过来。这个呢是video。刚刚看。六。下面有一个卡杠，杠HS杠video杠infor.com。最后是这个大纲内。agents。嗯。确认一下。这是一个，这是一个对吧，这两个呢都起来，嗯。这就是这两个数据落盘的一个a的进程啊，那接下来再往回推，我们需要把那个数据分发那个给它起下来。说。NG到康复。到卡夫卡，到卡夫卡。刚刚康复杠。这个木下面有一个。卡夫卡，卡夫卡点。com。我们之前少写一个这个F的这个强迫症，我给他改一下啊。来确认一下，确实少一个F，这个也不影响啊，只不过看起来有点别扭。好。这就可以了。怎么呢，重新来启动的啊，刚才也没启动成功啊no。b agent。等到。卡夫卡，卡夫卡，然后呢，刚刚。哎。卡夫卡，卡夫卡下面有一个卡夫卡，卡夫卡加。祷告，name isn&#39;。好。可以确认一下啊。对吧，这个也启动好。那最后呢，我们就要启动那个Bob的采集程了。接着来启动。牛逼了，杠C。比点两秒。注意啊，针对这个feel进程啊，正式启动的时候也需要使用这个no ho，把它放到后台运行在这里，我们为了一会儿使用方便，所以说我先在前台启动了，对吧。我们按一个回车把它启动起来，那接下来呢，我们就可以启动生成数据的程序了。注意在这呢，我们先执行那个generate real time for这个实时生成那个粉丝关注和取消关注的数据啊。来执行一下。</span><br></pre></td></tr></table></figure><h3 id="kafka-hdfs-user-active-conf"><a href="#kafka-hdfs-user-active-conf" class="headerlink" title="kafka-hdfs-user-active.conf"></a>kafka-hdfs-user-active.conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#source+channel+sink的名字</span><br><span class="line">agent.sources &#x3D; kafkaSource</span><br><span class="line">agent.channels &#x3D; kafka2Hdfs</span><br><span class="line">agent.sinks &#x3D; hdfsSink</span><br><span class="line"></span><br><span class="line"># 指定source使用的channel名字</span><br><span class="line">agent.sources.kafkaSource.channels &#x3D; kafka2Hdfs</span><br><span class="line"># 指定sink需要使用的channel的名字</span><br><span class="line">agent.sinks.hdfsSink.channel &#x3D; kafka2Hdfs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-------- kafkaSource相关配置-----------------</span><br><span class="line"># 定义消息源类型</span><br><span class="line">agent.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 定义kafka所在zk的地址</span><br><span class="line">agent.sources.kafkaSource.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line"># 配置消费的kafka topic</span><br><span class="line">agent.sources.kafkaSource.kafka.topics &#x3D; user_active</span><br><span class="line"># 配置消费者组的id</span><br><span class="line">agent.sources.kafkaSource.kafka.consumer.group.id &#x3D; user_active_con_1</span><br><span class="line"></span><br><span class="line">#------- fileChannel-1相关配置-------------------------</span><br><span class="line"># channel类型</span><br><span class="line">agent.channels.kafka2Hdfs.type &#x3D; file</span><br><span class="line">agent.channels.kafka2Hdfs.checkpointDir &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;user_active&#x2F;checkpoint</span><br><span class="line">agent.channels.kafka2Hdfs.dataDirs &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;user_active&#x2F;data</span><br><span class="line"></span><br><span class="line">#---------hdfsSink 相关配置------------------</span><br><span class="line">agent.sinks.hdfsSink.type &#x3D; hdfs</span><br><span class="line"># 注意, 我们输出到下面一个子文件夹datax中</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;user_active&#x2F;%Y%m%d</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat &#x3D; Text</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType &#x3D; DataStream</span><br><span class="line">agent.sinks.hdfsSink.hdfs.callTimeout &#x3D; 3600000</span><br><span class="line"></span><br><span class="line">#当文件大小为104857600字节时，将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize &#x3D; 104857600</span><br><span class="line">#events数据达到该数量的时候，将临时文件滚动成目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount &#x3D; 0</span><br><span class="line">#每隔N s将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval &#x3D; 3600</span><br><span class="line"></span><br><span class="line">#配置前缀和后缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix&#x3D;run</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileSuffix&#x3D;.data</span><br></pre></td></tr></table></figure><h3 id="kafka-hdfs-video-info-conf"><a href="#kafka-hdfs-video-info-conf" class="headerlink" title="kafka-hdfs-video-info.conf"></a>kafka-hdfs-video-info.conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#source+channel+sink的名字</span><br><span class="line">agent.sources &#x3D; kafkaSource</span><br><span class="line">agent.channels &#x3D; kafka2Hdfs</span><br><span class="line">agent.sinks &#x3D; hdfsSink</span><br><span class="line"></span><br><span class="line"># 指定source使用的channel名字</span><br><span class="line">agent.sources.kafkaSource.channels &#x3D; kafka2Hdfs</span><br><span class="line"># 指定sink需要使用的channel的名字</span><br><span class="line">agent.sinks.hdfsSink.channel &#x3D; kafka2Hdfs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#-------- kafkaSource相关配置-----------------</span><br><span class="line"># 定义消息源类型</span><br><span class="line">agent.sources.kafkaSource.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 定义kafka所在zk的地址</span><br><span class="line">agent.sources.kafkaSource.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03:9092</span><br><span class="line"># 配置消费的kafka topic</span><br><span class="line">agent.sources.kafkaSource.kafka.topics &#x3D; video_info</span><br><span class="line"># 配置消费者组的id</span><br><span class="line">agent.sources.kafkaSource.kafka.consumer.group.id &#x3D; video_info_con_1</span><br><span class="line"></span><br><span class="line">#------- fileChannel-1相关配置-------------------------</span><br><span class="line"># channel类型</span><br><span class="line">agent.channels.kafka2Hdfs.type &#x3D; file</span><br><span class="line">agent.channels.kafka2Hdfs.checkpointDir &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;video_info&#x2F;checkpoint</span><br><span class="line">agent.channels.kafka2Hdfs.dataDirs &#x3D; &#x2F;data&#x2F;filechannle_data&#x2F;video_info&#x2F;data</span><br><span class="line"></span><br><span class="line">#---------hdfsSink 相关配置------------------</span><br><span class="line">agent.sinks.hdfsSink.type &#x3D; hdfs</span><br><span class="line"># 注意, 我们输出到下面一个子文件夹datax中</span><br><span class="line">agent.sinks.hdfsSink.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;video_info&#x2F;%Y%m%d</span><br><span class="line">agent.sinks.hdfsSink.hdfs.writeFormat &#x3D; Text</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileType &#x3D; DataStream</span><br><span class="line">agent.sinks.hdfsSink.hdfs.callTimeout &#x3D; 3600000</span><br><span class="line"></span><br><span class="line">#当文件大小为104857600字节时，将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollSize &#x3D; 104857600</span><br><span class="line">#events数据达到该数量的时候，将临时文件滚动成目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollCount &#x3D; 0</span><br><span class="line">#每隔N s将临时文件滚动成一个目标文件</span><br><span class="line">agent.sinks.hdfsSink.hdfs.rollInterval &#x3D; 3600</span><br><span class="line"></span><br><span class="line">#配置前缀和后缀</span><br><span class="line">agent.sinks.hdfsSink.hdfs.filePrefix&#x3D;run</span><br><span class="line">agent.sinks.hdfsSink.hdfs.fileSuffix&#x3D;.data</span><br></pre></td></tr></table></figure><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后将hadoop集群启动起来，因为数据落盘会使用到HDFS</span><br></pre></td></tr></table></figure><h3 id="数据落盘-1"><a href="#数据落盘-1" class="headerlink" title="数据落盘"></a>数据落盘</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">好 到这为止，采集需要的配置都修改好了</span><br><span class="line">下面我们就来启动一下</span><br><span class="line">1：先启动数据落盘的flume</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252219426.png" alt="image-20230425221947341"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252220669.png" alt="image-20230425222021013"></p><h3 id="数据分发-1"><a href="#数据分发-1" class="headerlink" title="数据分发"></a>数据分发</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2：再启动数据分发的flume</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252225085.png" alt="image-20230425222459145"></p><h3 id="数据采集聚合-1"><a href="#数据采集聚合-1" class="headerlink" title="数据采集聚合"></a>数据采集聚合</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3：最后启动filebeat采集进程</span><br><span class="line"></span><br><span class="line">注意：针对filebeat进程，正式启动的时候也需要使用nohup 放在后台运行，在这里我们为了一会使用方便，所以先在前台启动。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252227824.png" alt="image-20230425222700964"></p><h3 id="执行数据生成程序"><a href="#执行数据生成程序" class="headerlink" title="执行数据生成程序"></a>执行数据生成程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">4：接下来启动生成数据的程序</span><br><span class="line">先执行GenerateRealTimeFollowData</span><br><span class="line">验证效果</span><br><span class="line">查看all_type_data_r2p40这个topic中的数据</span><br><span class="line"></span><br><span class="line">验证</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252230600.png" alt="image-20230425223043250"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252231054.png" alt="image-20230425223117407"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以执行成功了，那我们来验证一下结果。我们到那个卡夫卡里面来消费一下。基于console的。注意这个内容还是比较多的，但是我们可以大致看一下啊，你看。这是一条数据，是不是很乱呀？我们的日志数据没有这么乱吧，并且我们的日志数据里面也没有这个东西。对吧，我们的日志数据里面其实是这些东西在这，你看它其实啊，其实在这面又封装了一层，他把我这个具体的业务数据啊，给封装到这个message字段里面。注意这些字段的话，相对来讲是filebeat默认生成的，那这些字段呢，它不是我们需要的，我们希望啊，只记录我们的原始日志即可。那怎么解决这个问题呢？可以解决啊，我们需要在那个filebeat那个配置文件里面加一个配置。需要在filebeat中的output.kafka里面增加codec.format配置。在这相当于啊，你要对它输出这个数据啊，做一个格式化。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252235554.png" alt="image-20230425223504841"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CD format。做一个格式化。四六类形的。对。我们呢，只取它里面的这个东西，哎，问号首先呢，括括号中括号我们只取里面那个message。所以这个配置的意思呢，就是说我们从这个数据里面。你看它这个采集过来数据里面有很多字段，我们只需要message字段里面内容，其实这里面就是我们的原始的日志数据。然后把这个filebeat再停一下。你再来启动，我们再来执行这个生成数据的这个。看到没有？这个就正常了，这样就可以</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252237920.png" alt="image-20230425223717162"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们再来查看一下这个user_follow这个topic，这个看一看它里面能不能消费到数据，如果能消费到数据，就说明那个flume的数据分发过程是没有问题的啊</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252240353.png" alt="image-20230425224013711"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">收到了吧。注意你在这能收到就说明啊，我们中间那个flume的分发程序是OK的，他从那个大的topic里面把这个数据读出来，然后呢写到这里，这样就可以好。那接下来我们再来执行两个程序。一个是这个active。其实呢，我们之前已经执行过了，在这我们再重新执行一下啊。然后重新呢，再生成一批数据，就是往那个日志文件里面再写一批啊，因为你调用接口，这个接口就会记录了，以及这个video info。好，这个执行成功之后呀，注意我们就可以到那个HDPS里面去确认一下结果数据了。因为你这个执行之后啊，他呢，这个接口就会把那个日志记录到本地。这个接口呢，就会把那个日志数据啊，记录到这个本地的日文件里面，然后filebeat呢，发现你里面啊有新增数据，所以说他就把这个数据给读出来，读出来之后呢，把它呢，采集到那个大的topic，就是all type data那个topic里面。然后呢，这里面有数据之后，后面那个的数据分发程序，它就会读取这个数据，对吧，对这个数据做分发，然后呢，把这个数据分发到不同的那个子topic里面，那我们最后呢，还有一个flume数据落盘层。他呢，就会从那个子topic里面把那些数据呢读出来，最终呢落到hdfs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以下面我们来验证一下。看到没有，这个user active，对这个都是有的。好，这个目录来我们直接查看一下它里面的数据啊。使用管道来取，前一条取一条就行，好里面有数据。那说明是OK的，那其实对应的我可以直接把这个改一下，改成video。看一下他们的数据。也是有了。没有问题，好。那在这我们都可以获取到数据，那就说明了是没有问题的，这就意味着我们前面的整个数据采集流程是通的。那大家在下面做实验的时候呀，我估计啊，可能会遇到各种各样的问题啊，就是大家在操作这一块的时候，如果发现最终啊，看不到我们这个希望的结果。那你就需要一步一步去排查，你要确认一个数据到了哪一步。你先看一下那个大套贝里面有没有数据，然后呢，再看一下那个子套贝壳里面有没有数，如果子套贝格里面也有数据，那最终hps里面没数据，那肯定是你那个数据落盘程序有问题。所以说你需要一点点去分析啊。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252308990.png" alt="image-20230425230808125"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252309870.png" alt="image-20230425230907753"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252309790.png" alt="image-20230425230920558"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对这个filebeat呀，我再多说一点，这个filebeat它在采集日文件中的数据的时候呢，它会将日文件数据的采集的一个偏移量啊，记录到本地文件里。所以说它在这会记录一下，这样话你filebeat给重启之后，它呢会读取这个文件，然后呢，根据你上一次记录的off继续往下面消费。它可以保证，就算你那个filebeat要停了，那在它停的中间，你往那个日里面记录数据，它后期启动之后还可以把它呢采集过来是这样。注意了，如果说呢，我们想要让这个filebeat的重启之后啊，继续重新开始消费。这样怎么办呢？暴力一点的话，我们可以直接把这个。这个data目录啊，给它删掉，你删掉之后这些信息是不是就没了，没了之后它就认为它是一个新文件，就从头开始读了啊，这个需要注意一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252316813.png" alt="image-20230425231605565"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">还有一点就是我们在使用filebeat的时候啊，如果发现filebeat没有正常工作，这个时候呢，我们需要去查看filebeat的日志文件，来排查具体是什么问题，因为有时候有一些错误信息啊，他不会暴露到这个工作台上面，它会记录到日志文件里面。像few b的下面有一个move。看到没有，它下面有这个这文件，注意你看的话就看这个，它后面这个相当于是一些备份的啊。你直接看这个就行。这是最新的一些日志。这些呢，是之前的一些老的日志。如果说他有一些错误信息在这里面就可以看。你在这儿可能看不到，它不会暴露到这个地方啊，你说呢，我把这个飞票启动了，也没见他报错呀，结果呢，他也没有把数据采集到我的卡夫卡里面，那所以说你就需要来这儿来看啊，看那个日文面。好，那针对服务端日志和这个客端日志的一个数据采集啊，我们先讲到这儿，在这呢，大家主要掌握数据采集的整理思路，重点是里面那个数据聚合，数据分发这两个步骤，那个数据落盘呢，倒没什么特殊的，对吧，咱们之前已经用过很多次。</span><br></pre></td></tr></table></figure><h2 id="采集服务端数据库数据"><a href="#采集服务端数据库数据" class="headerlink" title="采集服务端数据库数据"></a>采集服务端数据库数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面呢，我们把这个服务端日志以及客户端日志呢采集过来，下面呢，我们还需要将服务端数据库的数据也采集过来。咱们前面分析了啊，由于历史粉丝关注数据呢，只需要导出一次，所以说呢，没必要使用sqoop，那还有一份数据呢。是那个每日的主播等级数据对吧？所以这份数据呢，我们在最开始的时候会将数据库里面的全量数据导出来一份，后期只需要根据表中的更新字段获取发生了变化的数据即可，这样每天需要导出的数据量就很小了，属于增量采集。这时候呢，可以选择使用sqoop，其实呢也可以选择使用shell脚本，sqoop的使用在上一个项目中我们已经用过了，所以接着呢，我们来讲点不一样的，我来使用shell脚本，将mysql中的数据导出来，所以说么，这两份数据我全部呢使用mysql脚本。把它倒出来。那首先呢，我们使用这个mysql里面那个-e这个命令，将这个具体的查询命令啊准备好。</span><br><span class="line"></span><br><span class="line">注意我其实在这呢，可以直接操作我Windows本地的那个MYSQL，这个里面它默上是有那个MYSQL客户端的，你如果没有的话，你就装一下啊，我们使用mysql -e后面的话就可以写具体的sql了。因为你现在也不是连本地的马，你是连其他机器的马，对吧。我们在这个list里面连接我们Windows里边那个马。那我们Windows的那个机器这个IP是也有2.168.182.1。这样就可以，这也可以编了哈。那这里面写了一个参考，我们先写那个历史粉丝关注数据啊。再来呢，它里面有两列，一个UID，一个呢是UID。from我们这个库的名称啊，是一个点。follow。零零，我们先查这一张表。看到没有，它是可以执行的。那以及呢，我们还要查询这个每日主播等级数据啊，就每天发生了变化的那些主播数据。注意它里面的话字段比较多。注意它里面有一个update time，就是更新字段啊。就这个，如果说这里面这个数据发生了变化。某一个字段被改了，那么这个就会变化，所以说我们每天呢，就根据它去抽取数据。up time。大于等于。所以我们这里面这个日期啊，目前的话看一下。那是2月1号对吧。26年2月1号，所以说我在里面这样来，我就把这些数据给它查出来，二六杠零二杠零幺。000000对吧。只要凌晨开始按着。update time。小于等于。2026杠零二杠零一。23:59:59，这样的话就可以把这些数据全都查出来。没问题吧，也是可以的啊。好，那这两条命令啊</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">历史粉丝关注数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252328190.png" alt="image-20230425232805742"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252328755.png" alt="image-20230425232851743"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252350277.png" alt="image-20230425235033202"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每日主播等级数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252351285.png" alt="image-20230425235117301"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252351462.png" alt="image-20230425235132504"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">验证成功之后呢，我们就需要把这个命令啊公布到脚本里面，首先是这个用户历史关注数据，注意这份数据啊。它其实呢是分表存取。你看我在这其实只是创建了一部分表，它有很多张。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252352125.png" alt="image-20230425235203139"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个分表逻辑是什么样子的，你看它里面存储的是什么呀，是一个用户的一个UID啊。所以说这样的，它会根据这个用户的UID来计算一个MD5值。你这个MD5值是什么样子的呢？给他搜一下。对吧，你看它的MD5值其实就是这样。就是类似于这么一串。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304252354258.png" alt="image-20230425235426902"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我们呢，会根据这个用户的UID去计算一个MD5值，然后呢获取最后两位字符，然后呢拼接到这个follow后面。组装一个新的表名。这样话呢，他会算一下你那个UID最后两位是什么，然后呢，存到对应的表里面。这是它一个分表逻辑啊，那你后期查的话呢，也会根据这个UID计算MD5获取最后两位，然后呢来前面呢拼那个follow下划线，后面的话拼上了两位，这样就找到它对应的一个表。</span><br><span class="line"></span><br><span class="line">注意这种组合呀。这两位你看它有可能是零到九以及a到z中的任意一个，所以说呢，这位它呢有36种情况，这一位呢也是36种情况，你26个英文字母加上零到九有十个，一共是36，每一位都三十六三十六乘以36是吧。1296张表。所以说呢，我们其实在实际过程中，我们需要将这1296张表中的数据全部都导出来，所以我在这呢，也没有建那么多张表，太多了啊，所以说在这建了一部分，在这大家知道它是一个分表的就行啊。然后呢，你要知道它这个分表规则。OK。这样的话，一会我们采集数据，我就先采集一张就行，但是呢，我会写脚本，让他可以支持把这个1296张表中的数据全部都采集出来。我们把那个脚本写好，但是具体采的时候，我们就只采这一张表就行啊，因为就它里面有数据。</span><br></pre></td></tr></table></figure><h3 id="extractFollower-sh"><a href="#extractFollower-sh" class="headerlink" title="extractFollower.sh"></a>extractFollower.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">好，下面我们来开发第一个脚本。我现在来写。简单号。它保存一下啊，给它起个名字叫attracted。要抽取follow。是脚本。只需要执行一次即可。你只需要抽取自己的是历史数据，后期的话我们还有一份实时数据，就可以实时维护了。针对1296张表。需要使用这个双层或循环。动态生成表明。I。12345678。所以后面还有呢a。FGH。I z KL mn pqr。ST。UVWXYZ，好吧，这题又是个直接画啊度。这是一层，你里面还要套一层呢。因为我们要批那个表明的最后两个字符嘛，对吧。G。这个我就不是文桥了。50块。好。那现在里面了，我来艾打印一下。就把那个表明啊给他批出来。哎。当然，这行吧。其实你在这儿只要能把这1000多张表的那个表明全都拼出来，那继续把它导出去，那不就很简单了吗？对吧？我先在这呢，先写一个导师的一个脚本。马杠u杠P杠H。18291。藏意后面是circle。这个UIDUID。from。video。零零。注意这样的话就可以获取那些数据了，然后我把这个数据呢，给它重定向。到这个里面这个soft。video recommend。就使用这个表名作为文件里面的前缀log，这样的话其实就可以把这张表数据给它导出来了。你如果把这个东西放到这儿。好把重要啊，我们一会执行不执行，你把里面改一下是不是就行了，你这个东西。还有那个。这东西，然后那个是不是可以了呀。你只要说这个循环执行完，其实就可以把所有数据全都倒出来。我在这通过IO把这个表名导一下就行，行吗。最后的话呢，我们只是把这一个标点数据给它打开就行。好，那接下来呢。先试一下啊，重新在这儿来建一个脚本。video&#39;。到了。OK，这样就可以了，那接下来我们来执行SH。看到没有，前面都打印出来了，没问题啊。那我们来确认一下，这个木下面有没有产生那个零零.log。产生了吧。看一下里面的数据。没问题吧，没问题啊好，</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 此脚本只需要执行一次即可</span><br><span class="line"></span><br><span class="line"># 针对1296张表，需要使用双层for循环动态生成表名</span><br><span class="line">for i in 0 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o p q r s t u v w x y z</span><br><span class="line">do</span><br><span class="line"> for j in 0 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o p q r s t u v w x y z</span><br><span class="line"> do</span><br><span class="line">echo follower_$&#123;i&#125;$&#123;j&#125;</span><br><span class="line">#mysql -uroot -padmin -h 192.168.182.1 -e &quot;select fuid,uid from video.follower_$&#123;i&#125;$&#123;j&#125;&quot; &gt;&gt; &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;follower_$&#123;i&#125;$&#123;j&#125;.log</span><br><span class="line"> done</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">mysql -uroot -padmin -h 192.168.182.1 -e &quot;select fuid,uid from video.follower_00&quot; &gt;&gt; &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;follower_00.log</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260002090.png" alt="image-20230426000249498"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260003322.png" alt="image-20230426000303163"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260003085.png" alt="image-20230426000343050"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260004849.png" alt="image-20230426000403681"></p><h3 id="extractUserLevel-sh"><a href="#extractUserLevel-sh" class="headerlink" title="extractUserLevel.sh"></a>extractUserLevel.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line"># 此脚本每天执行一次，添加到crontab或者Azkaban调度器中[每天0:30分开始执行]</span><br><span class="line"></span><br><span class="line"># 正常情况下获取昨天的数据，如果需要补数据，可以直接指定日期</span><br><span class="line">if [ &quot;X$1&quot; &#x3D;&#x3D; &quot;X&quot; ]</span><br><span class="line">then</span><br><span class="line">yesterday&#x3D;&#96;date --date&#x3D;&quot;1 days ago&quot; +%Y-%m-%d&#96;</span><br><span class="line">else</span><br><span class="line">yesterday&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">mysql -uroot -padmin -h 192.168.182.1 -e &quot;select * from video.cl_level_user where update_time &gt;&#x3D;&#39;$&#123;yesterday&#125; 00:00:00&#39; and update_time &lt;&#x3D; &#39;$&#123;yesterday&#125; 23:59:59&#39;&quot; &gt;&gt; &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;cl_level_user_$&#123;yesterday&#125;.log</span><br><span class="line"></span><br><span class="line"># 将数据上传到hdfs上，每天一个目录</span><br><span class="line"></span><br><span class="line"># 先在hdfs上创建日期目录</span><br><span class="line">hdfs dfs -mkdir -p &#x2F;data&#x2F;cl_level_user&#x2F;$&#123;yesterday&#x2F;&#x2F;-&#x2F;&#125;</span><br><span class="line"></span><br><span class="line"># 上传</span><br><span class="line">hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;video_recommend&#x2F;cl_level_user_$&#123;yesterday&#125;.log &#x2F;data&#x2F;cl_level_user&#x2F;$&#123;yesterday&#x2F;&#x2F;-&#x2F;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来开发第二个脚本。这个也是抽取数据了。他呢是抽取了一个user level，有这个主播等级的啊。注意在这个脚本中啊，我们需要将数据上传到hdfs上面，并且呢，这个脚本啊，也需要添加到Crontab或者azakaban那个调度器中，每天凌晨00:30了，然后执行一次。就是每天呢抽取一次啊。</span><br><span class="line"></span><br><span class="line">那下面我们写一下，就是正常情况下。每天凌晨获取昨天的数据。你如果呢，你需要补数据的话。可以直接指定日期。所以这个的话，我来获取一个日期。呃，一。等等a。那说明道一为空。then。还有这个yesterday。等于。加个反引号。刚刚date。days ago，一天以前就是昨天嘛，对吧，加号百分号Y杠百分号M，杠百分号D。这就过去昨天日期了。else，如果说呢不相等，那说明了多少？有值，有值的话呢？二。这样就快。那下面的话其实就可以把我们那个circle语句给它拿回来，那个搜有点长，我在这呢。到这儿拿一下。我们在前面是不是写过呀。对，这里面写这个星号就可以啊，没问题。这边需要改一下。把那个日期拼上。注意你说我在这成这个单引号，它不是不解析吗？大家注意外面还有一层双引号，咱们之前讲过对吧，双引号里面这个单引号这个会解析这个变量啊。我反过来又不行。那最后呢，我们把这个日志数据呢，给它重进现到这个目下面，在这我可以把前面这个复制一下是吧过来。后面的话呢，其实就是这个。表明了。使它来拼接一个热面。注意其在后面呢，你最好拼一个日期对吧，因为他每天都有一次。填了一个日期。点到这就可以。注意，那你到这还没完，我们还需要把这个日上传到HDS上面。杨淑玉上传。的X元。每天一个目录。所以你在这呢，你先在hps上创建日期目录，因为每天一个目录嘛，对吧。CS杠那个地可以加个杠P对吧。it。后面呢，我们就使用这个level user。然后后面把这个复制过来。大家注意这个日期格式呢，中间是带这个横杠的年月日，我们其实想获取这个不带横杠的。但是由于这他确实需要带横杠的，那我们这又不想要怎么办呢？那你就改一下呗，是吧。把横杠给替换掉就行，对吧，全部横杠替换成空，这样的话就是这个年月日中间不带任何分隔符了。这个用法咱们前面也讲过吧。还没上班。高foot。那就是这。把它呢，上传到这个目录下。这一块。</span><br><span class="line"></span><br><span class="line">好，注意，我这个脚本其实就是一个增量脚本，咱们前面说了，你在最开始的时候，其实啊，你还需要将这个表里面的之前的全量数据也给它导一次。那个脚本我这里先不写。我直接写了一个每日的增量脚本啊，大家要知道这个事情。因为我们现在这里面这个数据其实都是认为是一些增量数据啊，我直接。使用这个日期过滤，就可以把里面所有东西全都过滤出来。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260033394.png" alt="image-20230426003329063"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来确认一下。杠，我们来查一下这个date。小level user。有吧？再来查一下这个。没问题吧，有数据啊，那我们来看一下里面内容呗。对吧，这就是我们采集过来的内容。</span><br><span class="line"></span><br><span class="line">但是你在这需要注意点，它是有一个表头的。所以说呢，我们后期啊，在处理这个数据的时候，需要把这些数据给它归掉就行。那到此为止，服务端数据库中的数据我们就采集完毕了，那在这里面啊，大家需要熟悉我们的数据来源，以及数据最终的存储位置。后面我们的计算程序呢，都需要依赖于这些数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260034352.png" alt="image-20230426003427969"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304260034803.png" alt="image-20230426003455794"></p><h2 id="数据计算核心指标详细分析"><a href="#数据计算核心指标详细分析" class="headerlink" title="数据计算核心指标详细分析"></a>数据计算核心指标详细分析</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261043614.png" alt="image-20230426104322881"></p><h3 id="第一个任务"><a href="#第一个任务" class="headerlink" title="第一个任务"></a>第一个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大家好，前面我们把项目需要的数据都采集过来了，下面我们开始基于这些数据开发数据计算模块。由于我们需要计算的指标比较多，所以呢，我们先对需要计算的这些核心指标呢进行详细分析。来看一下这张图。我们从左往右分析，首先看左边第一列，这里面表示的是计算程序的数据源，其实就是咱们前面分析的那五种数据。服务端日，客端日加上服务端数据库数据。</span><br><span class="line"></span><br><span class="line">那既往右边看。注意，右边这里面列出来的都是具体的计算任务。那我们首先呢，来看一下第一个任务。它表示的是历史粉丝关注数据的初始化，这个任务负责把服务端数据库中的历史粉丝关注数据批量初始化到neo4j。这个任务呀，只需要运行一次即可，后面就不需要了，它属于一个临时的任务，因为后续的粉丝关注数据就通过第二个任务来实施维护了。</span><br></pre></td></tr></table></figure><h3 id="第二个任务"><a href="#第二个任务" class="headerlink" title="第二个任务"></a>第二个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们来看一下第二个任务。这个任务呢，是实时维护follow和UN follow关系，follow呢就是表示关注，UN follow表示取消关注。这个任务是需要一直运行的，他会实时维护粉丝和主播的关注情况。它的数据来源是kafka中的实时粉丝关注数据，这个是通过我们的日志采集程序采集过来的。那通过前面这两个任务呀，就可以将我们全平台的粉丝关注数据进行全量维护了。第一个任务负责历史粉丝关注数据的迁移，第二个任务负责实时维护，这样在neo4j中就维护了全量的粉丝关注数据。</span><br></pre></td></tr></table></figure><h3 id="第三个任务"><a href="#第三个任务" class="headerlink" title="第三个任务"></a>第三个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下第三个任务。他需要每天定时更新用户最新活跃时间。这个任务的主要作用呢，是为了维护所有用户的活跃时间。因为最终我们在统计三度关系的时候，如果是针对全平台所有的用户都计算，这样是没有意义的。为什么呢？我们来画一个图，再来分析一下这个三个关系的一个流程。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305201757733.png" alt="image-20230520175733543"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这个呢，是一个主播，主播一吧。好，这个主播呢，他有一些粉丝。这些粉丝是不是还可能会关注了其他主播呀？所以说在这呢，我们再画几个主播。你看这时候如果说有一个用户。当这个用户呢，你看他去关注这个主播一的时候。其实呢，我们需要把它对应的一个三度关系推荐给这个用户，对吧，其实就是这块这些主播对吧。它们两个之间的一个关系，它到它是两个关系，再到它是三种关系好。注意这里面这个主播以及粉丝呢，其实都属于用户，只是说呢，他的角色不一样吧。对于我们平台而言，所有的主播以及粉丝都属于用户。</span><br><span class="line"></span><br><span class="line">那我们在计算三的关系的时候，针对这里面这个主播和粉丝最好呀，都是最近活跃过的。如果说这些粉丝很长时间都不回来了。那么我们认为他的这些关注数据啊，这个参考价值就没那么大了。如果这些主播呀，也是很长时间都不活跃了，那么更不应该进行计算。因为最终假设把它计算出来的话，就会把它推荐给用户。那用户关注他了之后呢，就想去看一下他最近的开播视频。结果发现这个哥们呢，很久都没开播了，那你这种推荐就没有任何意义了，其实就浪费了一次机会对吧。</span><br><span class="line"></span><br><span class="line">所以后期啊，我们在计算三度关系的时候，会对这里面所有的用户，不管你是粉丝还是主播，都会对他的一个活跃时间进行过滤。当时我们在实际工作中使用的那个时间，是根据最近一周活跃过的用户来计算三种关系。就说你这里面这些主播以及粉丝都要是最近一周活跃过来，如果没有活跃过，我在计算三种关系的时候，就不把你们计算在内，直接把你刨除掉。OK，那这个就是第三个任务。这个任务呢，它是每天执行，因此每天凌晨根据昨天的主活数据来维护每个用户的活跃时间。当我们后期计算三个关系的时候，会根据这个条件过滤出来满足条件的用户。</span><br></pre></td></tr></table></figure><h3 id="第四个任务"><a href="#第四个任务" class="headerlink" title="第四个任务"></a>第四个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">好，接下来我们来看一下第四个任务。这个任务啊，主要是在neo4j中维护主播的最新等级信息，因为我们在计算三度关系的时候，还需要对最终计算出来的主播等级进行过滤。如果主播等级太低，那就没必要推荐了。这个任务呢，也是一个离线任务，每天执行一次即可，它的数据呢来源于hdfs，这个数据是我们通过脚本每天定时从服务端数据库中找出来的。</span><br></pre></td></tr></table></figure><h3 id="第五个任务"><a href="#第五个任务" class="headerlink" title="第五个任务"></a>第五个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下第五个任务，这个任务呢是每周一计算一次，每周一会到HDPS上面获取最近一个月的视频数据。计算最近一个月内视频评级满足3B加或者是2a加的主播。注意视频等级呢，主要有sabcd这五个等级，S是评级最高的，D是最差的。这里面的3B＋表示最近一个月至少要有三次开播，并且最近三次开播视频的评级需要是B级以上，包含B级。那这里面的2a＋表示最近一个月至少要有两次开播，并且最近两次开播视频的评级需要是a级以上，包含a级。这两个指标，一个是考虑主播的开播频次比较高，但是呢，视频的评级不是特别高，这样也是可以推荐的，相当于它是一种勤劳的主播。还有一种呢，是主播开播频次比较低，但是呢视频评级特别高，比较优秀，这种呢也可以推荐啊。所以说你只需要满足3B加或者2a加都是可以的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">那最终啊，我们把满足条件的主播计算出来，然后呢到neo4j中进行维护。给主播增加一个flag属性，如果主播最近的开播数据满足3B加或者是2A加，则把这个flag属性置为一。后面在计算三度关系的时候呢，会对主播的这个属性进行过滤，不满足条件呢就不推荐了。那到这为止，前面这五个任务啊，都是对这个neo4j它里面这个数据进行维护的。其中第一个任务呢是初始化数据的是执行一次。</span><br><span class="line"></span><br><span class="line">因此。第二个任务呢，是一个实时任务，需要一直执行，其实这个任务呢，也可以改为离线的，一天执行一次，我们当时主要考虑的是这样的neo4j中的那个粉丝关注数据啊，我们会有其他业务部门也在使用。所以说呢，我们就让这个任务实施维护了。那第三个和第四个任务呢，是每天只一次。第五个任务，本来啊，也考虑每天执行一次，但是呢，每天都计算最近一个月的数据，这样太频繁了，浪费计算资源，所以呢，我们就改为了每周一计算一次。因为我们最终的三个关系数据也是一周计算一次。所以这里面啊，这些离线任务，它的一个计算周期，其实最迟都是一周。所以说第三个和第四个任务其实也可以每周一计算一次。不过当时呢，是因为这些指标，其他业务部门在使用用户率的时候呢，也会用到，所以说呢，我们就每天执行一次。</span><br><span class="line"></span><br><span class="line">那我在这呢，说这么多呀，主要是为了让大家明白，为什么有的任务需要实时，为什么有的任务需要离线，以及离线的任务为什么还要分为一天进行一次和一周之间一次。这些都是由一些具体的业务环境而导致的。</span><br><span class="line"></span><br><span class="line">那到现在为止，你neo4j中的数据是这样。假设我们这里面创建的是一个user这个节点，对吧，这个节点的话，它俩具备以下这些属性。UID。以及呢，它里面有一个时间桌，代表它的一个活跃时间。就是最近的一个活跃时间。以及呢，有一个等级叫level对吧，主播等级。还有一个flag。表示啊，你这个主播最近这一个月，你这个视频平移是否满足3B加或者2a加，如果满足它的值就是一，不满足就是零了。</span><br><span class="line">好，所以说呢，经过前面这五个任务在neo4j里面，我维护的数据其实就是这样。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261105667.png" alt="image-20230426110524589"></p><h3 id="第六个任务"><a href="#第六个任务" class="headerlink" title="第六个任务"></a>第六个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下第六个任务。这个任务呢，是计算满足条件的主播的三度关系推荐列表。在计算的时候会根据前面几个任务的指标作为过滤条件。首先第一点在这里。他俩需要过滤出来最近一周里活跃过的。第二点呢，需要过滤出来主播等级大于四级的。第三点再过滤出来，最近一个月开播视频，满足这个3B加或者2a加这个条件。第四点，再过滤出来推荐列表中粉丝列表关注重合度大于二的。那根据这四点过滤之后呀，最终计算的三个关系列表数据才是我们需要的。</span><br><span class="line"></span><br><span class="line">第六个任务，计算好的数据会先存储到hdfs上面，这个任务是每周一计算一次。因为这个三种关系出现，数据的更新力度没有必要细化到每天。如果每天都计算一次，最终的结果数据变化也不大，并且每天都计算一次，对计算资源的要求也比较高，所以当时我们定的是每周计算一次。更新一次最新的三度关系列表数据。</span><br></pre></td></tr></table></figure><h3 id="第七个任务"><a href="#第七个任务" class="headerlink" title="第七个任务"></a>第七个任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据计算出来以后，会通过第七个任务把这个数据导出到MYSQ里面。这样就可以通过MYSQL对外提供数据了。这就是数据计算模块中的详细计算流程。下面呢，我把这个数据计算步骤啊做了一个汇总，我们来看一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261109573.png" alt="image-20230426110912322"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261109688.png" alt="image-20230426110948070"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261111274.png" alt="image-20230426111106202"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一步，历史粉丝关注，数据初始化。第二步，实时维护粉丝关注数据。第三步，每天定时更新主播等级。第四步，每天定时更新用户活跃时间。第五步，每周一计算最近一个月主播视频评级信息。第六步，每周一计算最近一周内主活主播的三组关系列表。第七步，三种关系列表数据导出到马，这个就是具体的我们的计算步骤。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261111021.png" alt="image-20230426111116924"></p><h2 id="数据计算之历史粉丝关注数据初始化-第一个任务"><a href="#数据计算之历史粉丝关注数据初始化-第一个任务" class="headerlink" title="数据计算之历史粉丝关注数据初始化(第一个任务)"></a>数据计算之历史粉丝关注数据初始化(第一个任务)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们首先来看一下数据计算中的第一步。历史粉丝关注出于初始化。咱们前面分析过啊，历史粉丝关的数据呢，来源于服务端数据库，并且呢，这些数据在存储的时候还是分表存储，一共1296张。这个表里的数据格式呢是一样的。fuid、uid，还有一个time stamp。这个fuid呢，表示关注者，其实呢就是粉丝。这个UID表示被关注者的UID，其实就是主播。这份数据呢，在前面开发数据采集模块的时候，我们已经生成过。那接下来我们就要把这个数据啊，初始化到neo4j中。那我们需要把之前生成的这个文件呢，上传到neo4j的一个import下面才可以使用。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261117713.png" alt="image-20230426111700471"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们之前导出的这个粉丝关注数据在这就这个文件，那我们需要把这个文件给它复制到import目下面。嗯了，好，这样就可以了。那下面呢，我们在初始化这个数据之前啊，我们把那个neo4j重新初始化一下，就把里面数据全给它删掉。因为之前里面有一些特殊数据。把那个data目录删掉。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261118389.png" alt="image-20230426111836337"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261118328.png" alt="image-20230426111852239"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">停一下。嗯。在其中。好，这样的话就可以了，注意因为你在这把那个data目录删了之后啊，你再重新启动之后呢，我们需要去修改一下密码。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261119828.png" alt="image-20230426111933953"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们需要到这儿来初始化数据了。我们在这先连到这个bin&#x2F;cypher-shell这里面。bin&#x2F;cypher-shell -a bolt:&#x2F;&#x2F;bigdata04:7687 -u neo4j -p admin。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261122789.png" alt="image-20230426112214786"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">好，那这里面呢，我们首先要针对这个关键字段建立索引，对吧。咱们前面是不是已经做过这种操作呀。所以下面呢，我们还需要执行一个这个。好，执行成功，那我们到界面上来确认一下数据。好，是可以的。</span><br><span class="line">neo4j&gt; CREATE CONSTRAINT ON (user:User) ASSERT user.uid IS UNIQUE;</span><br><span class="line">0 rows available after 281 ms, consumed after another 0 ms</span><br><span class="line">Added 1 constraints</span><br><span class="line">然后批量导入数据</span><br><span class="line">neo4j&gt; USING PERIODIC COMMIT 1000</span><br><span class="line">       LOAD CSV WITH HEADERS FROM &#39;file:&#x2F;&#x2F;&#x2F;follower_00.log&#39; AS line FIELDTERMINATOR &#39;\t&#39;</span><br><span class="line">       MERGE (viewer:User &#123; uid: toString(line.fuid)&#125;)</span><br><span class="line">       MERGE (anchor:User &#123; uid: toString(line.uid)&#125;)</span><br><span class="line">       MERGE (viewer)-[:follow]-&gt;(anchor);</span><br><span class="line">0 rows available after 791 ms, consumed after another 0 ms</span><br><span class="line">Added 11 nodes, Created 17 relationships, Set 11 properties, Added 11 labels</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261127580.png" alt="image-20230426112713190"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304261133414.png" alt="image-20230426113352894"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意，我们这儿呢，只初始化一个数据文件。那你说真的，我们实际工作中啊，我们有1000多个这种数据文件，你需要让我在这执行这个命令，执行1000多次吗？这肯定是不合理的。那如何一次性批量导入呢？可以这样来做啊。我们可以把这多个文件合并成一个大文件，对吧，你在Linux命行里面，你看这个文件，然后呢，通过这个右箭头&gt;&gt;重定向。把这个1000多个文件里面那个数据啊，整合到一个大文件里，这就可以了。好，那这样的话呢，针对数据计算的第一步初始化数据，这个我们就搞定了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v1.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十八周 直播平台三度关系推荐v1.0-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1.html</id>
    <published>2023-04-24T07:37:23.000Z</published>
    <updated>2023-05-25T03:23:53.809Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十八周-直播平台三度关系推荐v1-0-1"><a href="#第十八周-直播平台三度关系推荐v1-0-1" class="headerlink" title="第十八周 直播平台三度关系推荐v1.0-1"></a>第十八周 直播平台三度关系推荐v1.0-1</h1><h2 id="项目"><a href="#项目" class="headerlink" title="项目"></a>项目</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面我们开始正式学习直播平台三度关系推荐系统这个项目，这个项目分为1.0和2.0这两个版本。本周我们先学习1.0这个版本。首先我们来看一下项目效果。大家呢，可以在这里面扫码体验。这个就是我们直播平台的首页，当我们点击某一个主播，会进入到主播的详情页，我们在点击这个follow关注按钮的时候。这里面呢，会插入一个模块，它里面显示的是关注了此主播的人，也关注了哪些主播。这就是三度关系推进的效果。这页面上看起来只是把数据展现出来，很简单，但是具体这些数据是怎么来的，如何保证推荐的主播也是用户感兴趣的，这才是我们这个项目的核心内容。下面我们来看一下针对这个项目官方一点的介绍。</span><br><span class="line"></span><br><span class="line">在直播平台中，用户在主播页面关注该主播时。粉丝状态栏下方插入三度关系推荐模块，显示该主播的粉丝同时又关注了哪些主播。按照推荐重合度且满足一定的筛选条件进行择优展示，这样推荐的主播才是用户最可能喜欢的，可以帮助用户发现更多他喜欢的主播，促进用户活跃，进而挖掘用户消费潜力。这就是我们这个项目最终想要达到的效果。</span><br><span class="line"></span><br><span class="line">想要实现我们前面所说的三种关系推荐是需要由数据来支撑的，那么这些数据从哪里来呢？</span><br><span class="line">这就涉及到我们的第一块内容，数据采集。我们需要将项目中需要用的所有数据全部采集过来，包括离线数据和实时数据。</span><br><span class="line"></span><br><span class="line">这些数据采集过来以后，就需要涉及第二块内容了，数据存储，离线数据一般存储到分布式文件系统中，实时数据一般存储到消息队列中，数据存储起来以后，就需要涉及到数据计算了。</span><br><span class="line"></span><br><span class="line">数据计算模块，对前面存储起来的数据进行计算，分为离线计算和实时计算，计算之后的结果数据还会进行存储。</span><br><span class="line"></span><br><span class="line">那计算出来结果之后呢，就会涉及到数据展示了。将数据在页面中展示，查看最终的一个推荐效果。所以这个项目中的四大模块，它们之间的关系就是这样。在这里我们先从整体上对这个项目进行一个划分，后面还会有更详细的划分。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241546289.png" alt="image-20230424154642727"></p><h2 id="技术选型之数据采集"><a href="#技术选型之数据采集" class="headerlink" title="技术选型之数据采集"></a>技术选型之数据采集</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241549395.png" alt="image-20230424154922323"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">练好。接下来我们先针对这四个模块分析一下他们里面是需要用到的具体的技术框架，俗称技术选型。我们首先来看一下数据采集工具的选择。针对日志数据，目前业内常用的采集工具有下面这些。</span><br><span class="line"></span><br><span class="line">首先是这个apache开源的这个顶级项目flume。还有这个elastic公司开源logstash以及filebeat。那在这呢，我们把这个kafka也列出来了，但是kafka并不算是日志采集工具。只是说呢，它一般会和采集工具一块使用，所以呢，在这就一块列出。以及呢，这个sqoop组件。</span><br><span class="line"></span><br><span class="line">下面呢，我们来详细分析一下这个flume、logstash，还有filebeat这几个日志采集工具。</span><br><span class="line">首先呢，是flume。如果它是基于Java语言实现的。flume主要由source、channel、sink这三个组件组成。针对这三个组件中提供了很多实现。针对source，有基于文件的，基于socket的，基于kafka的等等，还有很多我们常用的数据源，flume几乎都提供了支持。这个channel提供了有常见的基于文件的，基于内存的。这个sink有基于hdfs的，基于kafka的等等，还有很多我们常用的存储系统几乎都提供了支持。就算是部分特殊的source和sink，flume没有提供支持，那么也没有关系，flume允许我们自定义这些组件。由于它也是基于Java的，所以开发这些自定义的组件也没有多大问题，我们都是Java程序员对吧？所以目前啊，在企业中，针对日志数据采集这一块，flume占据了主要地位。</span><br><span class="line"></span><br><span class="line">那接下来我们来看一下logstash。那是基于Jruby实现的。Jruby是ruby语言的Java实现。logstash的架构有点类似于flume，主要由输入、输出和过滤组成。这里的输入和输出类似于中的S和。那也提供了很多输入和输出的组件支持。常见的数据源和存储系统也都是支持的。并且带中还提供了强大的过滤功能，可以将采集到的数据进行一些处理之后再写出去。flume中的拦截器也可以实现类似的功能。logstash可以和elasticsearch、kibana轻松的实现一个日志收集检索、展现平台，非常方便。俗称ELK全家桶。</span><br><span class="line"></span><br><span class="line">那其实分析到这儿，我们会发现flume和那logstash还是非常相似的。但是呢，他们两个的典型应用场景是有一些区别的。logstash常用的场景是帮助运维人员采集服务器自身的运行日志，方便运维人员排查服务器的问题。这种场景下，对于数据的完整性和安全性要求不是特别高。因为logstash内部没有一个持久化的队列，所以在异常情况下是可能出现数据丢失的问题。而flume内部呢，是有自己的ack机制来去确保这个问题。所以说呢，它可以用于一些更重要的业务日志台词。</span><br><span class="line"></span><br><span class="line">那接下来我们再来看一下这个filebeat采集工具。是采用这个go语言开发的。它只支持文件数据采集。它可以将文件中的数据采集到，kafka，ES等等这些常见的存储系统。它会记录文件采集到的offset信息。就算filebeat的采集进程挂掉，也不会导致数据丢失，它下一次重新启动之后，还会延续之前的offset，继续往下面采。并且呢，这个filebeat呢，它还是一个轻量级的采集工具。咱们前面分析的这个flume，还有这个logstash，它们都是一些重量级的产品。在某些特定场景下，这个轻量级的组件可能会更加合适。filebeat和logstash同属于elastic这个公司，这些公司呢，提供了很多的这种Beat组件。filebeat的只是其中一个。因为我们的数据采集呢，主要是基于文件的，所以在这呢，我就只分析了这个filebeat。</span><br><span class="line"></span><br><span class="line">那到目前为止，这三个采集工具啊，我们都分析了一遍。logstash、flume属于重量级的组件。它们都是基于JVM虚拟机运行的。filebeat的呢是一个轻量级的组件，它是基于go语言。从语言层面来分析。go语言开发的程序，性能消耗是比那个基于JVM虚拟机运行的程序要小的。并且我们也在实际的服务器上进行了测试。相同数据规模下，filebeat的内存和CPU消耗是flume和logstash低的。</span><br><span class="line"></span><br><span class="line">那我们就直接选择filebeat了吗？并不是因为它的优点是性能消耗低，但是它的功能是有点弱的。所以我们在实际过程中会这样做。在前端业务机器上部署Filebeat的，将日志数据采集到消息队列里面。因为这个时候的要求是尽可能少的占用服务器资源，保证服务器上面的其他业务正常运行。数据到消息队列以后，后面我们可能还需要对数据进行一些简单的预处理，之后再存储到不同的地方。那所以在这个地方就可以使用flume了，因为提供了丰富的source和sink，并且也可以使用拦截器对数据进行一些简单的处理。这个时候就不需要太纠结性能消耗了，因为flume是部署在单独的服务器上面，不会对其他应用程序造成影响。在这呢，我们先简单画图看一下。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305171558502.png" alt="image-20230517155827983"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后这个呢，是前端业务机器啊。那如果说我们想采集这个机器上面的一个日志数据的话，最好呢，是部署一个性能消耗低的一个采集工具。因为这上面除此还有其他进程。所以说我们在这里面部署于一个filebeat的性能消耗比较低。还是有那么两台吧。好，那我们在每一台上面都部署一个Bo的，让他呢去采集当前机器里面的人数数据。采集到之后呢，把这个日志数据啊，放到我们的消息队列里面。这个数据进到这个消息页之后呢，后面我们其实就可以使用去对这些数据做一些简单的预处理，预处理之后呢，再把数据写到其他地方。所以接着呢就可以接一个flume，这个时候就不需要去考虑这个性能消耗。这个有可能，它可以直接去读它里面的数据，然后呢，再把数据再写进去，都是有可能的啊。类似于如果从第一个topic上面就是消费数据。把数据拿出来之后呢，对数据做一些处理，处理之后呢，再放到第二个topic都是OK的。以及呢，flume可以直接消费这个消息队列里面这个数据，然后呢，直接把数据呢，写到我们的hdfs分布式文件系统里面也是可以的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">所以在这里我们就需要选择Filebeat和flume了，</span><br><span class="line"></span><br><span class="line">这里的消息队列我们就直接使用kafka了，因为kafka是大数据领域中最常用的消息队列了。</span><br><span class="line"></span><br><span class="line">所以 我们最终的选择就是FileBeat+Kafka+Flume，这就是针对日志数据采集工具的选择。</span><br><span class="line"></span><br><span class="line">后面我们也会涉及到数据库数据的采集，在我们这个项目中，需要从数据库中采集的数据量比较小，可以选择使用sqoop，或者我们使用mysql -e命令直接导出数据也是可以的，上一个项目我们已经使用过sqoop采集mysql中的数据了，在这我们就使用一个不一样的，自己开发脚本使用mysql的命令导出数据。</span><br><span class="line">不过我们在最后是需要把HDFS中的结果数据导出到MySQL中，这个时候还是需要用到Sqoop的。</span><br></pre></td></tr></table></figure><h2 id="技术选型之数据存储"><a href="#技术选型之数据存储" class="headerlink" title="技术选型之数据存储"></a>技术选型之数据存储</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241618486.png" alt="image-20230424161849176"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面我们把数据采集工具分析完了，下面我们来分析一下数据存储系统的选择。我们采集到的数据最终会存储到分布式文件系统中，这个分布式文件系统一般就直接选择hadoop中的hdfs，这个就不需要额外的对比啊，因为我们在搭建大数据平台的时候，hdfs已经安装好了。并且它也可以和很多采集工具以及计算框架无缝衔接，所以大数据领域的分布式存储系统一般都直接使用hdfs。目前一些大的厂商也有分布式存储的一些服务，例如亚马逊的s3。我们也可以选择使用这些服务，但是这样的话，针对数据计算就不太友好了。分布式计算框架无法实现本地计算，因为数据和计算节点不在一块儿。所以在这呢，离线数据我们就使用HDFS来求我们的计算框架，计算的结果数据有一些是需要和前端交互的。这些数据呢，前期可以选择存储到MYSQL里。</span><br><span class="line"></span><br><span class="line">那我们在维护这个用户三度关系数据的时候呢，如果使用普通的关系型数据库进行存储的话，会造成很多数据冗余，并且查询起来也非常麻烦，所以一般啊会使用一些图数据库。这里面这个Graphx或者Gelly。Graphx是spark中的图计算。gelly属于flink中的图计算，它们只能实现分布式图计算，不能保存图数据，所以说呢，并不满足我们的需求。</span><br><span class="line"></span><br><span class="line">下面这几个neo4j、orientDB、JanusGraph这几个都是图数据库，它们几个又有什么区别呢？我们来看一下。在这里面，我通过这些层面。对这三个图数据库做了一些对比分析啊。其中这个neo4j啊，它是目前人气最高的图数据库，它可以支持高度扩展，完全支持acid acid是数据库里面的一个特性啊，neo4j啊，它提供的Cypher查询语言是比较人性化的，非常容易上手使用，并且 它支持社区版和商业版，社区版是开源的。社区版呢，不支持分布式，商业版支持分布式。neo4j入门，相当简单，学习成本比较低，并且比较稳定。还有就是neo4j，它对各种语言的支持也比较好，Java呀，python啊，这些语言它都支持，并且官方提供的还有一个connector插件，可以实现Spark直接操作neo4j非常方便，那在这呢，我们主要考虑到应用性以及快速上线这些特性，所以说neo4j是我们目前最优的选择。所以说呢，针对这些存储系统啊，我们最后的选择就是hdfs加上MYSQL加上neo4j。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241621070.png" alt="image-20230424162131164"></p><h2 id="技术选型之数据计算-数据展现"><a href="#技术选型之数据计算-数据展现" class="headerlink" title="技术选型之数据计算+数据展现"></a>技术选型之数据计算+数据展现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下数据计算框架的选择。目前啊，大数据领域最常用的几种计算框架包括mapreduce、storm、spark、flink。其中mapreduce是第一代计算引擎，它主要针对离线数据进行计算。由于mapreduce计算框架的模型啊是固定的，针对复杂的计算，需要开发多个mapreduce任务，代码量比较多，也比较麻烦，并且它的计算是基于磁盘的，计算效率也比较低，所以现在已经很少使用。</span><br><span class="line"></span><br><span class="line">接下来看一下storm这个计算框架，是一个比较早的实时计算框架，可以实现真正意义上的实时处理。在前期的数据计算领域立下了汗马功劳，但是由于此框架太过于独立，没有自己的生态圈，所以最近这几年呢，日渐没落。</span><br><span class="line"></span><br><span class="line">那接下来看一下spark的这个计算框架，它是一个分布式的内存计算框架，支持离线和实时数据计算，由于它是基于内存的，所以说呢，它的计算性能非常高。但是在这需要注意一下，虽然spark支持实时计算，但是它的实时计算并不是真正意义上的实时。这是由于它底层的计算模型决定。spark最快只能支持到秒级别的实时计算，相当于一秒执行一个小型的批处任务。</span><br><span class="line"></span><br><span class="line">最后我们来看一下flink这个计算框架，flink属于最近新兴起的一个流式计算框架，它侧重于的是实时计算。flink在支持实时计算的基础上，也可以实现离线数据计算，所以说flink也是支持离线和实时数据计算的。在我们这个项目中，既需要离线计算，也需要实时计算，所以单纯的使用mapreduce或者storm都不合适，并且呢，他们两个现在几乎呢已经快被淘汰了。用的非常少啊，所以说我们需要在Spark和flink中进行选择，当时我们在开发这个项目的时候，flink才刚出来。还不是很稳定，并且我们团队内部也是刚开始接触flink，之前我们一直是使用Spark，所以说为了保证项目快速稳定上线，我们当时决定先使用spark，等后期对项目进行迭代优化的时候再考虑使用flink。所以在这针对数据计算，我们选择Spark</span><br><span class="line"></span><br><span class="line">最后是这个数据展现模块，数据展现模块不需要我们实现。这块是由安卓开发组还有iOS开发组负责的，我们只需要把结果数据计算好，存储起来就可以。好，最后我们做一个总结啊，就我们前面啊，针对各个模块进行技术选型的时候，大家不要盲目的追星，我们要根据具体的业务场景和不同框架的特点进行选择，同时还要考虑已经在使用的成熟的框架，不要盲目追求一些所谓的好的新的框架，因为技术成本也要考虑。所以说，技术选型不单单是选技术，是要在结合业务场景的前提下进行选择。</span><br></pre></td></tr></table></figure><h2 id="项目整体架构"><a href="#项目整体架构" class="headerlink" title="项目整体架构"></a>项目整体架构</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241727480.png" alt="image-20230424172738182"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">大家好，前面我们把技术选型搞定了，下面我们来看一下项目的整体架构设计。在这里，我把这个项目分为了三个模块，数据采集、数据计算、存储以及数据展现。因为这个计算以后啊，就涉及到存储了，所以说呢，我把这两个划分到一块儿，那接下来我们来详细分析一下这个项目的整体流程。</span><br><span class="line"></span><br><span class="line">首先呢，看这个数据采集模块。数据采集模块的数据啊，包含两大类，一个是服端数据，还有一个是客户端数据。其中服务端数据啊，它里面包含服务器中接口调用时记录的日志数据和数据库中的数据。在这里需要注意一下，针对服务端日志数据的采集，是在提供接口服务的机器上部署filebeat来采集。这样机器会有上百台，在这里我们先用一台server01来表示。这里面DB呢，表示的就是MYSQL数据库。</span><br><span class="line"></span><br><span class="line">接下来呢，是客户端数据，就是用户使用APP的时候上报的一些用户行为日志。例如打开关闭APP以及呢在APP中的滑动点击等行为，其实呢都会记录日志。这些数据呢，客户端会通过接口定时上报。那接口收到这个请求之后呀，会把请求中包含的日志信息呢，记录到本地文件中，然后使用filebeat进行采集。也就是说呢，我们会在server02上去部署一个接口服务，接收客户端上报的日志数据。那这样其实就可以统一流程了。针对服务端日志和客户端日志，最终啊，都是通过这个filebeat的来进行采集。那filebeat呢，最终把这个数据啊，都采集到这个卡夫卡里面。那针对服务端数据库里面的一个数据啊，我们会通过脚本直接呢，把它导入到hdfs里面。</span><br><span class="line"></span><br><span class="line">那filebeat的采集的实时数据啊，导入到卡夫卡里面之后呢，还会通过flume。对这个数据进行一些分发处理，以及落盘到hdfs的操作。落盘就是存储的意思。那针对这里面我们刚才所说这个数据分发的一个详细内容，我们在后面开发数据采集模块的时候，会详细分析它的架构。这就是数据采集模块的主要内容。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来看一下数据计算，还有存储这个模块。计算模块主要呢是利用spark。针对卡夫卡中的数据呢，进行实时计算，针对hdfs中的数据啊，进行离线计算。那在计算的时候呢，它还会和这个noe4j这个图数据库进行交互。既会向里面写数据，也会从里面读数据。最终呢，会把这个spark计算的结果呀，使用sqoop导出到MYSQL里面。针对数据计算这一块，一共有六七种计算指标，具体直接计算指标我们在开发数据计算模块的时候会详细分析。这就是这块的一个流程。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后一个呢，是这个数据展现。那在这个模块里面，我们可以看到最终的一个项目效果，这里面其实就是一个手机端的一个项目。这个不是我们的重点。这就是我们这个项目的一个整体架构设计。注意，在这个架构里面其实存在三个主要的问题，第一个针对实时计算，Spark其实不是最优的选择，最好是使用flink。针对这个结果，数据的存储mysql也不是最优的选择。最好是使用redis。针对数据展现这一块，直接查mysql中的数据也不是最优的选择。最好是开发接口。对外提供接口查询数据。不过我们为了快速迭代上线，所以前期呢会使用相对来说比较简洁的架构，先把功能快速上线，后面再迭代优化。</span><br></pre></td></tr></table></figure><h2 id="Neo4j介绍及安装部署"><a href="#Neo4j介绍及安装部署" class="headerlink" title="Neo4j介绍及安装部署"></a>Neo4j介绍及安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">大家好，针对我们前面分析的这些技术组件，只有filebeat和neo4j我们没有使用过。不过非呢比较简单，它类似于在使用的时候主要是写配置文件，所以在后面用的时候我们再具体分析。下面我们就来学习一下neo4j的使用，让大家快速了解它，并掌握它的常见用。</span><br><span class="line"></span><br><span class="line">neo4j，它是一个高性能的图数据库。它和普通的关系型数据库是不一样的，它里面侧重于存储关系数据。针对各种明星之间错综复杂的关系，如果我们使用mysql这种数据库存储，在查询所有人之间的关系的时候是非常复杂的。但是使用neo4j这种数据库只需要一条命令就可以了。neo4j，它是一个嵌入式的基于磁盘的、具备完全的事物特性的持久化引擎。它将结构化数据存储在网络上，而不是表中。注意这块，这个网络，从数学角度我们可以把它称之为是图。这个并不是我们所说的4G网络，5G网络，不是这个意思。</span><br><span class="line"></span><br><span class="line">目前这个neo4j有两种发行版。一个呢是商业版，它是支持集群的，另一个是社区版。这个只支持单机。目前我们这个平台用户量啊，在三四千万这个规模，这个时候呢，我们使用单机也是足够用的。等后期单机无法支撑之后呢，再考虑使用商业版。那接下来我们来看一下neo4j的一个安装部署。用它支持在Windows以及Linux中进行安装，由于在实际过程中肯定是要在Linux中进行安装，所以说在这呢，我们就直接使用Linux环境。</span><br><span class="line"></span><br><span class="line">那下面呢，我们首先来下载一下。在这我已经起先打开了，因为它这个打开比较慢啊，你在这搜new，就这个new.com，这是它官网。接下之后把鼠标放在这个product上面，然后到这看到没有下载new，点那个。进入这个界面之后，注意。点这个。大陆的用户g serve。记下之后，注意，这呢是商业版。我们要用那个社区版的，就是这个。这是免费的。往下面走，你看他现在最新的版本呢，是4.1的，建议的话呢，我们可以往上面走一走，使用它之前比较稳定的是3.5的那个版本。往下边你看。3.5.21建议使用这个版本。看到没有，这是针对linknux或者麦克对吧？下这个是一个table包，如果想在Windows里面运行，你选那个Z。那我们在实际工作中，开发环境肯定是要用这个基于Linux的，所以说我就直接下载这个啊，你点这个就可以了，但是注意。你直接使用这个链接下载啊，很大概率可能会由于网络原因导致你下载失败。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304241747028.png" alt="image-20230424174708992"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">呃，建议的话不要用这个链接下怎么办。现在呢，还有一个链接。这个。点，建议大家使用这个链接下载。你直接把这个复制到你的那个浏览器里面，打开就可以，它就会自动开始下载。这个现在呢是比较快。sentence。我在这呢已经卸载过了，到时候呢也会把去的安装包发给大家，所以说你想下的话就去下一下，不想下就算了。</span><br><span class="line"></span><br><span class="line">那这个呢，我们就需要把这个安装包啊，上传到我们的bigdata04机器上面。我呢，提前已经上传过了。到这看一下。我放到对soft这个下面。有这个文件对吧，已经有了。那下面呢，我们呢，先解压。解压之后我们来吸到里面。接下来我们需要修改配置。注意它下面有一个conf目录。嗯。在看下面有一个neo4j.conf这个文件，我们就要改这个配置文件。这里面呢，其实也比较简单，我们只需要改两个地方。[root@bigdata04 soft]# cd neo4j-community-3.5.21&#x2F;conf</span><br><span class="line">[root@bigdata04 conf]# vi neo4j.conf</span><br><span class="line">...</span><br><span class="line">dbms.connectors.default_listen_address&#x3D;bigdata04</span><br><span class="line">dbms.connectors.default_advertised_address&#x3D;bigdata04</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">注意：这里的bigdata04是当前机器的主机名</span><br><span class="line"></span><br><span class="line">那接下来我们就可以去启动了。</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;soft&#x2F;neo4j-community-3.5.21</span><br><span class="line">[root@bigdata04 neo4j-community-3.5.21]# bin&#x2F;neo4j start</span><br><span class="line"></span><br><span class="line">起来之后呢，你可以通过JPS验证一下。看到没有，它确实有一个进程。</span><br><span class="line">访问neo4j的web界面：http:&#x2F;&#x2F;bigdata04:7474&#x2F;</span><br><span class="line">默认用户名和密码都是neo4j。</span><br><span class="line">你默认进来之后啊，你看它默认呢，会连那个bigdata04，这时候这个端口是7687，注意这个是真正连接neo4j这个服务的。</span><br><span class="line">注意：第一次访问的时候会提示修改密码，建议改为admin，因为那个默认的密码不安全。</span><br><span class="line">那如果说我们想把它停止掉，怎么停呢？很简单，你启动传一个start，那停止了就传一个stop。</span><br><span class="line">[root@bigdata04 neo4j-community-3.5.21]# bin&#x2F;neo4j stop</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;neo4j stop</span><br></pre></td></tr></table></figure><h2 id="Neo4j之添加数据"><a href="#Neo4j之添加数据" class="headerlink" title="Neo4j之添加数据"></a>Neo4j之添加数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用neo4j可以很方便的展示一些人物或者事物之间的错综复杂的关系。下面我们来看一张图。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242111885.png" alt="image-20230424211116004"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这张图里面展示了这些人物之间的关系。使用这种展示形式看起来是很清晰的。也方便理解，后期如果说我们想查询某一个人的一个关系链，也是很方便的。这些数据如果让你去使用MY数据存储是很繁琐的，并且呢，查询起来也很烦。那这在这里面呢，有几个概念我们需要明确一下。因为neo4j，它是一个图数据库。我们可以认为它里面存储的呢，都是图数据。这个图到底是一个什么东西呢？注意图呢，它是由点边和属性组成的。我们这个图里面这个圆圈呢，它就是一个点。这里面这个线呢，它就是一个边圆圈中的这个姓名呢，就是属性。以及这个边里面这个值呢，也是属性。就是点和边上面都可以设置属性。对，这个点呢，你还可以把它称为是节点。这个边的话，可以把它称为是关系。就类似于这两个人之间的一个关系。每个节点和关系，它都可以有一个或者是多个属性。比如这上面呢，可以保存多属性。</span><br><span class="line"></span><br><span class="line">那在这里面大家啊，先对这个neo4j有一个整体的认识，下面呢，我们开始具体学习neo4j中的具体使用。在这儿我们主要学习neo4j的以下操作。添加数据、查询数据、更新数据、建立索引、批量导入数据。主要是这五种。那下面呢，我们把这个neo4j给它起来</span><br></pre></td></tr></table></figure><h3 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h3><h4 id="create"><a href="#create" class="headerlink" title="create"></a>create</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">创建一个点</span><br><span class="line">create (p1:Person &#123;name:&quot;zs&quot;&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create (p2:Person &#123;name:&quot;ls&quot;&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create (p1:Person &#123;name:&quot;zs&quot;&#125;) -[:like]-&gt; (p2:Person &#123;name:&quot;ls&quot;&#125;)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242125348.png" alt="image-20230424212534229"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242125430.png" alt="image-20230424212513916"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242129575.png" alt="image-20230424212950366"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p1,p2并不会实际存储，用create创建关系，不会检查之前是否存在待创建关系同名的节点</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">表示每次都创建新的点或者边。第二个表示每次创建点或者编之前呢，会先查询一下，如果存在则不创建。那下面我们就来演示一下。由于在这儿我们需要挑一些密径，所以说呢，在这儿我简单记录一下，这样看起来会更加清晰。a。每次都创建新的点。左边。那我们来在这先创建一个点。嗯。我先把meeting在这写一下，create。注意后面怎么写呢？注意先写一个小括号。英文的啊。冒号后面呢，表示你要创建这个点，你要给它起一个类型，它是什么类型的，表示一个person。还是那些。那你这个person的话，你可以给它设置一些属性，就相当于你在这出现点的时候呢，可以里面给它加些属性，属性怎么加呢？括号括号。我们给它加一个name属性。冒号。后面是这个属性的值，叫张三。那我们在这呢，还可以给它起一个别名P，就类似呢，我们在这创建了一个判对象。判断对象里面有一个内部属性，它的值呢是张三，最终呢，给它起了一个变量的名称叫P。我们来执行一下。放到这里面，点了一个play去执行就可以。添加成功看到没有，创建一个note，创建一个节点，然后设置一个属性。OK，那接下来我们再来创建一个节点。还使用这个。改一下。P2吧。第。把这个拿过来执行一下。那现在的话呢，我们就有两个点了。但是这两个点之间呢，还没有什么关系。那我们想他们两个之间有关系的话，就需要给它们设置一个边了。这个边的话呢，同样可以使用这个create命令来创业。注意，看我怎么实现啊。VISA。对前面的话呢，就类似是这个对吧，把它拿过来。注意，我们想给他设置一个边。就是一个关系。我们想让这个PE啊，这个张三，让他去喜欢李四。表示它们间的关系吗？后面一个横杠中括号冒号，这是固定格式。后面给他写个like，表示呢，张三喜欢李四。一个横杠，一个右键轴。对，这个表示呢，是张三喜欢李四，所以说李四是在后面。对，它这个箭头是往右边指啊，比如他喜欢它，所以呢，在这两个点之间呢，给它设置一个关系叫like。比如张三喜欢的，我们来执行一下。成功了对吧，两个基点。是这两个属性。以及呢，创建了一个关系，或者说呢，是一个边都可以啊。注意，这条命令执行之后，我们可以到这个界面上点这个device。来看一下，你看这是节点，这是关系，这个呢是属性。这是我们创建的person，以及这个like的关系，以及你person里面name的一个属性，在这都可以查看。我们接着可以点那个like。看到没有？张三like李四。这样的话，就可以很清晰的看到他们两个之间的一个关系了。但是呢，这时候呢，你回到这儿来看一下。你看点了一个person。你发现啊，它有四个person。你看两个张三，两个李四。然后你往这个位置看，你看这是内幕李四对吧，所以我们现在啊，选中这个李四和ID，你看是一，那这个呢。李四，它ID是21，看到没有，这个ID是它自动生成的，是唯一的。你看他们两个还不一样，那就意味着这是两个节点，对吧，不是同一个，虽然说他们两个名字一样，但是他们不是同一个极点。那这是什么原因呢？注意，因为这个create呀，它每次呢都会创建新的激烈或者关系。所以说呢，最开始啊，我们使用这个create，你看创建了两个节点，P1还有P2，对这个P1还有P2这个东西它不会扯到就里面。这个以及这个name对应的值是会存储到里面的，这个相当于你给它起了一个别名而已，这个东西不会存进去。那我们之前你看在这创建了一个P1，又创建一个P2，现在我们出现了两个P对象。这是新建的。那接着呢，我们使用这条命令，看到没有。它相当于又创建了两个，这个杠三和离子。所以说呢，你这时候在你的纽扣针里面就有四个。那其实啊，我们在这是想给最开始创建的这两个机制增加一个这个like关系的。因为在实际过程中也会有这种需求，就是节点已经存在了，需要我们后期给他们指定关系。你这种写法，它相当会重新生成。那肯定是不满足我们需求的。这个时候该怎么做呢？咱们前面说了，谬杯里面除了有这个create meaning，它还有一个me meaning。这个命令呢，表示啊，在创建几点之前都会先查询一下，如果存在则不创建。我。这个命令。在创建节点之前都会。先查询一下。如果存在。则不创建。所以说这个默认命令啊，你就算是重复执行，他也不会产生重复的结果。注意你这个奎的命令。你重复执行，你执行一次，他就给你创建一个这个person这个节点。这个需要注意一下啊。那我们看一下me的话，后面的写法是一样的。后面这种写法是一样的。现在我要看起来清晰一些，我给它起个别名叫P3吧。其实都无所谓啊。注意那下面呢，我再写一个，这是P4对吧。这个名字给他改一下吧，这个叫Jack。同时呢，我们想要这个Jack呢，去like to。这样写much。第三。like。kiss。这样写就可以了。现在呢，我先复制这一行，拿过来来执行一下。拖延成功了对吧，创建一个节点，设置一个属性，那你说进行明我再执行一下，你可以直接点那个。再执行一下。看到没有，没有改变。点着来确认一下。看到没有，还是一个，所以说这个默认命令啊，你重复执行，它是不会重新创建的，因为在这的话，它呢，会根据你在这使用的这个名称去查一下，看看有没有重复的，如果有的话，他就不再创建了。所以说呢，在工作中啊，建议使用这个墨。可以避免重复。注意。这条命令你说我单独执行行不行啊不行，你必须要保证这条命令和前面两个一块来执行，因为在这这个变量，我前面说变量它是不会存到u里面的。他呢，只在当前绘画有效，所以说呢，这三条命令需要一块儿来执行。把它们放到一块儿，这样来执行就可以。来确认一下。看到没有，Tom Jack。你点那个也是OK的啊，一样的。这个。like to。对吧，我们记住那个person的话，等于之前我们是四个，现在又加了两个，一共是六个，没有问题。在这里其实还有另外一种写法，如果节点已经存在了，我们只需要创建关系，我们还可以使用那个match来实现。再让我们先简单用一下。那。那么可以查询之前。已有的。节点或者关系。那其实就是电或者是编了嘛。一样的意思啊。那接着呢，我们就想要这个Tom和这个也产生一个like关系。对，你看之前的话呢，是这个。Jack like Tom，那现在的话，我们想让Tom也去like Jack一下，互相习惯，这样的话就不是单相思了嘛，对吧。单相思最难受。所以说我们在这呢这样来做，使用ma先查询那个节点，因为那个节点之前已经创建过了，对吧。可以这样，那我们要给他起个名字叫a吧，还一个person。他的name呢？the Tom。对吧，我们要查两个人啊，要把这个汤还有这个都得查出来，对吧，要找到这两个人，重新再给他们加一个关系。B。name。Jack。把他们两个都查出来，注意，查出来之后注意。a。然后a就是Tom，然后Tom呢去like。B注意这里面这个a还有B，只是为了在这去使用，没有其他含义，你给它起个什么XY也是可以的。来我们来执行一下，注意他们两个也需要一起来执行，要不然那你这个a他是找不到他的。好，执行成功，你看创建了一个关系。这个还是六对吧，没有变。只不过这时候你看没有，汤姆也喜欢Jack克，Jack克也喜欢汤姆，他们两个就互相喜欢了。所以说呢，我们就可以通过match呢，去查询之前已有的机械信息，然后再通过墨创建关系就行。这样也不会额外产生重复的节点。所以说呢，这两种方式啊都可以，你使用这种方式也行，使用这种方式也行。按需选择即可。效果是一样的。</span><br></pre></td></tr></table></figure><h4 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个命令在创建节点前都会先查询一下，如果存在则不创建</span><br><span class="line"></span><br><span class="line">merge (p3:Person &#123;name:&quot;jack&quot;&#125;)</span><br><span class="line">merge (p4:Person &#123;name:&quot;tom&quot;&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">merge (p3) -[:like]-&gt; (p4)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242135053.png" alt="image-20230424213515502"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242135568.png" alt="image-20230424213535288"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">把上述三条命令一起执行</span><br></pre></td></tr></table></figure><h4 id="match"><a href="#match" class="headerlink" title="match"></a>match</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">可以查询之前已有的节点(点)或者关系(边)</span><br><span class="line"></span><br><span class="line">match(a:Person &#123;name:&quot;tom&quot;&#125;),(b:Person &#123;name:&quot;jack&quot;&#125;)</span><br><span class="line">merge (a) -[:like]-&gt; (b)</span><br><span class="line"></span><br><span class="line">这种a,b只是别名，只在当前会话有效；这两个要一起执行，不然第二个命令找不到a,b</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242143938.png" alt="image-20230424214333671"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242144207.png" alt="image-20230424214401568"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">之前jack-&gt;tom</span><br><span class="line">现在jack&lt;-&gt;tom</span><br><span class="line"></span><br><span class="line">等同于之前的三条merge命令</span><br></pre></td></tr></table></figure><h2 id="Neo4j之查询数据"><a href="#Neo4j之查询数据" class="headerlink" title="Neo4j之查询数据"></a>Neo4j之查询数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下用户类中如何查询数据。针对这块我们主要学习以下内容，首先呢，学习一下这个match和return的用法，它们呢可以实现查看满足条件的数据，并且返回。以及最后我们会讲两个案例，如何查询二度关系和三度关系。</span><br></pre></td></tr></table></figure><h3 id="match-return"><a href="#match-return" class="headerlink" title="match+return"></a>match+return</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match(p:Person &#123;name:&quot;tom&quot;&#125;) return p</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">咱们前面呢说过这个match啊，它可以进行一个查询，下面咱们就来继续使用一下。这个呢，其实有点类似于mysql中的select。在这需要注意一下，match不能单独存在。咱们前面在使用的时候，那后面跟着也是有一个merge命令的。如果我们只想查询一些数据，并且把这个数据返回过去，呃，如何实现呢？</span><br><span class="line">就可以使那个match加return。就是查看满足条件数据，并且返回，那下面呢，我们就来查询一条数据。现在我们想查询一下这个Tom这条数据。nice。PAR。指定属性name。对吧，我们就想查他，那查出来之后呢，想要把这个结果啊给返回，怎么返回呢，后面加个return。这样这样就可来我们来执行一下。没问题吧，查出来了对吧，把那个汤姆查出来。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242150553.png" alt="image-20230424215018223"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们来查询一些复杂一点的内容。首先呢，在这我们来初始化一些数据。好，这些数据呢，刚才我已经把它复制过来了，就这些数据，注意这里面创建点的这些操作和创建边的操作需要在一个会话里面一起执行啊。否则它是无法识别这些变量的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242152676.png" alt="image-20230424215221280"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在这你看其实相当于我们初始化ABCXYZ是吧，这几个用户。在他们之间呢，给他加了一些关注关系，其实就类似于直播平台里面用户和主播之间的一个关注关系。好在这把这批数据给他做一下初始化，直接复制过来。好创建成功对吧，六个节点六个属性六个关系。那首先呢，现在我们要做一个查询。假设呢，这些都是主播，你看a follow了B对吧，a关注了B，那B的话，我们可以认为它是一个主播对吧？那所以说呢，这样我们要查询某个主播的粉丝信息。我们就查询这哥们儿，你看有这个a和C都关注了他对吧，他的名字呢叫B。B。好，那这时候怎么实现注意。查询嘛，使用。name。我们要查询这个用户，他的一些粉丝。注意看我下面怎么写，相当于啊，是别人关注了他，按照我们之前教的这种写法，就类似于这种，你说把这个拿过来，注意这个箭头是往右指的，也就是说呢，是他操作了别人，他关注了别人。我们现在要查的是谁关注了它，所以说了这个方向不是往右的往左。对吧？我们要查询哪些人关注到这个user b。所以后边的话呢，零。有的。这时候它后面呢，就不需要加这个括，括号里面也不需要指定什么属性了。相当于我们就要查询到底是谁关注了这个user b。这个人具体是谁，我们现在还不知道呢，所以说呢，后面也不需要加一些具体的限定。OK。那个范围就可以。来，我们来执行一下。看到没有a和C。你回过头来看一下。AC对吧，你看。a的话，它这个名字就是大a嘛，对吧，这是大C没问题。所以说呢，我们是可以查出来的。那其实这种写法呀，它还有一种写法。还有一种写法，矢量。mass。把他们反过来。把它放到前面。这个不是注意改一下。对吧。就是谁去follow了这个user。这样写也是可以的。如果说你感觉这种写法比较别扭，你可以用这种写，对吧，谁关着B。这样把它返回过来就可以，效果是一样。11下午啊。如果说我们只想返回满足条件的那个粉丝的一个name值，你看这个相当于它整个把这个几点都给返回来，如果说我们只想返回它里面这个内的属性的值怎么办呢？也简单。值返为零。没问题吧，也是可以的。好，这个其实啊，就是我们要查询的那个主播的二度关系。为什么这样说呢？我们来分析一下啊。你看这个时候呢，是这样的我。这个呢，是这个主播B。后面呢，是主播B的一个粉丝。那这个时候我和这个主播B的粉丝，我们之间是不是就属于一个二度关系呢？因为我和主播B我们之间呢，是一度关系。主币和粉丝之间呢，也是一度关系，但是我和这些粉丝之间就属于二度关系，我们是通过这个主币来认识。好，那我们在这个项目中呢，是想实现三个关系推荐。也就是说呀，当我要关注某个主播的时候，你呢，要给我推荐这个主播的粉丝，又关注了哪些主播，你把那些主播推荐给我。因为我和那些主播之间才属于三种关系。二的关系。三度关系。看到没有，这个时候我和主播N之间就属于三种关系。看到没有，我和他是一组，和它是二度，和它呢也是三组。那这个三的关系该如何查询呢？其实咱们刚才这个呀，查询的就是二楼关系，其实你只需要把这个主播币的粉丝查出来就行了，后期谁去关注主播币，那这个粉丝和那个人，他们之间是不是就是二度关系。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242153262.png" alt="image-20230424215318019"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查询某个人的粉丝</span><br><span class="line">match (:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (n:User) return n</span><br><span class="line"></span><br><span class="line">另一种写法</span><br><span class="line">match (n:User) -[:follow]-&gt; (:User &#123;name:&quot;B&quot;&#125;) return n</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242156274.png" alt="image-20230424215640635"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查询某个人的粉丝只返回name(属性)值</span><br><span class="line">match (:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (n:User) return n.name</span><br><span class="line"></span><br><span class="line">另一种写法</span><br><span class="line">match (n:User) -[:follow]-&gt; (:User &#123;name:&quot;B&quot;&#125;) return n.name</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AB%E5%91%A8-%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0-1%5Cimage-20230424215940138.png" alt="image-20230424215940138">)<img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242159113.png" alt="image-20230424215940184"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这些属于二度关系(查询我关注的人的粉丝的信息)</span><br><span class="line">我-&gt;主播-&gt;粉丝</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">三度关系(给我推荐我关注的主播的粉丝关注的人)</span><br><span class="line">我-&gt;主播-&gt;粉丝-&gt;主播N</span><br><span class="line"></span><br><span class="line">match (a:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt;(c:User) return a.name as aname,b.name as bname,c.name as cname</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242213902.png" alt="image-20230424221314630"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那我们现在想查三度关系，那其实就是查出来主播B的粉丝又关注了哪些主播。只要把这些数据查出来就可以了。我们返回一下。看到没有？这是主播B，有这么几个粉丝啊，A和C看到没有？他有a和C这两个粉丝，分别关注了XYZ这三个主播，其中呢，A和C这两个粉丝呢，都关注了Y这个主播。那这个时候你再给我推荐深度关系的时候呢，就应该从这里面去挑了，那这个时候是不是应该把这个B的粉丝关注比较多的主播推荐给我呢？你看a和C都关注了Y，你是不是应该把它推荐给我呢？在这里，理论上来说，Y这个主播最有可能是我喜欢。这个Z和X呢，它那个可能性啊，就没那么大，所以说呢，在这里面啊，我们在获取这个三度关系的时候呢，针对这个c.name里面这个结果呀，最好呢是做一下过滤。我们统计一下c.name里面相同主播出现的次数。然后呢，按照倒序排序。最终再取一个topN是不是就可以了？重合度越多的说明了越有可能是我喜欢的。因为相当于我去关注这个主播B了，相当于我喜欢主播B。你看a和C这两个粉丝也关注他了，说明他们两个也喜欢他，那有可能我和这个a还有C这两个粉丝这个口味是一样的。那他们两个同时呢，又都关注了Y这个主播，所以说呢，Y这个主播也是最有可能是我喜欢的。所以说呢，这个其实就是三的关系，最终想要达到一个效果。</span><br><span class="line"></span><br><span class="line">好，那根据我们刚才分析，你在这儿还想对它做一个什么聚合，对吧，再做个排序，这东西怎么实现呢？我们来看一下。其实这个麦呀，后面。也支持什么抗就是抗函数。奥特曼。排序的以及呢，这个厘米上对吧，取多少条这些命令啊，都是支持的。那所以说呢，在这个基础之上，我们可以做一些调整。所以这时候你要把这个去掉。只保留这个a name，还有c name就行。然后后面注意你后面返回多个列的话，中间有多少个开啊。抗的星。也是做一个求和啊，后面order by。some。DSC倒序排序。那我们再做一个厘米。倒序排序之后呢，我们取前几条，这样不就是top n了吗，对吧。来，我们在这儿实现一下。看到没有？他最终统计的这个数量，你看对不对。这个Y西里Y嘛，是吧，两次。Z是一次，X是一次对吧，没问题吧。没问题，那所以说这个时候你其实可以把这个稍微改一下，你改成厘米则点一吧，因为这两个值都是一样的，我们就厘米的一就取了一条。没问题吧，是可以的。注意你这里面啊，你用康兴也行，或者说呢，你用那个什么呀，这种写法。对吧，这样也可以啊，都是一样的效果。要消毒一下。可以使用。它或者它效果是一样的。OK。那其实这里面啊，你看这里面就相当于我们根据这个a，还有这个c name去做一些分组。然后呢，使用抗的。去做了一个求和统计，每组的一个数据行数。好把这个加个a是吧，少了一个a啊。这是一样，这只是一个变量名称啊，无所谓。改过来之后呢，看起来顺眼一些对吧，有强迫症的话，感觉这里面少一个字母，感觉很难受对吧。好，这就是二度关系，还有三度关系的一个查询了。对，这里面呢，其实啊，我们还可以使用where去加一些过滤条件。就实现一下过滤。注意。你想使用where也可以啊。这个where需要放在。return前面因为你这个return啊，就直接返回了呀，你这个where啊，肯定是要放到return之前的，先过滤再返回嘛，对吧。把这个复制过来。注意现在前面加一个外过滤。我们过一下，where name。对，你在这还不能用那个C里，C里姆是在后面定义对吧，我们前面的话还只能用C点内。不等于X吗？把X这个过滤掉行吗？你先回到这儿。对吧，还是0.3。你看现在这个C里是不是有一个X呀。好，我们在它基础上加列过滤。对吧，C点内就是不等于X对吧。看到没有，只有Y和Z啊。OK，所以说呢，这里面也是可以用这个where或者条件的。OK，这个其实就是我们这个没忽略里面的一个查询操作啊。注意这里面这个查询语法呀，其实啊，就是用户这里面的S法语法，这个塞法语法呀，在查询的时候，你看其实有些地方它和circle那个查询还是有点类似啊。所以说呢，这种写法还是比较简单易用的，最起码看起来是比较清晰的。很直啊，所以说呢，我们学起来上手也很快。这就是我们当时为什么选择这个newd啊，这个查询语言确实用起来比较方便。</span><br></pre></td></tr></table></figure><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实match后面也支持count()、order by、limit等命令</span><br><span class="line">match (a:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt;(c:User) return a.name as aname,c.name as cname,count(*) as sum order by sum desc limit 3</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242224375.png" alt="image-20230424222424329"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：这里count(*)等同于count(cname)</span><br><span class="line"></span><br><span class="line">相当于对aname,和cname做了分组，在操作</span><br></pre></td></tr></table></figure><h4 id="where"><a href="#where" class="headerlink" title="where"></a>where</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：where放到return之前</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实match后面也支持count()、order by、limit等命令</span><br><span class="line">match (a:User &#123;name:&quot;B&quot;&#125;) &lt;-[:follow]- (b:User) -[:follow]-&gt;(c:User) where c.name &lt;&gt; &quot;X&quot; return a.name as aname,c.name as cname,count(*) as sum order by sum desc limit 3</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242231619.png" alt="image-20230424223142375"></p><h2 id="Neo4j之更新数据"><a href="#Neo4j之更新数据" class="headerlink" title="Neo4j之更新数据"></a>Neo4j之更新数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下如何在应用中去更新数据。更新数据这块啊，其实总结一下有两种情况。第一种呢，就是更新节点的属性，使用match和set实现，你把它先查出来，然后呢使用set命令去修改。第二种啊，就是更新节点之间的关系，也就是边。这个其实就是删除边。我们使用那个ma和D的视线把它查出来，把它删掉。其实呢，你使用这个match和吉delete也可以实现删除节点。对吧，我们把这个节点查出来，然后把它删掉。那下面呢，我们就来演示一下。首先我们看一下就是如何修改节点中的属性。那。有了。name。我使用那个X这个用户吧，行吧。把它查出来之后呢，后面sa.H。等于八，注意如果说它里面没有这个属性，那就把这个属性给加上去，如果有这个属性了，那把这个属性值改成18。看到没有添加一个属性。现在我们可以查一下它。你看a。看到没有，它里面一个name是XH是什么？具有刚才我们给他加了一个属性啊。那接下来我们看一下如何删除关系。match。然后呢？name。a，对，你前面这个变量呢，你能用到了，那你就给它起个变量，如果你用不到，那你就不用起。我们在这呢用不到，所以说就不给它起变量，变成清不洗都无所谓啊。follow。我们看一下之前那个数据这个a。你看它其实呢，关注了BXY对吧，那我们随便找一个吧。name。X吧，对吧，它对它呢也有一个分关系。注意，我们最终啊，想把他们两个之间那个follow关系给它删掉。那怎么办，这时候啊，你要给这个合作关系啊，也起一个别名。起个名称，这样的话在后面呢，使用第一层。这样就可以把它给删掉。看到没有，删除了一个关系。点包。这时候这个a是不是就没有关注那个X了呀。你可以到这儿来查一下。看到没有对吧，X它现在就没有人去follow。就变成一个孤家寡人了，对吧，又没有连到这里面。OK，这就是用户内容针对更新数据的相关操作。如果说你想去删除一条数据啊，就删除一个节点，那你把它查出来对吧，把它查出来后面呢，直接给它就类似这种。前面呢写这个，后面呢加上一个a，就可以把这个name等于X的这个user给它删掉。这个呢，给大家留一个作业，下一周我们自己操作一下行吧，我这个呢就不再演示这个。</span><br></pre></td></tr></table></figure><h3 id="更新节点属性"><a href="#更新节点属性" class="headerlink" title="更新节点属性"></a>更新节点属性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">match (a:User &#123;name:&quot;X&quot;&#125;) set a.age &#x3D; 18</span><br><span class="line"></span><br><span class="line">有更新，无则添加</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242247278.png" alt="image-20230424224750421"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242248429.png" alt="image-20230424224819117"></p><h3 id="更新节点之间的关系-边"><a href="#更新节点之间的关系-边" class="headerlink" title="更新节点之间的关系(边)"></a>更新节点之间的关系(边)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match (:User &#123;name:&quot;A&quot;&#125;) -[r:follow]-&gt; (:User &#123;name:&quot;X&quot;&#125;) delete r</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242250704.png" alt="image-20230424225000475"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242250214.png" alt="image-20230424225026098"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意:删除节点</span><br><span class="line">match (a:User &#123;name:&quot;A&quot;&#125;) delete a</span><br></pre></td></tr></table></figure><h2 id="Neo4j之建立索引-批量导入数据"><a href="#Neo4j之建立索引-批量导入数据" class="headerlink" title="Neo4j之建立索引+批量导入数据"></a>Neo4j之建立索引+批量导入数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下neo4j中的索引</span><br><span class="line"></span><br><span class="line">neo4j中的索引可以细分为两种</span><br></pre></td></tr></table></figure><h3 id="普通索引"><a href="#普通索引" class="headerlink" title="普通索引"></a>普通索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">普通索引 CREATE INDEX ON :User(name)</span><br><span class="line"></span><br><span class="line">第一种是普通索引，使用create index 可以实现，指定给节点中的某个属性建立索引，</span><br><span class="line">具体建立索引的依据是后期我们在查询的时候是否需要在where中根据这个属性进行过滤，如果需要则建立索引，如果不需要则不建立索引。</span><br></pre></td></tr></table></figure><h3 id="唯一索引"><a href="#唯一索引" class="headerlink" title="唯一索引"></a>唯一索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">唯一约束 CREATE CONSTRAINT ON (u:User) ASSERT u.id IS UNIQUE</span><br><span class="line"></span><br><span class="line">第二种索引称之为唯一约束，类似于mysql数据库中主键的唯一约束。</span><br><span class="line">使用CREATE CONSTRAINT可以实现</span><br><span class="line">CREATE CONSTRAINT ON (u:User) ASSERT u.id IS UNIQUE</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那这两种在使用的时候具体该如何选择呢？</span><br><span class="line"></span><br><span class="line">如果某个字段的值是唯一的，并且后期也需要根据这个字段进行过滤操作，那么就可以建立唯一约束，唯一约束的查询性能比索引更快</span><br></pre></td></tr></table></figure><h3 id="批量导入数据"><a href="#批量导入数据" class="headerlink" title="批量导入数据"></a>批量导入数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们将学习一下neo4j如何批量导入数据</span><br><span class="line">针对项目一开始的时候有一批海量数据需要导入，我们就不能使用前面讲的那种命令一条一条导入了，性能太差，我们需要有一个批量导入的方式来快速导入这一批数据。</span><br><span class="line">neo4j批量导入数据有两种方式，一种是这个batch import的，还有一种呢是load csv</span><br><span class="line"></span><br><span class="line">第一种这个batch import，它呢需要组装三个文件，导入性能呢比较快，但是呢比较麻烦。</span><br><span class="line"></span><br><span class="line">第二种这个load csv呢，它呢只需要把数据组装到一个CSV文件即可。导入性能没有batch import快，但是也没有我们想象中的那么慢，还是可以接受的啊，它的优点呢，就说使用起来很方便，直接把所有需要的数据直接都组装到一个CSV文件即可。</span><br><span class="line"></span><br><span class="line">那在这里啊，我们考虑到一个易用性。由于我们的原始数据都在MYSQL中，我们可以通过MYSQL命令直接把数据导出为一个文件。所以接着呢，我们直接使用load CSV会更加的方便。</span><br><span class="line"></span><br><span class="line">但是在这有一点需要注意。在load csv中啊，我们如果使用到了merge或者match这些命令的时候，我们需要确认关键字段是否有索引，否则呢，性能会很差，怎么理解呢？来看一下。就针对这种match，或者咱们前面讲的那种merge。你这个merge，它在执行的时候，其实呢，它会根据这个name看看有没有这条数据，对吧，如果没有的话，它才会新增，如果有的话，他就不会再新增了，所以说呢，它需要根据name这个字段去查询数据，那所以说呢，你就需要根据name了。去建立索引，如果你没有建立索引，后期用户内容数据量大之后，这块平行效率会很差。以及这个match也是一样的，match里面你看没有，你这个其实是根据那个name去查的。所以这时候的话，这个name字段对吧，它也是需要有索引。以及这个where后面这些过滤条件，对吧，也是需要有索引。如果这些关键字段没有建立索引的话，那其实这些操作它就相当于是一个全表扫描了，所以说呢，你数据量越多，它的查询性能会越差。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如：merge(a:User &#123;name:“A”&#125;)，此时就需要提前对User中的name字段建立索引，否则在进行初始化的时候，数据量大了之后，初始化的性能会很差，因为merge在执行的时候会查询name等于A的数据在不在neo4j中，如果name字段没有建立索引，则会执行全表扫描。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那接着啊，我们先把这个neo4j的数据啊给它清空了，那如何清空数据呢。接着呢，就给大家一种暴力的方式啊。你把它的data目录给删了。因为他的所有数据啊，都放到那个date目录里面。然后呢，重启一下neo4j。stop一下。再启动一下，那接着呢，我们来看一下，我准备了一个测试的一个数据文件。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181652001.png" alt="image-20230518165227850"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">fuiduid</span><br><span class="line">10011000</span><br><span class="line">10011004</span><br><span class="line">10011005</span><br><span class="line">10012001</span><br><span class="line">10021000</span><br><span class="line">10021004</span><br><span class="line">10022001</span><br><span class="line">10031000</span><br><span class="line">10031004</span><br><span class="line">10061000</span><br><span class="line">10061005</span><br><span class="line">20021004</span><br><span class="line">20021005</span><br><span class="line">20022004</span><br><span class="line">20031000</span><br><span class="line">20031005</span><br><span class="line">20032004</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个文件里面一共有两列。fuid是关注者。uid是被关注者。你可以把它认为是主播，这是观众对吧。那我们需要把这个数据啊，给他做一下初始化。那怎么初始化呢，注意。你想要对这个数据做初始化的话，你首先啊，需要把这个文件上传到NEO4J_HOME的import目录下才可以使用。注意你必须要放在这个目录下面才能使用啊，否则neo4j会找不到。这个需要注意一下，那下面呢，我们就把这个批量导入命令啊，先给他写一下。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 neo4j-community-3.5.21]# bin&#x2F;cypher-shell -a bolt:&#x2F;&#x2F;bigdata04:7687 -u neo4j -p admin</span><br><span class="line">Connected to Neo4j 3.5.21 at bolt:&#x2F;&#x2F;bigdata04:7687 as user neo4j.</span><br><span class="line">Type :help for a list of available commands or :exit to exit the shell.</span><br><span class="line">Note that Cypher queries must end with a semicolon.</span><br><span class="line">neo4j&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">首先针对关键字段建立索引</span><br><span class="line">neo4j&gt; CREATE CONSTRAINT ON (user:User) ASSERT user.uid IS UNIQUE;</span><br><span class="line">0 rows available after 281 ms, consumed after another 0 ms</span><br><span class="line">Added 1 constraints</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">然后批量导入数据</span><br><span class="line">neo4j&gt; USING PERIODIC COMMIT 1000</span><br><span class="line">       LOAD CSV WITH HEADERS FROM &#39;file:&#x2F;&#x2F;&#x2F;follower_demo.log&#39; AS line FIELDTERMINATOR &#39;\t&#39;</span><br><span class="line">       MERGE (viewer:User &#123; uid: toString(line.fuid)&#125;)</span><br><span class="line">       MERGE (anchor:User &#123; uid: toString(line.uid)&#125;)</span><br><span class="line">       MERGE (viewer)-[:follow]-&gt;(anchor);</span><br><span class="line">0 rows available after 791 ms, consumed after another 0 ms</span><br><span class="line">Added 11 nodes, Created 17 relationships, Set 11 properties, Added 11 labels</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在neo4j的web界面上也可以执行，需要添加:auto命令。</span><br><span class="line">解释：</span><br><span class="line"></span><br><span class="line">PERIODIC COMMIT 1000：每1000条提交一次，这个参数非常关键，如果在数据量很大的情况下内存无法同时加载很多数据，所以需要批量提交事务，这样可以减小任务失败的风险，并且也可以提高数据导入的速度，当然这需要设置一个合适的数量。</span><br><span class="line">WITH HEADERS：是否使用列名，如果文件中有列名，则可以加这个参数，这样在读取数据的时候就会忽略第一行</span><br><span class="line">FIELDTERMINATOR ‘\t’：指定文件中的字段分隔符</span><br><span class="line">然后我们到页面上看一下导入的数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305181659391.png" alt="image-20230518165932413"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：此时我们会发现在页面中的圆圈中没有显示数据的具体内容，之前我在用3.2版本的时候是没有这个问题的，现在使用新的3.5版本之后会发现页面显示的时候会出现这种问题，这个问题倒没什么影响，就是在页面中看起来不太方便而已。</span><br><span class="line"></span><br><span class="line">通过我的测试发现</span><br><span class="line">如果我们在添加节点数据的时候，给节点指定一个name属性，那么name属性的值默认会显示在这个圆圈里面，如果不是name字段，则不显示。这个应该是新版本的一些特性。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">批量初始化数据</span><br><span class="line"></span><br><span class="line">针对关键字段建立索引</span><br><span class="line">create constraint on (user:User) assert user.uid is unique;</span><br><span class="line"></span><br><span class="line">批量导入语句</span><br><span class="line">using periodic commit 1000</span><br><span class="line">这什么意思呢？注意这个表示呀，可以是每1000条起交一次，这个表示设置一个事物提交的一个大小啊。这个参数呢非常关键，如果说你这个数据量非常大，那你把这个全量的数据全部都读出来，全部都放到内存里面，这样的话内存可能扛不住，所以说呢，建议呢批量去提交事务。这样可以减小任务失败的风险，并且呢，也可以提高数据导入的速度。当然，这需要设置一个合适的数量，这个数量太大或者太小其实都不合适啊，就类似于我们平时往mysql里面批量提交数据一样，提交数据也是一批一批的。</span><br><span class="line">load csv with headers from &#39;file:&#x2F;&#x2F;&#x2F;follower_demo.log&#39; as line fieldterminator &#39;\t&#39;</span><br><span class="line">注意这个其实呢，就相当于从本地这个根目录下面读取了。我们之前把这个文件放到那个neo4j的import目录下了，注意你只要放到了import目录下面，那其实呢就是相当于是从根目录读取，这个是neo4j来设定的。他就会读取这个文件里面内容，一次读一行，一行数据这个字段之间是分隔符这指定一下。那这样的话，其实前面这两行呢，基本就把这个功能属性设置好了，</span><br><span class="line"></span><br><span class="line">merge (viewer:User &#123;uid: toString(line.fuid)&#125;)</span><br><span class="line">把那个第一列取出来，注意外面这个呢，我们使用的是一个tostring，它是一个函数啊，你本来一读出来之后呢，这个是一个数字，我们要把它转成一个字符串啊，因为本身我们这个UID就是一个字符串。</span><br><span class="line"></span><br><span class="line">merge (anchor:User &#123;uid: toString(line.uid)&#125;)</span><br><span class="line">下面这个呢，是一个主播anchor。</span><br><span class="line"></span><br><span class="line">merge (viewer) -[:follow]-&gt; (anchor);</span><br><span class="line">那下面把他们之间的关系给watch。V。WS。安。这样就可，那我们来执行一下这个命令，注意这个命令呢，你可以在这个外部界面去执行。在这执行也可以啊，但是在这执行的时候呢。你执行这条命令可以直接执行，但是你在执行这个时候，它会提示让你在前面加一个什么auto，有一个自动提交事务。这是一种方式，这个给大家留个作业，下一周呢自己实验一下，我呢先不用这种方式。我用哪种方式呢？我就直接在我们的雷命令行里面去做。注意咱们之前不是把这个六库率给它重新删了相，那重新启动了吗？现在于是一个新的六扣率了，那所以说你在这啊。还需要去修改一下密码。相当于我们把那个对的目录删了之后，它就是一套新的东西。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;cypher-shell -a bolt:&#x2F;&#x2F;bigdata04:7687 -u neo4j -p admin</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242334611.png" alt="image-20230424233404885"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242334532.png" alt="image-20230424233420853"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242335175.png" alt="image-20230424233527600"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304242337963.png" alt="image-20230424233725604"></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那这个大家有没有感觉到有一些看起来不太一样的地方？咱们前面在创建这个节点的时候，你看节点上面是不是显示的那个内蒙的一个值啊。那你说我现在其实也有一个UID的值啊，你为什么这边没显示出来呢。注意了啊，之前啊，我在用这个纽破利的3.2那个版本的时候呢，是没有这个问题的，那现在呢，使用新的这个3.5这个版本之后啊，发现它这个页面显示的时候呢。会出现这种现象。这种现象啊，倒也没什么影响啊，就说了我们在页面中看起来啊，不太方便而已。那通过我的测试发现呀。我们在添加节点数据的时候呢，如果说你给这个节点啊，指定的一个内部属性，那么内部属性的值默认会显示在这个圆圈里面。如果不是内部的一个字段。就不显示你现在有这个UID字段，不显示这个呢，相当于是这个新版本的一些特性啊。不过这倒不影响啊，只不过说在这看起来啊，有点不太习惯，它这个数据呢，你看它其实存点对吧，UID这个值都是有的啊。好。下面呢，我们来验证一下，我们把UID这个属性的一个名称啊，给它改一下，把它改成name，看看这个值啊，会不会显示到这个圆圈里面。那个只是一个显示形式而已啊，我们来验证一下。验证呢很简单，把这个复制出来一份。对吧，然后在这呢改一下。看了什么？这个测试的。这块呢，这个属性名称改成name。这个也是name对吧。好，这个时候呢，我就在这里面来执行，注意我直接拿过来执行啊，它其实呢会报错。先看一下。看到没有？我搞错了。它下面有个提示。就说啊，你需要加一个什么呀，冒号凹凸。这样才行。所以说呢，也就意味着在它前面啊，加一个冒号。而是自行提交。这就可以了。该成功了，你看这又有了。这样的一个test。看到没有，这样就显示了。是什么？遇到这个问题啊，也不要太感到惊讶，这个只是在新版本上做一些改动，之前那个老版本是没有问题的，就是我们之前线上那个版本是OK的啊。后来呢，给大家在这讲的时候，我们用了一个新的版本，稍微新一点就有一些变化啊。其实两个效果是完全一样的啊，这个只是在这显示而已啊。就是看起来清晰一些，这样可能看起来不太清晰啊，有点B。那这样的话，我们现在就实现了一个批量数据的一个初始化，其实就很简单了，现在呢，我们后期啊，可以把我们想要初始化那些数据啊呃，提前导成这种文件。然后在这写一个这个P，触发一个脚本就OK。所以说呢，那CSV这种方式还是比较方便的，你直接把你需要的数据全部都组装到这一个文件里。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="直播平台三度关系推荐v1.0" scheme="http://tianyong.fun/tags/%E7%9B%B4%E6%92%AD%E5%B9%B3%E5%8F%B0%E4%B8%89%E5%BA%A6%E5%85%B3%E7%B3%BB%E6%8E%A8%E8%8D%90v1-0/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-Spark Streaming-6</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-6.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-6.html</id>
    <published>2023-04-23T14:22:49.000Z</published>
    <updated>2023-04-26T10:44:42.994Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-Spark-Streaming-6"><a href="#第十一周-Spark性能优化的道与术-Spark-Streaming-6" class="headerlink" title="第十一周 Spark性能优化的道与术-Spark Streaming-6"></a>第十一周 Spark性能优化的道与术-Spark Streaming-6</h1><h2 id="SparkStreaming-wordcount程序开发"><a href="#SparkStreaming-wordcount程序开发" class="headerlink" title="SparkStreaming wordcount程序开发"></a>SparkStreaming wordcount程序开发</h2><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面我们来学习一下Spark中的Spark streaming。针对Spark Streaming，我们主要讲一些基本的用法，因为目前在实时计算领域，flink的应用场景会更多。Spark streaming啊，它是Spark Core API的一种扩展。它可以用于进行大规模、高吞吐量、容错的实时数据流的处理。大家注意这个实时啊，属于近实时。最小可以支持秒级别的实时处理。</span><br><span class="line"></span><br><span class="line">那spark streaming的工作原理呢？是这样的。它呢会接收实时输入的数据流，然后呢，将数据啊拆分成多个Batch。比如呢，每收集一秒数据给它封装为一个batch，然后将每个batch呢交给这个spark计算引擎进行处理。最后呢，会产生出一个结果数据流。这个结果数据流里面数据呢，也是由一个一个的batch所组成的，所以说呢，spark streaming的实时处理，其实呢，就是一小批一小批的处理。那下面呢，我们就来开发一个spark streaming的实时wordcount程序来感受一下。</span><br><span class="line"></span><br><span class="line">现在我们来创建一个项目。it。点击这个auto。把它那个基本环境再配置一下，因为我们在这呢，也要写了一个SC，所以说在这点右键。到这儿。在这下面创建一个GALA。对吧，然后注意右键把它视为S对吧。接下来到这个depends里面，注意添加那个scholar的SDK，注意针对这个我们需要添加02:11的。因为我们使用的那个卡夫卡集群，它那个是02:11编译的啊。那个SC的版本，所以说呢，在这儿使用02:11。好，这样就行了。那下面呢，我们需要找一下它对应的一个依赖，SPA，人命的依赖。现在我们来说一下。就那个。用02:11的，注意我们之前用的是2.4.3这个版本。S。把这个除掉是吧。好。那这样基本环境就OK了。下面呢，我们来开发这个word的程序，在这呢，先建一个package。I1克点。八。the stream count。需求呢，是这样的。通过socket。模拟产生数据。实时计算数据中。单词出现的次数。这个没方法。好，那在这注意，我们需要先创建一个streaming。context。然后呢，指定数据处理。间隔。所以五秒吧。因为我们前面说了，你SPA死命，他这个实时处理，其实还是一小批一小批的处理，所以说你需要指定它这个一小批这个间隔是多少秒。那现在我们直接利用一个streaming。这边呢，首先传一个。配一样康复。注意第二个呢，才是这个距离的时间叫。五秒。SSC吧。注意，那我们在上面来创建这个。mark。康复配置对象。嗯。嗯。了，我们现在本意来执行。注意咱们之前啊，开发这个发个离线代码的时候，我们呢，穿的都是logo对吧，注意这时候呢。你需要这样来写LOGO2。什么意思呢？所以。止住了。LOCAL2。表示启动两个进程。一个进程。否则读取。数据源的数据一个进程。负责处理数据。ABB name。好，这样就行了。嗯。好，接下来我们来通过socket。获取实时产生的数据。B04端口9001。这个可以叫SRD。是吧，这里面也是RDD啊。下面我们就对接收到的数据使用。空格进行切割。转换成单个单词。改个e lines RD。第二，find map。加你的SP。按空格切就行啊。这样返回的就是wasd是里面呢包含了每个单词。这样把每个单词。转换成。淘宝兔的形式啊。what map对吧？这个RD。下面来执行reduce。BYK操作啊，所以基于K进行求和。嗯。it is by k。嗯。有我。啊。下面呢，将这个结果数据打印到控制台。嗯。认识。对吧，你看这个代码是不是也和咱们前面写那个link代码很像呀。启动任务。嗯。注意它下面这种写法不太一样，和那个SPA离线写法都不一样啊。等待任务形式啊。嗯。啊。好，这样的话你就可以开启一个发达人命实时流处理程序了。那我们在这呢，把这个socket给它打开。来执行。这个没事啊，还是那个when you choose啊，这个不用管了。把这个日志清一下，好，下面注意在这我来输入点数据。右。the me。回来看到没有， hello2161。只要说你输入的那两项数据在它的一个时间段之内，对吧，在五秒之内，它其实就把它切到一块儿了。这就可以啊。所以说呢，你可以这样理解，它相当于是每隔五秒把前五秒的数据给你封装成一个batch。然后后面呢，其实执行的就类似于Spark核心的那个代码Spark I的对吧。其实就是离线的那一套。它前面的话是按照时间去切这个小批，后面的话把你一小批一小批去处理，这样的话可以达到一个进食时的一个效果啊，所以说这个就是方向十命它的一个执行的原理。好，这是实现，接下来呢，我们使用加来实现一下。键package。SPA。world can&#39;t。加。把这个需求拿过来。那下面呢，是一个密方法，好，那接下注意首先还是要获取这套什么使命contact这些东西。先获取。创建。sten。啊。所以这里面你去Java这面你要获取这个。Java。streaming。context。嗯。后面呢，传的还是一个。时间。R。u减。second。嗯嗯。SIC。那上面还是要创建这个SPA配对项啊。嗯。but。嗯。master。LOCAL2。name。好，这样也可以。注意这块报错。对，报错了，一般是你那个包引错了。你可以看一下，把鼠标放到这个上面，你看。说什么这是什么Java FX里面什么？这是有问题的。对吧，我们用的话肯定是用Spark里面。对，其实啊，你这后面是少了一个S啊。嗯。这样也可以。你看这个时候用的是发使命里面的。好。下面是通过。获取实时产生的数据。soirit。这个点零四。等零一。来阿。对接收到的数据使用空格进行切割。转换成。三个单词。嗯。哪一点find map？对，那这里面的话，我们就需要写一个函数了啊。你有一个map function。然后要返回一个swim。我们就不把那个map的代码也写进去啊。这样的话，它是一个。我可以这样来直接来写ari。there as list，它里面呢，直接line there。后面做。直接。这样就可以啊，因为它最终返回一个联系啊。这种写法啊，它返回的速度，这样把这个速度转成list，再把它转成这个就可以。我咋？那接下来是把每个。单词转换。喂。double two的形式。我1MAP。所以呢，这里面也是需要写一个函数的啊。你有一个T。嗯。这个呢是in。嗯。因为你最终要法是一个P2列，这就淘宝里面第一列，淘宝里面第二列。new。double two。这呢，其实就是一个over了，把名字改一下啊，看起来清晰点。war。一。这个呢，叫派。安你。接下来实行。YK。嗯。reduce。function。I1 I2。这个呢，就叫word count。最后将结果数据啊引到。台。我们直接使用那个不好1D啊。这里面呢，我们给它传一个你一个VID方。这里面其实也好办，这个呢，就是一个。higher。不好意思。然后这里面的话，再给它传一个VD方式。对，这个就是具体那个他。嗯。嗯。相量二。这样的话就可以把里面这些数据啊，给它迭代出来。然后呢，就剩下最后这个。行任务。start。还有一个，等待任务停止。好一场，好一起。嗯。嗯。好，这就可以了。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket模拟产生数据，实时计算数据中单词出现的次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCountScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      <span class="comment">//注意：此处的local[2]表示启动2个进程，一个进程负责读取数据源的数据，一个进程负责处理数据</span></span><br><span class="line">      .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      .setAppName(<span class="string">"StreamWordCountScala"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建StreamingContext，指定数据处理间隔为5秒</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//通过socket获取实时产生的数据</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD = ssc.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对接收到的数据使用空格进行切割，转换成单个单词</span></span><br><span class="line">    <span class="keyword">val</span> wordsRDD = linesRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把每个单词转换成tuple2的形式</span></span><br><span class="line">    <span class="keyword">val</span> tupRDD = wordsRDD.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行reduceByKey操作</span></span><br><span class="line">    <span class="keyword">val</span> wordcountRDD = tupRDD.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将结果数据打印到控制台</span></span><br><span class="line">    wordcountRDD.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232245799.png" alt="image-20230423224530561"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232245196.png" alt="image-20230423224547229"></p><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket模拟产生数据，实时计算数据中单词出现的次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamWordCountJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//创建SparkConf配置对象</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">                .setAppName(<span class="string">"StreamWordCountJava"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建StreamingContext</span></span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过socket获取实时产生的数据</span></span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; linesRDD = ssc.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//对接收到的数据使用空格进行切割，转换成单个单词</span></span><br><span class="line">        JavaDStream&lt;String&gt; wordsRDD = linesRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//把每个单词转换为tuple2的形式</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairRDD = wordsRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行reduceByKey操作</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; wordCountRDD = pairRDD.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> i1 + i2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//将结果数据打印到控制台</span></span><br><span class="line">        wordCountRDD.foreachRDD(<span class="keyword">new</span> VoidFunction&lt;JavaPairRDD&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(JavaPairRDD&lt;String, Integer&gt; pair)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                pair.foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        System.out.println(tup._1+<span class="string">"---"</span>+tup._2);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//启动任务</span></span><br><span class="line">        ssc.start();</span><br><span class="line">        <span class="comment">//等待任务停止</span></span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SparkStreaming整合Kafka"><a href="#SparkStreaming整合Kafka" class="headerlink" title="SparkStreaming整合Kafka"></a>SparkStreaming整合Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这个sparkstreaming和kafka的整合。我们的需求是这样的，使用sparkstreaming实时消费kafka中的数据。这种场景也是比较常见的。注意，在这你想使用kafka，我们需要引入对应的一个依赖。它那个依赖是什么呢？到官网来看一下，进到官网点那个。文档SPA人命。你这里面来搜一下啊。就梳里的卡夫卡往下边走。到这块。看到没有，把这个点开右键啊，打开一个新页面。看到没有？你这个SPA命想要和卡夫卡进行交互，想要从卡夫卡里面去消费数据，你需要添加对应的依赖。这个呢是针对0.8.2.1级以上的，这个呢是针对卡夫卡零点十几以上的，那我们用的话肯定要用这个了，要用新一点的啊。那就这个东西。你可以。把它复制过来，到这儿来搜一下。就这个。2.4.3。格拉，02:11。把这个拿回来就可以了。嗯。这样就可以了。那下面呢，我们就来写一下具体的一个代码。swim。卡不卡？bug消费。卡夫卡中的数据。嗯。首先呢，还是希望见。streaming。嗯。context。康复。然后呢，还有一个second。嗯。有没有？我们还使用这个五秒。具体这个时间间隔啊，需要根据你们的业务而定啊。等于你有一个Spark。said master。LOCAL2。set name。嗯。获取消费卡夫卡的数据流。那怎么获取呢？现在我们需要用的这个卡夫卡u。csa。direct。这里面需要传两个泛型参数，three。SH。这里面呢，首先把这个SSC传给他。嗯，所以接下来需要第二个参数。什么那个location。street。这个。它这边会有提示啊。对吧，这几个参数。好使，用它呢，来调一个P开头的这个。就第一个就行。接下来只听下面那个。street。用这个。这个发型呢，还是string？对，这里面你要指定一下topic，对他接触的是一个topic s啊是一个。然后后面呢是一个map map里面传的是卡夫卡的一些参数。这里面这些都是固定写法啊。那下面我们就要呈现这两个。指定。指定卡夫卡的配置信息。好不好？等于一个map。object。我们直接在里面给它初始化就行啊。显得不报错。还有这个topic，在这一个它指定性。嗯。只要topics。注意，在这我们需要传一个。里面呢，可以同时使用多套贝。也就是说，它可以同时从多个topic里面去读取数据，都是可以的。那我们在这一个，我们就写一个就行。那既然把这个参数给它完善一下啊，嗯，首先需要指定卡不卡的。broke地址信息。would strive。B01。9092。零二。9092。039092。嗯。接下来我们需要指定那个K的序列化类型。K点。s Li。展开一个反序变化啊D。Siri。a。对啊。你如果怕拼数的话，可以先把后面这个写。后面的话，我们使用这个class of。spring。size对吧。可以把这个给它复制过去，把这个D改成小写就行了啊。这样也可以。嗯。还有这个value的。序列化类型。嗯。那就把这个改一下就行，改成V。都是死顿类型啊对，这就是咱们前面指定的那个对吧，配合没有一个泛型啊。嗯。那下面来指定那个。消费者ID。就那个YD。胳膊的ID。好，下面呢，再指定一下消费策略。there there。这块呢，只能一个。最后我们来指定一个自动提交。在设置啊。enable。there also？好。这样就可以了。这是一些核心的参数。OK，这样的话就可以从它里面去读取数据啊，这样就可以获取到一个类似于卡夫卡。我在这里面，我们可以把它称为。嗯，这是他们的一个概念啊。这个数据流。那下面我们就可以处理数据了。后面你可以调map啊，map啊这些算子去处理就可以。嗯。这样每次获取到一条数据啊，每次几的一条数据。然后把里面数据迭代出来之后呢，把它封装成一个他报，因为它本身呢是一个record一行记录，那我们在这呢。对的点。先获取的，你们K。然后再获取value。嗯。在这我们就把这个数据打印出来。将数据。印到后来。因为其实你在这只要能获取到数据，你后期你想做map map reduce go是不是都可以呀，对吧，那个就没什么区别了。启动任务。start。等待任务停止。嗯。嗯，好。那下面呢，我们把这个运行起来。但是呢，你发现这块他报错了。看到没有？31行。遇到问题不要怕啊，这个问题我们要排查一下。他说这个类型啊，有点问题。他呢，发现了是一个布尔类型，结果他需要了是一个OB。所以这块的话，你需要在这这样来指定一下。强制执行类型Java点拉点布尔。嗯。这样就可以了啊来执行。好，这样就可以了。那接下来呢，我们来开启一个生产者，往里面写点儿数据。其中一个卡不卡，剩下的我里面写着数据啊。其控制台的生产者。嗯。hello Spark。嗯。看到没有打一回啊。我把它停一下啊。注意。它这个呢，we know，为什么呀。因为现在我们卡卡里面数据啊，其实只有value是没有那个K的啊，所以说你K答出来是no，我们G的那些数据啊，一般都放到value里面啊。就是放到外。这个K的话是为了判定你这个数据到底是放哪个分区里面啊，一般会传一个K。所以说我们那种说法一般是不传的，然后随机分啊。这是没有问题的。那这样的话，我们就可以把那个卡字卡里面数据给他消费出来。是吧，那后面就可以实现你的业务逻辑。OK。那接下来呢，我们使用这个加代码来实现一下。卡夫卡。加了。嗯。把这个注释拿过来。好，首先呢，获取这个streaming context。在指定读取数据的。时间间隔为五秒。嗯。有一个Java。streaming context。好点。这个大家看五秒。嗯。new。said master。logo。嗯。下载APP name。把这个拿过来，嗯。而且这个变量Co。嗯。那接下来我们来获取消费卡夫卡的数据流。还是那个卡不卡。great。direct，那首先SC。后面还是一样的。嗯。嗯。嗯。嗯。嗯。好，接下来是这个。consumer。这个。there。首先是一个topics，还有一个。搞不搞？注意这块啊。你需要指定泛型，你这个泛型写到哪了。我们在SKY面里面是放在这个位置，但是在这里面你写这还是不对的啊，你在写前面。three。W。好。接下来创建这两个啊，把这个topic，还有这个卡夫卡。在线行。指令要读取的。名称。先写这个。three。topics。嗯。挨着。七。那接下来是这个。有一个map。object。好，不搞。下面就往里面添加参数了啊。RI。service。我们俩复制一下吧。这个又是提花。嗯。嗯嗯。第二个呢，是这个K的这个虚化类型。所以这个你别导错包了啊，你要导这个。巴阿巴奇，看到没有，卡布卡点common这个body。name。嗯。嗯嗯。说不爱你。嗯。also。offset。there reet。at。ne。点点commit。好，这样就可以了。of Australia。好，那接下来数数去。嗯。嗯嗯。选一个map，因为一个function。我们最终返回是一个double two。里面是一个string。LW。好，这个就是一个record。我们可以在这直接。你了一个。two，嗯。你告你。six。嗯。我看点160。这样转换成淘宝之后，后期用起来也方便。SP对吧。将数据打印的。启动任务，嗯。start。等待任务停止。嗯。好一场。嗯。嗯。好，这样就可以了，来。把它执行一下。好看没有，这是之前那条数据啊。我们可以再往里面加一条。哈哈哈。可以吧，也是可以的啊。好，这就是Java代码的一个实现。好，那针对Spark命这一块呢，我们暂时就讲到这儿，因为后期大部分的实施计算需求，我们需要使用link去实现了。在这呢，我们是把这个SPA命最常见那个消费卡不卡数据这种案例呢给大家讲一下。</span><br></pre></td></tr></table></figure><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spark 消费Kafka中的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"StreamKafkaScala"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定Kafka的配置信息</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Object</span>](</span><br><span class="line">      <span class="comment">//kafka的broker地址信息</span></span><br><span class="line">      <span class="string">"bootstrap.servers"</span>-&gt;<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>,</span><br><span class="line">      <span class="comment">//key的序列化类型</span></span><br><span class="line">      <span class="string">"key.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//value的序列化类型</span></span><br><span class="line">      <span class="string">"value.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//消费者组id</span></span><br><span class="line">      <span class="string">"group.id"</span>-&gt;<span class="string">"con_2"</span>,</span><br><span class="line">      <span class="comment">//消费策略</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span>-&gt;<span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">//自动提交offset</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span>-&gt;(<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//指定要读取的topic的名称</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"t1"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">    <span class="keyword">val</span> kafkaDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    kafkaDStream.map(record=&gt;(record.key(),record.value()))</span><br><span class="line">      <span class="comment">//将数据打印到控制台</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232301727.png" alt="image-20230423225935601"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232301407.png" alt="image-20230423225949397"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spark 消费Kafka中的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//创建StreamingContext，指定读取数据的时间间隔为5秒</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">                .setAppName(<span class="string">"StreamKafkaJava"</span>);</span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafka的配置信息</span></span><br><span class="line">        HashMap&lt;String, Object&gt; kafkaParams = <span class="keyword">new</span> HashMap&lt;String, Object&gt;();</span><br><span class="line">        kafkaParams.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"key.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"value.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"group.id"</span>,<span class="string">"con_2"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"auto.offset.reset"</span>,<span class="string">"latest"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"enable.auto.commit"</span>,<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定要读取的topic名称</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        topics.add(<span class="string">"t1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; kafkaStream = KafkaUtils.createDirectStream(</span><br><span class="line">                ssc,</span><br><span class="line">                LocationStrategies.PreferConsistent(),</span><br><span class="line">                ConsumerStrategies.&lt;String, String&gt;Subscribe(topics, kafkaParams)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">//处理数据</span></span><br><span class="line">        kafkaStream.map(<span class="keyword">new</span> Function&lt;ConsumerRecord&lt;String, String&gt;, Tuple2&lt;String,String&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(ConsumerRecord&lt;String, String&gt; record)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(record.key(),record.value());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();<span class="comment">//将数据打印到控制台</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//启动任务</span></span><br><span class="line">        ssc.start();</span><br><span class="line">        <span class="comment">//等待任务停止</span></span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink新版本1.12以上-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-2.html</id>
    <published>2023-04-20T08:46:48.000Z</published>
    <updated>2023-06-02T08:37:04.596Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="Flink新版本1-12以上-2"><a href="#Flink新版本1-12以上-2" class="headerlink" title="Flink新版本1.12以上-2"></a>Flink新版本1.12以上-2</h1><h2 id="State-状态-的容错与一致性"><a href="#State-状态-的容错与一致性" class="headerlink" title="State(状态)的容错与一致性"></a>State(状态)的容错与一致性</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了State的原理和在代码中的使用。下面我们来深入分析一下State的容错和一致性。</span><br><span class="line"></span><br><span class="line">4.1 State的容错与一致性</span><br><span class="line">针对一个Flink流式任务，如何保证这个任务故障后恢复到之前的运行状态？</span><br><span class="line">注意：这里所说的恢复到之前的运行状态是表示将算子中计算的中间结果恢复到任务停止之前的样子。</span><br><span class="line"></span><br><span class="line">其实咱们前面也提到过，想要实现状态的这种容错效果，需要借助于checkpoint机制。</span><br><span class="line"></span><br><span class="line">因为checkpoint可以将状态数据持久化保存到外部存储系统中，这样任务恢复时，可以基于之前存储到外部的状态数据进行恢复。</span><br><span class="line"></span><br><span class="line">针对流式计算任务，在故障后恢复状态数据的时候会涉及到三种语义：</span><br><span class="line"></span><br><span class="line">至少一次：At-least-once，这种语义可能会导致数据恢复时重复处理数据。</span><br><span class="line">至多一次：At-most-once，这种语义可能会导致数据恢复时丢失数据。</span><br><span class="line">仅一次：Exactly-once，这种语义可以保证数据只对结果影响一次，可以保证结果的准确性，不会出现重复或者丢失的情况。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：这里的仅一次语义，表示数据对最终结果的影响只有一次，并不是说数据只被处理一次。</span><br><span class="line"></span><br><span class="line">如果想要实现流式计算任务中数据的一致性，其实就是想要在流式计算任务中实现这种仅一次语义。</span><br></pre></td></tr></table></figure><h3 id="流计算中Exactly-once语义的多种实现思路"><a href="#流计算中Exactly-once语义的多种实现思路" class="headerlink" title="流计算中Exactly-once语义的多种实现思路"></a>流计算中Exactly-once语义的多种实现思路</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对流式计算任务中Exactly-once语义的实现思路其实是有多种的，下面我们来分析一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021237900.png" alt="image-20230602123745704"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">第一种思路：借助于At-least-once语义，再加上去重功能来实现。这种实现思路需要我们程序员自己来维护一个去重功能，因为At-least-once语义可以保证数据不丢，但是可能会出现数据重复，这样当任务故障后恢复的时候，对重复的数据进行去重，这样就可以间接实现Exactly-once这种仅一次语义了。</span><br><span class="line"></span><br><span class="line">这种实现思路属于中等难度，因为需要我们自己开发一个去重功能，保证数据不重复，对应的这个功能也会有一些性能开销，所以性能开销这方面属于中等。</span><br><span class="line"></span><br><span class="line">第二种思路：借助于At-least-once语义，再加上幂等操作，这种实现思路需要依赖于第三方存储系统的特性，也就是说这个第三方存储系统需要支持幂等操作，幂等操作表示一条命令重复执行多次，对最终的结果只有一次影响。</span><br><span class="line">举个例子：针对redis中的set命令，我们执行set a 1 ,这条命令不管执行多少次，a的值始终都是1，那么这个命令我们就可以认为是幂等操作。</span><br><span class="line">针对redis的incr命令，我们执行incr a，此时每执行一次，a的值就会加1，所以多次执行会导致结果发生变化，那么这个命令就不是幂等操作。</span><br><span class="line">这样在使用At-least-once语义的时候，他可能会导致数据重复，借助于幂等操作也是可以实现Exactly-once这种仅一次语义的。</span><br><span class="line"></span><br><span class="line">这种实现思路比较简单，对应的性能开销也比较低，不需要我们额外维护什么功能。</span><br><span class="line"></span><br><span class="line">第三种思路：借助于状态和checkpoint机制来实现，这种实现思路其实我们前面已经分析过了，他是可以实现仅一次语义的。</span><br><span class="line"></span><br><span class="line">这种实现思路也比较简单，对应的性能开销也比较低。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对这三种实现思路，下面有几张图我们来看一下，加深一下理解：</span><br><span class="line"></span><br><span class="line">思路1：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021240656.png" alt="image-20230602124044160"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">相当于我们在中间的算子中需要借助于外部的存储系统实现一个去重功能，当任务故障后恢复的时候，遇到重复数据在处理的时候可以进行去重，这样可以保证实现仅一次语义的效果。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">思路2：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021241563.png" alt="image-20230602124129333"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时需要依赖于外部存储系统的幂等特性，咱们前面以redis数据库为例进行了分析，如果是使用hbase数据库呢？</span><br><span class="line">hbase中rowkey是唯一的，我们只要保证重复数据的rowkey不变，那么执行多次put操作也是可以保证结果数据不变的。</span><br><span class="line"></span><br><span class="line">思路3：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021241575.png" alt="image-20230602124152921"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时是借助于状态和checkpoint机制实现，任务运行期间，定期对状态生成快照，任务故障恢复时基于之前的快照数据恢复状态，这样可以实现仅一次语义。</span><br></pre></td></tr></table></figure><h3 id="如何实现Flink任务的端到端一致性？"><a href="#如何实现Flink任务的端到端一致性？" class="headerlink" title="如何实现Flink任务的端到端一致性？"></a>如何实现Flink任务的端到端一致性？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Flink可以借助于Checkpoint机制保证Flink系统内部状态的一致性。</span><br><span class="line"></span><br><span class="line">但是一个Flink流计算任务需要有Source、系统内部(也就是算子)，以及Sink这三部分组成。</span><br><span class="line"></span><br><span class="line">那这个时候应该如何实现Flink流计算任务整个链条的一致性保证？（也可以称为是端到端的一致性）</span><br><span class="line">端到端的一致性，意味着要保证从Source端到系统内部、再到最终的Sink端整个阶段的一致性。</span><br><span class="line">每一个阶段负责保证自己的一致性，整个端到端的一致性级别就取决于所有阶段中一致性最弱的那个阶段了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">那下面我们来具体分析一下这三个阶段如何保证自己的一致性？</span><br><span class="line"></span><br><span class="line">首先是Source端：这个时候外部数据源需要支持任务故障恢复时的数据重放机制，不能说数据取出来之后数据源中就没有了，如果数据取出来之后任务失败了，肯定还是要重新再取一次的，所以说，外部数据源需要支持数据重放机制，在流计算任务中常用的外部数据源是kafka，kafka是可以支持数据重放机制的，我们可以通过控制消费者的消费偏移量实现数据重新消费。</span><br><span class="line"></span><br><span class="line">接下来是Flink系统内部，这块主要是依赖于Checkpoint机制实现。</span><br><span class="line"></span><br><span class="line">最后是Sink端：此时目的地存储系统需要保证数据恢复时不会重复写入。</span><br><span class="line"></span><br><span class="line">针对Sink端功能的具体实现包括两种方式，也就是说如何保证写入目的地存储系统时不会出现重复？</span><br><span class="line">第一种方式是：幂等写入</span><br><span class="line">幂等写入其实我们前面已经分析过了，就是说一个操作，可以重复执行很多次，但是只会导致一次结果更改，也就是说，后面再重复执行就不起作用了。</span><br><span class="line"></span><br><span class="line">第二种方式是：事务写入</span><br><span class="line">需要构建事务来写入外部系统，构建的事务对应着 checkpoint，等到 checkpoint 真正完成的时候，才把所有对应的结果写入目的地存储系统中。</span><br><span class="line"></span><br><span class="line">针对事务写入这种机制大致有两种实现方式：</span><br><span class="line"></span><br><span class="line">第一种实现方式：预写日志(WAL)的方式，这种方式无法100%保证Exactly-once语义，可能会出现数据重复。</span><br><span class="line">怎么理解呢？</span><br><span class="line">预写日志这种方式的通用性比较强，它会使用Operator State来存储数据，在任务发生故障时可以恢复，不会导致数据丢失，几乎适合所有外部系统，但是不能提供100%端到端的仅一次语义。</span><br><span class="line">因为基于预写日志的写入方式在某些极端情况下可能会将数据写入多次。</span><br><span class="line">例如：如果外部系统不支持原子性的写入多条数据，那么在向外部系统写入数据的时候可能就会出现部分数据已经写入，但是此时任务出现了故障，导致剩余一部分数据没有写入的情况。当下次恢复的时候会重写全部数据，这样数据就会出现部分重复。</span><br><span class="line"></span><br><span class="line">第二种实现方式：两阶段提交(2PC)的方式，这种方式可以100%保证Exactly-once语义。</span><br><span class="line">如果外部系统自身就支持事务（比如MySQL、Kafka），可以使用两阶段提交的方式，这样可以保证端到端的一致性。</span><br><span class="line">两阶段提交这种方式可以提供端到端的一致性保证，但是它的代价也是非常明显的，就是牺牲了延迟。输出数据不再是实时写入到外部系统，而是分批次地提交。</span><br><span class="line">目前来说，没有完美的故障恢复和仅一次语义保障机制，对于开发者来说，我们需要在不同需求之间权衡取舍。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对WAL预写日志这种方式，他提供的有一个接口是GenericWriteAheadSink，我们来看一下这个接口的大致实现：</span><br><span class="line">这是一个抽象类，目前还没有具体的实现。</span><br><span class="line">他里面用到了ListState，在initializeState方法中可以看到这个ListState属于Operator State这种类型。</span><br><span class="line">这个ListState负责在checkpoint的时候保存目前等待写出去的数据，便于任务故障时的数据恢复。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">private transient ListState&lt;PendingCheckpoint&gt; checkpointedState;</span><br><span class="line"></span><br><span class="line">checkpointedState &#x3D;</span><br><span class="line">        context.getOperatorStateStore()</span><br><span class="line">                .getListState(</span><br><span class="line">                        new ListStateDescriptor&lt;&gt;(</span><br><span class="line">                                &quot;pending-checkpoints&quot;, new JavaSerializer&lt;&gt;()));</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">两阶段提交这种方式会用到TwoPhaseCommitSinkFunction这个接口：</span><br><span class="line">这个接口里面主要有几个比较重要的方法：</span><br><span class="line">&#x2F;&#x2F;开启一个新事务</span><br><span class="line">protected abstract TXN beginTransaction() throws Exception;</span><br><span class="line">&#x2F;&#x2F;预提交-对应的就是第一阶段的提交</span><br><span class="line">protected abstract void preCommit(TXN transaction) throws Exception;</span><br><span class="line">&#x2F;&#x2F;提交-对应的是第二阶段的提交，此时数据才会真正提交到外部存储系统中，数据才对外可见</span><br><span class="line">protected abstract void commit(TXN transaction);</span><br><span class="line">&#x2F;&#x2F;回滚事务</span><br><span class="line">protected abstract void abort(TXN transaction);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们在使用Flink向kafka中写入数据想要保证数据一致性的时候就会用到这个接口，等后面我们再具体分析这个接口在kafka中的具体实现。</span><br></pre></td></tr></table></figure><h2 id="Checkpoint-快照-机制详解"><a href="#Checkpoint-快照-机制详解" class="headerlink" title="Checkpoint(快照)机制详解"></a>Checkpoint(快照)机制详解</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Checkpoint机制是Flink中实现State容错和一致性最核心的功能。</span><br><span class="line">它能够根据配置周期性地基于流计算中的状态数据生成快照，从而将这些状态数据定期持久化存储下来，当Flink程序一旦意外故障崩溃时，在重新运行程序的时候可以有选择地从这些快照进行恢复，从而修正因为故障带来的数据异常。</span><br><span class="line"></span><br><span class="line">默认情况下Checkpoint机制是处于禁用状态的，如果想要开启需要在代码层面进行设置。</span><br><span class="line"></span><br><span class="line">Checkpoint支持两种语义级别：Exactly-once和At-least-once，其中Exactly-once是默认的语义级别。</span><br><span class="line"></span><br><span class="line">Exactly-once语义对于大多数应用来说是合适的。</span><br><span class="line">At-least-once语义可能用在某些延迟超低的应用程序（始终延迟为几毫秒），相对而言，At-least-once语义不需要保证数据的强一致性，所以数据的传输延迟会比较低。</span><br><span class="line">如果某个需求对数据的准确度要求不是特别高，但是需要计算和传输的数据量比较大，还需要低延迟，那么可以考虑使用At-least-once语义。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来具体看一下如何在代码中开启Checkpoint，以及Checkpoint相关的一些核心配置。</span><br><span class="line">创建package：com.imooc.scala.checkpoint</span><br><span class="line"></span><br><span class="line">核心代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.storage.<span class="type">FileSystemCheckpointStorage</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">ExecutionCheckpointingOptions</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Checkpoint相关配置</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CheckpointCoreConf</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint，并且指定自动执行的间隔时间</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">60</span>*<span class="number">2</span>)<span class="comment">//2分钟</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//高级选项</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//设置语义模式为EXACTLY_ONCE（默认就是EXACTLY_ONCE）</span></span><br><span class="line">    cpConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//设置两次Checkpoint之间的最小间隔时间(设置为5秒：表示Checkpoint完成后的5秒内不会开始生成新的Checkpoint)</span></span><br><span class="line">    cpConfig.setMinPauseBetweenCheckpoints(<span class="number">1000</span>*<span class="number">5</span>)<span class="comment">//5秒</span></span><br><span class="line">    <span class="comment">//设置最多允许同时运行几个Checkpoint(默认值为1，也建议使用默认值1，这样可以减少资源占用)</span></span><br><span class="line">    cpConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//设置一次Checkpoint的执行超时时间，达到超时时间后会被取消执行，可以避免Checkpoint执行时间过长</span></span><br><span class="line">    cpConfig.setCheckpointTimeout(<span class="number">1000</span>*<span class="number">60</span>*<span class="number">6</span>)<span class="comment">//6分钟</span></span><br><span class="line">    <span class="comment">//设置允许的连续Checkpoint失败次数(默认值为0，表示Checkpoint只要执行失败任务也会立刻失败)</span></span><br><span class="line">    <span class="comment">//偶尔的Checkpoint失败不应该导致任务执行失败，可能是由于一些特殊情况(网络问题)导致Checkpoint失败</span></span><br><span class="line">    <span class="comment">//应该设置一个容错值，如果连续多次Checkpoint失败说明确实是有问题了，此时可以让任务失败</span></span><br><span class="line">    cpConfig.setTolerableCheckpointFailureNumber(<span class="number">3</span>)</span><br><span class="line">    <span class="comment">//设置在手工停止任务时是否保留之前生成的Checkpoint数据(建议使用RETAIN_ON_CANCELLATION)</span></span><br><span class="line">    <span class="comment">//RETAIN_ON_CANCELLATION：在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    <span class="comment">//DELETE_ON_CANCELLATION：只有在任务故障时才会保留，如果手工停止任务会删除之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    <span class="comment">//支持JobManagerCheckpointStorage(默认)和FileSystemCheckpointStorage</span></span><br><span class="line">    <span class="comment">//JobManagerCheckpointStorage：表示Checkpoint后的状态数据存储在JobManager节点的JVM堆内存中</span></span><br><span class="line">    <span class="comment">//FileSystemCheckpointStorage：表示Checkpoint后的状态数据存储在文件系统中(可以使用分布式文件系统HDFS)</span></span><br><span class="line">    <span class="comment">//可以简写为：cpConfig.setCheckpointStorage("hdfs://bigdata01:9000/flink-chk001")</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="keyword">new</span> <span class="type">FileSystemCheckpointStorage</span>(<span class="string">"hdfs://bigdata01:9000/flink-chk001"</span>))</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这些配置是针对单个任务有效的，并且需要放到具体的任务中才有意义。目前这个类里面的代码是不完整的，无法正常执行，等后面我们把这些代码拿到对应的任务中去使用。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">好的，我来解释一下。在Flink中，Checkpoint是用来保存程序运行状态的一种机制。env.enableCheckpointing(1000*60*2)是用来设置Checkpoint的间隔时间，即每隔多长时间进行一次Checkpoint。cpConfig.setMinPauseBetweenCheckpoints(1000*5)是用来设置两次 Checkpoint之间的最小暂停时间，即如果上一次Checkpoint完成后，距离下一次 Checkpoint的间隔时间小于这个值，则会等待直到满足最小暂停时间后再进行下一次 Checkpoint。</span><br><span class="line"></span><br><span class="line">这两个设置是相互独立的，可以根据实际情况进行调整。例如，如果你希望程序在出现故障时能够快速恢复，则可以缩短Checkpoint的间隔时间；如果你希望减少 Checkpoint 对程序性能的影响，则可以增加两次Checkpoint之间的最小暂停时间。</span><br></pre></td></tr></table></figure><h3 id="保存多个Checkpoint"><a href="#保存多个Checkpoint" class="headerlink" title="保存多个Checkpoint"></a>保存多个Checkpoint</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">默认情况下，如果在任务中开启了Checkpoint，则Flink只会保留最近成功生成的1份Checkpoint数据。</span><br><span class="line">当Flink程序故障重启时，可以从最近的这份Checkpoint数据来进行恢复。</span><br><span class="line"></span><br><span class="line">但是我们希望能够保留多份Checkpoint数据，并能够根据实际需要选择其中一份进行恢复，这样会更加灵活。</span><br><span class="line">例如：我们发现最近2个小时的数据处理有问题，希望将整个状态还原到2小时之前，这样就需要找到2个小时之前的Checkpoint数据进行恢复了。</span><br><span class="line"></span><br><span class="line">Flink可以支持保留多份Checkpoint数据，需要在Flink的配置文件flink-conf.yaml中，添加如下配置，指定最多需要保存最近多少份Checkpoint数据</span><br><span class="line">这个配置针对整个客户端有效，只要是在这个客户端上提交的任务，都会使用这个配置。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# vi flink-conf.yaml </span><br><span class="line">...</span><br><span class="line">state.checkpoints.num-retained: 20</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="从Checkpoint进行恢复"><a href="#从Checkpoint进行恢复" class="headerlink" title="从Checkpoint进行恢复"></a>从Checkpoint进行恢复</h3><h4 id="手工停止的任务-恢复数据"><a href="#手工停止的任务-恢复数据" class="headerlink" title="手工停止的任务,恢复数据"></a>手工停止的任务,恢复数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面我们分析了Checkpoint的详细配置，下面我们就结合具体的案例来演示一下Checkpoint是如何对状态数据进行持久化保存的，并且当任务故障后，我们如何基于Checkpoint产生的数据进行恢复</span><br><span class="line"></span><br><span class="line">首先我们重新开发一个有状态的单词计数案例，并且在代码中开启checkpoint。</span><br><span class="line"></span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 有状态的单词计数 + Checkpoint</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStateWithCheckpointDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="string">"hdfs://bigdata01:9000/flink-chk/wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WordCountStateWithCheckpointDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">修改项目的依赖配置，将所有依赖的scope都设置为provided。</span><br><span class="line"></span><br><span class="line">打jar包。</span><br><span class="line"></span><br><span class="line">将生成的jar包上传到bigdata04机器中。</span><br><span class="line"></span><br><span class="line">在bigdata04机器上开启Socket</span><br><span class="line"></span><br><span class="line">向集群中提交此任务。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时a出现2次，b出现了1次。</span><br><span class="line"></span><br><span class="line">此时可以在任务界面中查看checkpoint的执行情况：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021510607.png" alt="image-20230602151049919"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line"></span><br><span class="line">Trriggered：表示截止到目前checkpoint触发的次数。</span><br><span class="line">In Progress：表示目前正在执行的checkpoint数量。</span><br><span class="line">Completed：表示成功执行结束的checkpoint次数。</span><br><span class="line">Failed：表示执行失败的checkpoint次数，这里显示为2的意思是失败了2次，这是因为我们现在设置的checkpoint间隔时间太短了，只有10秒，任务提交上去之后很快就会触发checkpoint，此时Flink任务可能还没有初始化完成，所以会出现一些失败次数，等Flink任务正常运行起来之后就没问题了。实际工作中，我们会把checkpoint的间隔时间设置为分钟级别，一般是2分钟，5分钟之类的，这样就不会出现这种问题了。</span><br><span class="line">Restored：0表示这个任务没有基于之前的checkpoint数据启动，显示为1表示基于之前的checkpoint数据启动。</span><br><span class="line">Path：表示当前任务的checkpoint数据保存目录，注意：同一个任务每次启动生成的checkpoint数据目录都不一样，因为这个路径里面用到了flink的任务id，任务id是每次都会重新生成的。</span><br><span class="line"></span><br><span class="line">此时到hdfs中查看一下具体生成的checkpoint目录</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 ~]# hdfs dfs -ls &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;</span><br><span class="line">Found 22 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:23 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-27</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:23 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-28</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:23 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-29</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-30</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-31</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-32</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-33</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-34</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:24 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-35</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-36</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-37</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-38</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-39</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-40</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:25 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-41</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-42</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-43</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-44</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-45</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:26 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-46</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:18 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;shared</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2027-11-14 17:18 &#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;taskowned</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这里会显示最近20次checkpoint生成的数据目录。</span><br><span class="line"></span><br><span class="line">接下来我们来手工停止任务。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021517531.png" alt="image-20230602151730020"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：如果刚才是由于任务内部故障导致的任务停止，则Flink会基于默认的重启策略自动重启，在自动重启的时候会自动使用最新生成的那一份checkpoint数据进行恢复。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">但是刚才是我们手工停止的任务，那么任务在重启的时候想要恢复数据就需要手工指定从哪一份checkpoint数据启动。</span><br><span class="line">如果没有特殊情况就选择使用最新的那一份checkpoint数据进行恢复即可。</span><br><span class="line"></span><br><span class="line">先开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">选择一份checkpoint数据重启任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]#flink1-15 run -m yarn-cluster -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink-chk&#x2F;wordcount&#x2F;10fbadc413d0ae5c1b91bb460e969117&#x2F;chk-54&#x2F;_metadata  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：-s后面指定的是某一份checkpoint数据，这个时候任务启动的时候会基于这份checkpoint数据进行恢复，这个路径必须写hdfs的全路径，必须要有hdfs路径的前缀，否则会被识别成linux本地路径。</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，查看任务界面中的checkpoint信息：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021520073.png" alt="image-20230602152033829"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">此时图中显示的Restored为1，说明这个任务是基于之前的checkpoint数据恢复启动的。</span><br><span class="line">checkpoint ID目前为55，说明这个任务会延续之前的Checkpoint ID，继续递增增长。</span><br><span class="line">最下面的Latest Restore：ID为54，这个编号是启动任务时使用的Checkpoint 数据的ID编号。</span><br><span class="line">Restore Time表示恢复的时间。</span><br><span class="line">Type：SavePoint，这是因为我们是手工重启的，会显示为Savepoint，如果是任务故障时自动重启的，这里会显示为checkpoint。具体Savepoint什么含义，后面我们会具体分析。</span><br><span class="line">Path：这个表示启动任务时使用的checkpoint数据路径。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">此时在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line"></span><br><span class="line">查看任务日志：</span><br><span class="line">(a,3)</span><br><span class="line">(b,2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这样就说明此任务在启动的时候恢复到了之前的状态，因为上一次任务停止之前，a出现了2次，b出现了1次。这次任务重启后，可以累加之前状态中记录的次数。这样就实现了任务故障后的数据恢复，可以保证流计算中数据的准确性，如果没有使用状态和checkpoint，当任务重启后，所有的数据都会归0。</span><br><span class="line"></span><br><span class="line">当重启后的程序正常运行后，他还会按照Checkpoint的配置进行运行，继续生成Checkpoint数据。</span><br></pre></td></tr></table></figure><h4 id="任务自动重启时checkpoint数据的自动恢复"><a href="#任务自动重启时checkpoint数据的自动恢复" class="headerlink" title="任务自动重启时checkpoint数据的自动恢复"></a>任务自动重启时checkpoint数据的自动恢复</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">下面我们来演示一下任务在运行期间，任务内部故障导致的任务自动重启时checkpoint数据的自动恢复。</span><br><span class="line">重新开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交此任务。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br><span class="line"></span><br><span class="line">此时在任务界面中查看一下taskmanager节点信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021526216.png" alt="image-20230602152607843"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">从这个图里面可以看出来，taskmanager进程目前运行在bigdata02上。</span><br><span class="line">我们模拟一个故障场景，由于集群节点异常导致bigdata02节点宕机了，那么运行在这个节点上的taskmanager进程肯定也就没了。</span><br><span class="line"></span><br><span class="line">在这里我到bigdata02上使用kill命令直接把taskmanager进程杀掉就可以模拟这个场景了。</span><br><span class="line">首先在bigdata02上执行jps命令查看目前的进程信息</span><br><span class="line"></span><br><span class="line">[root@bigdata02 ~]# jps</span><br><span class="line">1680 NodeManager</span><br><span class="line">1570 DataNode</span><br><span class="line">11815 YarnTaskExecutorRunner</span><br><span class="line">11992 Jps</span><br><span class="line">11674 YarnJobClusterEntrypoint</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">其实11815 YarnTaskExecutorRunner 这个就是taskmanager进程了。</span><br><span class="line"></span><br><span class="line">想要进一步确认的话可以使用jps -ml命令：</span><br><span class="line">[root@bigdata02 ~]# jps -ml</span><br><span class="line">1680 org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line">12033 sun.tools.jps.Jps -ml</span><br><span class="line">1570 org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">11815 org.apache.flink.yarn.YarnTaskExecutorRunner -D taskmanager.memory.network.min&#x3D;67108864b -D taskmanager.cpu.cores&#x3D;1.0 -D taskmanager.memory.task.off-heap.size&#x3D;0b -D taskmanager.memory.jvm-metaspace.size&#x3D;268435456b -D external-resources&#x3D;none -D taskmanager.memory.jvm-overhead.min&#x3D;201326592b -D taskmanager.memory.framework.off-heap.size&#x3D;134217728b -D taskmanager.memory.network.max&#x3D;67108864b -D taskmanager.memory.framework.heap.size&#x3D;134217728b -D taskmanager.memory.managed.size&#x3D;241591914b -D taskmanager.memory.task.heap.size&#x3D;26843542b -D taskmanager.numberOfTaskSlots&#x3D;1 -D taskmanager.memory.jvm-overhead.max&#x3D;201326592b --configDir . -Dblob.server.port&#x3D;32854 -Djobmanager.rpc.address&#x3D;bigdata02 -Djobmanager.memory.jvm-overhead.min&#x3D;201326592b -Dtaskmanager.resource-id&#x3D;container_1826267805863_0011_01_000002 -Dweb.port&#x3D;0 -Djobmanager.memory.off-heap.size&#x3D;134217728b -Dweb.tmpdir&#x3D;&#x2F;tmp&#x2F;flink-web-95fcafd5-13cb-44f8-adbf-e3e42f09aa15 -Dinternal.taskmanager.resource-id.metadata&#x3D;bigdata02:38033 -Djobmanager.rpc.port&#x3D;36024 -Dr</span><br><span class="line">11674 org.apache.flink.yarn.entrypoint.YarnJobClusterEntrypoint -D jobmanager.memory.off-heap.size&#x3D;134217728b -D jobmanager.memory.jvm-overhead.min&#x3D;201326592b -D jobmanager.memory.jvm-metaspace.size&#x3D;268435456b -D jobmanager.memory.heap.size&#x3D;469762048b -D jobmanager.memory.jvm-overhead.max&#x3D;201326592b</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">确定了是哪个进程之后，使用kill 命令杀掉进程。</span><br><span class="line">直接强制杀进程</span><br><span class="line">[root@bigdata02 ~]# kill -9 11815</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个时候回到任务界面，可以看到界面中已经不显示这个taskmanager了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021529424.png" alt="image-20230602152917231"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">稍等一会任务会重启：</span><br><span class="line">此时发现任务中有一些失败的task，这是因为刚才taskmanager进程挂掉之后，socket也停止了，任务重启后，连不上socket了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021530253.png" alt="image-20230602153006041"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">重新开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再看任务界面中的checkpoint相关的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021530596.png" alt="image-20230602153045307"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">可以发现此时下面的restore部分有信息了，表示是基于之前的第36份数据进行启动的，此时type就是checkpoint了。</span><br><span class="line"></span><br><span class="line">这个时候我们在socket中再模拟产生一条数据a</span><br><span class="line"></span><br><span class="line">再验证一下输出结果，发现是没有问题的，是基于之前的状态进行累加的。</span><br><span class="line"></span><br><span class="line">(a,3)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这就是针对checkpoint数据的自动恢复和手动恢复。</span><br></pre></td></tr></table></figure><h4 id="Savepoint详解"><a href="#Savepoint详解" class="headerlink" title="Savepoint详解"></a>Savepoint详解</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">咱们前面讲到的Checkpoint是为了保证应用在出现故障时可以顺利重启恢复。</span><br><span class="line"></span><br><span class="line">而Savepoint是为了有计划的备份任务，实现任务升级后可恢复。</span><br><span class="line"></span><br><span class="line">任务升级主要包括：增减并行度、调整业务逻辑、以及升级Flink版本时的任务迁移。</span><br><span class="line"></span><br><span class="line">Flink通过Savepoint功能可以做到程序升级后，继续从升级前的那个点开始执行计算，保证数据不中断。</span><br><span class="line"></span><br><span class="line">Savepoint会生成全局，一致性快照，可以保存数据源offset，operator操作状态等信息，可以从应用在过去任意做了savepoint的时刻开始继续消费。</span><br><span class="line"></span><br><span class="line">Savepoint的生成算法和Checkpoint是完全一样的，所以可以把Savepoint认为是包含了一些额外元数据的Checkpoint，所以Savepoint本质上是特殊的Checkpoint。</span><br><span class="line"></span><br><span class="line">Savepoint和Checkpoint可以同时执行，互不影响，Flink不会因为正在执行Checkpoint而推迟Savepoint的执行。</span><br></pre></td></tr></table></figure><h4 id="Checkpoint-VS-Savepoint"><a href="#Checkpoint-VS-Savepoint" class="headerlink" title="Checkpoint VS Savepoint"></a>Checkpoint VS Savepoint</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对Checkpoint 和 Savepoint的详细区别，在这里我整理了一个表格，我们来看一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021536214.png" alt="image-20230602153615972"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">中文翻译：Checkpoint可以翻译为检查点 或者快照。Savepoint可以翻译为保存点。</span><br><span class="line"></span><br><span class="line">触发方式：Checkpoint是由JobManager定时触发快照并自动清理，不需要用户干预，当任务故障重启时自动恢复。Savepoint面向用户，完全根据用户的需要触发与清理，恢复的时候也是根据需求进行恢复。</span><br><span class="line"></span><br><span class="line">作用：Checkpoint主要是为了实现任务故障恢复的，他的侧重点是容错，当Flink作业意外失败，重启时可以从之前生成的CheckPoint自动恢复运行，不影响作业逻辑的准确性。SavePoint侧重点是维护，当Flink作业需要在人工干预下手动重启、升级、或者迁移时，先将状态整体写入可靠存储，维护完毕之后再从SavePoint恢复，他属于有计划的备份。</span><br><span class="line"></span><br><span class="line">特点：Checkpoint属于轻量级的快照，因为Checkpoint的频率往往比较高，所以Checkpoint的存储格式非常轻量级，但作为权衡牺牲了一切可移植的东西，例如：不保证改变并行度和升级的兼容性。Savepoint属于重量级的快照，他会以二进制的形式存储所有状态数据和元数据，执行起来比较慢而且贵，但是能够保证程序的可移植性 ，例如并行度改变或代码升级之后，仍然能正常恢复。</span><br><span class="line">Checkpoint是支持增量快照的（如果状态数据存储在RocksDB里面），对于超大状态的 作业而言可以降低写入成本。Savepoint并不会连续自动触发，所以不支持增量，只支 持全量。</span><br></pre></td></tr></table></figure><h4 id="Savepoint保证程序可移植性的前提条件"><a href="#Savepoint保证程序可移植性的前提条件" class="headerlink" title="Savepoint保证程序可移植性的前提条件"></a>Savepoint保证程序可移植性的前提条件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">咱们刚才说了，Savepoint能够保证程序的可移植性，可以保证在代码升级之后，仍然可以恢复数据。</span><br><span class="line">但是他想要实现这个功能的前提条件是需要保证任务中所有有状态的算子都配置好下面这两个参数：</span><br><span class="line">第一个参数是：算子唯一标识。</span><br><span class="line">第二个参数是：算子最大并行度，这个参数只针对使用了keyed State的算子。</span><br><span class="line"></span><br><span class="line">这两个参数会被固化到Savepoint数据中，不可更改，如果新任务中这两个参数发生了变化，就无法从之前生成的Savepoint数据中启动并恢复数据了，只能选择丢弃之前的状态从头开始运行。</span><br><span class="line"></span><br><span class="line">下面我们来具体分析一下这两个参数</span><br></pre></td></tr></table></figure><h5 id="算子唯一标识"><a href="#算子唯一标识" class="headerlink" title="算子唯一标识"></a>算子唯一标识</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">首先是算子唯一标识这个参数：</span><br><span class="line"></span><br><span class="line">默认情况下Flink会给每个算子分配一个唯一标识。</span><br><span class="line">但是这个标识是根据前面算子的标识并且结合一些规则生成的，这也就意味着任何一个前置算子发生改变都会导致该算子的标识发生变化 。</span><br><span class="line"></span><br><span class="line">例如：我们添加或者删除一个算子，这样后面算子的唯一标识就变了，就不可控了。</span><br><span class="line"></span><br><span class="line">只要任务中的算子唯一标识发生了变化，Savepoint保存的状态数据基本上就无法用来恢复了，因为之前保存的算子标识和现在最新的算子标识不一样了。</span><br><span class="line"></span><br><span class="line">咱们前面说过，Savepoint会以二进制的形式存储所有状态数据和元数据，这里的算子唯一标识就属于元数据中的内容。当Flink任务从Savepoint启动时，会利用算子的唯一标识将Savepoint保存的状态映射到新任务对应的算子中，只有当新任务的算子唯一标识和Savepoint数据中保存的算子标识相同时，状态才能顺利恢复。</span><br><span class="line"></span><br><span class="line">所以说如果我们没有给有状态的算子手工设置唯一标识，那么在任务升级时就会受到很多限制。</span><br><span class="line"></span><br><span class="line">看下面这个图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021539955.png" alt="image-20230602153912620"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">上面是最开始的任务版本，里面有Source、Map和Sink这三个组件，假设这三个组件都是有状态的，我们没有给这些组件手工设置唯一标识，使用的是默认的自动生成的。</span><br><span class="line">这里的唯一标识其实就是uid。</span><br><span class="line">假设source自动生成的唯一标识是uid-001，Map自动生成的唯一标识是uid-002，Sink自动生成的唯一标识是uid-003。</span><br><span class="line"></span><br><span class="line">当这个任务运行了一段时间之后，我们的业务逻辑发生了变化，所以对代码做了一些修改，增加了一个Flatmap组件，看下面这个图：</span><br><span class="line">此时还是使用的默认生成的唯一标识，那此时source自动生成的唯一标识可能还是uid-001，map自动生成的唯一标识也还是uid-002，但是flatmap自动生成的唯一标识可能是uid-003，最后sink自动生成的唯一标识就可能是uid-004了。</span><br><span class="line"></span><br><span class="line">这样新任务中sink的唯一标识和之前任务中sink的唯一标识就不一样了，那么再基于之前任务生成的savepoint数据就会导致无法恢复了。</span><br><span class="line"></span><br><span class="line">为了能够在任务的不同版本之间顺利升级，我们需要通过 uid(…) 方法手动的给算子设置uid。</span><br><span class="line">类似这样的：</span><br><span class="line">source.uid(...)</span><br><span class="line">transform.uid(...)</span><br><span class="line">sink.uid(...)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在DataSouce、中间的转换算子、DataSink上面都可以设置uid。</span><br><span class="line"></span><br><span class="line">其实也没必要给所有的组件都设置uid，最重要的是给包含了状态的组件设置uid，没有状态的组件也不会涉及到数据恢复，就没必要设置了。</span><br><span class="line"></span><br><span class="line">看这块代码中的设置</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021540022.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">source和map中维护了状态，所以需要设置uid，其它的就不用设置了，让程序自动生成即可</span><br><span class="line"></span><br><span class="line">注意：在同一个任务内部，uid不能重复。</span><br><span class="line"></span><br><span class="line">此时我们手工指定uid之后，后期就算在任务中新增了一个组件，之前的组件的uid也不会变化了，任务基于之前生成的Savepoint数据启动的时候依然是可以恢复数据的。</span><br></pre></td></tr></table></figure><h5 id="算子最大并行度"><a href="#算子最大并行度" class="headerlink" title="算子最大并行度"></a>算子最大并行度</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来分析一下第二个参数：算子最大并行度</span><br><span class="line"></span><br><span class="line">Flink中Keyed State类型的状态数据在恢复时，是按照KeyGroup为单位恢复的，每个KeyGroup 中包含一部分key的数据。</span><br><span class="line">咱们前面在讲状态的扩缩容模式的时候也提到过KeyGroup，说的是一回事。</span><br><span class="line">针对Keyed State，状态在扩缩容的时候会以KeyGroup为单位进行重新分配。</span><br><span class="line"></span><br><span class="line">KeyGroup的个数等于算子的最大并行度。</span><br><span class="line">注意：算子的最大并行度并不是算子的并行度，</span><br><span class="line"></span><br><span class="line">算子的最大并行度是通过setMaxParallelism()方法设置的，</span><br><span class="line">算子的并行度是通过setParallelism()方法设置的。</span><br><span class="line"></span><br><span class="line">当我们设置的算子并行度大于算子最大并行度时，任务在重启的时候有些并行度就分配不到KeyGroup了，这样会导致Flink任务无法从Savepoint恢复数据。</span><br><span class="line"></span><br><span class="line">注意：此时也是无法从Checkpoint中恢复数据的。</span><br><span class="line"></span><br><span class="line">那这个算子最大并行度应该如何设置呢？</span><br><span class="line">可以通过这两种方式，一种全局的，一种局部的</span><br><span class="line"></span><br><span class="line">设置全局算子最大并行度：env.setMaxParallelism()</span><br><span class="line">设置某个算子最大并行度：.map(..).setMaxParallelism()</span><br><span class="line">那咱们之前如果没有设置过的话，这些算子的最大并行度默认是多少呢？</span><br><span class="line"></span><br><span class="line">先说结果，算子最大并行度默认是128，最大是32768。env.setMaxParallelism()</span><br><span class="line"></span><br><span class="line">我们来看一下这块逻辑的代码：</span><br><span class="line">算子的最大并行度可以在算子上独立设置，也可以通过env全局设置。</span><br><span class="line">通过这个方法作为入口查看底层源码：</span><br><span class="line">env.setMaxParallelism()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">往下面追踪，可以追踪到这里：</span><br><span class="line">public StreamExecutionEnvironment setMaxParallelism(int maxParallelism) &#123;</span><br><span class="line">    Preconditions.checkArgument(</span><br><span class="line">            maxParallelism &gt; 0</span><br><span class="line">                    &amp;&amp; maxParallelism &lt;&#x3D; KeyGroupRangeAssignment.UPPER_BOUND_MAX_PARALLELISM,</span><br><span class="line">            &quot;maxParallelism is out of bounds 0 &lt; maxParallelism &lt;&#x3D; &quot;</span><br><span class="line">                    + KeyGroupRangeAssignment.UPPER_BOUND_MAX_PARALLELISM</span><br><span class="line">                    + &quot;. Found: &quot;</span><br><span class="line">                    + maxParallelism);</span><br><span class="line"></span><br><span class="line">    config.setMaxParallelism(maxParallelism);</span><br><span class="line">    return this;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这是我们手工设置算子最大并行度时使用的方法，这里面会用到KeyGroupRangeAssignment这个类。</span><br><span class="line"></span><br><span class="line">在KeyGroupRangeAssignment这个类中有一个方法：computeDefaultMaxParallelism</span><br><span class="line">这个方法会计算默认的算子最大并行度</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public static int computeDefaultMaxParallelism(int operatorParallelism) &#123;</span><br><span class="line"></span><br><span class="line">    checkParallelismPreconditions(operatorParallelism);</span><br><span class="line"></span><br><span class="line">    return Math.min(</span><br><span class="line">            Math.max(</span><br><span class="line">                    MathUtils.roundUpToPowerOfTwo(</span><br><span class="line">                            operatorParallelism + (operatorParallelism &#x2F; 2)),</span><br><span class="line">                    DEFAULT_LOWER_BOUND_MAX_PARALLELISM),</span><br><span class="line">            UPPER_BOUND_MAX_PARALLELISM);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这块代码我们可以拿出来运行一下，测试一下效果</span><br><span class="line">创建一个Object：TestDefaultMaxParallelism</span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">MathUtils</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 测试默认算子最大并行度</span></span><br><span class="line"><span class="comment"> * 结论：Flink生成的算子最大并行度介于128和32768之间</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestDefaultMaxParallelism</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//这个参数表示当前算子(任务)的并行度</span></span><br><span class="line">    <span class="keyword">val</span> operatorParallelism = <span class="number">1</span></span><br><span class="line">    <span class="comment">//最小值：2的7次方=128</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">DEFAULT_LOWER_BOUND_MAX_PARALLELISM</span>:<span class="type">Int</span> = <span class="number">1</span> &lt;&lt; <span class="number">7</span></span><br><span class="line">    <span class="comment">//最大值：2的15次方=32768</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">UPPER_BOUND_MAX_PARALLELISM</span>:<span class="type">Int</span> = <span class="number">1</span> &lt;&lt; <span class="number">15</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//计算算子最大并行度</span></span><br><span class="line">    <span class="keyword">val</span> res = <span class="type">Math</span>.min(</span><br><span class="line">      <span class="type">Math</span>.max(</span><br><span class="line">        <span class="type">MathUtils</span>.roundUpToPowerOfTwo(</span><br><span class="line">          operatorParallelism + (operatorParallelism / <span class="number">2</span>)),</span><br><span class="line">        <span class="type">DEFAULT_LOWER_BOUND_MAX_PARALLELISM</span>),</span><br><span class="line">      <span class="type">UPPER_BOUND_MAX_PARALLELISM</span>)</span><br><span class="line">    println(res)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">运行代码可以发现，如果Flink任务中算子的并行度比较小的时候，则算子最大并行度默认就是128。</span><br><span class="line"></span><br><span class="line">如果Flink任务中算子的并行度比较大，则会按照这个公式生成对应的值，这个值不是固定的。</span><br><span class="line"></span><br><span class="line">所以说如果我们想要保证Savepoint的可移植性，那么最好是手工设置一个固定的算子最大并行度。</span><br></pre></td></tr></table></figure><h6 id="算子最大并行度-注意事项"><a href="#算子最大并行度-注意事项" class="headerlink" title="算子最大并行度-注意事项"></a>算子最大并行度-注意事项</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">在工作中还遇到过一个关于算子最大并行度的问题，当时我们在计算某一个业务指标的时候，这个任务中也用到了基于keyed state类型的状态，这个业务指标前期数据量比较小，所以最开始给这个任务设置的全局并行度为20，也没有单独给这个任务中的算子设置最大并行度，根据前面的分析，此时默认生成的算子最大并行度就是128了。</span><br><span class="line">后期随着平台用户规模的增长，这个业务的数据量呈指数级增长，使用之前的并行度来处理增长后的数据规模就有点力不从心了，导致数据出现了积压，我们尝试调整任务的并行度，当把任务的并行度调整为128以上时，发现任务无法从checkpoint和savepoint进行恢复。</span><br><span class="line">这就是所谓的任务并行度调不上去了。</span><br><span class="line">这是因为我们前面分析的，当算子并行度大于算子最大并行度时，任务在重启的时候有些并行度就分配不到KeyGroup了，这样会导致Flink任务无法从Checkpoint或者Savepoint恢复数据。</span><br><span class="line">此时想要提高计算能力，只有一种方法，那就是放弃状态中保存的数据，不从状态中恢复，直接单独启动这个任务，这样是可以提高并行度的。</span><br><span class="line"></span><br><span class="line">其实合理的一点方案是这样的：我们在开发基于keyed state类型的有状态的任务的时候，需要提前预估一下这个任务后期可能处理的数据规模会达到哪种级别，提前设置一个合适的算子最大并行度。这样在前期数据量小的时候，我们可以给任务设置一个比较小的并行度，也不浪费资源，后期数据量上来之后，再调整为一个比较大的并行度。</span><br><span class="line"></span><br><span class="line">所以针对一个keyed state类型的有状态的Flink任务，它未来能扩展到的最大并行度其实取决于这个任务第一次启动时设置的算子最大并行度。</span><br></pre></td></tr></table></figure><h4 id="手工触发Savepoint"><a href="#手工触发Savepoint" class="headerlink" title="手工触发Savepoint"></a>手工触发Savepoint</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面对Savepoint的原理有了一定的理解，下面我们来看一下Savepoint的使用</span><br><span class="line">Savepoint需要手工触发，需要使用这种形式的命令：</span><br><span class="line">bin&#x2F;flink savepoint jobId hdfs:&#x2F;&#x2F;IP:9000&#x2F;flink&#x2F;sap -yid yarnAppId</span><br><span class="line"></span><br><span class="line">注意：针对flink on yarn模式一定要指定-yid参数。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面我们分别来演示一下：</span><br><span class="line">开启一个新的socket</span><br><span class="line">nc -l 9001</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">手工触发savepoint。</span><br><span class="line"></span><br><span class="line">注意：需要获取flink任务id和对应的yarn applicationid</span><br><span class="line"></span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint 09ded8897f6a33eda7bdbb32e42046 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0014</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时到任务界面查看，可以看到这里显示的savepoint信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021555606.png" alt="image-20230602155500749"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">手工停止这个任务。</span><br></pre></td></tr></table></figure><h3 id="从Savepoint进行恢复"><a href="#从Savepoint进行恢复" class="headerlink" title="从Savepoint进行恢复"></a>从Savepoint进行恢复</h3><h4 id="正常恢复"><a href="#正常恢复" class="headerlink" title="正常恢复"></a>正常恢复</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">从Savepoint进行恢复时使用的命令和前面我们讲的手动从checkpoint恢复的命令是一样的。</span><br><span class="line">其实这个命令本来就是Savepoint提供的，只不过也是支持基于checkpoint的数据进行恢复。</span><br><span class="line"></span><br><span class="line">接下来我们尝试使用之前生成的savepoint数据来重启恢复任务。</span><br><span class="line">开启一个新的socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line"></span><br><span class="line">向集群中提交任务，需要通过-s参数指定savepoint的数据目录。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-09ded8-e14f515ce7af&#x2F;_metadata  -c com.imooc.scala.checkpoint.WordCountStateWithCheckpointDemo -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,3)</span><br><span class="line"></span><br><span class="line">这样就说明任务正常基于savepoint的数据恢复到了之前的状态。</span><br></pre></td></tr></table></figure><h4 id="异常恢复"><a href="#异常恢复" class="headerlink" title="异常恢复"></a>异常恢复</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">咱们前面讲到了在某些特殊情况下会导致任务无法从Savepoint中恢复。</span><br><span class="line">下面来针对两个比较常见的故障场景进行分析：</span><br><span class="line">故障情况1：未手工设置uid，重启时任务中增加了新的算子</span><br><span class="line">故障情况2：未手工设置uid，重启时算子并行度发生了变化</span><br></pre></td></tr></table></figure><h5 id="故障情况1"><a href="#故障情况1" class="headerlink" title="故障情况1"></a>故障情况1</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">首先演示一下第一种故障情况：</span><br><span class="line">创建package：com.imooc.scala.savepoint</span><br><span class="line"></span><br><span class="line">原始代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.savepoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 故障情况1：未手工设置uid，重启时任务中增加了新的算子</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStateForSavepoint1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="string">"hdfs://bigdata01:9000/flink-chk/wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WordCountStateForSavepoint1"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">我们首先基于这份代码生成一份savepoint数据，然后再对这个代码进行修改，增加一个算子，看看还能不能基于之前的数据进行恢复。</span><br><span class="line"></span><br><span class="line">编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -c com.imooc.scala.savepoint.WordCountStateForSavepoint1 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br><span class="line"></span><br><span class="line">手工触发savepoint。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint d552f822307d4b1ce2e991f4cbb0d065 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0016</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">然后停止任务。</span><br><span class="line"></span><br><span class="line">修改代码，在任务中增加一个算子，其他代码不变。</span><br><span class="line">其实增加的这个算子对结果数据没有任何影响。</span><br><span class="line"></span><br><span class="line">val keyedStream &#x3D; text.flatMap(_.split(&quot; &quot;))</span><br><span class="line">  .map((_, 1))</span><br><span class="line">  .map(tup&#x3D;&gt;(tup._1,tup._2))</span><br><span class="line">  .keyBy(_._1)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">重新编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">基于之前生成的savepoint数据进行恢复。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-d552f8-ec06489b0100&#x2F;_metadata  -c com.imooc.scala.savepoint.WordCountStateForSavepoint1 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时发现任务提交上去之后会自动失败。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021603217.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">查看日志中的错误信息。</span><br><span class="line"></span><br><span class="line">注意：此时至少需要开启Hadoop的historyserver服务。</span><br><span class="line"></span><br><span class="line">核心错误日志是这一行：</span><br><span class="line">Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Failed to rollback to checkpoint&#x2F;savepoint hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-d552f8-ec06489b0100. Cannot map checkpoint&#x2F;savepoint state for operator c27dcf7b54ef6bfd6cff02ca8870b681 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">日志中的意思是说不能把savepoint中的状态数据映射到uid发送了变化的算子中。</span><br><span class="line">如果你想忽略这个问题，可以指定--allowNonRestoredState，这样会忽略掉无法映射的状态数据，强制启动。</span><br><span class="line"></span><br><span class="line">想要解决这个问题，咱们之前分析了，需要手工设置算子的uid，至少是要指定有状态的算子的uid。</span><br><span class="line">修改之前的代码，在有状态的map算子后面设置uid，并且删掉之前增加的map算子。</span><br><span class="line">完整代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.savepoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 故障情况1：未手工设置uid，并且任务中增加了新的算子</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStateForSavepoint1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="string">"hdfs://bigdata01:9000/flink-chk/wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).uid(<span class="string">"vs_map001"</span>).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WordCountStateForSavepoint1"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -c com.imooc.scala.savepoint.WordCountStateForSavepoint1 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">手工触发savepoint。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint ebb9f0efa183e6b3d57f497a4da86ec2 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0020</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">然后停止任务。</span><br><span class="line"></span><br><span class="line">再修改代码，增加一个map算子</span><br><span class="line">val keyedStream &#x3D; text.flatMap(_.split(&quot; &quot;))</span><br><span class="line">  .map((_, 1))</span><br><span class="line">  .map(tup&#x3D;&gt;(tup._1,tup._2))</span><br><span class="line">  .keyBy(_._1)</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">重新编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">基于之前生成的savepoint数据进行恢复。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-ebb9f0-ae46290b2f2c&#x2F;_metadata  -c com.imooc.scala.savepoint.WordCountStateForSavepoint1 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时发现任务可以正常启动。</span><br><span class="line">查看任务界面中的信息，可以看到任务是基于之前的savepoint数据进行恢复的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021611292.png" alt="image-20230602161106105"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在socket中模拟产生数据：</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line"></span><br><span class="line">看到这个结果就说明数据是正常恢复了。</span><br></pre></td></tr></table></figure><h5 id="故障情况2"><a href="#故障情况2" class="headerlink" title="故障情况2"></a>故障情况2</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来演示一下第二种故障情况：</span><br><span class="line"></span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.savepoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 未手工设置uid，重启时算子并行度发生了变化</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountStateForSavepoint2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line">    <span class="comment">//获取Checkpoint的配置对象</span></span><br><span class="line">    <span class="keyword">val</span> cpConfig = env.getCheckpointConfig</span><br><span class="line">    <span class="comment">//在任务故障和手工停止任务时都会保留之前生成的Checkpoint数据</span></span><br><span class="line">    cpConfig.setExternalizedCheckpointCleanup(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置Checkpoint后的状态数据的存储位置</span></span><br><span class="line">    cpConfig.setCheckpointStorage(<span class="string">"hdfs://bigdata01:9000/flink-chk/wordcount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WordCountStateForSavepoint2"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">这个代码就是一个有状态的单词计数，没有给map这个有状态的算子设置uid。</span><br><span class="line">我一会先基于默认并行度1启动任务，并且生成savepoint，然后在基于savepoint恢复的时候调整并行度，看看会出现什么情况</span><br><span class="line"></span><br><span class="line">编译打包。</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -c com.imooc.scala.savepoint.WordCountStateForSavepoint2 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line">(a,1)</span><br><span class="line">(b,1)</span><br><span class="line">(a,2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">手工触发savepoint。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint 1a871d083497ae7ab775c168a5f75f6d hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0023</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">然后停止任务。</span><br><span class="line"></span><br><span class="line">再开启一个新的socket</span><br><span class="line"></span><br><span class="line">基于之前生成的savepoint数据进行恢复。</span><br><span class="line">通过-p 指定全局并行度为2,</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -p 2 -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-1a871d-b2fce8500a75&#x2F;_metadata  -c com.imooc.scala.savepoint.WordCountStateForSavepoint2 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">结果发现任务执行失败</span><br><span class="line">查看任务报错日志，发现和故障情况1里面的错误信息一样</span><br><span class="line">Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: </span><br><span class="line">Failed to rollback to checkpoint&#x2F;savepoint hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-1a871d-b2fce8500a75. Cannot map checkpoint&#x2F;savepoint state for operator c27dcf7b54ef6bfd6cff02ca8870b681 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">那也就意味着，改变并行度时，也会影响算子默认生成的uid。</span><br><span class="line">所以想要支持在恢复状态的时候修改并行度，需要给有状态的算子手工设置uid。</span><br><span class="line"></span><br><span class="line">修改代码，给这个有状态的map算子手工设置uid。</span><br><span class="line">keyedStream.map(new RichMapFunction[(String,Int),(String,Int)] &#123;</span><br><span class="line">  ......</span><br><span class="line">    &#125;).uid(&quot;vs_map001&quot;).print()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">重新编译打包</span><br><span class="line"></span><br><span class="line">上传jar包。</span><br><span class="line"></span><br><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">向集群中提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -c com.imooc.scala.savepoint.WordCountStateForSavepoint2 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">当任务正常启动之后，在socket中模拟产生数据：</span><br><span class="line"></span><br><span class="line">到任务界面查看输出结果信息：</span><br><span class="line"></span><br><span class="line">手工触发savepoint。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 savepoint 5e63967dbad8dbc32be5dcd77edcae88 hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap -yid application_1826267805863_0027</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">然后停止任务。</span><br><span class="line"></span><br><span class="line">再开启一个新的socket</span><br><span class="line"></span><br><span class="line">基于之前生成的savepoint数据进行恢复。</span><br><span class="line">通过-p指定全局并行度为2,</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster -p 2 -s hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;sap&#x2F;savepoint-5e6396-b23c7c08f013&#x2F;_metadata  -c com.imooc.scala.savepoint.WordCountStateForSavepoint2 -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">此时任务可以正常启动并恢复数据。</span><br><span class="line">查看任务界面，可以看到并行度变成2了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021617129.png" alt="image-20230602161722870"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在socket中模拟产生数据</span><br><span class="line">a</span><br><span class="line">查看输出结果</span><br><span class="line">2&gt; (a,3)</span><br><span class="line">能看到这个输出，说明任务成功恢复执行了。</span><br></pre></td></tr></table></figure><h2 id="State-Backend-状态的存储方式"><a href="#State-Backend-状态的存储方式" class="headerlink" title="State Backend(状态的存储方式)"></a>State Backend(状态的存储方式)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">咱们前面分析过，状态数据默认是存储在taskmanager节点的jvm堆内存中。</span><br><span class="line">当然还有一种选择是存储在本地的rocksdb数据库中。</span><br><span class="line"></span><br><span class="line">具体状态的存储位置，是由state backend来控制的：</span><br><span class="line"></span><br><span class="line">目前Flink提供了两种State Backend：</span><br><span class="line"></span><br><span class="line">看下面这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306021625881.png" alt="image-20230602162512730"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一种是HashMapStateBackend，这是默认的State Backend，这种存储方式会将状态数据存储在TaskManager节点的JVM堆内存中。</span><br><span class="line"></span><br><span class="line">第二种是EmbeddedRocksDBStateBackend，这种存储方式是借助于内嵌的RocksDB数据库。</span><br><span class="line">Rocksdb这个数据库中的数据会存储在对应TaskManager节点的本地磁盘文件中，此时状态数据会作为本地磁盘上的序列化字节存在。</span><br></pre></td></tr></table></figure><h3 id="HashMapStateBackend"><a href="#HashMapStateBackend" class="headerlink" title="HashMapStateBackend"></a>HashMapStateBackend</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HashMapStateBackend这种存储方式，他的优点是，基于内存读写数据，效率高，不需要涉及序列化和反序列化。</span><br><span class="line"></span><br><span class="line">但是它也有缺点，基于内存操作的话，内存大小是有限的，所以不适合存储超大状态的数据，适合用于常规任务，就是状态数据量中等的任务，例如：分钟级别的窗口聚合操作。</span><br><span class="line"></span><br><span class="line">Flink在执行checkpoint的时候，每次都会从HashMapStateBackend中获取全量数据进行持久化。</span><br></pre></td></tr></table></figure><h3 id="EmbeddedRocksDBStateBackend"><a href="#EmbeddedRocksDBStateBackend" class="headerlink" title="EmbeddedRocksDBStateBackend"></a>EmbeddedRocksDBStateBackend</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">EmbeddedRocksDBStateBackend这种存储方式，他的优点是解决了内存受限的问题，所以可以适用于超大状态的任务，例如：天级别的窗口聚合操作。</span><br><span class="line"></span><br><span class="line">但是它也有缺点，它在存储状态数据的时候使用的是内嵌的Rocksdb数据库，Rocksdb数据库的数据是存储在磁盘文件中的，所以需要操作磁盘，那么它的效率就不如内存高了，并且基于磁盘的读写操作还需要涉及到数据的序列化和反序列，也会影响性能，所以这种方式适合用于对状态读写性能要求不是特别高的任务。</span><br><span class="line"></span><br><span class="line">针对EmbeddedRocksDBStateBackend这种存储方式，它可以支持增量checkpoint，这样可以进一步提高Checkpoint性能。当然了，他也可以支持全量checkpoint。</span><br><span class="line"></span><br><span class="line">所以在工作中，针对常规的任务还是建议使用HashMapStateBackend，如果某个任务需要在状态里面维护大量的数据，可以考虑使用EmbeddedRocksDBStateBackend。</span><br></pre></td></tr></table></figure><h3 id="State-Backend的配置"><a href="#State-Backend的配置" class="headerlink" title="State Backend的配置"></a>State Backend的配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">在flink的执行环境中通过setStateBackend方法进行设置。</span><br><span class="line">可以给这个方法传递new HashMapStateBackend()或者new EmbeddedRocksDBStateBackend(true)。</span><br><span class="line"></span><br><span class="line">注意：new EmbeddedRocksDBStateBackend(true)中的true表示开启增量checkpoint。如果不传参数，则是全量checkpoint。</span><br><span class="line"></span><br><span class="line">在代码中的具体使用是这样的：</span><br><span class="line">创建package：com.imooc.scala.statebackend</span><br><span class="line"></span><br><span class="line">想要使用rockdb数据库，需要引入对应的依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-statebackend-rocksdb&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.15.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;!--&lt;scope&gt;provided&lt;&#x2F;scope&gt;--&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于WordCountStateWithCheckpointDemo代码复制一份进行修改</span><br><span class="line"></span><br><span class="line">Object：WordCountStateWithStateBackendDemo</span><br><span class="line"></span><br><span class="line">核心代码是这些：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;设置StateBackend</span><br><span class="line">&#x2F;&#x2F;默认使用HashMapStateBackend</span><br><span class="line">&#x2F;&#x2F;此时State数据保存在TaskManager节点的JVM堆内存中</span><br><span class="line">&#x2F;&#x2F;env.setStateBackend(new HashMapStateBackend())</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;此时State数据保存在TaskManager节点内置的RocksDB数据库中(TaskManager节点的本地磁盘文件中)</span><br><span class="line">&#x2F;&#x2F;在EmbeddedRocksDBStateBackend的构造函数中指定参数true会开启增量Checkpoint【建议设置为true】</span><br><span class="line">env.setStateBackend(new EmbeddedRocksDBStateBackend(true))</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;注意：具体Checkpoint后的状态数据存储在哪里是由setCheckpointStorage控制的</span><br><span class="line">env.getCheckpointConfig.setCheckpointStorage(new FileSystemCheckpointStorage(&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink-chk001&quot;))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使用的时候和之前使用默认的HashMapStateBackend没什么区别。</span><br></pre></td></tr></table></figure><h2 id="State的生存时间-TTL"><a href="#State的生存时间-TTL" class="headerlink" title="State的生存时间(TTL)"></a>State的生存时间(TTL)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">默认情况下State数据会一直存在，如果存储了过多状态数据，可能会导致内存溢出(针对HashMapStateBackend)。</span><br><span class="line"></span><br><span class="line">因此从Flink 1.6版本开始引入了State TTL特性。类似于Redis 中的TTL机制，超时自动删除。</span><br><span class="line"></span><br><span class="line">TTL特性可以支持对KeyedState中过期状态数据的自动清理，当状态中的某条数据到了过期时间，这条数据会被Flink自动删除，这样就有效解决了状态数据在无干预情况下无限增长导致内存溢出的问题。</span><br><span class="line"></span><br><span class="line">例如：我们在实时统计一段时间内的数据指标的时候，需要在状态中做去重，但是过了这段时间之后，之前的状态数据就没有用了，这样就可以用状态的ttl机制实现自动清理，当然我们也可以通过代码逻辑清空状态中的历史数据。</span><br><span class="line"></span><br><span class="line">针对OperatorState类型而言，基本上不需要自动清理，以我们之前开发的OperatorState_MyBufferSinkDemo代码为例。</span><br><span class="line">在MyBufferSink中，平时处理的数据是存储在任务内部的本地缓存中。</span><br><span class="line">只有在触发checkpoint的时候，才会把本地缓存中的数据写入到状态中，当下次触发checkpoint的时候我们会在代码中将之前的状态数据清空。所以状态中存储的数据不会一直增长，这样就没必要设置了状态的TTL了。</span><br><span class="line"></span><br><span class="line">数据过期判断依据：上次修改的时间戳 + 我们设置的状态TTL &gt; 当前时间戳</span><br><span class="line">如果满足这个条件，那么这条状态数据就过期了。</span><br><span class="line"></span><br><span class="line">本质上来讲，状态的TTL功能其实就是给每个Keyed State增加了一个“时间戳”，而 Flink 在状态创建、写入或读取的时候可以更新这个时间戳，并且判断状态是否过期。如果状态过期，还会根据可见性参数，来决定是否返回已过期但还未清理的状态。</span><br><span class="line">状态的清理并不是即时的，而是使用了一种 Lazy 的算法来实现，从而减少状态清理对性能的影响。</span><br><span class="line"></span><br><span class="line">举个例子，我要将一个 String 类型的数据存储到ValueState类型的状态中：</span><br><span class="line">如果没有设置状态的TTL ，则直接将 String 类型的数据存储到ValueState中。</span><br><span class="line">如果设置了状态的TTL，则Flink会将&lt;String, Long&gt;这种结构的数据存储到ValueState中，其中Long为时间戳，用于判断状态是否过期。</span><br><span class="line"></span><br><span class="line">在代码层面进行分析的话，是这样的。</span><br><span class="line">如果没有设置状态的TTL，我们存储 String 类型的数据使用的是ValueState。</span><br><span class="line">当我们设置了状态的TTL，那么就需要使用ValueState对应的TtlValueState。</span><br><span class="line"></span><br><span class="line">来看一下TtlValueState的源码：</span><br><span class="line">他里面有一个update方法：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void update(T value) throws IOException &#123;</span><br><span class="line">    accessCallback.run();</span><br><span class="line">    original.update(wrapWithTs(value));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">update方法中会调用wrapWithTs(value)，看一下wrapWithTs这个方法的实现：</span><br><span class="line">其实在这里可以发现，TtlUtils.wrapWithTs方法接收了两个参数，一个是value，另外一个是当前时间戳。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;V&gt; TtlValue&lt;V&gt; wrapWithTs(V value) &#123;</span><br><span class="line">    return TtlUtils.wrapWithTs(value, timeProvider.currentTimestamp());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">继续往下面查看TtlUtils.wrapWithTs方法</span><br><span class="line">static &lt;V&gt; TtlValue&lt;V&gt; wrapWithTs(V value, long ts) &#123;</span><br><span class="line">    return new TtlValue&lt;&gt;(value, ts);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">最终发现它返回了一个TtlValue数据类型，来看一下这个类的实现：</span><br><span class="line">public class TtlValue&lt;T&gt; implements Serializable &#123;</span><br><span class="line">    private static final long serialVersionUID &#x3D; 5221129704201125020L;</span><br><span class="line"></span><br><span class="line">    @Nullable private final T userValue;</span><br><span class="line">    private final long lastAccessTimestamp;</span><br><span class="line"></span><br><span class="line">    public TtlValue(@Nullable T userValue, long lastAccessTimestamp) &#123;</span><br><span class="line">        checkArgument(!(userValue instanceof TtlValue));</span><br><span class="line">        this.userValue &#x3D; userValue;</span><br><span class="line">        this.lastAccessTimestamp &#x3D; lastAccessTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Nullable</span><br><span class="line">    public T getUserValue() &#123;</span><br><span class="line">        return userValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public long getLastAccessTimestamp() &#123;</span><br><span class="line">        return lastAccessTimestamp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">这个类里面封装了具体的数据和对应的时间戳。</span><br><span class="line">这个类里面的lastAccessTimestamp就表示上次访问的时间戳，这样就可以基于这个时间戳判断这个数据是否过期了。</span><br><span class="line"></span><br><span class="line">这个是TtlValueState的实现，对应的还有TtlMapState、TtlListState等。</span><br><span class="line"></span><br><span class="line">下面我们来具体演示一下TTL的使用：</span><br><span class="line"></span><br><span class="line">创建package：com.imooc.scala.ttl</span><br><span class="line"></span><br><span class="line">完整代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.ttl</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.<span class="type">StateTtlConfig</span>.<span class="type">TtlTimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">StateTtlConfig</span>, <span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * State TTL的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StateTTLDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启Checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">1000</span>*<span class="number">10</span>)<span class="comment">//为了观察方便，在这里设置为10秒执行一次</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    keyedStream.map(<span class="keyword">new</span> <span class="type">RichMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">      <span class="comment">//声明一个ValueState类型的状态变量，存储单词出现的总次数</span></span><br><span class="line">      <span class="keyword">private</span> <span class="keyword">var</span> countState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">       * @param parameters</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//设置TTL机制相关配置</span></span><br><span class="line">        <span class="keyword">val</span> ttlConfig = <span class="type">StateTtlConfig</span></span><br><span class="line">          <span class="comment">//1：指定状态的生存时间</span></span><br><span class="line">          .newBuilder(<span class="type">Time</span>.seconds(<span class="number">10</span>))</span><br><span class="line">          <span class="comment">//2：指定什么时候触发更新延长状态的TTL时间</span></span><br><span class="line">          <span class="comment">//OnCreateAndWrite：仅在创建和写入时触发TTL时间更新延长。</span></span><br><span class="line">          <span class="comment">//OnReadAndWrite：表示在读取的时候也会触发(包括创建和写入)</span></span><br><span class="line">          .setUpdateType(<span class="type">StateTtlConfig</span>.<span class="type">UpdateType</span>.<span class="type">OnCreateAndWrite</span>)</span><br><span class="line">          <span class="comment">//3：过期数据是否可访问</span></span><br><span class="line">          <span class="comment">//NeverReturnExpired：表示永远不会返回过期数据(可能会存在数据已过期但是还没有被清理)</span></span><br><span class="line">          <span class="comment">//ReturnExpiredIfNotCleanedUp：表示数据只要没有被删除，就算过期了也可以被访问</span></span><br><span class="line">          .setStateVisibility(<span class="type">StateTtlConfig</span>.<span class="type">StateVisibility</span>.<span class="type">NeverReturnExpired</span>)</span><br><span class="line">          <span class="comment">//4：TTL的时间语义</span></span><br><span class="line">          <span class="comment">//判断数据是否过期时使用的时间语义，默认使用处理时间，目前只支持这一种</span></span><br><span class="line">          .setTtlTimeCharacteristic(<span class="type">TtlTimeCharacteristic</span>.<span class="type">ProcessingTime</span>)</span><br><span class="line">          <span class="comment">//5：过期数据删除策略</span></span><br><span class="line">          <span class="comment">//cleanupFullSnapshot：全量删除</span></span><br><span class="line">          <span class="comment">//此时只有当任务从checkpoint或者savepoint恢复时才会删除所有过期数据</span></span><br><span class="line">          <span class="comment">//这种方式其实并不能真正解决使用HashMapStateBackend时的内存压力问题，只有定时重启恢复才可以解决</span></span><br><span class="line">          <span class="comment">//注意：这种方式不适合Rocksdb中的增量Checkpoint方式</span></span><br><span class="line">          <span class="comment">//.cleanupFullSnapshot()</span></span><br><span class="line">          <span class="comment">//cleanupIncrementally：针对内存的增量删除方式</span></span><br><span class="line">          <span class="comment">//增量删除策略只支持基于内存的HashMapStateBackend，不支持EmbeddedRocksDBStateBackend</span></span><br><span class="line">          <span class="comment">//它的实现思路是在所有数据上维护一个全局迭代器。当遇到某些事件（例如状态访问）时会触发增量删除</span></span><br><span class="line">          <span class="comment">//cleanupSize=100 和 runCleanupForEveryRecord=true 表示每访问一个状态数据就会向前迭代遍历100条数据并删除其中过期的数据</span></span><br><span class="line">          .cleanupIncrementally(<span class="number">100</span>,<span class="literal">true</span>)</span><br><span class="line">          <span class="comment">//针对Rocksdb的增量删除方式</span></span><br><span class="line">          <span class="comment">//当Rocksdb在做Compact(合并)的时候删除过期数据</span></span><br><span class="line">          <span class="comment">//每Compact(合并)1000个Entry之后，会从Flink中查询当前时间戳，用于判断这些数据是否过期</span></span><br><span class="line">          <span class="comment">//.cleanupInRocksdbCompactFilter(1000)</span></span><br><span class="line">          .build</span><br><span class="line">        <span class="comment">//注册状态</span></span><br><span class="line">        <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">          <span class="string">"countState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">          classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment">//开启TTL机制</span></span><br><span class="line">        <span class="comment">//注意：开启TTL机制会增加状态的存储空间，因为在存储状态的时候还需要将状态的上次修改时间一起存储</span></span><br><span class="line">        valueStateDesc.enableTimeToLive(ttlConfig)</span><br><span class="line">        countState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>)): (<span class="type">String</span>,<span class="type">Int</span>) = &#123;</span><br><span class="line">        <span class="comment">//从状态中获取这个key之前出现的次数</span></span><br><span class="line">        <span class="keyword">var</span> lastNum = countState.value()</span><br><span class="line">        <span class="keyword">val</span> currNum = value._2</span><br><span class="line">        <span class="comment">//如果这个key的数据是第一次过来，则将之前出现的次数初始化为0</span></span><br><span class="line">        <span class="keyword">if</span>(lastNum == <span class="literal">null</span>)&#123;</span><br><span class="line">          lastNum = <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//汇总出现的次数</span></span><br><span class="line">        <span class="keyword">val</span> sum = lastNum+currNum</span><br><span class="line">        <span class="comment">//更新状态</span></span><br><span class="line">        countState.update(sum)</span><br><span class="line">        <span class="comment">//返回单词及单词出现的总次数</span></span><br><span class="line">        (value._1,sum)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StateTTLDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">这个可以可以直接在idea中运行。</span><br><span class="line">先开启socket：</span><br><span class="line"></span><br><span class="line">启动代码。</span><br><span class="line"></span><br><span class="line">在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">可以看到任务输出结果：</span><br><span class="line">2&gt; (b,1)</span><br><span class="line">6&gt; (a,1)</span><br><span class="line">由于我们设置了状态的TTL时间是10秒，所以10秒之后，再模拟产生一条数据 a</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a</span><br><span class="line">看一下输出的数据效果，发现此时a的次数还是1，说明之前在状态中存储的那个a已经过期了。</span><br><span class="line">6&gt; (a,1)</span><br><span class="line">立刻再模拟产生一条数据 a</span><br><span class="line">a</span><br><span class="line">此时发现输出的a的次数就是2了，在数据过期前是可以使用的，并且使用之后，数据的过期时间会对应的往后顺延，因为我们重新向状态中写入了a的数据，那么a对应的时间戳就会被修改为当前最新时间。</span><br><span class="line">6&gt; (a,2)</span><br><span class="line">这就是状态的TTL。</span><br><span class="line"></span><br><span class="line">其实在Flink 的DataStream API 中，状态的TTL功能的应用场景还是比较少的。状态的TTL功能在Flink SQL中是被大规模应用的，除了窗口类操作和ETL之类的任务之外，其余的Flink SQL任务基本都需要用到状态的TTL机制。</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Flink任务中设置了TTL 和不设置TTL 的状态是不兼容的，大家在使用的时候时一定要注意。避免出现任务从 Checkpoint &#x2F;Savepoint无法恢复的情况。但是我们是可以去修改TTL的时间的，因为修改时长并不会改变状态的存储结构。</span><br><span class="line">如果试图使用设置了TTL的状态恢复先前没有设置TTL时生成的状态，将导致任务恢复失败。</span><br></pre></td></tr></table></figure><h2 id="Window中的数据存在哪里？"><a href="#Window中的数据存在哪里？" class="headerlink" title="Window中的数据存在哪里？"></a>Window中的数据存在哪里？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">简单来说是内存中，严格来说其实是存储在状态中</span><br><span class="line"></span><br><span class="line">核心内容是在WindowOperator这个类里面。</span><br><span class="line"></span><br><span class="line">下面我们来从源码层面来具体分析一下。</span><br><span class="line">随便找一个类，在keyBy后面调用window(..)方法。</span><br><span class="line">@PublicEvolving</span><br><span class="line">def window[W &lt;: Window](assigner: WindowAssigner[_ &gt;: T, W]): WindowedStream[T, K, W] &#x3D; &#123;</span><br><span class="line">  new WindowedStream(new WindowedJavaStream[T, K, W](javaStream, assigner))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">进入new WindowedJavaStream()</span><br><span class="line">这里面会创建WindowOperatorBuilder。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public <span class="type">WindowedStream</span>(<span class="type">KeyedStream</span>&lt;<span class="type">T</span>, <span class="type">K</span>&gt; input, <span class="type">WindowAssigner</span>&lt;? <span class="keyword">super</span> <span class="type">T</span>, <span class="type">W</span>&gt; windowAssigner) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.input = input;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.builder =</span><br><span class="line">            <span class="keyword">new</span> <span class="type">WindowOperatorBuilder</span>&lt;&gt;(</span><br><span class="line">                    windowAssigner,</span><br><span class="line">                    windowAssigner.getDefaultTrigger(input.getExecutionEnvironment()),</span><br><span class="line">                    input.getExecutionConfig(),</span><br><span class="line">                    input.getType(),</span><br><span class="line">                    input.getKeySelector(),</span><br><span class="line">                    input.getKeyType());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">进入WindowOperatorBuilder这个类：</span><br><span class="line">在这个类里面有一个buildWindowOperator方法，他会创建WindowOperator，</span><br><span class="line">private &lt;ACC, R&gt; WindowOperator&lt;K, T, ACC, R, W&gt; buildWindowOperator(</span><br><span class="line">        StateDescriptor&lt;? extends AppendingState&lt;T, ACC&gt;, ?&gt; stateDesc,</span><br><span class="line">        InternalWindowFunction&lt;ACC, R, K, W&gt; function) &#123;</span><br><span class="line"></span><br><span class="line">    return new WindowOperator&lt;&gt;(</span><br><span class="line">            windowAssigner,</span><br><span class="line">            windowAssigner.getWindowSerializer(config),</span><br><span class="line">            keySelector,</span><br><span class="line">            keyType.createSerializer(config),</span><br><span class="line">            stateDesc,</span><br><span class="line">            function,</span><br><span class="line">            trigger,</span><br><span class="line">            allowedLateness,</span><br><span class="line">            lateDataOutputTag);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">进入WindowOperator</span><br><span class="line">这个类中有一个核心方法是processElement，处理窗口中的数据</span><br><span class="line">在这个方法中可以看到里面用到了windowState</span><br><span class="line">windowState.setCurrentNamespace(stateWindow);</span><br><span class="line">windowState.add(element.getValue());</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">查看windowState的介绍：</span><br><span class="line">这是一个聚合类型的状态。</span><br><span class="line">从注释上可以发现，windowState负责存储窗口内的状态，每个窗口一个命名空间。</span><br><span class="line">&#x2F;** The state in which the window contents is stored. Each window is a namespace *&#x2F;</span><br><span class="line">private transient InternalAppendingState&lt;K, W, IN, ACC, ACC&gt; windowState;</span><br><span class="line">从这里可以看出来，窗口中的数据其实是在状态中维护的。</span><br><span class="line"></span><br><span class="line">当窗口触发执行完成之后，窗口中维护的状态数据是会被删除的。</span><br><span class="line">他里面有一个clearAllState方法，当窗口执行结束后这个方法会被触发</span><br><span class="line">他里面会调用windowState.clear(); 清空状态中的数据。</span><br><span class="line">private void clearAllState(</span><br><span class="line">        W window, AppendingState&lt;IN, ACC&gt; windowState, MergingWindowSet&lt;W&gt; mergingWindows)</span><br><span class="line">        throws Exception &#123;</span><br><span class="line">    windowState.clear();</span><br><span class="line">    triggerContext.clear();</span><br><span class="line">    processContext.window &#x3D; window;</span><br><span class="line">    processContext.clear();</span><br><span class="line">    if (mergingWindows !&#x3D; null) &#123;</span><br><span class="line">        mergingWindows.retireWindow(window);</span><br><span class="line">        mergingWindows.persist();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">所以窗口中的状态不会一直增加，除非你的窗口非常大，例如：天级别的窗口，这种情况的话最好是使用rocksdb存储状态，避免内存溢出。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-Flink新版本1.12以上-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%96%B0%E7%89%88%E6%9C%AC1.12%E4%BB%A5%E4%B8%8A-1.html</id>
    <published>2023-04-20T08:46:48.000Z</published>
    <updated>2023-06-02T08:22:04.622Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="Flink新版本1-12以上-1"><a href="#Flink新版本1-12以上-1" class="headerlink" title="Flink新版本1.12以上-1"></a>Flink新版本1.12以上-1</h1><h2 id="Flink新版本新特性介绍"><a href="#Flink新版本新特性介绍" class="headerlink" title="Flink新版本新特性介绍"></a>Flink新版本新特性介绍</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">下面针对Flink最近几个版本的更新的重要新特性进行汇总，帮助大家快速了解一下每个版本的差异。</span><br><span class="line">1：Flink 1.12版本重要新特性</span><br><span class="line"></span><br><span class="line">在DataStreamAPI 中支持批处理(批流一体化)。</span><br><span class="line">DataSetAPI被标记为过时。</span><br><span class="line">针对离线计算需求，建议使用Table API和SQL，或者使用DataStreamAPI 中的批处理模式。</span><br><span class="line">增加新的DataSinkAPI。</span><br><span class="line">扩展了KafkaSQL Connector，可以支持Upsert模式。</span><br><span class="line">…</span><br><span class="line">2：Flink 1.13版本重要新特性</span><br><span class="line"></span><br><span class="line">正式弃用旧的Flink Planner。</span><br><span class="line">相关SQL日期函数的修正，解决了时区的问题。</span><br><span class="line">例如：增加了 timestamp_ltz类型。</span><br><span class="line">正式弃用 Mesos 支持。</span><br><span class="line">提升Hive SQL的兼容性。</span><br><span class="line">增强DataStream API 和 Table API 的交互。</span><br><span class="line">Flink SQL Client 的改进。</span><br><span class="line">…</span><br><span class="line">3：Flink 1.14版本重要新特性</span><br><span class="line"></span><br><span class="line">移除旧的Flink Planner，将Blink Planner作为默认实现。</span><br><span class="line">批流一体化功能的完善。</span><br><span class="line">Checkpoint机制的改进。</span><br><span class="line">性能优化与效率提升。</span><br><span class="line">Table API &amp; SQL的功能完善与升级。</span><br><span class="line">…</span><br><span class="line">4：Flink 1.15版本重要新特性</span><br><span class="line"></span><br><span class="line">批流一体化功能的进一步完善。</span><br><span class="line">针对KafkaConnector模块提供了KafkaSource和KafkaSink。</span><br><span class="line">后续会移除FlinkKafkaConsumer和FlinkKafkaProducer。</span><br><span class="line">Flink SQL中添加了对JSON相关函数的支持。</span><br><span class="line">对 Flink 的运维操作进行了简化。</span><br><span class="line">…</span><br></pre></td></tr></table></figure><h2 id="快速上手使用Flink1-15"><a href="#快速上手使用Flink1-15" class="headerlink" title="快速上手使用Flink1.15"></a>快速上手使用Flink1.15</h2><h3 id="开发Flink-1-15代码"><a href="#开发Flink-1-15代码" class="headerlink" title="开发Flink 1.15代码"></a>开发Flink 1.15代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">首先创建一个新的maven项目：db_flink15</span><br><span class="line">修改基础环境：</span><br><span class="line">引入scala2.12，选择本地安装的java1.8、修改编译级别为8</span><br><span class="line">在src&#x2F;main中添加scala目录</span><br><span class="line">在src&#x2F;main&#x2F;scala中创建package：com.imooc.scala</span><br><span class="line"></span><br><span class="line">引入flink相关依赖，注释掉依赖中的&lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br></pre></td></tr></table></figure><h4 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.15.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;!-- &lt;scope&gt;provided&lt;&#x2F;scope&gt; --&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-clients&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.15.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;!-- &lt;scope&gt;provided&lt;&#x2F;scope&gt; --&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- log4j的依赖 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;!-- &lt;scope&gt;provided&lt;&#x2F;scope&gt; --&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;!-- &lt;scope&gt;provided&lt;&#x2F;scope&gt; --&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在项目的resource目录下添加log4j.properties文件，文件内容如下：</span><br><span class="line">log4j.rootLogger&#x3D;warn,stdout</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout &#x3D; org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Target &#x3D; System.out</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br></pre></td></tr></table></figure><h4 id="WordCountScala"><a href="#WordCountScala" class="headerlink" title="WordCountScala"></a>WordCountScala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.RuntimeExecutionMode</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 单词计数案例</span><br><span class="line"> * 注意：基于最新Flink批流一体化的API开发</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object WordCountScala &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    &#x2F;&#x2F;获取执行环境</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    &#x2F;&#x2F;指定处理模式，默认支持流处理模式，也支持批处理模式。</span><br><span class="line">    &#x2F;**</span><br><span class="line">     * STREAMING：流处理模式，默认。</span><br><span class="line">     * BATCH：批处理模式。</span><br><span class="line">     * AUTOMATIC：让系统根据数据源是否有界来自动判断是使用STREAMING还是BATCH。</span><br><span class="line">     *</span><br><span class="line">     * 建议在客户端中使用flink run提交任务的时候通过-Dexecution.runtime-mode&#x3D;BATCH指定</span><br><span class="line">     *&#x2F;</span><br><span class="line">    env.setRuntimeMode(RuntimeExecutionMode.AUTOMATIC)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;指定DataSource</span><br><span class="line">    val text &#x3D; env.socketTextStream(&quot;bigdata04&quot;, 9001)&#x2F;&#x2F;当处理模式指定为AUTOMATIC时，会按照流处理模式执行</span><br><span class="line">    &#x2F;&#x2F;val text &#x3D; env.readTextFile(&quot;D:\\data\\hello.txt&quot;)&#x2F;&#x2F;当处理模式指定为AUTOMATIC时，会按照批处理模式执行</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;指定具体的业务逻辑</span><br><span class="line">    import org.apache.flink.api.scala._</span><br><span class="line">    val wordCount &#x3D; text.flatMap(_.split(&quot; &quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      &#x2F;&#x2F;针对时间窗口，目前官方建议使用window，主要是为了让用户指定窗口触发是使用处理时间 or 事件时间</span><br><span class="line">      .window(TumblingProcessingTimeWindows.of(Time.seconds(2)))</span><br><span class="line">      .sum(1)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;指定DataSink</span><br><span class="line">    wordCount.print().setParallelism(1)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;执行程序</span><br><span class="line">    env.execute(&quot;WordCountScala&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="在已有的大数据集群中集成Flink-1-15环境"><a href="#在已有的大数据集群中集成Flink-1-15环境" class="headerlink" title="在已有的大数据集群中集成Flink 1.15环境"></a>在已有的大数据集群中集成Flink 1.15环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1：下载Flink1.15版本的安装包，上传到bigdata04机器的&#x2F;data&#x2F;soft目录中。</span><br><span class="line">2：解压</span><br><span class="line">3：在·&#x2F;etc&#x2F;profile·中配置·HADOOP_CLASSPATH·环境变量</span><br><span class="line">如果之前在安装·Flink1.11·版本环境的时候配置过了，则忽略此步骤。</span><br><span class="line">[root@bigdata04 soft]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">...</span><br><span class="line">export HADOOP_CLASSPATH&#x3D;&#96;$&#123;HADOOP_HOME&#125;&#x2F;bin&#x2F;hadoop classpath&#96;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4：使用Flink中yarn-session的方式验证一下此Flink客户端是否正常</span><br><span class="line">[root@bigdata04 flink-1.15.0]# bin&#x2F;yarn-session.sh -jm 1024m -tm 1024m -d</span><br><span class="line"></span><br><span class="line">提交之后到YARN上验证一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311504599.png" alt="image-20230531150430359"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">任务的web界面上显示的是Flink1.15版本，说明是没有问题的。</span><br><span class="line">5：最后停止这个yarn-session</span><br></pre></td></tr></table></figure><h3 id="向YARN中同时提交Flink-1-11-和1-15版本的代码"><a href="#向YARN中同时提交Flink-1-11-和1-15版本的代码" class="headerlink" title="向YARN中同时提交Flink 1.11 和1.15版本的代码"></a>向YARN中同时提交Flink 1.11 和1.15版本的代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">为了便于在YARN中同时提交Flink 1.11和1.15版本的代码，我们可以参考之前在讲spark3.x版本扩展内容时候的思路，对flink1.15版本中需要用到的脚本名称进行修改，增加版本名称后缀，便于分辨和使用。</span><br><span class="line">提交代码时常用的是yarn-session.sh和flink这两个脚本，修改这两个脚本的名称</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;soft&#x2F;flink-1.15.0&#x2F;bin</span><br><span class="line">[root@bigdata04 bin]# mv yarn-session.sh yarn-session-1-15.sh</span><br><span class="line">[root@bigdata04 bin]# mv flink flink1-15</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">然后为了便于在任何目录中使用这些脚本，需要在&#x2F;etc&#x2F;profile中配置环境变量</span><br><span class="line"></span><br><span class="line">注意：针对之前的Flink 1.11.1版本对应的环境变量是这样的</span><br><span class="line">[root@bigdata04 bin]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">...</span><br><span class="line">export FLINK_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;flink-1.11.1</span><br><span class="line">export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$FLINK_HOME&#x2F;bin:&#x2F;data&#x2F;so</span><br><span class="line">ft&#x2F;flink-1.15.0&#x2F;bin:$SPARK_HOME&#x2F;bin:$HIVE_HOME&#x2F;bin:&#x2F;data&#x2F;soft&#x2F;spark-3.</span><br><span class="line">2.1-bin-hadoop3.2&#x2F;bin:$PATH</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">现在还需要针对Flink 1.15.0版本配置对应的环境变量，可以直接在PATH中指定即可</span><br><span class="line">[root@bigdata04 bin]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">...</span><br><span class="line">export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$FLINK_HOME&#x2F;bin:&#x2F;data&#x2F;so</span><br><span class="line">ft&#x2F;flink-1.15.0&#x2F;bin:$SPARK_HOME&#x2F;bin:$HIVE_HOME&#x2F;bin:&#x2F;data&#x2F;soft&#x2F;spark-3.</span><br><span class="line">2.1-bin-hadoop3.2&#x2F;bin:$PATH</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 bin]# source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">接下来验证这两个版本中yarn-session的使用是否正常</span><br><span class="line">1：使用flink1.11.1版本启动一个yarn-session</span><br><span class="line">[root@bigdata04 ~]# yarn-session.sh -jm 1024m -tm 1024m -d</span><br><span class="line"></span><br><span class="line">在启动窗口可以看到如下日志</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;flink-1.11.1&#x2F;lib&#x2F;log4j-slf4j-impl-2.12.1.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;slf4j-log4j12-1.7.25.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这里面可以看到确实使用的是1.11.1版本。</span><br><span class="line"></span><br><span class="line">启动之后查看对应的任务页面，发现显示的是1.11.1版本</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311510294.png" alt="image-20230531151053162"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">查看jobmanager中的信息也可以验证是使用的1.11.1版本</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311511091.png" alt="image-20230531151114768"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311511275.png" alt="image-20230531151148156"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">停止此任务</span><br></pre></td></tr></table></figure><h4 id="Flink1-15-0版本的代码"><a href="#Flink1-15-0版本的代码" class="headerlink" title="Flink1.15.0版本的代码"></a>Flink1.15.0版本的代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2：使用flink1.15.0版本启动一个yarn-session</span><br><span class="line"></span><br><span class="line">在启动窗口可以看到如下日志</span><br><span class="line">(按照上述步骤验证)</span><br><span class="line"></span><br><span class="line">停止此任务</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3：配置Flink 1.15的historyserver</span><br><span class="line">目前任务停止后无法查看任务界面，需要配置并开启flink的historyserver</span><br><span class="line">之前针对flink 1.11.1版本我们已经配置过historyserver了，在这里先把他的historyserver服务启动起来。</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;historyserver.sh start</span><br><span class="line"></span><br><span class="line">注意：针对Flink的每个版本都需要启动一个对应的historyserver服务，如果一个客户端节点上部署了多个flink的版本，则需要启动多个historyserver服务。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">可以选择在一个节点上启动多个Flink版本的historyserver服务，也可以选择把多个historyserver服务放到多个不同的节点上。</span><br><span class="line">如果是在一个节点上启动多个historyserver服务，则需要修改historyserver服务的端口，每个historyserver服务使用一个独立的端口。</span><br><span class="line"></span><br><span class="line">在这里需要修改flink 1.15版本的相关配置</span><br><span class="line">修改flink 1.15中conf目录下的flink-conf.yaml配置文件，内容如下：</span><br><span class="line">[root@bigdata04 conf]# vi flink-conf.yaml </span><br><span class="line">...</span><br><span class="line">jobmanager.archive.fs.dir: hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;completed-jobs-1-15&#x2F;</span><br><span class="line">historyserver.web.address: bigdata04</span><br><span class="line">historyserver.web.port: 8083</span><br><span class="line">historyserver.archive.fs.dir: hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;completed-jobs-1-15&#x2F;</span><br><span class="line">historyserver.archive.fs.refresh-interval: 10000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意：在这里针对flink1.15版本使用的historyserver端口是8083，flink1.11.1使用的是8082。</span><br><span class="line"></span><br><span class="line">还有就是flink1.15版本使用的hdfs路径也改了一下，使用的是completed-jobs-1-15，不要和flink1.11.1版本使用相同的路径，否则后期会出现错乱。</span><br><span class="line"></span><br><span class="line">再启动flink1.15中的historyserver服务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# bin&#x2F;historyserver.sh start</span><br><span class="line">[INFO] 1 instance(s) of historyserver are already running on bigdata04.</span><br><span class="line">Starting historyserver daemon on host bigdata04.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在启动的时候会提示在这个节点上已经成功启动了一个historyserver，不要紧， 通过jps验证一下这个是否启动成功。</span><br><span class="line">[root@bigdata04 flink-1.15.0]# jps</span><br><span class="line">2289 HistoryServer</span><br><span class="line">3650 HistoryServer</span><br><span class="line"></span><br><span class="line">发现有两个HistoryServer，具体哪个是Flink1.15启动的，可以使用jps -ml查看</span><br><span class="line">[root@bigdata04 flink-1.15.0]# jps -ml</span><br><span class="line">2289 org.apache.flink.runtime.webmonitor.history.HistoryServer --configDir &#x2F;data&#x2F;soft&#x2F;flink-1.11.1&#x2F;conf</span><br><span class="line">3650 org.apache.flink.runtime.webmonitor.history.HistoryServer --configDir &#x2F;data&#x2F;soft&#x2F;flink-1.15.0&#x2F;conf</span><br><span class="line">4067 sun.tools.jps.Jps -ml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">还需要启动Hadoop集群的historyserver服务。</span><br><span class="line">在bigdata01上执行。</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br><span class="line"></span><br><span class="line">在bigdata02上执行。</span><br><span class="line">[root@bigdata02 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br><span class="line"></span><br><span class="line">在bigdata03上执行。</span><br><span class="line">[root@bigdata03 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">4：向YARN集群中正式提交Flink1.15.0版本的代码</span><br><span class="line">基于前面刚开发的代码进行打包，打包之前需要在pom.xml中先添加编译和打包的配置</span><br><span class="line"></span><br><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;!-- 编译插件 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;!-- scala编译插件 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;descriptorRefs&gt;</span><br><span class="line">                    &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                &lt;archive&gt;</span><br><span class="line">                    &lt;manifest&gt;</span><br><span class="line">                        &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                        &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                    &lt;&#x2F;manifest&gt;</span><br><span class="line">                &lt;&#x2F;archive&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">    &lt;&#x2F;plugins&gt;</span><br><span class="line">&lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">修改已有依赖的scope配置，全部设置为provided</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.15.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-clients&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.15.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;!-- log4j的依赖 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">执行打包操作</span><br><span class="line"></span><br><span class="line">启动socket服务</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line"></span><br><span class="line">将jar包上传到flink1.15目录下，然后使用flink run命令提交任务</span><br><span class="line">[root@bigdata04 flink-1.15.0]# flink1-15 run -m yarn-cluster  -c com.imooc.scala.WordCountScala -yjm 1024 -ytm 1024 db_flink15-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b</span><br><span class="line">a</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311525852.png" alt="image-20230531152500101"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">停止后任务后再去查看任务界面发现也是可以的。</span><br><span class="line"></span><br><span class="line">注意：任务停止后需要稍微等一会才可以在这里刷新出来任务列表。</span><br></pre></td></tr></table></figure><h4 id="Flink1-11-1版本的代码"><a href="#Flink1-11-1版本的代码" class="headerlink" title="Flink1.11.1版本的代码"></a>Flink1.11.1版本的代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">5：向YARN集群中正式提交Flink1.11.1版本的代码</span><br><span class="line"></span><br><span class="line">接下来验证一下Flink1.11.1版本的代码是否正常。</span><br><span class="line">基于之前开发的代码重新打包，上传到bigdata04中的flink1.11.1目录中</span><br><span class="line"></span><br><span class="line">注意：需要将里面的依赖的scope暂时都设置为provided。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">启动socket服务</span><br><span class="line"></span><br><span class="line">提交任务</span><br><span class="line">[root@bigdata04 flink-1.11.1]# flink run -m yarn-cluster  -c com.imooc.scala.stream.SocketWindowWordCountScala -yjm 1024 -ytm 1024 db_flink-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">模拟生成数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311529454.png" alt="image-20230531152920001"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">停止后任务后再去查看任务界面发现也是可以的。</span><br><span class="line"></span><br><span class="line">注意：任务停止后需要稍微等一会才可以在这里刷新出来任务列表。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311530588.png" alt="image-20230531153010268"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311530210.png" alt="image-20230531153022755"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到这为止，bigdata04客户端节点中可以同时支持提交flink1.11和1.15版本的代码。</span><br></pre></td></tr></table></figure><h2 id="Flink-1-15之State-状态-的容错与一致性"><a href="#Flink-1-15之State-状态-的容错与一致性" class="headerlink" title="Flink 1.15之State(状态)的容错与一致性"></a>Flink 1.15之State(状态)的容错与一致性</h2><h3 id="什么是State-状态"><a href="#什么是State-状态" class="headerlink" title="什么是State(状态)"></a>什么是State(状态)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">State中文翻译为状态。</span><br><span class="line"></span><br><span class="line">在Flink流计算中的某些业务场景下，状态是非常重要的。</span><br><span class="line">那下面我们首先来分析一下到底什么是状态。</span><br><span class="line"></span><br><span class="line">比较官方一点的定义是这样的：当前流计算任务执行过程中需要用到之前的数据，那么之前的数据就可以称之为状态。</span><br><span class="line"></span><br><span class="line">所以说针对流计算任务中的状态其实可以理解为历史流数据。</span><br><span class="line"></span><br><span class="line">举个生活中的例子：我们每个人的大脑中存储的昨天、前天的信息也可以认为是状态。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">状态在代码层面的体现其实就是一种存储数据的数据结构。类似于Java中的list、map之类的数据结构。</span><br><span class="line"></span><br><span class="line">它的出现主要是为了解决流(实时)计算中的两大问题：</span><br><span class="line">第一个问题：解决流计算中需要使用历史流数据的问题</span><br><span class="line">例如一些流计算的去重场景，当然也可以借助于外部存储系统来实现去重。</span><br><span class="line">状态的引入可以实现不依赖外部存储系统来存储中间数据，最终实现去重操作。</span><br><span class="line"></span><br><span class="line">第二个问题：解决流计算中数据一致性的问题(单纯使用状态是解决不了的，需要结合Checkpoint机制一起实现)</span><br><span class="line">例如金融数据的流计算场景，对结果的准确度要求比较高，需要保证任务故障重启后结果数据依然是准确的，不能出现重复或者丢失数据的情况。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面来通过两个图加深一下理解</span><br><span class="line"></span><br><span class="line">第一个图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311536495.png" alt="image-20230531153628847"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这个图里面显示了Flink实现流计算去重的业务流程，source组件接入实时数据流，中间在具体的算子中对实时数据进行去重，这里借助于State实现去重，也就是将source传输过来的数据存储到State中，只保留不重复的数据，最后通过Sink组件将需要的结果数据写出去。</span><br><span class="line"></span><br><span class="line">刚才我们说了，这里其实也可以通过外部存储系统来实现去重</span><br><span class="line">例如：我们可以通过redis实现数据去重的效果，这样其实也是可以的，但是有点大材小用了，引入外部存储系统会增加Flink任务的复杂度，使用State这个轻量级解决方案是比较合适的。</span><br><span class="line"></span><br><span class="line">大家可能还有一个疑问，针对这里实现的去重功能，我在Flink中直接new一个基于内存的set集合来存储历史接收到的数据，是不是也可以实现数据去重的效果？</span><br><span class="line">是可以的，这样也可以实现数据去重的效果，但是如果任务发生了异常，重启之后基于内存的set集合中的数据就没了，这样会导致任务重启后数据无法恢复到之前的样子。</span><br><span class="line"></span><br><span class="line">state中存储的数据默认也是存放在内存中的，不过state借助于checkpoint机制可以将内存中的数据持久化到HDFS中，这样可以实现任务重启后State数据的恢复。</span><br><span class="line"></span><br><span class="line">接下来来看一下第二个图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311539652.png" alt="image-20230531153924351"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">这个图里面显示了Flink在对金融数据实现实时累加求和时的业务场景，source组件对接的是kafka中的数据，中间通过算子实现数据累加求和，将聚合后的结果数据存储到State中，最终通过sink组件将聚合后的结果写出去。</span><br><span class="line">为了保证数据累加求和计算任务失败重启后数据的准确性，除了需要在State中维护每次累加求和的中间结果，还需要维护消费者对应的offset偏移量信息。</span><br><span class="line"></span><br><span class="line">在这个图里面</span><br><span class="line">当Source组件消费到第1条数据1的时候，此时offset偏移量会等于1，后面的算子接收到第1条数据1的时候会进行累加求和，产生的结果还是1。此时Source组件会在State中写入offset&#x3D;1，同时算子也会在State中写入累加后的结果1。假设这个时候触发了checkpoint操作，那么就会把State中存储的数据持久化到HDFS中，便于后期使用。</span><br><span class="line"></span><br><span class="line">当Source组件消费到第2条数据3的时候，此时offset偏移量会等于2，后面的算子接收到第2条数据3的时候会和上次的结果累加求和，所以产生的结果是1+3&#x3D;4。此时Source组件会在State中写入offset&#x3D;2，同时算子也会在State中写入累加后的结果4。假设这个时候也触发了checkpoint操作，也会把State中存储的数据持久化到HDFS中。</span><br><span class="line"></span><br><span class="line">当Source组件消费到第3条数据5的时候，此时offset偏移量会等于3，后面的算子接收到第3条数据5的时候会和上次的结果累加求和，所以产生的结果是4+5&#x3D;9。此时Source组件会在State中写入offset&#x3D;3，同时算子也会在State中写入累加后的结果9。假设这个时候也触发了checkpoint操作，也会把State中存储的数据持久化到HDFS中。</span><br><span class="line"></span><br><span class="line">后面再来新数据的话也是按照这个流程去执行。</span><br><span class="line"></span><br><span class="line">如果Flink任务将第4条数据消费出来了，并且发送到后面的算子中进行计算，但是在计算的时候却由于网络异常出现了问题，导致任务异常停止，当网络恢复正常后，我们重启任务，此时可以基于上一次checkpoint持久化的数据恢复任务中的State。这样就可以将消费者的offset重置为3，算子中累加的结果重置为9，接下来继续正常消费第4条数据进行计算，这样就可以保证数据的准确性了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：如果我们使用的是Java中的map、list之类数据结构来存储offset偏移量和算子的中间结果，checkpoint的时候无法将这些数据持久化到HDFS中，只有State中的数据才可以，这是Flink框架默认实现的机制，并不是所有的数据都可以被checkpoint持久化的，正因为如此，我们才需要使用State。</span><br></pre></td></tr></table></figure><h3 id="离线计算是否需要State-状态-？"><a href="#离线计算是否需要State-状态-？" class="headerlink" title="离线计算是否需要State(状态)？"></a>离线计算是否需要State(状态)？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在离线计算的时候我们没有提到过状态，但是在实时计算中却经常提到状态，这是为什么？</span><br><span class="line"></span><br><span class="line">状态主要是为了保存历史数据，并且保证结果数据的一致性。</span><br><span class="line"></span><br><span class="line">针对离线计算，所有历史数据都已经到了，不需要单独保存，如果在离线计算中任务失败了，重跑一遍程序就可以了，基本上没有影响。所以可以认为离线计算是不需要维护状态的。</span><br><span class="line"></span><br><span class="line">但是针对实时计算，数据是源源不断产生的，实时计算任务是7x24小时执行的，如果实时计算任务失败了，想要恢复数据，总不能从头再重新计算一遍把，这样肯定是不现实的。就算数据源是Kafka，支持重跑数据，但是这个任务可能已经运行了很多天了，重新把之前的数据再计算一遍也需要消耗很长时间，这样在数据时效层面上来说也是不合理的，因为这个数据恢复时间太长了。</span><br><span class="line">如果数据源是基于socket的，任务失败的时候还会导致数据丢失。</span><br><span class="line"></span><br><span class="line">所以说实时计算任务失败之后可能会导致源数据、以及中间结果数据丢失，这个时候想要在一定时效内保证数据的准确性就需要借助于状态实现了。</span><br></pre></td></tr></table></figure><h3 id="State相关概念整体概览"><a href="#State相关概念整体概览" class="headerlink" title="State相关概念整体概览"></a>State相关概念整体概览</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">State涉及的相关概念比较多，所以在这里我们首先从全局层面分析一下这些概念，这样可以构建一个全局观，便于后面的深入理解。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311553801.png" alt="image-20230531155313232"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">首先看图里面蓝色的方框，主要包括JobManager和TaskManager。</span><br><span class="line">JobManager对应的是Flink集群的主节点，TaskManager对应的是Flink集群的从节点。</span><br><span class="line"></span><br><span class="line">假设我们向Flink集群中提交了一个流计算任务，这个任务中包含了Source、Map、和Sink这三块。</span><br><span class="line">其中Source是负责从Kafka中消费数据的，为了保证任务重启后的数据准确性，Source组件会把消费到的Offset偏移量数据保存到State中。</span><br><span class="line">Map算子会把计算的中间结果也保存到State中，Sink也会把某些数据保存到State中，这样可以便于后期的数据恢复，保证数据的准确性。</span><br><span class="line"></span><br><span class="line">默认情况下，Source、Map、和Sink写入到State中的数据会存储在TaskManager节点中的JVM堆内存中。</span><br><span class="line">当然官方也可以支持将这些State数据存储到Rocksdb这个内嵌数据库中，Rocksdb数据库中的数据会存储在对应节点的本地磁盘文件中。</span><br><span class="line"></span><br><span class="line">当满足一定时机的情况下，Flink任务会触发checkpoint操作，当执行checkpoint操作的时候，会将默认存储在TaskManager节点JVM堆内存中的State数据保存到另外一个地方，这个地方可以是JobManager的JVM堆内存中，也可以是分布式文件系统中(例如HDFS)。</span><br><span class="line"></span><br><span class="line">初始的State数据的存储位置是由State Backend这个功能控制的。</span><br><span class="line">checkpoint时State数据的存储位置是由checkpoint storage这个功能控制的。</span><br><span class="line"></span><br><span class="line">这就是涉及到状态的Flink任务的整体执行流程以及checkpoint时的流程，在这大家先有一个初步的了解，这里面的细节内容后面我们会进一步详细分析。</span><br></pre></td></tr></table></figure><h3 id="State-状态-的类型"><a href="#State-状态-的类型" class="headerlink" title="State(状态)的类型"></a>State(状态)的类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面我们对状态有了基本的了解，其实状态对应的就是数据，这些数据会涉及到存储和恢复。</span><br><span class="line">任务正常运行的时候会向状态中写入数据，在任务重启时会涉及到状态的恢复。</span><br><span class="line"></span><br><span class="line">Flink从管理层面对这些状态进行了划分，大致划分出两种类型的状态：</span><br><span class="line">一种是Raw State，中文翻译为原生状态</span><br><span class="line">另外一种是Managed State，中文翻译为托管状态。</span><br></pre></td></tr></table></figure><h3 id="Raw-State-VS-Managed-State"><a href="#Raw-State-VS-Managed-State" class="headerlink" title="Raw State VS Managed State"></a>Raw State VS Managed State</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这两种类型的状态有什么区别吗？</span><br><span class="line">下面我们来详细分析一下，看这个表格：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311554256.png" alt="image-20230531155439184"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">在这里会从3个角度进行分析：</span><br><span class="line">1：从管理方式这个角度上进行分析</span><br><span class="line">原生状态是需要开发者自己管理的，需要自己实现数据的序列化。</span><br><span class="line"></span><br><span class="line">而托管状态是由Flink Runtime进行托管管理的，Flink已经实现好了存储和恢复这些功能，不需要我们自己实现序列化。</span><br><span class="line">从这里可以看出来，原生状态需要自己实现相关的功能，使用起来比较复杂。而托管状态已经被封装好了，使用起来比较方便。</span><br><span class="line"></span><br><span class="line">2：从数据结构这个角度上进行分析</span><br><span class="line">原生状态只支持字节数组类型，而托管状态则支持多种数据结构，例如：MapState、ListState、ValueState等。</span><br><span class="line">从这里可以看出来原生状态支持的是最基础的数据类型，比较灵活。而托管状态主要支持常见的数据类型。不过托管状态中支持的那些常见的数据类型在工作中其实已经足够使用了。</span><br><span class="line"></span><br><span class="line">3：从使用场景这个角度上进行分析</span><br><span class="line">原生状态主要在自定义Operator时使用，Operator是DataSource、Transform、以及DataSink这三块的统称。而托管状态在所有数据流场景中都是可以使用的，也就是说托管状态的应用场景是包含了原生状态的应用场景。</span><br><span class="line"></span><br><span class="line">这就是原生状态和托管状态的区别。</span><br><span class="line">在实际工作中，基本上只会用到托管状态，而不会用到原生状态。</span><br><span class="line"></span><br><span class="line">所以后面我们主要分析托管状态的使用。</span><br></pre></td></tr></table></figure><h4 id="托管状态-Managed-State-的类型"><a href="#托管状态-Managed-State-的类型" class="headerlink" title="托管状态(Managed State)的类型"></a>托管状态(Managed State)的类型</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对托管状态，从作用域层面进行划分，还可以再细分为两种类型：</span><br><span class="line">一种是keyed State，还有一种是Operator State。</span><br><span class="line"></span><br><span class="line">这两种类型的State中支持的数据结构是不一样的，来看一下这个表格</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311557837.png" alt="image-20230531155737405"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keyed State可以支持ValueState、ListState、ReducingState、AggregatingState、MapState这5种数据结构，这些数据结构也可以称为原语，原语是一个官方名词，不太好理解，我还是喜欢将它称为是数据结构。</span><br><span class="line">从名字上可以看出来，ListState其实就相当于是一个List列表了，MapState相当于是一个Map集合，具体这些数据类型的使用后面我们再具体分析，在这里先有个基本认识就可以了。</span><br><span class="line"></span><br><span class="line">Operator State可以支持ListState、UnionListState和BroadcastState这3种数据结构。</span><br></pre></td></tr></table></figure><h5 id="Keyed-State-VS-Operator-State"><a href="#Keyed-State-VS-Operator-State" class="headerlink" title="Keyed State VS Operator State"></a>Keyed State VS Operator State</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Keyed State和Operator State除了在支持的数据结构层面有区别，还有一些其他的区别，看这个表格：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311558118.png" alt="image-20230531155849233"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">在这里会从4个角度进行分析：</span><br><span class="line">1：从State使用场景这个角度上进行分析</span><br><span class="line">针对Keyed State，它只能应用在基于KeyedStream的数据流中，在Flink中，普通的数据流是DataStream，在DataStream后面调用keyBy算子之后，返回的就是KeyedStream数据流，那也就是说Keyed State只能应用在做过keyBy之后的数据流里面。</span><br><span class="line"></span><br><span class="line">而Operator State可以应用在所有数据流中，包括keyedStream。</span><br><span class="line"></span><br><span class="line">2：从State分配方式这个角度上进行分析</span><br><span class="line">针对Keyed State，因为是基于key进行分组的数据流，相同key的数据会进入到同一个子任务中被处理，此时每个相同的Key共享一个State实例。</span><br><span class="line"></span><br><span class="line">针对Operator State，需要兼容所有类型的数据流，所以此时算子的同一个子任务共享一个State实例，和key无关。</span><br><span class="line"></span><br><span class="line">3：从State创建方式这个角度上进行分析</span><br><span class="line">针对Keyed State，需要借助于getRuntimeContext这个对象来创建。</span><br><span class="line">针对Operator State，需要借助于context这个对象来创建。</span><br><span class="line"></span><br><span class="line">4：从State扩缩容模式这个角度上进行分析</span><br><span class="line">什么是扩缩容模式？</span><br><span class="line">通俗一点来说，其实就是在任务故障后恢复的时候，算子的并行度发生了变化，可能增加了并行度，或者减少了并行度。</span><br><span class="line">针对无状态的算子，扩缩容很容易，没有什么影响。</span><br><span class="line">但是针对有状态的算子，并行度发生改变之后，状态在恢复的时候会涉及到重新分组，需要将状态数据分配到和之前数量不相等的算子任务中。</span><br><span class="line"></span><br><span class="line">针对Keyed State，它会以KeyGroup为单位重新分配状态数据，KeyGroup其实就是包含了多个key的一个分组。</span><br><span class="line">针对Operator State，它会均匀分配状态数据，或者是广播分配，具体要看你使用的是哪种数据类型了。</span><br></pre></td></tr></table></figure><h6 id="Keyed-State类型的状态的扩缩容模式"><a href="#Keyed-State类型的状态的扩缩容模式" class="headerlink" title="Keyed State类型的状态的扩缩容模式"></a>Keyed State类型的状态的扩缩容模式</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对扩缩容模式，下面有几个图，我们来看一下，详细分析一下</span><br><span class="line"></span><br><span class="line">首先是针对Keyed State类型的状态的扩缩容模式</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311559880.png" alt="image-20230531155938701"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">图中左边的task1和task2表示是keyBy后面连接的某个算子的2个并行实例，表示这个算子的并行度为2。</span><br><span class="line">中间的 task1 、task2和task3表示任务重启后，将keyBy后面那个算子的并行度改为了3。</span><br><span class="line">右边的task1表示是任务又一次重启后，将keyBy后面那个算子的并行度又改为了1。</span><br><span class="line">这个图里面显示的是Flink任务中的同一个算子，在任务重启后，算子并行度发生变化时状态的分配情况。</span><br><span class="line"></span><br><span class="line">基于Keyed State类型状态的算子在扩缩容时会根据新的算子并行度数量对状态重新分配，不过为了降低状态数据在不同任务之间的迁移成本，Flink对这些状态做了分组，会按照所有状态的key进行分组，划分成多个keyGroup，每个keyGroup内部包含一部分key的状态，以keyGroup为单位重新分配状态数据。</span><br><span class="line"></span><br><span class="line">在这个图里面最开始算子的并行度为2，会产生2个task，task1和task2。其中task1里面的k1和k3这两个key的状态被划分到了一个keyGroup，k5和k7这两个key的状态被划分到了一个keyGroup，k9和k11这两个key的状态被划分到了一个keyGroup。</span><br><span class="line">Task2里面也是类似这样的效果，也划分出了一些keyGroup。</span><br><span class="line"></span><br><span class="line">此时当这个任务异常结束，重新启动的时候，我们对这个算子的并行度进行了调整，改成了3，相当于增加了并行度。此时这个算子在运行时会产生3个task，分别是这里面的task1 、task2和task3。</span><br><span class="line">这个时候在对状态数据进行恢复的时候就需要将之前2个task产生的状态数据恢复到最新的3个task中，如图所示，这些状态数据会按照keyGroup为单位分配到这3个task中。</span><br><span class="line"></span><br><span class="line">当任务重启运行了一段时间之后，由于某些异常情况导致任务又停止了，重启再启动的时候，我们将算子的并行度改为了1，相当于减少了并行度。此时这个算子在运行时只会产生1个task。</span><br><span class="line">这个时候在对状态数据进行恢复的时候就需要将之前3个task产生的状态数据恢复到这1个task中，如图所示，这些状态数据会按照keyGroup为单位分配到这1个task中。</span><br><span class="line"></span><br><span class="line">这就是Keyed State类型的状态在任务重启并行度发生了变化时状态的分配方式。</span><br></pre></td></tr></table></figure><h6 id="Operator-State类型的状态的扩缩容模式"><a href="#Operator-State类型的状态的扩缩容模式" class="headerlink" title="Operator State类型的状态的扩缩容模式"></a>Operator State类型的状态的扩缩容模式</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下Operator State类型的状态的扩缩容模式，</span><br><span class="line">在这里需要针对Operator State中的不同数据结构进行单独分析：</span><br><span class="line"></span><br><span class="line">首先是Operator State中的ListState，看这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311600510.png" alt="image-20230531160009335"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如果算子中使用了Operator State类型中的ListState这种状态，那么算子在扩缩容时会对ListState中的数据重新分配。</span><br><span class="line">大致流程是这样的：这个算子的所有并行运行的task中的ListState数据会被统一收集起来，然后均匀分配给更多的task或者更少的task。</span><br><span class="line"></span><br><span class="line">最开始这些ListState中的状态数据在2个task中维护。</span><br><span class="line">后面任务重启，并行度变成3以后，会将之前的状态数据分配给这3个task。</span><br><span class="line">当后面任务重启，并行度变成1以后，会将之前的状态数据都分配这1个task。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来是针对Operator State中的UnionListState，看这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311600921.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">UnionListState底层其实就是ListState，唯一的区别就是在扩缩容时状态数据的分配策略不一样。</span><br><span class="line">UnionListState会在扩缩容时把里面的所有状态数据全部广播发送给新任务。</span><br><span class="line">最开始这些状态数据在2个task中维护。</span><br><span class="line">后面任务重启，并行度变成3以后，之前task1和task2维护的状态数据都会分配给这3个task。</span><br><span class="line">当后面任务重启，并行度变成1以后，前面3个task都会将自己维护的数据分配给这1个task。</span><br><span class="line">所以针对UnionListState这种方式，任务重启恢复状态数据的时候，每个子任务都会收到所有的数据，但是这个子任务可以根据一定的策略选择操作部分状态数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后一个是Operator State中的BroadcastState，看这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311600611.png" alt="image-20230531160052830"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BroadcastState在扩缩容时会把状态广播发送给所有的新任务。</span><br><span class="line">那这种方式和UnionListState有什么区别？</span><br><span class="line">针对UnionListState这种方式，假设算子A的并行度为2，那么会产生2个task，这2个task中维护的状态数据是不一样的，当任务重启之后，如果并行度发生了变化，那么算子A的每个子任务都可以接收到之前2个task中维护的状态数据。</span><br><span class="line"></span><br><span class="line">针对BroadcastState这种方式，假设算子A的并行度为2，那么这2个task中的数据是完全一样的，当任务重启之后，如果并行度增加了，只需要基于某一个task中的状态数据复制到新的task中即可。如果任务重启后并行度减少了，只需要简单的去掉多余的task即可。</span><br></pre></td></tr></table></figure><h5 id="Keyed-State详解"><a href="#Keyed-State详解" class="headerlink" title="Keyed State详解"></a>Keyed State详解</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">首先看一下针对Keyed State的解释</span><br><span class="line">Keyed State是基于KeyedStream上的状态，在普通数据流后面调用keyBy之后可以获取到一个KeyedStream数据流。</span><br><span class="line">此时状态是和特定的key绑定的。</span><br><span class="line"></span><br><span class="line">针对KeyedStream上的每个Key，Flink都会维护一个状态实例。</span><br><span class="line"></span><br><span class="line">看一下下面这个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311637004.png" alt="image-20230531163721767"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">图中左边的Source表示是数据源，这个组件的并行度为2，所以Source产生了2个task。</span><br><span class="line">右边的Stateful表示是有状态的算子，这个算子的并行度也是2，所以也产生了2个task。</span><br><span class="line"></span><br><span class="line">假如数据源按照ID这个列作为Key进行了keyBy分组，形成了一个KeyedStream数据流，其中ID的值为A\B\C\D这种英文字母。</span><br><span class="line">此时这个数据流中所有ID为A的key共享一个状态，可以访问和更新这个状态，以此类推，每个Key对应一个自己的状态。</span><br><span class="line"></span><br><span class="line">在这个图里面Stateful 1这个task实例维护了A\B\Y这些key的状态数据。Stateful 2这个task实例维护了D\E\Z这些key的状态数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了：无论是Keyed State还是Operator State，Flink的状态都是基于本地的，也就是说每个算子子任务维护着这个子任务对应的状态存储，子任务之间的状态不支持互相访问。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在这个图里面，Stateful 1和Stateful 2这两个子任务虽然都属于同一个算子，但是他们是2个独立的子任务，所以这2个子任务之间的状态数据也是不支持互相访问的。</span><br></pre></td></tr></table></figure><h5 id="Keyed-State中支持的数据结构"><a href="#Keyed-State中支持的数据结构" class="headerlink" title="Keyed State中支持的数据结构"></a>Keyed State中支持的数据结构</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来具体分析一下Keyed State中支持的常见数据结构，也可以称之为状态的原语。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311642457.png" alt="image-20230531164250345"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ValueState:存储类型为T的单值状态，T是一个泛型。这个状态与对应的key绑定，是最简单的状态了。它里面可以存储任意类型的值，这个值也可以是一个复杂数据结构。它可以通过update方法更新状态的值，通过value()方法获取状态值。</span><br><span class="line"></span><br><span class="line">ListState:表示是一个列表状态，列表里面存储多个类型为T的元素。可以通过add方法往列表中添加数据；也可以通过get()方法返回一个Iterable列表来遍历状态数据。</span><br><span class="line"></span><br><span class="line">ReducingState:存储一个聚合后类型为T的单值状态，这种状态通过用户传入的reduceFunction，每次调用add方法添加值的时候，会调用reduceFunction，最后合并到一个单一的状态值，有点类似于ValueState，都是存储单个数值的。</span><br><span class="line"></span><br><span class="line">AggregatingState&lt;IN , OUT&gt;：存储一个聚合后类型为OUT的单值状态，它和ReducingState的区别是ReducingState在聚合时接收的数据类型和最终产生的聚合结果数据类型是一致的。但是AggregatingState在聚合时接收的数据类型和最终产生的聚合结果数据类型可以不一样，AggregatingState里面使用了两个泛型，IN代表聚合时传入的数据类型，OUT表示最终产生的结果数据类型。</span><br><span class="line"></span><br><span class="line">MapState&lt;UK, UV&gt;:可以存储key-value类型的多个元素，key和value可以是任何类型。用户可以通过put或putAll方法添加元素。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在这需要注意一点：我们前面分析的这些State对象，只是用于和状态进行交互(例如：更新、删除、清空、查询等操作)，真正的状态值，有可能是存储在内存中，也可能是rocksdb对应的本地磁盘中，相当于我们只是持有了状态的句柄。</span><br><span class="line"></span><br><span class="line">类似于我们在Java中new了一个对象a1，这个a1只是一个引用，真正的对象是存储在JVM堆内存中的，a1只是持有了这个对象在堆内存中的内存地址值。</span><br><span class="line"></span><br><span class="line">在实际工作中，其实最常用的State就是keyed State，keyed State中最常用的其实主要是ValueState、ListState和MapState这3个。</span><br><span class="line"></span><br><span class="line">其它State很少用，包括后面我们要讲到的Operator State。</span><br></pre></td></tr></table></figure><h5 id="Keyed-State的使用案例"><a href="#Keyed-State的使用案例" class="headerlink" title="Keyed State的使用案例"></a>Keyed State的使用案例</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面我们针对Keyed State的原理有了一定的了解，下面我们来真正实操一下，掌握Keyed State在工作中的实际应用：</span><br></pre></td></tr></table></figure><h6 id="温度告警-ValueState"><a href="#温度告警-ValueState" class="headerlink" title="温度告警-ValueState"></a>温度告警-ValueState</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">首先看第一个案例：温度告警</span><br><span class="line">大致需求是这样的，某机房内的多个设备会实时上报温度信息，在Flink任务内部需要对设备最近两次的温度进行对比，如果温差超过了20度，则需要发送告警信息，说明设备出问题了。</span><br><span class="line"></span><br><span class="line">在这里我们可以把设备的唯一标识ID字段作为keyby分组的key，这样的话在Flink内部就只需要维护设备的温度即可，温度值是一个数字，数字属于一个普通的单值，所以可以考虑使用ValueState。</span><br><span class="line"></span><br><span class="line">下面开始开发代码，</span><br><span class="line">创建package：com.imooc.scala.state</span><br><span class="line">创建object：KeyedState_AlarmDemo</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.state</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.&#123;<span class="type">MapFunction</span>, <span class="type">RichFlatMapFunction</span>, <span class="type">RichMapFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 温度告警：ValueState</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KeyedState_AlarmDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据格式为 设备ID,温度</span></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    text.map(line=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> tup = line.split(<span class="string">","</span>)</span><br><span class="line">      (tup(<span class="number">0</span>),tup(<span class="number">1</span>).toInt)</span><br><span class="line">    &#125;).keyBy(_._1)</span><br><span class="line">      .flatMap(<span class="keyword">new</span> <span class="type">RichFlatMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),<span class="type">String</span>] &#123;</span><br><span class="line">        <span class="comment">//声明一个ValueState类型的状态变量，存储设备上一次收到的温度数据</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> lastDataState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">         * @param parameters</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="comment">//注册状态</span></span><br><span class="line">          <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">            <span class="string">"lastDataState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">            classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">          )</span><br><span class="line">          lastDataState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>), out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="comment">//初始化</span></span><br><span class="line">          <span class="keyword">if</span>(lastDataState.value() == <span class="literal">null</span>)&#123;</span><br><span class="line">            lastDataState.update(value._2)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">//获取上次温度</span></span><br><span class="line">          <span class="keyword">val</span> tmpLastData = lastDataState.value()</span><br><span class="line">          <span class="comment">//如果某个设备的最近两次温差超过20度，则告警</span></span><br><span class="line">          <span class="keyword">if</span>(<span class="type">Math</span>.abs(value._2 - tmpLastData) &gt;= <span class="number">20</span>)&#123;</span><br><span class="line">            out.collect(value._1+<span class="string">"_温度异常"</span>)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">//更新状态</span></span><br><span class="line">          lastDataState.update(value._2)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"KeyedState_AlarmDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在这段代码中，确实没有设置状态存储位置或状态后端。默认情况下，Flink会使用内存作为状态后端，将状态存储在TaskManager的JVM堆上。如果要使用其他状态后端，例如RocksDB，需要在Flink配置文件中进行配置或在代码中显式设置。这段代码可能只是一个简单的示例，用于演示如何使用&#96;ValueState&#96;。</span><br><span class="line"></span><br><span class="line">无论您使用哪种状态后端（RocksDB或文件系统），都需要启用checkpoint才能在发生故障时恢复状态。状态后端负责管理运行时状态的存储和访问，而checkpoint则负责将状态数据定期保存到持久化存储中，以便在发生故障时可以从最近的checkpoint恢复状态。因此，如果您希望您的应用程序能够在发生故障时恢复状态，那么建议启用checkpoint。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">s1,50</span><br><span class="line">s2,10</span><br><span class="line">s3,10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">一直到这，程序还是打印不出任何数据，说明没有触发温度异常的逻辑。</span><br><span class="line">因为这里会针对每个key维护一个状态，现在输入的3个key都是不一样的，所以没有触发。</span><br><span class="line"></span><br><span class="line">只有再输入一个s1的值，才可能会触发</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">s1,10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时程序触发打印操作了</span><br><span class="line"></span><br><span class="line">s1_温度异常</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">为了加深大家的理解，我在代码里面增加一些调试代码，打印一些中间数据，便于我们观察分析：</span><br><span class="line">1：首先将任务的并行度调整一下，默认我的windows是8核的，所以并行度为8，为了看起来清晰，我就在代码层面将任务的并行度设置为8。</span><br><span class="line">2：在flatmap中增加一些打印操作，将当前的线程id、初始的状态值，以及上次的状态值打印一下。</span><br><span class="line"></span><br><span class="line">修改后的代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.state</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.&#123;<span class="type">MapFunction</span>, <span class="type">RichFlatMapFunction</span>, <span class="type">RichMapFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 温度告警：ValueState</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KeyedState_AlarmDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line">    <span class="comment">//设置任务全局并行度为8</span></span><br><span class="line">    env.setParallelism(<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据格式为 设备ID,温度</span></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    text.map(line=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> tup = line.split(<span class="string">","</span>)</span><br><span class="line">      (tup(<span class="number">0</span>),tup(<span class="number">1</span>).toInt)</span><br><span class="line">    &#125;).keyBy(_._1)</span><br><span class="line">      .flatMap(<span class="keyword">new</span> <span class="type">RichFlatMapFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>),<span class="type">String</span>] &#123;</span><br><span class="line">        <span class="comment">//声明一个ValueState类型的状态变量，存储设备上一次收到的温度数据</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> lastDataState: <span class="type">ValueState</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 任务初始化的时候这个方法执行一次</span></span><br><span class="line"><span class="comment">         * @param parameters</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="comment">//注册状态</span></span><br><span class="line">          <span class="keyword">val</span> valueStateDesc = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Int</span>](</span><br><span class="line">            <span class="string">"lastDataState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">            classOf[<span class="type">Int</span>]<span class="comment">//指定状态中存储的数据类型</span></span><br><span class="line">          )</span><br><span class="line">          lastDataState = getRuntimeContext.getState(valueStateDesc)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>), out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          println(<span class="string">"线程ID："</span>+<span class="type">Thread</span>.currentThread().getId+<span class="string">",接收到数据："</span>+value)</span><br><span class="line"><span class="comment">//打印当前的线程ID和接收到的数据</span></span><br><span class="line">          <span class="comment">//初始化</span></span><br><span class="line">          <span class="keyword">if</span>(lastDataState.value() == <span class="literal">null</span>)&#123;</span><br><span class="line">            lastDataState.update(value._2)</span><br><span class="line">            println(<span class="string">"lastDataState is null"</span>)<span class="comment">//打印初始的状态为null</span></span><br><span class="line">          &#125;</span><br><span class="line">          println(<span class="string">"lastDataState is "</span>+ lastDataState.value())<span class="comment">//打印上次的状态值</span></span><br><span class="line">          <span class="comment">//获取上次温度</span></span><br><span class="line">          <span class="keyword">val</span> tmpLastData = lastDataState.value()</span><br><span class="line">          <span class="comment">//如果某个设备的最近两次温差超过20度，则告警</span></span><br><span class="line">          <span class="keyword">if</span>(<span class="type">Math</span>.abs(value._2 - tmpLastData) &gt;= <span class="number">20</span>)&#123;</span><br><span class="line">            out.collect(value._1+<span class="string">"_温度异常"</span>)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">//更新状态</span></span><br><span class="line">          lastDataState.update(value._2)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"KeyedState_AlarmDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">重启开启socket</span><br><span class="line"></span><br><span class="line">运行代码。</span><br><span class="line">在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">s1,50</span><br><span class="line"></span><br><span class="line">控制台打印的数据如下：</span><br><span class="line">线程ID：91,接收到数据：(s1,50)</span><br><span class="line">lastDataState is null</span><br><span class="line">lastDataState is 50</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">flatmap算子的并行度为8，同时会有8个线程处理，通过keyBy对数据流分组之后，相同规则的数据会进入到同一个线程中。</span><br><span class="line">这里就说明s1,50这条数据被线程ID为91的线程处理了，s1这个key是第一次过来，对应的状态为null，所以这里打印出来了null，当状态为null时，程序会把当前的数据赋值给状态，所以下面打印的上次状态值就是50了。</span><br><span class="line"></span><br><span class="line">在之前的socket中再模拟产生一条数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">s2,10</span><br><span class="line"></span><br><span class="line">控制台打印的数据如下：</span><br><span class="line">线程ID：84,接收到数据：(s2,10)</span><br><span class="line">lastDataState is null</span><br><span class="line">lastDataState is 10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">此时说明这条数据被线程ID为84的线程处理了，s2这个key也是第一次过来的，所以初始也为null，后面给他赋值之后是10。</span><br><span class="line"></span><br><span class="line">在之前的socket中再模拟产生一条数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">s3,10</span><br><span class="line"></span><br><span class="line">控制台打印的数据如下：</span><br><span class="line">线程ID：91,接收到数据：(s3,10)</span><br><span class="line">lastDataState is null</span><br><span class="line">lastDataState is 10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时显示的线程ID也是91，说明s3这条数据和s1那条数据都是由同一个线程处理的，但是注意此时s3对应的状态默认依然是null，因为s3也是第一次过来，虽然ts3和s1都是由同一个线程处理的，但是状态是和key绑定的，所以s3对应的状态依然是null。</span><br><span class="line">所以前面咱们说过，KeyedSate是和key绑定的，针对KeyedStream上的每个Key，Flink都会维护一个状态实例。</span><br><span class="line"></span><br><span class="line">如果我们把ValueState换成一个普通的int变量，就不是这样的效果了，当s3这条数据过来的时候就可以获取到之前s1存储的数据50了，因为普通的变量在同一个线程里面是共享的。</span><br></pre></td></tr></table></figure><h6 id="直播间数据统计-MapState"><a href="#直播间数据统计-MapState" class="headerlink" title="直播间数据统计-MapState"></a>直播间数据统计-MapState</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看第二个案例：直播间数据统计</span><br><span class="line">大致需求是这样的，需要统计平台中每个主播在直播间内收到的礼物信息、点赞、关注等指标，以直播间为单位进行统计。</span><br><span class="line"></span><br><span class="line">由于用户每次开播都会生成一个新的直播间vid，并且我们需要基于这个直播间vid统计它里面的数据指标，这些数据指标包含多个维度，例如：礼物对应的数量、点赞对应的数据量、关注对应的数量，这种数据就适合使用MapState进行存储了，每一个直播间的数据指标存储到一个MapState中。</span><br><span class="line"></span><br><span class="line">下面开始开发代码，</span><br><span class="line">创建object：KeyedState_VideoDataDemo</span><br><span class="line"></span><br><span class="line">由于数据格式是JSON格式的，所以引入fastjson依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.2.68&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;!--&lt;scope&gt;provided&lt;&#x2F;scope&gt;--&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.state</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">MapState</span>, <span class="type">MapStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">KeyedProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 直播间数据统计-MapState</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KeyedState_VideoDataDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据格式</span></span><br><span class="line">    <span class="comment">//送礼数据：&#123;"type":"gift","uid":"1001","vid":"29901","value":100&#125;</span></span><br><span class="line">    <span class="comment">//关注数据：&#123;"type":"follow","uid":"1001","vid":"29901"&#125;</span></span><br><span class="line">    <span class="comment">//点赞数据：&#123;"type":"like","uid":"1001","vid":"29901"&#125;</span></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    text.map(line=&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> videoJsonData = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">      <span class="keyword">val</span> vid = videoJsonData.getString(<span class="string">"vid"</span>)</span><br><span class="line">      <span class="keyword">val</span> videoType = videoJsonData.getString(<span class="string">"type"</span>)</span><br><span class="line">      <span class="comment">//也可以使用if语句实现</span></span><br><span class="line">      videoType <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">"gift"</span> =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> value = videoJsonData.getIntValue(<span class="string">"value"</span>)</span><br><span class="line">          (vid,videoType,value)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; (vid,videoType,<span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).keyBy(_._1)<span class="comment">//注意：后面也可以使用flatmap算子，在这里换一种形式，使用低级API process</span></span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>,(<span class="type">String</span>,<span class="type">String</span>,<span class="type">Int</span>),(<span class="type">String</span>,<span class="type">String</span>,<span class="type">Int</span>)] &#123;</span><br><span class="line">        <span class="comment">//声明一个MapState类型的状态变量，存储用户的直播间数据指标</span></span><br><span class="line">        <span class="comment">//MapState中key的值为gift\follow\like，value的值为累加后的结果</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">var</span> videoDataState: <span class="type">MapState</span>[<span class="type">String</span>,<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="comment">//注册状态</span></span><br><span class="line">          <span class="keyword">val</span> mapStateDesc = <span class="keyword">new</span> <span class="type">MapStateDescriptor</span>[<span class="type">String</span>,<span class="type">Int</span>](</span><br><span class="line">            <span class="string">"videoDataState"</span>,<span class="comment">//指定状态名称</span></span><br><span class="line">            classOf[<span class="type">String</span>],<span class="comment">//指定key的类型</span></span><br><span class="line">            classOf[<span class="type">Int</span>]<span class="comment">//指定value的类型</span></span><br><span class="line">          )</span><br><span class="line">          videoDataState = getRuntimeContext.getMapState(mapStateDesc)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: (<span class="type">String</span>, <span class="type">String</span>, <span class="type">Int</span>),</span><br><span class="line">                                    ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>, <span class="type">Int</span>), (<span class="type">String</span>, <span class="type">String</span>, <span class="type">Int</span>)]#<span class="type">Context</span>,</span><br><span class="line">                                    out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> videoType = value._2</span><br><span class="line">          <span class="keyword">var</span> num = value._3</span><br><span class="line">          <span class="comment">//判断状态中是否有这个数据</span></span><br><span class="line">          <span class="keyword">if</span>(videoDataState.contains(videoType))&#123;</span><br><span class="line">            num += videoDataState.get(videoType)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">//更新状态</span></span><br><span class="line">          videoDataState.put(videoType,num)</span><br><span class="line">          out.collect((value._1,videoType,num))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"KeyedState_VideoDataDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">在Flink程序中，要使用Keyed State，必须先对数据流进行keyBy操作。keyBy操作会将数据流分区，使得具有相同键的数据被发送到同一个算子实例进行处理。</span><br><span class="line"></span><br><span class="line">在keyBy之后，可以使用flatMap或process函数来访问和操作Keyed State。这两种函数都可以访问Keyed State，但它们之间有一些区别。</span><br><span class="line"></span><br><span class="line">&#96;flatMap&#96;函数允许您对输入数据进行一对多的转换。它接收一个输入元素，并可以生成零个、一个或多个输出元素。它通常用于对数据流进行扁平化处理。</span><br><span class="line"></span><br><span class="line">process函数提供了更多的控制能力。它允许您访问时间戳、watermark和其他元数据，并且可以注册定时器来执行基于时间的操作。它通常用于更复杂的事件驱动应用程序。</span><br><span class="line"></span><br><span class="line">因此，在选择使用flatMap还是process函数时，需要根据您的应用程序需求来决定。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">除了&#96;flatMap&#96;和&#96;process&#96;函数之外，还可以使用&#96;reduce&#96;和&#96;aggregate&#96;函数来访问和操作Keyed State。</span><br><span class="line"></span><br><span class="line">&#96;reduce&#96;函数允许您对数据流中的元素进行归约操作，以计算每个键的累积结果。它需要一个ReduceFunction，该函数指定如何将两个输入元素合并为一个输出元素。</span><br><span class="line"></span><br><span class="line">&#96;aggregate&#96;函数与&#96;reduce&#96;函数类似，但提供了更多的灵活性。它需要一个AggregateFunction，该函数由三个方法组成：&#96;createAccumulator&#96;、&#96;add&#96;和&#96;getResult&#96;。这些方法分别用于创建累加器、将输入元素添加到累加器中并计算累积结果。</span><br><span class="line"></span><br><span class="line">这些函数都可以访问Keyed State，并根据您的应用程序需求选择使用。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">开启一个新的socket</span><br><span class="line"></span><br><span class="line">运行代码。</span><br><span class="line">在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">&#123;&quot;type&quot;:&quot;gift&quot;,&quot;uid&quot;:&quot;1001&quot;,&quot;vid&quot;:&quot;29901&quot;,&quot;value&quot;:100&#125;</span><br><span class="line">&#123;&quot;type&quot;:&quot;follow&quot;,&quot;uid&quot;:&quot;1001&quot;,&quot;vid&quot;:&quot;29901&quot;&#125;</span><br><span class="line">&#123;&quot;type&quot;:&quot;like&quot;,&quot;uid&quot;:&quot;1001&quot;,&quot;vid&quot;:&quot;29902&quot;&#125;</span><br><span class="line">&#123;&quot;type&quot;:&quot;like&quot;,&quot;uid&quot;:&quot;1001&quot;,&quot;vid&quot;:&quot;29901&quot;&#125;</span><br><span class="line">&#123;&quot;type&quot;:&quot;gift&quot;,&quot;uid&quot;:&quot;1001&quot;,&quot;vid&quot;:&quot;29901&quot;,&quot;value&quot;:100&#125;</span><br><span class="line">&#123;&quot;type&quot;:&quot;like&quot;,&quot;uid&quot;:&quot;1001&quot;,&quot;vid&quot;:&quot;29904&quot;&#125;</span><br><span class="line"></span><br><span class="line">控制台打印的数据如下</span><br><span class="line">7&gt; (29901,gift,100)</span><br><span class="line">7&gt; (29901,follow,1)</span><br><span class="line">6&gt; (29902,like,1)</span><br><span class="line">7&gt; (29901,like,1)</span><br><span class="line">7&gt; (29901,gift,200)</span><br><span class="line">6&gt; (29904,like,1)</span><br></pre></td></tr></table></figure><h6 id="订单数据补全-ListState"><a href="#订单数据补全-ListState" class="headerlink" title="订单数据补全-ListState"></a>订单数据补全-ListState</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看第三个案例：订单数据补全</span><br><span class="line">大致需求是这样的，某外卖平台需要开发一个实时订单消息推送功能，当用户下单，并且成功支付后向商家推送一条消息。</span><br><span class="line">其中下单数据是一个数据流，支付数据是另外一个数据流。</span><br><span class="line">这个时候就需要对两个数据流进行关联了，当同一个订单相关的数据都到齐之后向外推送消息。</span><br><span class="line">针对这个需求，我们计划使用ListState实现。</span><br><span class="line"></span><br><span class="line">下面开始开发代码，</span><br><span class="line">创建object：KeyedState_OrderDataDemo</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.state</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichFlatMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ListState</span>, <span class="type">ListStateDescriptor</span>, <span class="type">ValueState</span>, <span class="type">ValueStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.<span class="type">KeyedCoProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 订单数据补全-ListState （双流Join）</span></span><br><span class="line"><span class="comment"> * 订单数据流 + 支付数据流</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KeyedState_OrderDataDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//数据格式为</span></span><br><span class="line">    <span class="comment">//订单数据流：&#123;"pid":"1001","pname":"n1"&#125;</span></span><br><span class="line">    <span class="comment">//支付数据流：&#123;"pid":"1001","pstatus":"success"&#125;</span></span><br><span class="line">    <span class="keyword">val</span> orderText = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">val</span> payText = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9002</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//解析订单数据流</span></span><br><span class="line">    <span class="keyword">val</span> orderTupleData = orderText.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> orderJsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">      <span class="keyword">val</span> pid = orderJsonObj.getString(<span class="string">"pid"</span>)</span><br><span class="line">      <span class="keyword">val</span> pname = orderJsonObj.getString(<span class="string">"pname"</span>)</span><br><span class="line">      (pid, pname)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//解析支付数据流</span></span><br><span class="line">    <span class="keyword">val</span> payTupleData = payText.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> payJsonObj = <span class="type">JSON</span>.parseObject(line)</span><br><span class="line">      <span class="keyword">val</span> pid = payJsonObj.getString(<span class="string">"pid"</span>)</span><br><span class="line">      <span class="keyword">val</span> pstatus = payJsonObj.getString(<span class="string">"pstatus"</span>)</span><br><span class="line">      (pid, pstatus)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对两个流进行分组+connect连接(也可以先对两个流分别调用keyBy，再调用connect，效果一样)</span></span><br><span class="line">    orderTupleData.connect(payTupleData)</span><br><span class="line">        .keyBy(<span class="string">"_1"</span>,<span class="string">"_1"</span>)<span class="comment">//field1表示第1个流里面的分组字段，field2表示第2个流里面的分组字段</span></span><br><span class="line">        .process(<span class="keyword">new</span> <span class="type">KeyedCoProcessFunction</span>[<span class="type">String</span>,(<span class="type">String</span>,<span class="type">String</span>),(<span class="type">String</span>,<span class="type">String</span>),(<span class="type">String</span>,<span class="type">String</span>,<span class="type">String</span>)] &#123;</span><br><span class="line">          <span class="comment">//声明两个ListState类型的状态变量，分别存储订单数据流和支付数据流</span></span><br><span class="line">          <span class="comment">/**</span></span><br><span class="line"><span class="comment">           * 注意：针对这个业务需求，pid在一个数据流中是不会重复的，其实使用ValueState也是可以的，</span></span><br><span class="line"><span class="comment">           * 因为在这里已经通过keyBy基于pid对数据分组了，所以只需要在状态中存储pname或者pstatus即可。</span></span><br><span class="line"><span class="comment">           * 但是如果pid数据在一个数据流里面会重复，那么就必须要使用ListState了，这样才能存储指定pid的多条数据</span></span><br><span class="line"><span class="comment">           */</span></span><br><span class="line">          <span class="keyword">private</span> <span class="keyword">var</span> orderDataState: <span class="type">ListState</span>[(<span class="type">String</span>,<span class="type">String</span>)] = _</span><br><span class="line">          <span class="keyword">private</span> <span class="keyword">var</span> payDataState: <span class="type">ListState</span>[(<span class="type">String</span>,<span class="type">String</span>)] = _</span><br><span class="line"></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="comment">//注册状态</span></span><br><span class="line">            <span class="keyword">val</span> orderListStateDesc = <span class="keyword">new</span> <span class="type">ListStateDescriptor</span>[(<span class="type">String</span>,<span class="type">String</span>)](</span><br><span class="line">              <span class="string">"orderDataState"</span>,</span><br><span class="line">              classOf[(<span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">val</span> payListStateDesc = <span class="keyword">new</span> <span class="type">ListStateDescriptor</span>[(<span class="type">String</span>,<span class="type">String</span>)](</span><br><span class="line">              <span class="string">"payDataState"</span>,</span><br><span class="line">              classOf[(<span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">            )</span><br><span class="line">            orderDataState = getRuntimeContext.getListState(orderListStateDesc)</span><br><span class="line">            payDataState = getRuntimeContext.getListState(payListStateDesc)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">//处理订单数据流</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement1</span></span>(orderTup: (<span class="type">String</span>, <span class="type">String</span>),</span><br><span class="line">                                       ctx: <span class="type">KeyedCoProcessFunction</span>[<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>), (<span class="type">String</span>, <span class="type">String</span>), (<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]#<span class="type">Context</span>,</span><br><span class="line">                                       out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="comment">//获取当前pid对应的支付数据流，关联之后输出数据，（可能是支付数据先到）</span></span><br><span class="line">            payDataState.get().forEach(payTup=&gt;&#123;</span><br><span class="line">              out.collect((orderTup._1,orderTup._2,payTup._2))</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="comment">//将本次接收到的订单数据添加到状态中，便于和支付数据流中的数据关联</span></span><br><span class="line">            orderDataState.add(orderTup)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">//处理支付数据流</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement2</span></span>(payTup: (<span class="type">String</span>, <span class="type">String</span>),</span><br><span class="line">                                       ctx: <span class="type">KeyedCoProcessFunction</span>[<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>), (<span class="type">String</span>, <span class="type">String</span>), (<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]#<span class="type">Context</span>,</span><br><span class="line">                                       out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="comment">//获取当前pid对应的订单数据流，关联之后输出数据，（可能是订单数据先到）</span></span><br><span class="line">            orderDataState.get().forEach(orderTup=&gt;&#123;</span><br><span class="line">              out.collect((orderTup._1,orderTup._2,payTup._2))</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="comment">//将本次接收到的订单数据添加到状态中，便于和订单数据流中的数据关联</span></span><br><span class="line">            payDataState.add(payTup)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"KeyedState_OrderDataDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">开启两个新的socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">[root@bigdata04 ~]# nc -l 9002</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">运行代码。</span><br><span class="line"></span><br><span class="line">在socket 9001中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">&#123;&quot;pid&quot;:&quot;1001&quot;,&quot;pname&quot;:&quot;n1&quot;&#125;</span><br><span class="line"></span><br><span class="line">在socket 9002中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9002</span><br><span class="line">&#123;&quot;pid&quot;:&quot;1003&quot;,&quot;pstatus&quot;:&quot;success&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">此时两条数据不是同一个订单的，所以程序没有任何输出</span><br><span class="line"></span><br><span class="line">接下来继续在socket 9001中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">&#123;&quot;pid&quot;:&quot;1003&quot;,&quot;pname&quot;:&quot;n3&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">此时可以看到控制台输出一条数据</span><br><span class="line">5&gt; (1003,n3,success)</span><br><span class="line"></span><br><span class="line">接下来继续在socket 9002中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9002</span><br><span class="line">&#123;&quot;pid&quot;:&quot;1001&quot;,&quot;pstatus&quot;:&quot;success&quot;&#125;</span><br><span class="line"></span><br><span class="line">此时可以看到控制台输出一条数据</span><br><span class="line">4&gt; (1001,n1,success)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">这样就实现了订单数据流和支付数据流的数据关联，这就是Flink中的双流Join，双流Join还有很多中实现方式，后面我们会有一个单独的章节详细分析双流Join。</span><br><span class="line"></span><br><span class="line">注意：这个案例中两个数据流的数据会一直存储在状态中，随着时间的增长，状态会越来越多，状态如果是存储在内存中的话，内存可能就扛不住了，最终导致内存溢出。</span><br><span class="line"></span><br><span class="line">针对这个问题有两个解决方案</span><br><span class="line"></span><br><span class="line">我们自己基于业务层面从状态中清理掉不用的数据，例如两份数据join到一起之后，就删除状态中的数据，这样可以保证状态中的数据不会一直无限递增。</span><br><span class="line">状态设置一个失效机制，官方提供的有一个TTL机制，可以给状态设置一个生存时间 ，过期自动删除，这个TTL机制我们后面再具体分析。</span><br><span class="line"></span><br><span class="line">在这里我们只是演示了状态的存储，针对状态的故障恢复我们还没有演示，因为我们使用状态的很大一个原因就是考虑到状态可以实现故障后的数据恢复。等后面讲到状态一致性的时候我们再来具体演示基于状态的任务故障后，状态数据是如何恢复的。</span><br></pre></td></tr></table></figure><h5 id="Keyed-State的使用形式总结"><a href="#Keyed-State的使用形式总结" class="headerlink" title="Keyed State的使用形式总结"></a>Keyed State的使用形式总结</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">在程序中想要使用Keyed State，大致可以通过下面这几种形式：</span><br><span class="line"></span><br><span class="line">第一种：通过重写RichXXXFunction，在里面创建和操作状态。例如针对map算子可以使用RichMapFunction，针对flatmap算子可以使用RichFlatMapFunction等。</span><br><span class="line">在这里使用对应的RichFunction主要是因为它里面提供了getRuntimeContext这个上下文，通过getRuntimeContext可以获取Keyed State。</span><br><span class="line"></span><br><span class="line">第二种：通过process()这种低级API。</span><br><span class="line">主要是因为process中也可以获取到getRuntimeContext这个上下文。</span><br><span class="line"></span><br><span class="line">第三种：其实还通过mapWithState()、flatMapWithState()等直接带有状态的算子，这种算子里面针对Keyed State直接进行了封装，使用起来更加方便，但是它有一定的局限性。</span><br><span class="line">首先是这些带有状态的算子只能应用在keyBy算子之后</span><br><span class="line">还有就是这些带有状态的算子里面封装的其实都是ValueState这种状态，不能自己控制使用哪种状态。</span><br><span class="line"></span><br><span class="line">下面我们来演示一下这种状态的使用：</span><br><span class="line">以常用的单词计数这个需求为例</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306011745279.png" alt="image-20230601174451755"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Scala有一种特殊的数据类型，叫做Option。</span><br><span class="line">Option有两种值，一种是Some，表示有值，一种是None，表示没有值</span><br><span class="line">Option通常会用于模式匹配中，用于判断某个变量是有值还是没有值，这比null来的更加简洁明了</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.state</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用MapWithState实现带有状态的单词计数案例</span></span><br><span class="line"><span class="comment"> * 思考问题：有状态的单词计数代码 和  无状态的单词计数代码有什么区别？</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KeyedState_MapWithStateDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> keyedStream = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * T：当前数据流中的数据类型</span></span><br><span class="line"><span class="comment">     * R：返回数据类型</span></span><br><span class="line"><span class="comment">     * S：State中存储的数据类型</span></span><br><span class="line"><span class="comment">     * [(String,Int),Int] (String,Int):R    Int:S</span></span><br><span class="line"><span class="comment">     * 注意：在这里State默认是ValueState，ValueState中可以存储多种数据类型。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    keyedStream.mapWithState[(<span class="type">String</span>,<span class="type">Int</span>),<span class="type">Int</span>]((in: (<span class="type">String</span>,<span class="type">Int</span>),count: <span class="type">Option</span>[<span class="type">Int</span>])=&gt;&#123;</span><br><span class="line">      count <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">//（t1,t2）</span></span><br><span class="line">        <span class="comment">//t1: 表示这个map操作需要返回的数据</span></span><br><span class="line">        <span class="comment">//t2: 表示State中目前的数据</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(c) =&gt; ((in._1,in._2+c),<span class="type">Some</span>(in._2+c))<span class="comment">//第2及以上次数，返回累加后的数据，更新状态;c是count简写，用count也行</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; ((in._1,in._2),<span class="type">Some</span>(in._2))<span class="comment">//第1次接收到数据，直接返回数据，初始化状态</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"KeyedState_MapWithStateDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分析一下mapWithState方法的底层实现</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapWithState</span></span>[<span class="type">R</span>: <span class="type">TypeInformation</span>, <span class="type">S</span>: <span class="type">TypeInformation</span>](</span><br><span class="line">      fun: (<span class="type">T</span>, <span class="type">Option</span>[<span class="type">S</span>]) =&gt; (<span class="type">R</span>, <span class="type">Option</span>[<span class="type">S</span>])): <span class="type">DataStream</span>[<span class="type">R</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (fun == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NullPointerException</span>(<span class="string">"Map function must not be null."</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> cleanFun = clean(fun)</span><br><span class="line">  <span class="keyword">val</span> stateTypeInfo: <span class="type">TypeInformation</span>[<span class="type">S</span>] = implicitly[<span class="type">TypeInformation</span>[<span class="type">S</span>]]</span><br><span class="line">  <span class="keyword">val</span> serializer: <span class="type">TypeSerializer</span>[<span class="type">S</span>] = stateTypeInfo.createSerializer(getExecutionConfig)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> mapper = <span class="keyword">new</span> <span class="type">RichMapFunction</span>[<span class="type">T</span>, <span class="type">R</span>] <span class="keyword">with</span> <span class="type">StatefulFunction</span>[<span class="type">T</span>, <span class="type">R</span>, <span class="type">S</span>] &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> stateSerializer: <span class="type">TypeSerializer</span>[<span class="type">S</span>] = serializer</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(in: <span class="type">T</span>): <span class="type">R</span> = &#123;</span><br><span class="line">      applyWithState(in, cleanFun)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  map(mapper)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以及它里面封装的状态：ValueState</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">StatefulFunction</span>[<span class="type">I</span>, <span class="type">O</span>, <span class="type">S</span>] <span class="keyword">extends</span> <span class="title">RichFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">val</span> stateSerializer: <span class="type">TypeSerializer</span>[<span class="type">S</span>]</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> state: <span class="type">ValueState</span>[<span class="type">S</span>] = _</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">applyWithState</span></span>(in: <span class="type">I</span>, fun: (<span class="type">I</span>, <span class="type">Option</span>[<span class="type">S</span>]) =&gt; (<span class="type">O</span>, <span class="type">Option</span>[<span class="type">S</span>])): <span class="type">O</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> (o, s: <span class="type">Option</span>[<span class="type">S</span>]) = fun(in, <span class="type">Option</span>(state.value()))</span><br><span class="line">    s <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(v) =&gt; state.update(v)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; state.update(<span class="literal">null</span>.asInstanceOf[<span class="type">S</span>])</span><br><span class="line">    &#125;</span><br><span class="line">    o</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(c: <span class="type">Configuration</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> info = <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">S</span>](<span class="string">"state"</span>, stateSerializer)</span><br><span class="line">    state = getRuntimeContext().getState(info)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">开启一个新的socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line"></span><br><span class="line">启动程序。</span><br><span class="line">在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a b </span><br><span class="line"></span><br><span class="line">控制台打印的结果如下：</span><br><span class="line">2&gt; (b,1)</span><br><span class="line">6&gt; (a,1)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">再模拟产生一条数据</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">控制台打印的结果如下：</span><br><span class="line">6&gt; (a,2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">从这里可以看出来，整个代码的执行流程是没有问题的。</span><br><span class="line"></span><br><span class="line">注意：这种方式了解即可，以后看到了这种写法知道是什么意思就行，个人不建议在工作中使用方式，因为这种方式不好理解，不属于通俗易懂的代码。</span><br></pre></td></tr></table></figure><h5 id="Operator-State-详解"><a href="#Operator-State-详解" class="headerlink" title="Operator State 详解"></a>Operator State 详解</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下针对Operator State的解释</span><br><span class="line">Operator State表示是和算子绑定的状态，与Key无关。所以说Operator State可以应用在任何类型的数据流上。</span><br><span class="line"></span><br><span class="line">此时算子的同一个子任务共享一个状态实例，流入这个算子子任务的数据可以访问和更新这个状态实例。</span><br><span class="line"></span><br><span class="line">看一下下面这个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306011818673.png" alt="image-20230601181849199"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">图中左边的Source表示是数据源，这个组件的并行度为2，会产生2个task。</span><br><span class="line">右边的Stateful这个有状态的算子的并行度也是2，对应也会产生2个task。</span><br><span class="line"></span><br><span class="line">此时Source-1的数据都会进入到Stateful -1这个子任务中，Stateful -1会维护一个状态实例，他接收到的A\B\Y这几个数据会存储到同一个状态实例中，A\B\Y这些数据会共享同一个状态实例。</span><br><span class="line"></span><br><span class="line">对应的Source-2的数据都会进入到Stateful -2这个子任务中，Stateful -2会维护一个状态实例，他接收到的D\E\Z这几个数据会存储到同一个状态实例中。D\E\Z这些数据会共享同一个状态实例。</span><br><span class="line"></span><br><span class="line">从这个图里面可以清晰的看出来Operator State和Keyed State之间的区别。</span><br><span class="line"></span><br><span class="line">在实际工作中Operator State的实际应用场景不如Keyed State多。Operator State经常被用在Source或Sink组件中，用来保存流入数据的偏移量或者对输出的数据做缓存，以保证Flink应用的Exactly-Once语义。</span><br><span class="line"></span><br><span class="line">其中有一个典型的应用场景是Flink从Kafka中消费数据，这个时候会用到FlinkKafkaConsumerBase这个接口，我们之前在讲Flink中的DataSource的时候讲到过，针对kafka的DataSource可以提供仅一次语句，想要提供仅一次语句，那么这个DataSource中就需要维护状态了，通过状态来维护消费偏移量信息。</span><br><span class="line"></span><br><span class="line">FlinkKafkaConsumerBase中实际上会维护消费者消费的topic名称、分区编号和offset偏移量这些信息，这些数据会使用Operator State类型的状态进行存储，类似图中显示的这样：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306011823622.png" alt="image-20230601182307162"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">实际上这里面会用到Operator State中的UnionListState这种状态，其实就是一个基于List列表的状态。</span><br><span class="line"></span><br><span class="line">接下来我们来具体分析一下FlinkKafkaConsumerBase的源码，需要先引入flink-connector-kafka这个依赖。</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.15.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;!--&lt;scope&gt;provided&lt;&#x2F;scope--&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在FlinkKafkaConsumerBase这个类的第201行有这么一行代码：</span><br><span class="line">private transient ListState&lt;Tuple2&lt;KafkaTopicPartition, Long&gt;&gt; unionOffsetStates;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这里用到了ListState，咱们刚才说的他使用的是UnionListState，其实UnionListState的底层就是ListState，唯一的区别就是任务故障恢复时状态数据的传输方式不一样，咱们之前在讲Operator State的扩缩容模式的时候详细分析过。</span><br><span class="line"></span><br><span class="line">这个ListState中存储的是Tuple2数据结构，Tuple2中的第1列是KafkaTopicPartition。</span><br><span class="line">KafkaTopicPartition中存储的是Kafka中指定topic的名称和分区的编号。</span><br><span class="line">查看KafkaTopicPartition的源码：</span><br><span class="line">private final String topic;</span><br><span class="line">private final int partition;</span><br><span class="line"></span><br><span class="line">Tuple2中的第2列是Long类型，其实存储的就是指定Topic对应分区的消费偏移量。</span><br></pre></td></tr></table></figure><h6 id="Operator-State中支持的数据结构"><a href="#Operator-State中支持的数据结构" class="headerlink" title="Operator State中支持的数据结构"></a>Operator State中支持的数据结构</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来具体分析一下 Operator State中支持的常见数据结构：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306011827274.png" alt="image-20230601182743126"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">由于Operator State中存储的是算子的同一个子任务中的状态数据，所以提供的都是可以存储多条数据的状态，没有提供ValueState这种状态。</span><br><span class="line"></span><br><span class="line">ListState：表示是一个列表类型的状态，存储类型为T的多个元素，T是一个泛型。</span><br><span class="line">UnionListState：底层就是ListState。</span><br><span class="line"></span><br><span class="line">ListState和UnionListState的区别在于任务故障后恢复数据时：ListState是将整个状态列表按照负载均衡算法均匀分布到各个算子子任务上，每个算子子任务得到的是整个列表的子集；而UnionListState会按照广播的方式，将整个列表发送给每个算子子任务。</span><br><span class="line"></span><br><span class="line">ListState和UnionListState的区别在源码层面也可以看到，看一下OperatorStateStore这个接口的内容：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ListState&lt;S&gt; getListState(ListStateDescriptor&lt;S&gt; stateDescriptor) throws Exception;</span><br><span class="line">ListState&lt;S&gt; getUnionListState(ListStateDescriptor&lt;S&gt; stateDescriptor) throws Exception;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">通过这块代码可以看出来，ListState和UnionListState对应的数据结构是一样的。</span><br><span class="line"></span><br><span class="line">BroadcastState：主要存储K-V类型的多个元素，存储的数据格式和MapState一样，但是它在恢复数据的时候会广播发送数据。</span><br><span class="line">BroadcastState属于OperatorState的一种特殊类型，主要是为了实现同一个算子的多个子任务共享一个State。</span><br></pre></td></tr></table></figure><h5 id="Operator-State的使用案例"><a href="#Operator-State的使用案例" class="headerlink" title="Operator State的使用案例"></a>Operator State的使用案例</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面我们针对Operator State的原理有了一定的了解，下面我们来实操一下，掌握Operator State在工作中的应用：</span><br></pre></td></tr></table></figure><h6 id="ListState的使用"><a href="#ListState的使用" class="headerlink" title="ListState的使用"></a>ListState的使用</h6><h6 id="Flink中自带的有状态的Source"><a href="#Flink中自带的有状态的Source" class="headerlink" title="Flink中自带的有状态的Source"></a>Flink中自带的有状态的Source</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">首先我们来分析一个Flink内部已有的Source：FromElementsFunction</span><br><span class="line">当我们使用env.fromElements或者env.fromCollection时，底层其实会用到FromElementsFunction这个数据源。</span><br><span class="line">代码路径如下所示：</span><br><span class="line">env.fromCollection() --&gt; javaEnv.fromCollection(collection, typeInfo) --&gt; SourceFunction&lt;OUT&gt; function &#x3D; new FromElementsFunction&lt;&gt;(data) --&gt; FromElementsFunction(Iterable&lt;T&gt; elements)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在FromElementsFunction这个类中有一个ListState：checkpointedState</span><br><span class="line"></span><br><span class="line">private transient ListState&lt;Integer&gt; checkpointedState;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">checkpointedState这个状态主要在initializeState和snapshotState这两个方法里面用到了。</span><br><span class="line"></span><br><span class="line">initializeState和snapshotState这两个方法有什么作用吗？</span><br><span class="line"></span><br><span class="line">在分析这两个方法之前，我们需要再重新梳理一下状态这块的概念</span><br><span class="line"></span><br><span class="line">从本质上来说，状态属于Flink算子子任务中的一种本地数据，为了保证这份数据的可恢复性，需要借助于Checkpoint机制来将状态数据持久化输出到分布式文件系统中。</span><br><span class="line"></span><br><span class="line">状态相关的主要逻辑有2块：</span><br><span class="line">1：在Checkpoint时将算子子任务中的状态数据写入到外部存储中，这个过程可以简称为：snapshot。</span><br><span class="line">2：在任务初始化或重启时，以一定的逻辑从外部存储中读取状态数据，并且恢复为算子子任务的本地数据，这个过程可以简称为restore。</span><br><span class="line"></span><br><span class="line">针对Keyed State来说，它对这2块内容做了完善的封装，我们程序员可以开箱即用。</span><br><span class="line"></span><br><span class="line">但是对于Operator State来说，每个算子子任务管理自己的Operator State，或者说每个算子子任务上的数据流共享同一个状态实例，可以访问和修改该状态。算子子任务上的数据在程序重启、扩缩容等场景下不能保证百分百的一致性。简单来说，就是Flink应用重启后，某个数据流中的元素不一定会和上次一样，还能流入到对应的子任务上。</span><br><span class="line">因此，我们需要根据自己的业务场景来设计snapshot和restore的逻辑。为了实现这两块的业务逻辑，Flink提供了最为基础的CheckpointedFunction接口。</span><br><span class="line"></span><br><span class="line">所以在这我们可以发现其实FromElementsFunction这个类就实现了CheckpointedFunction这个接口。</span><br><span class="line"></span><br><span class="line">CheckpointedFunction这个接口中提供了两个方法：</span><br><span class="line">void snapshotState(FunctionSnapshotContext context) throws Exception;</span><br><span class="line">void initializeState(FunctionInitializationContext context) throws Exception;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">其中snapshotState对应的就是刚才我们所分析的那个snapshot过程。</span><br><span class="line">initializeState对应的就是刚才我们所分析的那个restore过程。</span><br><span class="line"></span><br><span class="line">那我们来看一下FromElementsFunction里面针对这两个过程是怎么实现的：</span><br><span class="line"></span><br><span class="line">首先看一下snapshotState这个方法：</span><br><span class="line">这个方法会在checkpoint时触发执行，将checkpointedState中的数据持久化到外部存储中。</span><br><span class="line"></span><br><span class="line">public void snapshotState(FunctionSnapshotContext context) throws Exception &#123;</span><br><span class="line">    Preconditions.checkState(</span><br><span class="line">            this.checkpointedState !&#x3D; null,</span><br><span class="line">            &quot;The &quot; + getClass().getSimpleName() + &quot; has not been properly initialized.&quot;);</span><br><span class="line">    &#x2F;&#x2F;先清空一下checkpointedState中之前保存的数据</span><br><span class="line">    this.checkpointedState.clear();</span><br><span class="line">&#x2F;&#x2F;然后把目前正在处理的数据保存到checkpointedState中</span><br><span class="line">    this.checkpointedState.add(this.numElementsEmitted);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下initializeState这个方法：</span><br><span class="line">这个方法会在任务第一次启动，以及后续重启时执行，主要是为了从外部存储中读取状态数据，并且把状态数据恢复成本地数据。</span><br><span class="line"></span><br><span class="line">public void initializeState(FunctionInitializationContext context) throws Exception &#123;</span><br><span class="line">    Preconditions.checkState(</span><br><span class="line">            this.checkpointedState &#x3D;&#x3D; null,</span><br><span class="line">            &quot;The &quot; + getClass().getSimpleName() + &quot; has already been initialized.&quot;);</span><br><span class="line">    &#x2F;&#x2F;注册状态</span><br><span class="line">    this.checkpointedState &#x3D;</span><br><span class="line">            context.getOperatorStateStore()</span><br><span class="line">                    .getListState(</span><br><span class="line">                            new ListStateDescriptor&lt;&gt;(</span><br><span class="line">                                    &quot;from-elements-state&quot;, IntSerializer.INSTANCE));</span><br><span class="line">   &#x2F;&#x2F;判断任务是否是重启</span><br><span class="line">    if (context.isRestored()) &#123;</span><br><span class="line">        List&lt;Integer&gt; retrievedStates &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">&#x2F;&#x2F;从外部存储中读取状态数据，存储为本地数据</span><br><span class="line">        for (Integer entry : this.checkpointedState.get()) &#123;</span><br><span class="line">            retrievedStates.add(entry);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; given that the parallelism of the function is 1, we can only have 1 state</span><br><span class="line">        Preconditions.checkArgument(</span><br><span class="line">                retrievedStates.size() &#x3D;&#x3D; 1,</span><br><span class="line">                getClass().getSimpleName() + &quot; retrieved invalid state.&quot;);</span><br><span class="line">        &#x2F;&#x2F;从本地数据中获取第一条数据赋值给numElementsToSkip </span><br><span class="line">        this.numElementsToSkip &#x3D; retrievedStates.get(0);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：在initializeState里面注册状态时，首先会根据指定的状态名称到状态的外部存储中检查一下是否存在一个和当前状态名称相同的状态，如果存在，则尝试对状态进行恢复，如果不存在，则默认初始化状态。</span><br><span class="line"></span><br><span class="line">这个是Flink中自带的有状态的Source。</span><br></pre></td></tr></table></figure><h6 id="自定义一个有状态的Sink"><a href="#自定义一个有状态的Sink" class="headerlink" title="自定义一个有状态的Sink"></a>自定义一个有状态的Sink</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们通过自定义一个有状态的Sink来感受一下CheckpointedFunction的具体使用：</span><br><span class="line"></span><br><span class="line">需求是这样的：</span><br><span class="line">我们想要实现一个批量输出的功能，此时可以考虑在Sink组件内部定义一个缓存，但是还要保证数据一定会输出到外部系统。这个时候就需要借助于状态实现了，通过snapshotState定期将批量缓存的数据保存到状态中，如果程序出现了故障，重启后还可以从状态中将未输出的数据读取到缓存中，继续输出到外部系统。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.state</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.<span class="type">RuntimeExecutionMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 通过自定义Sink实现批量输出</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OperatorState_MyBufferSinkDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(<span class="type">RuntimeExecutionMode</span>.<span class="type">AUTOMATIC</span>)</span><br><span class="line">    <span class="comment">//设置并行度为2</span></span><br><span class="line">    env.setParallelism(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .addSink(<span class="keyword">new</span> <span class="type">MyBufferSink</span>())</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"OperatorState_MyBufferSinkDemo"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.state</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.state.&#123;<span class="type">ListState</span>, <span class="type">ListStateDescriptor</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.&#123;<span class="type">FunctionInitializationContext</span>, <span class="type">FunctionSnapshotContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.checkpoint.<span class="type">CheckpointedFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.<span class="type">SinkFunction</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义批量输出Sink</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyBufferSink</span> <span class="keyword">extends</span> <span class="title">SinkFunction</span>[(<span class="type">String</span>,<span class="type">Int</span>)] <span class="keyword">with</span> <span class="title">CheckpointedFunction</span></span>&#123;</span><br><span class="line">  <span class="comment">//声明一个ListState类型的状态变量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> checkpointedState: <span class="type">ListState</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">//定义一个本地缓存</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> bufferElements = <span class="type">ListBuffer</span>[(<span class="type">String</span>, <span class="type">Int</span>)]()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Sink的核心处理逻辑，将接收到的数据输出到外部系统</span></span><br><span class="line"><span class="comment">   * 接收到一条数据，这个方法就会执行一次</span></span><br><span class="line"><span class="comment">   * @param value</span></span><br><span class="line"><span class="comment">   * @param context</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(value: (<span class="type">String</span>, <span class="type">Int</span>), context: <span class="type">SinkFunction</span>.<span class="type">Context</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//将接收到的数据保存到本地缓存中</span></span><br><span class="line">    bufferElements += value</span><br><span class="line">    <span class="comment">//当本地缓存大小到达一定阈值时，将本地缓存中的数据一次性输出到外部系统</span></span><br><span class="line">    <span class="keyword">if</span> (bufferElements.size == <span class="number">2</span>) &#123;</span><br><span class="line">      println(<span class="string">"======start======"</span>)</span><br><span class="line">      <span class="keyword">for</span> (element &lt;- bufferElements) &#123;</span><br><span class="line">        println(element)</span><br><span class="line">      &#125;</span><br><span class="line">      println(<span class="string">"======end======"</span>)</span><br><span class="line">      <span class="comment">//清空本地缓存中的数据</span></span><br><span class="line">      bufferElements.clear()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 将本地缓存中的数据保存到状态中，在执行checkpoint时，会将状态中的数据持久化到外部存储中</span></span><br><span class="line"><span class="comment">   * @param context</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">snapshotState</span></span>(context: <span class="type">FunctionSnapshotContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//将上次写入到状态中的数据清空</span></span><br><span class="line">    checkpointedState.clear()</span><br><span class="line">    <span class="comment">//将最新的本地缓存中的数据写入到状态中</span></span><br><span class="line">    <span class="keyword">for</span> (element &lt;- bufferElements) &#123;</span><br><span class="line">      checkpointedState.add(element)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 初始化或者恢复状态</span></span><br><span class="line"><span class="comment">   * @param context</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeState</span></span>(context: <span class="type">FunctionInitializationContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//注册状态</span></span><br><span class="line">    <span class="keyword">val</span> descriptor = <span class="keyword">new</span> <span class="type">ListStateDescriptor</span>[(<span class="type">String</span>, <span class="type">Int</span>)](</span><br><span class="line">      <span class="string">"buffered-elements"</span>,</span><br><span class="line">      classOf[(<span class="type">String</span>,<span class="type">Int</span>)]</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//此时借助于context获取OperatorStateStore，进而获取ListState</span></span><br><span class="line">    checkpointedState = context.getOperatorStateStore.getListState(descriptor)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果是重启任务，需要从外部存储中读取状态数据并写入到本地缓存中</span></span><br><span class="line">    <span class="keyword">if</span>(context.isRestored) &#123;</span><br><span class="line">      checkpointedState.get().forEach(e=&gt;&#123;</span><br><span class="line">        bufferElements += e</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">开启一个新的Socket</span><br><span class="line"></span><br><span class="line">启动代码。</span><br><span class="line">在socket中模拟产生数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">此时控制台没有输出数据。</span><br><span class="line"></span><br><span class="line">再产生一条数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">b</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">此时控制台依然没有输出数据。</span><br><span class="line">为什么产生两条数据了还是没有输出？这是因为此时我们给程序设置的并行度为2。所以sink组件会产生2个子任务，这两条数据现在被划分到了两个子任务中，每个子任务只收到了1条数据，所以不满足输出数据的条件。</span><br><span class="line">如果把任务的并行度修改为1这个时候就会输出数据了。</span><br><span class="line"></span><br><span class="line">再产生一条数据：</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">此时发现控制台输出数据了：</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;start&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">(a,1)</span><br><span class="line">(a,1)</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;end&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这样就通过自定义Sink实现了有状态的批量输出功能。</span><br></pre></td></tr></table></figure><h6 id="UnionListState的使用"><a href="#UnionListState的使用" class="headerlink" title="UnionListState的使用"></a>UnionListState的使用</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对UnionListState的使用，在Kafka Connector中的FlinkKafkaConsumerBase里面有应用，我们来看一下对应的代码：</span><br><span class="line">前面其实我们已经简单分析了一些内容了，这个时候再来查看FlinkKafkaConsumerBase这个类，理解会更深一些：</span><br><span class="line">public abstract class FlinkKafkaConsumerBase&lt;T&gt; extends RichParallelSourceFunction&lt;T&gt;</span><br><span class="line">        implements CheckpointListener, ResultTypeQueryable&lt;T&gt;, CheckpointedFunction &#123;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在这里可以发现这个类其实也实现了CheckpointedFunction接口。</span><br><span class="line">那我们来看一下FlinkKafkaConsumerBase中对应的snapshotState和initializeState这两个方法的实现</span><br><span class="line"></span><br><span class="line">首先来看snapshotState方法：</span><br><span class="line">snapshotState方法里面的逻辑大致就是先清空之前unionOffsetStates中的数据，然后写入最新的数据。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">final</span> void snapshotState(<span class="type">FunctionSnapshotContext</span> context) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (!running) &#123;</span><br><span class="line">        <span class="type">LOG</span>.debug(<span class="string">"snapshotState() called on closed source"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">//清空之前写入的状态数据</span></span><br><span class="line">        unionOffsetStates.clear();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="type">AbstractFetcher</span>&lt;?, ?&gt; fetcher = <span class="keyword">this</span>.kafkaFetcher;</span><br><span class="line">        <span class="keyword">if</span> (fetcher == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="comment">// the fetcher has not yet been initialized, which means we need to return the</span></span><br><span class="line">            <span class="comment">// originally restored offsets or the assigned partitions</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">Map</span>.<span class="type">Entry</span>&lt;<span class="type">KafkaTopicPartition</span>, <span class="type">Long</span>&gt; subscribedPartition :</span><br><span class="line">                    subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line"><span class="comment">//向状态中写入数据</span></span><br><span class="line">                unionOffsetStates.add(</span><br><span class="line">                        <span class="type">Tuple2</span>.of(</span><br><span class="line">                                subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (offsetCommitMode == <span class="type">OffsetCommitMode</span>.<span class="type">ON_CHECKPOINTS</span>) &#123;</span><br><span class="line">                <span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call</span></span><br><span class="line">                <span class="comment">// can happen</span></span><br><span class="line">                <span class="comment">// on this function at a time: either snapshotState() or</span></span><br><span class="line">                <span class="comment">// notifyCheckpointComplete()</span></span><br><span class="line">                pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">HashMap</span>&lt;<span class="type">KafkaTopicPartition</span>, <span class="type">Long</span>&gt; currentOffsets = fetcher.snapshotCurrentState();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (offsetCommitMode == <span class="type">OffsetCommitMode</span>.<span class="type">ON_CHECKPOINTS</span>) &#123;</span><br><span class="line">                <span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call</span></span><br><span class="line">                <span class="comment">// can happen</span></span><br><span class="line">                <span class="comment">// on this function at a time: either snapshotState() or</span></span><br><span class="line">                <span class="comment">// notifyCheckpointComplete()</span></span><br><span class="line">                pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">Map</span>.<span class="type">Entry</span>&lt;<span class="type">KafkaTopicPartition</span>, <span class="type">Long</span>&gt; kafkaTopicPartitionLongEntry :</span><br><span class="line">                    currentOffsets.entrySet()) &#123;</span><br><span class="line"><span class="comment">//向状态中写入数据</span></span><br><span class="line">                unionOffsetStates.add(</span><br><span class="line">                        <span class="type">Tuple2</span>.of(</span><br><span class="line">                                kafkaTopicPartitionLongEntry.getKey(),</span><br><span class="line">                                kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (offsetCommitMode == <span class="type">OffsetCommitMode</span>.<span class="type">ON_CHECKPOINTS</span>) &#123;</span><br><span class="line">            <span class="comment">// truncate the map of pending offsets to commit, to prevent infinite growth</span></span><br><span class="line">            <span class="keyword">while</span> (pendingOffsetsToCommit.size() &gt; <span class="type">MAX_NUM_PENDING_CHECKPOINTS</span>) &#123;</span><br><span class="line">                pendingOffsetsToCommit.remove(<span class="number">0</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">接下来再看一下initializeState方法</span><br><span class="line">initializeState方法里面的逻辑大致就是初始化或者获取unionOffsetStates，然后再判断任务是不是重启，是的话就把数据给本地变量。</span><br><span class="line">这里面比较特殊的就是在获取状态时使用的是getUnionListState这个方法，这个方法返回的就是UnionListState这种类型的状态了。这种状态咱们前面分析过，他的底层其实就是ListState、唯一的区别就是任务在故障恢复时状态数据的传输方式不一样。</span><br><span class="line">这里面用到了UnionListState，说明FlinkKafkaConsumer在故障恢复的时候每个子任务都可以获取到之前所有子任务中维护的状态数据，这样做的目的是为了便于重新给每个子任务分配需要消费的topic分区信息。</span><br><span class="line">此时Flink任务在重启的时候，每个FlinkKafkaConsumer子任务都可以获取到待消费的kafka中指定topic的所有分区信息和对应的消费偏移量信息，具体某一个FlinkKafkaConsumer子任务在运行的时候会按照一定的策略选择一个或者多个分区进行消费。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">final</span> void initializeState(<span class="type">FunctionInitializationContext</span> context) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">OperatorStateStore</span> stateStore = context.getOperatorStateStore();</span><br><span class="line">    <span class="comment">//初始化或者获取unionOffsetStates，这里用到了getUnionListState</span></span><br><span class="line">    <span class="keyword">this</span>.unionOffsetStates =</span><br><span class="line">            stateStore.getUnionListState(</span><br><span class="line">                    <span class="keyword">new</span> <span class="type">ListStateDescriptor</span>&lt;&gt;(</span><br><span class="line">                            <span class="type">OFFSETS_STATE_NAME</span>,</span><br><span class="line">                            createStateSerializer(getRuntimeContext().getExecutionConfig())));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">        restoredState = <span class="keyword">new</span> <span class="type">TreeMap</span>&lt;&gt;(<span class="keyword">new</span> <span class="type">KafkaTopicPartition</span>.<span class="type">Comparator</span>());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// populate actual holder for restored state</span></span><br><span class="line"><span class="comment">//从外部存储中获取状态数据，恢复到本地变量中</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">Tuple2</span>&lt;<span class="type">KafkaTopicPartition</span>, <span class="type">Long</span>&gt; kafkaOffset : unionOffsetStates.get()) &#123;</span><br><span class="line">            restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">LOG</span>.info(</span><br><span class="line">                <span class="string">"Consumer subtask &#123;&#125; restored state: &#123;&#125;."</span>,</span><br><span class="line">                getRuntimeContext().getIndexOfThisSubtask(),</span><br><span class="line">                restoredState);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">LOG</span>.info(</span><br><span class="line">                <span class="string">"Consumer subtask &#123;&#125; has no restore state."</span>,</span><br><span class="line">                getRuntimeContext().getIndexOfThisSubtask());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这就是UnionListState的使用。</span><br></pre></td></tr></table></figure><h6 id="BroadcastState的使用"><a href="#BroadcastState的使用" class="headerlink" title="BroadcastState的使用"></a>BroadcastState的使用</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">针对BroadcastState的使用，一个典型的应用案例就是两个流连接的场景。</span><br><span class="line">假设其中一个数据流是 事件数据流，它属于普通的数据流，里面是一些用户行为数据。</span><br><span class="line"></span><br><span class="line">另外一个数据流是 配置数据流，它不是普通的数据流，它是广播数据流，里面是一些映射关系数据。</span><br><span class="line"></span><br><span class="line">需求是使用配置数据流中的映射关系数据去完善事件数据流中的用户行为数据。</span><br><span class="line"></span><br><span class="line">这个需求来源于某直播平台，这个直播平台会在多个国家运营，如果每一个国家都使用一套运营策略，会比较麻烦，运营成本比较高，意义也不是特别大。为了方便运营管理，所以平台内部提出了大区这个概念，可以将一些国家划分到同一个大区里面，同一个大区使用相同的运营策略。</span><br><span class="line">那么对应的就有国家和大区之间的映射关系，这个映射关系不是一成不变的，他会随着平台的发展而发生变化。</span><br><span class="line">在用户行为数据中针对用户的基础数据里面只有用户所属的国家信息，没有包含大区信息，因为国家和大区的关系是可变的，但是在做报表统计的时候，是需要以大区维度进行统计的。</span><br><span class="line">所以针对实时报表这种场景，就需要对用户行为数据中的国家信息进行实时关联转换了。</span><br><span class="line"></span><br><span class="line">为了加深理解，来看一下下面这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202306011848126.png" alt="image-20230601184843285"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这个图里面有两个实时数据流。</span><br><span class="line">上面的事件数据流里面是用户的行为数据。</span><br><span class="line">下面的配置数据流里面是国家和大区之间的最新映射关系。</span><br><span class="line">在程序中需要将配置数据流广播出去，转换为BroadcastState，然后将两份数据流连接到一起，这样在处理事件数据流中的用户行为数据的时候，可以获取到BroadcastState，基于这份状态数据对用户行为数据中的国家信息进行转换。</span><br><span class="line"></span><br><span class="line">下面我们来基于这个需求开发一下对应的代码。</span><br><span class="line"></span><br><span class="line">针对事件数据流在实际工作中基本上是来源于kafka的，在这里为了演示方便，我们来开发一个自定义的Source模拟产生数据。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.state</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">RichSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 事件数据流-自定义Source</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyStreamSource</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>[<span class="type">String</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 初始化方法，只执行一次</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param parameters</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Source的核心方法，负责源源不断的产生数据</span></span><br><span class="line"><span class="comment">   * @param ctx</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">    <span class="keyword">while</span> (isRunning)&#123;</span><br><span class="line">      <span class="comment">//&#123;"dt":"2026-01-01 10:11:11","countryCode":"US","data":[&#123;"type":"s1","score":0.3,"level":"A"&#125;,&#123;"type":"s2","score":0.2,"level":"B"&#125;]&#125;</span></span><br><span class="line">      <span class="keyword">val</span> time = sdf.format(<span class="keyword">new</span> <span class="type">Date</span>)</span><br><span class="line">      <span class="keyword">val</span> line_prefix = <span class="string">"&#123;\"dt\":\""</span>+time+<span class="string">"\",\"countryCode\":\""</span></span><br><span class="line">      <span class="keyword">val</span> line_suffix = <span class="string">"\",\"data\":[&#123;\"type\":\"s1\",\"score\":0.3,\"level\":\"A\"&#125;,&#123;\"type\":\"s2\",\"score\":0.2,\"level\":\"B\"&#125;]&#125;"</span></span><br><span class="line">      <span class="keyword">val</span> countryCodeArr = <span class="type">Array</span>(<span class="string">"US"</span>,<span class="string">"PK"</span>,<span class="string">"KW"</span>)</span><br><span class="line">      <span class="keyword">val</span> num = <span class="type">Random</span>.nextInt(<span class="number">3</span>)</span><br><span class="line">      ctx.collect(line_prefix+countryCodeArr(num)+line_suffix)</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)<span class="comment">//每隔1秒产生一条数据，控制一下数据产生速度</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 任务停止的时候执行一次</span></span><br><span class="line"><span class="comment">   * 这里主要负责控制run方法中的循环</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    isRunning = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   *任务停止的时候执行一次</span></span><br><span class="line"><span class="comment">   * 这里主要负责关闭在open方法中创建的连接</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对配置数据流在实际工作中基本上是来源于Redis或者MySQL，在这里为了演示方便，我们来开发一个自定义的Source模拟从Redis中获取数据。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.state</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">RichSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 配置数据流-自定义Source</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRedisSource</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>[mutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">String</span>]]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 初始化方法，只执行一次</span></span><br><span class="line"><span class="comment">   * @param parameters</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//TODO 创建Redis数据库连接</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Source的核心方法，负责源源不断的产生数据</span></span><br><span class="line"><span class="comment">   * @param ctx</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning)&#123;</span><br><span class="line">      <span class="comment">//TODO 需要从Redis中获取这些映射关系</span></span><br><span class="line">      <span class="keyword">val</span> resMap = mutable.<span class="type">Map</span>(<span class="string">"US"</span>-&gt;<span class="string">"AREA_US"</span>,<span class="string">"PK"</span>-&gt;<span class="string">"AREA_AR"</span>,<span class="string">"KW"</span>-&gt;<span class="string">"AREA_AR"</span>)</span><br><span class="line">      ctx.collect(resMap)</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">5000</span>)<span class="comment">//每隔5秒更新一次配置数据</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 任务停止的时候执行一次</span></span><br><span class="line"><span class="comment">   * 这里主要负责控制run方法中的循环</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    isRunning = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   *任务停止的时候执行一次</span></span><br><span class="line"><span class="comment">   * 这里主要负责关闭在open方法中创建的连接</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//TODO 关闭Redis数据库连接</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.state</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON</span><br><span class="line">import org.apache.flink.api.common.RuntimeExecutionMode</span><br><span class="line">import org.apache.flink.api.common.state.MapStateDescriptor</span><br><span class="line">import org.apache.flink.streaming.api.functions.co.BroadcastProcessFunction</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.util.Collector</span><br><span class="line"></span><br><span class="line">import scala.collection.JavaConverters.mapAsJavaMap</span><br><span class="line">import scala.collection.mutable</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * BroadcastState在两个流连接中的应用（双流Join）</span><br><span class="line"> * 这个场景类似于：一个事实表（事件数据流） left join 一个维度表（配置数据流）</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object OperatorState_BroadcastStateDemo &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    env.setRuntimeMode(RuntimeExecutionMode.AUTOMATIC)</span><br><span class="line"></span><br><span class="line">    import org.apache.flink.api.scala._</span><br><span class="line">    &#x2F;&#x2F;构建第1个数据流：事件数据流</span><br><span class="line">    val eventStream &#x3D; env.addSource(new MyStreamSource)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;构建第2个数据流：配置数据流</span><br><span class="line">    val confStream &#x3D; env.addSource(new MyRedisSource)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;将配置数据流广播出去，变成广播数据流，并且注册一个MapState</span><br><span class="line">    val countryAreaMapStateDescriptor &#x3D; new MapStateDescriptor[String,String](</span><br><span class="line">      &quot;countryArea&quot;,</span><br><span class="line">      classOf[String],</span><br><span class="line">      classOf[String]</span><br><span class="line">    )</span><br><span class="line">    val broadcastStream &#x3D; confStream.broadcast(countryAreaMapStateDescriptor)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;将两个流进行连接</span><br><span class="line">    val broadcastConnectStream &#x3D; eventStream.connect(broadcastStream)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;处理连接后的流</span><br><span class="line">    broadcastConnectStream.process(new BroadcastProcessFunction[String,mutable.Map[String,String],String]&#123;</span><br><span class="line">      &#x2F;&#x2F;处理事件数据流中的数据</span><br><span class="line">      override def processElement(value: String, ctx: BroadcastProcessFunction[String, mutable.Map[String, String], String]#ReadOnlyContext, out: Collector[String]): Unit &#x3D; &#123;</span><br><span class="line">        val jsonObj &#x3D; JSON.parseObject(value)</span><br><span class="line">        val countryCode &#x3D; jsonObj.getString(&quot;countryCode&quot;)</span><br><span class="line">        &#x2F;&#x2F;取出广播状态中的数据</span><br><span class="line">        val broadcastState &#x3D; ctx.getBroadcastState(countryAreaMapStateDescriptor)</span><br><span class="line">        val area &#x3D; broadcastState.get(countryCode)</span><br><span class="line">        &#x2F;&#x2F;任务刚开始执行的时候broadcastState中的数据为空，所以获取不到数据</span><br><span class="line">        if(area!&#x3D;null)&#123;</span><br><span class="line">          jsonObj.put(&quot;countryCode&quot;,area)</span><br><span class="line">          out.collect(jsonObj.toJSONString)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      &#x2F;&#x2F;处理广播后的配置数据流中的数据</span><br><span class="line">      override def processBroadcastElement(value: mutable.Map[String, String], ctx: BroadcastProcessFunction[String, mutable.Map[String, String], String]#Context, out: Collector[String]): Unit &#x3D; &#123;</span><br><span class="line">        &#x2F;&#x2F;获取BroadcastState</span><br><span class="line">        val broadcastState &#x3D; ctx.getBroadcastState(countryAreaMapStateDescriptor)</span><br><span class="line">        &#x2F;&#x2F;清空BroadcastState中的数据</span><br><span class="line">        broadcastState.clear()</span><br><span class="line">        &#x2F;&#x2F;重新写入最新的映射关系数据</span><br><span class="line">        broadcastState.putAll(mapAsJavaMap(value))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;OperatorState_BroadcastStateDemo&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">运行程序，结果发现程序报错，查看错误日志发现提示的是scala的问题：</span><br><span class="line">Exception in thread &quot;main&quot; java.util.concurrent.ExecutionException: scala.tools.reflect.ToolBoxError: reflective compilation has failed: cannot initialize the compiler due to java.lang.NoClassDefFoundError: Could not initialize class scala.tools.nsc.Properties$</span><br><span class="line">at org.apache.flink.shaded.guava30.com.google.common.util.concurrent.AbstractFuture.getDoneValue(AbstractFuture.java:566)</span><br><span class="line">at org.apache.flink.shaded.guava30.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:527)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">经过排查发现这个问题主要是因为我们代码里面用到了mapAsJavaMap这种功能，此时需要引入scala相关的依赖包：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scala-reflect&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">2.12</span><span class="number">.11</span>&lt;/version&gt;</span><br><span class="line">    &lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scala-compiler&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">2.12</span><span class="number">.11</span>&lt;/version&gt;</span><br><span class="line">    &lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">2.12</span><span class="number">.11</span>&lt;/version&gt;</span><br><span class="line">    &lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">重新运行程序，发现可以正常运行，并且输出数据：</span><br><span class="line"></span><br><span class="line">3&gt; &#123;&quot;dt&quot;:&quot;2022-05-29 18:11:07&quot;,&quot;data&quot;:[&#123;&quot;score&quot;:0.3,&quot;level&quot;:&quot;A&quot;,&quot;type&quot;:&quot;s1&quot;&#125;,&#123;&quot;score&quot;:0.2,&quot;level&quot;:&quot;B&quot;,&quot;type&quot;:&quot;s2&quot;&#125;],&quot;countryCode&quot;:&quot;AREA_US&quot;&#125;</span><br><span class="line">4&gt; &#123;&quot;dt&quot;:&quot;2022-05-29 18:11:08&quot;,&quot;data&quot;:[&#123;&quot;score&quot;:0.3,&quot;level&quot;:&quot;A&quot;,&quot;type&quot;:&quot;s1&quot;&#125;,&#123;&quot;score&quot;:0.2,&quot;level&quot;:&quot;B&quot;,&quot;type&quot;:&quot;s2&quot;&#125;],&quot;countryCode&quot;:&quot;AREA_AR&quot;&#125;</span><br><span class="line">5&gt; &#123;&quot;dt&quot;:&quot;2022-05-29 18:11:09&quot;,&quot;data&quot;:[&#123;&quot;score&quot;:0.3,&quot;level&quot;:&quot;A&quot;,&quot;type&quot;:&quot;s1&quot;&#125;,&#123;&quot;score&quot;:0.2,&quot;level&quot;:&quot;B&quot;,&quot;type&quot;:&quot;s2&quot;&#125;],&quot;countryCode&quot;:&quot;AREA_AR&quot;&#125;</span><br><span class="line">6&gt; &#123;&quot;dt&quot;:&quot;2022-05-29 18:11:10&quot;,&quot;data&quot;:[&#123;&quot;score&quot;:0.3,&quot;level&quot;:&quot;A&quot;,&quot;type&quot;:&quot;s1&quot;&#125;,&#123;&quot;score&quot;:0.2,&quot;level&quot;:&quot;B&quot;,&quot;type&quot;:&quot;s2&quot;&#125;],&quot;countryCode&quot;:&quot;AREA_AR&quot;&#125;</span><br><span class="line">7&gt; &#123;&quot;dt&quot;:&quot;2022-05-29 18:11:11&quot;,&quot;data&quot;:[&#123;&quot;score&quot;:0.3,&quot;level&quot;:&quot;A&quot;,&quot;type&quot;:&quot;s1&quot;&#125;,&#123;&quot;score&quot;:0.2,&quot;level&quot;:&quot;B&quot;,&quot;type&quot;:&quot;s2&quot;&#125;],&quot;countryCode&quot;:&quot;AREA_AR&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这就是BroadcastState的常用应用场景。</span><br><span class="line"></span><br><span class="line">注意：针对这个需求，因为没有对数据流进行keyBy分组，如果不对配置数据流进行广播，那么在处理事件数据流中的数据的时候，每一个子任务无法获取到配置数据流中的所有映射关系，所以只能通过全量广播，这样才能让事件数据流的每一个子任务都可以获取到所有的配置数据流中的数据，最终实现数据关联转换。</span><br></pre></td></tr></table></figure><h5 id="Operator-State的使用形式总结"><a href="#Operator-State的使用形式总结" class="headerlink" title="Operator State的使用形式总结"></a>Operator State的使用形式总结</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在程序中想要使用Operator State，主要通过实现CheckpointedFunction这个接口，然后实现接口中的initializeState和snapshotState函数。</span><br><span class="line"></span><br><span class="line">可以开发一些自定义的类去继承SourceFunction、SinkFunction或者MapFunction，同时实现CheckpointedFunction这个接口，这样也可以在里面使用状态了。</span><br><span class="line"></span><br><span class="line">注意：CheckpointedFunction这种方式，既可以操作Operator State，也可以操作Keyed State，在代码中同时操作这两种类型的State也是可以的。</span><br><span class="line"></span><br><span class="line">因为在initializeState中，通过context既可以获取KeyedState，又可以获取OperatorState。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">context.getKeyedStateStore</span><br><span class="line">context.getOperatorStateStore</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-2.html</id>
    <published>2023-04-20T08:46:43.000Z</published>
    <updated>2023-06-01T04:49:13.801Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十七周-Flink极速上手篇-Flink高级进阶之路-2"><a href="#第十七周-Flink极速上手篇-Flink高级进阶之路-2" class="headerlink" title="第十七周 Flink极速上手篇-Flink高级进阶之路-2"></a>第十七周 Flink极速上手篇-Flink高级进阶之路-2</h1><h2 id="Kafka-Connector"><a href="#Kafka-Connector" class="headerlink" title="Kafka-Connector"></a>Kafka-Connector</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们来看一下flink中针对kafka connect的专题，提供了很多的connect组件，其中应用比较广泛的就是kafka这个connect。我们就针对kafka在flink的应用做详细的分析。针对flink流处里啊，最常用的组件就是kafka。原始日志数据产生后，会被日志采集工具采集到kafka中，让flink去处理。处理之后的数据可能也会继续写入到kafka中。kafka可以作为flink的datasource和datasink来使用。并且kafka中的partition机制和flink的并行度机制可以深度结合，提高数据的读取效率和写入效率。那我们想要在flink中使用kafka，需要添加对应的依赖。(先在flink官网中找到依赖的名字，再到maven中去找符合的版本)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-connector-kafka_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="Kafka-Consumer的使用"><a href="#Kafka-Consumer的使用" class="headerlink" title="Kafka Consumer的使用"></a>Kafka Consumer的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那在具体执行这个代码之前啊，我们先需要把那个zookeeper集群，还有kafka集群给他起来。我这些相关的服务呢，已经起来了。这是入K班了，这是卡不卡都已经起来啊。好，那下面注意，我们还需要做一件事情。因为我们在这里面呢，用到了一个T1这个topic，所以说在这我们需要去创建这个topic。找一下之前的命令。嗯。其一。这个分区设置为五，后面因子是为二。发现一个T。好，创建成功。那下面呢，我们就可以去启动代码。启动电板之后，那我们需要往那个卡夫卡里面模拟产生数据。这个时候呢，我们可以启动一个基于控制台的一个生产者来模拟产生数据。嗯。使用这个卡不卡console producer。把这个复制一下。好，这个套背上就是T1。那这个时候呢，我们接着就来模拟产生数据。hello。看到没有消费到。再加一个。hello。没问题吧，是可以的，这样的话我们就可以消费卡夫卡中的数据了。好，这个是代码实践，接下来我们使用这个Java代码来实现一下。先创建一个package。嗯。stream。搞不搞？SS。把这个复制过来。嗯。嗯嗯。嗯。好，首先呢，还是获取一个连环应。execution。因为第2GET。因为。下面的env.S。嗯。在这儿，我们需要去利用这个。Li。卡不卡？three。那这里面啊，传一个topic。嗯。第一，嗯。第二个，你有一个simple。视频，game。第三个pop。嗯嗯。有一个薄。嗯。嗯嗯嗯。首先呢，我们在里面set property。我可以把这个呢直接拿过来。嗯。好，下面呢，said。格布利。卡不卡星本。把它拿过来。嗯。感注释，这个就是指定卡夫卡作为S。嗯。这是指令。普林格卡夫卡consumer的相关配置。接下来呢，我们将读取到的数据啊，一到控制台。嗯。嗯。嗯嗯。嗯。嗯。包的异常。嗯。来把这个启动起来。好，那我们在这边呢，再模拟产生的数据。哈哈哈。没问题吧，是可以的。好，这就是Java代码的一个实现。</span><br></pre></td></tr></table></figure><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">      env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>() prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在运行代码之前，需要先启动zookeeper集群和kafka集群</span><br><span class="line">在kafka中创建topic：t1</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic t1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">然后启动代码</span><br><span class="line">再启动一个Kafka 的 console生产者模拟产生数据，验证效果。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304231050885.png" alt="image-20230423105004127"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.kafkaconnector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaSourceJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">        String topic = <span class="string">"t1"</span>;</span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">        prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), prop);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafka作为source</span></span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.addSource(kafkaConsumer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将读取到的数据打印到控制台</span></span><br><span class="line">        text.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"StreamKafkaSourceJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="KafkaConsumer消费策略设置"><a href="#KafkaConsumer消费策略设置" class="headerlink" title="KafkaConsumer消费策略设置"></a>KafkaConsumer消费策略设置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对卡夫卡康消费数据的时候会有一些策略，我们来看一下。首先这个是默认的消费策略。下面还有一个ear，从最早的开始消费，latest，从最新的开始消费。已经呢，现在要出来一个C呢。按照指定的时间戳往后面开始消费。下面呢，我们来演示一下。我直接在这里面来设置一下。嗯。卡夫卡consumer。的消费策略设置。这个其实我们在讲卡不卡的时候也详细分解过啊，其实是类似的。首先我们看下这个默认策略。嗯。直接使用它来设置点带。start from group of，注意这个呢，其实就默认你不设置它默就类。它是什么意思呢？它会读取。group ID。对应。保存的outside。开始消费数据。那读取不到的话呢。则根据卡夫卡中。这个参数auto点。reite。参数的值开始。消费数据。因为如果说你是第一次使用这个消费者，那么他之前肯定是没有保存这个对应的office的信息，那这样的话呢，他就会根据这个参数的值来开始进行消费。那这个值的话，它那要么是early latest对吧，要么是从最新的，要么是从最近的。那下一次的话呢，他就会根据你之前指定的这个global ID对应的保存的那个开始往下面继续消费数据。那既然下面这个呢，是从那个最早的记录开始，消费主义啊，不搞consumer there that。from earliest。从最早的记录。开始消费。独具。忽略。你提交。信息。这样的话，他就不管你有没有提交，都会每次都从那个最早的数据开始消费。那对了，还有一个从。最新的记录开始消费。也是忽略这个已提交的奥赛的信息。嗯。start。home latest。嗯。那还有一个是从指定的时间戳开始消费数据。对于每个分区。其时间戳大于或等于指定时间戳的记录。江北。作为70位。嗯。that。大的from。这里面你给他传一个时间戳就行了啊，我这边随便写一个行吗？这就是这几种测量啊。其实我们在讲卡不卡的时候，也详细分析过这几种词，那在那就把这个默认的给它打开吧。就你这个呢，你在这儿设置不设置，它其实都是一个默认的策略。这个呢，就是针对这里面这个卡夫卡抗性板，它这个消费策略的一个设置。其实咱们在工作中啊，一般最常见的，那其实就是一种默认的技术。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304231057297.png" alt="image-20230423105719815"></p><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="KafkaConsumer的容错"><a href="#KafkaConsumer的容错" class="headerlink" title="KafkaConsumer的容错"></a>KafkaConsumer的容错</h3><h4 id="Flink-Checkpoint"><a href="#Flink-Checkpoint" class="headerlink" title="Flink Checkpoint"></a>Flink Checkpoint</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下Flink中也有checkpoint机制，Checkpoint是Flink实现容错机制的核心功能，它能够根据配置周期性地基于流中各个算子任务的State来生成快照，从而将这些State数据定期持久化存储下来，当Flink程序一旦意外崩溃时，重新运行程序时可以有选择地从这些快照进行恢复，从而修正因为故障带来的程序数据异常。</span><br><span class="line"></span><br><span class="line">当CheckPoint机制开启的时候，Consumer会定期把Kafka的offset信息还有其它算子任务的State信息一块保存起来</span><br><span class="line">当Job失败重启的时候，Flink会从最近一次的CheckPoint中进行恢复数据，重新消费Kafka中的数据</span><br><span class="line"></span><br><span class="line">为了能够使用支持容错的Consumer，需要开启checkpoint</span><br><span class="line">那如何开启呢？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;每隔5000 ms执行一次Checkpoint(设置Checkpoint的周期)</span><br><span class="line">env.enableCheckpointing(5000)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那针对这个呢，它还有一些相关的配置。那我接着呢，把这个配置拿过来。把这个复制一下。搞一下包。这个是。针对checkpoint的相关配置。下面这个参数的意思呢？表示设置一下checkpoint的一个语义，它可以提供这种锦一词的语义。下面这个呢，表示两次切之间它的一个时间间隔。这个呢，表示呢，必须要在指定时间之内完成one。其实就是给这个check呢，设置一个超时时间，超过这个时间了就被丢弃了。下面这个呢，表示呢，同一时间只允许执行一个checkpoint。下面这个三注意。他呢表示呀，当我们对这个link程序执行一个cancel之后，就是把这个link程序停掉之后，我们呢，会保留这个这个波段数据，这样的话，我们可以根据实际需要，后期呢来恢复这些数据。这是它相关的一些配置啊，</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span><br><span class="line">env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)</span><br><span class="line">&#x2F;&#x2F;确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span><br><span class="line">env.getCheckpointConfig.setMinPauseBetweenCheckpoints(500)</span><br><span class="line">&#x2F;&#x2F;Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span><br><span class="line">env.getCheckpointConfig.setCheckpointTimeout(60000)</span><br><span class="line">&#x2F;&#x2F;同一时间只允许执行一个Checkpoint</span><br><span class="line">env.getCheckpointConfig.setMaxConcurrentCheckpoints(1)</span><br><span class="line">&#x2F;&#x2F;表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span><br><span class="line">env.getCheckpointConfig.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">env.enableCheckpointing(5000)用于设置checkpoint的时间间隔，即每5000毫秒触发一次checkpoint。而env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500)用于设置两次checkpoint之间的最短时间间隔，即两次checkpoint之间至少要间隔500毫秒。</span><br><span class="line"></span><br><span class="line">这意味着，如果某一次checkpoint花费了超过4500毫秒的时间，那么下一次checkpoint将不会立即开始，而是会等待至少500毫秒后才开始。</span><br></pre></td></tr></table></figure><h4 id="State数据存储的位置"><a href="#State数据存储的位置" class="headerlink" title="State数据存储的位置"></a>State数据存储的位置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">最后还有一个配置，设置State数据存储的位置</span><br><span class="line">默认情况下，State数据会保存在TaskManager的内存中，Checkpoint执行时，会将State数据存储在JobManager的内存中。</span><br><span class="line"></span><br><span class="line">具体的存储位置取决于State Backend的配置，Flink一共提供了3种存储方式</span><br><span class="line"></span><br><span class="line">MemoryStateBackend</span><br><span class="line">State数据保存在Java堆内存中，执行Checkpoint的时候，会把State的快照数据保存到JobManager的内存中，基于内存的State Backend在生产环境下不建议使用。</span><br><span class="line"></span><br><span class="line">FsStateBackend</span><br><span class="line">State数据保存在TaskManager的内存中，执行Checkpoint的时候，会把State的快照数据保存到配置的文件系统中，可以使用HDFS等分布式文件系统。</span><br><span class="line"></span><br><span class="line">RocksDBStateBackend</span><br><span class="line">RocksDB跟上面的都略有不同，它会在本地文件系统中维护State，State会直接写入本地RocksDB中。同时它需要配置一个远端的文件系统（一般是HDFS），在做Checkpoint的时候，会把本地的数据直接复制到远端的文件系统中。故障切换的时候直接从远端的文件系统中恢复数据到本地。RocksDB克服了State受内存限制的缺点，同时又能够持久化到远端文件系统中，推荐在生产环境中使用。</span><br></pre></td></tr></table></figure><h5 id="RocksDBStateBackend"><a href="#RocksDBStateBackend" class="headerlink" title="RocksDBStateBackend"></a>RocksDBStateBackend</h5><h6 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">所以在这里我们使用第三种：RocksDBStateBackend</span><br><span class="line">针对RocksDBStateBackend需要引入依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-statebackend-rocksdb_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;设置状态数据存储的位置</span><br><span class="line">env.setStateBackend(new RocksDBStateBackend(&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flink&#x2F;checkpoints&quot;,true))</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">      env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106563.png" alt="image-20230423210612299"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106374.png" alt="image-20230423210628262"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106556.png" alt="image-20230423210654942"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232107250.png" alt="image-20230423210749485"></p><h4 id="Kafka-Consumers-Offset自动提交"><a href="#Kafka-Consumers-Offset自动提交" class="headerlink" title="Kafka Consumers Offset自动提交"></a>Kafka Consumers Offset自动提交</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Kafka Consumers Offset自动提交机制需要根据Job是否开启Checkpoint来区分。</span><br><span class="line">CheckPoint关闭时：通过参数enable.auto.commit和auto.commit.interval.ms控制</span><br><span class="line">CheckPoint开启时：执行CheckPoint的时候才会提交offset，此时kafka中的自动提交机制就会被忽略</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232113418.png" alt="image-20230423211307520"></p><h3 id="KafkaProducer的使用"><a href="#KafkaProducer的使用" class="headerlink" title="KafkaProducer的使用"></a>KafkaProducer的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下在flink中如何向kafka中写数据，此时需要用到kafka producer。</span><br><span class="line"></span><br><span class="line">所有的数据都写入指定topic的一个分区里面。注意，他会把所有数据写到这个topic的一个分区。那这样的话，其实呢，在我们实习当中，这样是不合适的啊。我们使用操作的肯定是要使用多个分区，你要把数据分别写到不同的分区里面，这样的话后期我们去消费也可以并行消费，提高消费能力，对吧？那你如果都搞一个分区里面，那其实相当于我这个topic卡就一个分区。这样后期我这个处理能力是有限制的，如果不想自定义分区器，也不想使用默认的，可以直接使用null即可</span><br></pre></td></tr></table></figure><h4 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaProducer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.internals.<span class="type">KafkaSerializationSchemaWrapper</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.partitioner.<span class="type">FlinkFixedPartitioner</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink向Kafka中生产数据 </span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSinkScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启checkpoint</span></span><br><span class="line">    <span class="comment">//env.enableCheckpointing(5000)</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaProducer的相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t2"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为sink</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     KafkaSerializationSchemaWrapper的几个参数</span></span><br><span class="line"><span class="comment">     1：topic：指定需要写入的topic名称即可</span></span><br><span class="line"><span class="comment">     2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中</span></span><br><span class="line"><span class="comment">     默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面</span></span><br><span class="line"><span class="comment">     如果不想自定义分区器，也不想使用默认的，可以直接使用null即可</span></span><br><span class="line"><span class="comment">     3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳</span></span><br><span class="line"><span class="comment">     如果写入了，那么在watermark的案例中，使用extractTimestamp()提起时间戳的时候</span></span><br><span class="line"><span class="comment">     就可以直接使用previousElementTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> kafkaProducer = <span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">KafkaSerializationSchemaWrapper</span>[<span class="type">String</span>](topic, <span class="literal">null</span>, <span class="literal">false</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()), prop, <span class="type">FlinkKafkaProducer</span>.<span class="type">Semantic</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    text.addSink(kafkaProducer)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSinkScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">所以，如果我们不需要自定义分区器的时候，直接传递为null即可，不要使用FlinkFixedPartitioner，它会将数据都写入到topic的一个分区中。</span><br><span class="line"></span><br><span class="line">将FlinkFixedPartitioner设置为null</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232135912.png" alt="image-20230423213547295"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232137221.png" alt="image-20230423213734278"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232138202.png" alt="image-20230423213834818"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmak里查看</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232144712.png" alt="image-20230423214449177"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果指定的默认FlinkFixedPartitioner</span><br><span class="line"></span><br><span class="line">new FlinkFixedPartitioner[String]()</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202305311234314.png" alt="image-20230531123033483"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.kafkaconnector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink向Kafka中生产数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaSinkJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定FlinkKafkaProducer相关配置</span></span><br><span class="line">        String topic = <span class="string">"t2"</span>;</span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafak作为sink</span></span><br><span class="line">        FlinkKafkaProducer&lt;String&gt; kafkaProducer = <span class="keyword">new</span> FlinkKafkaProducer&lt;&gt;(topic, <span class="keyword">new</span> KafkaSerializationSchemaWrapper&lt;String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">new</span> SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE);</span><br><span class="line">        text.addSink(kafkaProducer);</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"StreamKafkaSinkJava"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="KafkaProducer的容错"><a href="#KafkaProducer的容错" class="headerlink" title="KafkaProducer的容错"></a>KafkaProducer的容错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如果Flink开启了CheckPoint，针对FlinkKafkaProducer可以提供EXACTLY_ONCE的语义保证</span><br><span class="line"></span><br><span class="line">可以通过semantic参数来选择三种不同的语义：</span><br><span class="line">Semantic.NONE、Semantic.AT_LEAST_ONCE【默认】、Semantic.EXACTLY_ONCE</span><br><span class="line"></span><br><span class="line">来看一下。注意看到没有，我们刚才指的就是一个锦一四啊。但是这个时候注意你还要开启这个table。开启这个了。下面那些参数我暂时就先不指定了，行吗？好，那下面呢，我们来执行一下。来开启这个稍的。好注意这时候呢，给大家看一个比较神奇的现象。你看刚才我们把这个搜下打开啊，结果它停了，你再把它打开。他还会请。看没有？什么原因呢？那时候我这块也没有报错呀。注意这个呢，是因为这个原因。我们之前啊，在这加了一个logo的配置文件，对吧。注意这个日级别，我之前给它改成error。我们把这个调一下。调成那个警告级别。因为这个时候有一些日他没有打出来警告信息看不到啊。来再启动把这个打开。对，他这个其实应该是error级别的，但是他写的什么写的不太好，他把这个日志写成那种warning级别，警告级别的，所以说呢，我们之前使用那个error级别的，监控不到这些日志信息啊。啊，停一下吧。</span><br><span class="line"></span><br><span class="line">来分析一下啊。不要往后面看这。还有什么呀，这个事物时间比这个博客里面配置的这个时间还要大。就是说，生产者中设置的事物超时时间大于卡夫卡博客中设置的事物超时时间。因为卡夫卡服务中默认事物的超时时间是15分钟，但是呢，弗林格卡夫卡保留它里面设置的事物超时间默认是一小时，这个仅一次语义啊，它需要依赖这个事物。如果从Li应用程序崩溃到完全重启的时间超过了卡夫卡的事物超时时间，那么将会有数据丢失，所以我们需要合理的配置事物超时时间。因此，在使用这个仅一次语义之前，建议增加卡夫卡博克中这个transaction.max.timeout.ms的值。把这个值啊给它调大。那下面呢，我们就来修改一下卡夫卡里面这个配置，这个配置在哪啊，其实就那个server.properties里面啊。买那个可以试一下。它里面是没有这个参数的，你直接在这把它拿过来。给它做个值。我们也给它改成一小时吧。这个你转换成毫秒是3600000。那么是五个零啊，这样的话就一小时。把这个复制一下。对，这个集群里面所有机器都要改啊。嗯。好，可以了，注意改完之后我们需要重写。那你先把这个卡夫卡集群停掉。好停掉之后再去启动，启动的话，我们使用它这个命令啊。前面加了一个GMX，这样的话我们可以使那个CMA来减轻它里面一些信息啊。嗯。好，这个起来了。嗯。嗯。这个呢也可以啊。好，这个也可以了。那接下来我们重新再执行这个样本，对吧，把这个再看一下。嗯。看到没有，此时他就不报错了啊，我们可以在这来验证一下，先确一下里面的数据对吧。是这样。5211。这个停了，因为刚才我们把那个卡夫卡停掉之后啊，这个c map哎，就停掉了。把它起来。所以这个没不对。嗯。嗯。第三。对吧，这里面是这了来。我们输点作业。好变了吧，对吧。说明这个数据写进来了，并且这块呢也没报错啊。OK，这样就可以了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232152824.png" alt="image-20230423215255115"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaProducer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.internals.<span class="type">KafkaSerializationSchemaWrapper</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.partitioner.<span class="type">FlinkFixedPartitioner</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink向Kafka中生产数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSinkScala</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//开启checkpoint</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaProducer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t3"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为sink</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    KafkaSerializationSchemaWrapper的几个参数</span></span><br><span class="line"><span class="comment">    1：topic，指定需要写入的topic名称即可</span></span><br><span class="line"><span class="comment">    2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中，</span></span><br><span class="line"><span class="comment">    默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面</span></span><br><span class="line"><span class="comment">    如果不想自定义分区器，也不想使用默认的，可以直接使用null即可</span></span><br><span class="line"><span class="comment">    3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳，</span></span><br><span class="line"><span class="comment">    如果写入了，那么在watermark的常见中，使用extractTimestamp()提取时间戳的时候，</span></span><br><span class="line"><span class="comment">    就可以直接使用previousElementTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp</span></span><br><span class="line"><span class="comment">    4：SerializationSchema，数据解析规则，默认使用string类型的数据解析规则即可</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> kafkaProducer = <span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">KafkaSerializationSchemaWrapper</span>[<span class="type">String</span>](topic, <span class="literal">null</span>,<span class="literal">false</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()), prop, <span class="type">FlinkKafkaProducer</span>.<span class="type">Semantic</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    text.addSink(kafkaProducer)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSinkScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">注意：此时执行代码会发现无法正常执行，socket打开之后，启动代码，会发现socket监听会自动断开，表示代码执行断开了</span><br><span class="line"></span><br><span class="line">log4j.rootLogger&#x3D;warn,stdout</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout &#x3D; org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Target &#x3D; System.out</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br><span class="line"></span><br><span class="line">但是此时在idea中看不到任何报错信息，主要是因为我们之前把日志级别改为error级别了，把日志级别调整为warn之后就可以看到报错信息了</span><br><span class="line"></span><br><span class="line">2020-08-12 19:21:59,759 [Sink: Unnamed (3&#x2F;8)] [org.apache.flink.runtime.taskmanager.Task] [WARN] - Sink: Unnamed (3&#x2F;8) (1b621d88e460877995ad37d34379c166) switched from RUNNING to FAILED.</span><br><span class="line">org.apache.kafka.common.KafkaException: Unexpected error in InitProducerIdResponse; The transaction timeout is larger than the maximum value allowed by the broker (as configured by transaction.max.timeout.ms).</span><br><span class="line">at org.apache.kafka.clients.producer.internals.TransactionManager$InitProducerIdHandler.handleResponse(TransactionManager.java:1151)</span><br><span class="line">at org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler.onComplete(TransactionManager.java:1074)</span><br><span class="line">at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)</span><br><span class="line">at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:569)</span><br><span class="line">at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)</span><br><span class="line">at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:425)</span><br><span class="line">at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:311)</span><br><span class="line">at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:244)</span><br><span class="line">at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">提示生产者中设置的事务超时时间大于broker中设置的事务超时时间。</span><br><span class="line"></span><br><span class="line">因为Kafka服务中默认事务的超时时间是15min，但是FlinkKafkaProducer里面设置的事务超时时间默认是1h。EXACTLY_ONCE模式依赖于事务，如果从Flink应用程序崩溃到完全重启的时间超过了Kafka的事务超时时间，那么将会有数据丢失，所以我们需要合理地配置事务超时时间，因此在使用EXACTLY_ONCE模式之前建议增加Kafka broker中transaction.max.timeout.ms 的值。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">下面我们需要修改kafka中的server.properties配置文件</span><br><span class="line">bigdata01、bigdata02、bigdata03都需要修改</span><br><span class="line"></span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# vi config&#x2F;server.properties</span><br><span class="line">...</span><br><span class="line">transaction.max.timeout.ms&#x3D;3600000</span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# vi config&#x2F;server.properties</span><br><span class="line">...</span><br><span class="line">transaction.max.timeout.ms&#x3D;3600000</span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# vi config&#x2F;server.properties</span><br><span class="line">...</span><br><span class="line">transaction.max.timeout.ms&#x3D;3600000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">改完配置文件之后，重启kafka</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties </span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties </span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties </span><br><span class="line"></span><br><span class="line">重新执行Flink代码，此时就不报错了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-1.html</id>
    <published>2023-04-20T08:44:51.000Z</published>
    <updated>2023-05-31T03:07:28.547Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十七周-Flink极速上手篇-Flink高级进阶之路-1"><a href="#第十七周-Flink极速上手篇-Flink高级进阶之路-1" class="headerlink" title="第十七周 Flink极速上手篇-Flink高级进阶之路-1"></a>第十七周 Flink极速上手篇-Flink高级进阶之路-1</h1><h2 id="Window的概念和类型"><a href="#Window的概念和类型" class="headerlink" title="Window的概念和类型"></a>Window的概念和类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">大家好，前面我们学习了flink中的基本概念，集群部署以及核心API的使用，下面我们来学习一下flink中的高级特性的使用。首先，我们需要掌握中的window、time以及whatermark使用。然后我们需要掌握kafka-connector使用，这个是针对kafka一个专题。最后我们会学习一下Spark中的流式计算sparkStreaming，之前在学习spark的时候我们没有涉及这块，在这儿我们和flink一块来学习，可以加深理解，因为它们都是流式计算引擎。</span><br><span class="line"></span><br><span class="line">下面呢，我们首先进入第一块flink中的window和time。flink认为批处理是流处理的一个特例，所以flink底层引擎是一个流式引擎，这上面呢实现了流处理和批处理。而window呢，就是从流处理到批处理的一个桥梁。通常来讲啊，这个window啊，是一种可以把无界数据切割为有界数据块的手段，例如对流动的所有元素进行计数是不可能的，因为通常流是无限的。或者呢，可以称之为是无界了。所以说流上的聚合需要由window来划分范围，比如计算过去五分钟或者最后100个元素的和。</span><br><span class="line"></span><br><span class="line">window可以是以时间驱动的time window，例如每30秒，或者是以数据驱动的count window，例如每100个元素。DataStream API提供了基于time和count的window。同时，由于某些特殊的需要，dataStreamAPI也提供了定制化的window操作，供用户自定义window。</span><br><span class="line"></span><br><span class="line">这个window呀，根据类型可以分为这两种。第一种是滚动窗口，它呢表示窗口内的数据没有重叠，第二种呢是滑动窗口，它呢表示窗口内的数据有重叠。</span><br><span class="line"></span><br><span class="line">那下面我们来看个图分析一下，首先看这个滚动窗口，这个S轴呢是一个时间轴，你看这个是一个窗口的大小，这是WINDOW1 window2 window3，注意每个窗口内的数据是没有重叠的，这个就是滚动窗口。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201704808.png" alt="image-20230420170417289"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面来看这个滑动窗口，这个S轴呢，还是一个时间轴，你看这个是一个window的大小。这个表示是一个window的滑动间隔，这是WINDOW1这个红色的，它这个窗口从这到这儿，下面这个呢，WINDOW2，注意这个窗口它是从这儿到这儿，这个蓝色的看到没有，它里面呢，包含了WINDOW1里面的一部分数据。那你看WINDOW3 window3里面它包含了WINDOW2里面的一部分数据，所以说这个滑动窗口，它们每个窗口之间呀，会有数据重叠，这个就这两种窗口它的一个区别</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201705935.png" alt="image-20230420170515739"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我针对这个窗口的类型做了一个汇总。你看这是window window下面有time window有count window还有自定义window，那这些window再往下面你看它呢，可以实现滚动窗口或者滑动窗口，对吧？不管你是基于time的，还是基于count的，还是自定义的，你们都可以实现滚动窗口或者是滑动窗口。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201707926.png" alt="image-20230420170745169"></p><h3 id="TimeWindow的使用"><a href="#TimeWindow的使用" class="headerlink" title="TimeWindow的使用"></a>TimeWindow的使用</h3><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这些window的具体应用，首先来看第一个time window。time window呢是根据时间对数据流切分窗口，time window可以支持滚动窗口和滑动窗口。</span><br><span class="line"></span><br><span class="line">其中它有这么两种用法，来看一下time window。</span><br><span class="line">timeWindow(Time.seconds(10))</span><br><span class="line">注意，首先这个。他呢是表示。滚动窗口的窗口大小为十秒。对每十秒内的数据,进行聚合计算。这个呢，其实就是设置一个滚动窗口。</span><br><span class="line"></span><br><span class="line">timeWindow(Time.seconds(10),Time.seconds(5))</span><br><span class="line">那下面这个呢，对应的它设置的就是一个滑动窗口，因为它除了有一个窗口大小，它还滑动一个间隔。表示滑动窗口的窗口大小为十秒,滑动间隔为五秒,就是每隔五秒计算前十秒内的数据，所以说是两种用法，一种是滚动，一种是滑动。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TimeWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TimeWindowOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TimeWindow之滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    <span class="comment">/*text.flatMap(_.split(" "))</span></span><br><span class="line"><span class="comment">      .map((_,1))</span></span><br><span class="line"><span class="comment">      .keyBy(0)</span></span><br><span class="line"><span class="comment">      //窗口大小</span></span><br><span class="line"><span class="comment">      .timeWindow(Time.seconds(10))</span></span><br><span class="line"><span class="comment">      .sum(1)</span></span><br><span class="line"><span class="comment">      .print()*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TimeWindow之滑动窗口：每隔5秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">        .timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>),<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print()</span><br><span class="line">    env.execute(<span class="string">"TimeWindowOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeWindow滚动窗口</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201729458.png" alt="image-20230420172901278"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201729476.png" alt="image-20230420172914398"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeWindow滑动窗口，黑色第一次输入，蓝色第二次输入</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201732506.png" alt="image-20230420173241489"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第三次打印，蓝色</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201736543.png" alt="image-20230420173625064"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TimeWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeWindowOpJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//TimeWindow之滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        <span class="comment">/*text.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">            @Override</span></span><br><span class="line"><span class="comment">            public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span><br><span class="line"><span class="comment">                    throws Exception &#123;</span></span><br><span class="line"><span class="comment">                String[] words = line.split(" ");</span></span><br><span class="line"><span class="comment">                for (String word: words) &#123;</span></span><br><span class="line"><span class="comment">                    out.collect(new Tuple2&lt;String, Integer&gt;(word,1));</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;).keyBy(0)</span></span><br><span class="line"><span class="comment">                //窗口大小</span></span><br><span class="line"><span class="comment">                .timeWindow(Time.seconds(10))</span></span><br><span class="line"><span class="comment">                .sum(1)</span></span><br><span class="line"><span class="comment">                .print();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//TimeWindow之滑动窗口：每隔5秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word: words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">10</span>),Time.seconds(<span class="number">5</span>))</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"TimeWindowOpJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CountWindow的使用"><a href="#CountWindow的使用" class="headerlink" title="CountWindow的使用"></a>CountWindow的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下count的Window的使用，count Window是根据元素个数对数据流切分窗口。count window也可以支持滚动窗口和滑动窗口。</span><br><span class="line">countWindow(5)表示滚动窗口的大小,是五个元素。也就是当窗口中填满五个元素的时候，就会对窗口进行计算</span><br><span class="line">countWindow(5,1)</span><br><span class="line">表示滑动窗口的窗口大小是五个元素，滑动的间隔为一个元素，也就是说每新增一个元素就会对前面五个元素计算一次</span><br><span class="line"></span><br><span class="line">那我们再验证一下。有有有。写四个啊。看到没有，这个又要直行了。对吧，你后面再加哈，注意。此时呢，它就不会再执行了，因为你是一个滚动窗口啊，最终呢，你再满足有五个元素之后，它才会重新执行，这是一个滚动窗口。把这个听一下。把这个注意事项啊，我们给它加进来，这是一个解释啊。由于我们在这里使用了可以。会相对数据分组。如果某个分组对应的数据窗口，数据窗口内达到了五个元素，这个窗口才会被主发执行，如果你不使用KPI的话，他就不会在这儿做区分了，所以他接收到所有的数据，在这儿会统一计算。不过那个时候你就需要使用这个count window or这个咱们后面再分析啊，接着我们先使用这个K方式，后面呢直接使用这个count window，好，这是一个滚动窗口，下面呢，我们来实现一个滑动窗口。它的豌豆之滑动窗口。每隔一个元素计算一次前五个元素。map。空格切一下。小点一。零零，它的window，注意第一个参数是窗口大小，第二个是滑动间隔。嗯。窗口大小。第二个参数。滑动间隔。一。BA。好，那接着要把上面这个的读调，嗯。把这个socket呢，再给它打开。嗯。好，那我们到这儿来数数句，hello you。注意它直行了，为什么呀，因为它的滑动间隔是一，只要间隔一个元素，它就会执行，它呢会往前推找五个元素，但是它前面并没有五个元素，就只有这一个，所以说最终的结果呢，就是这样好。那下面呢，我继续往里面添加元素。hello，你。看那个效果，看到没有，hello就两次了，me是一次对吧，hello已经变成两次了，那下面我们还按照刚才这个逻辑。hello，加三次，你看加三次，它其实最终呢，输出了三条如玉，这次是三，这次是四，这次是五。没问题吧，因为你新增一条数据，它就会往前推五条数据去统计。嗯。看到没有2345。这也是可以的啊，然后再加个什么，hello。还是50。you。为什么一直是五次呢？因为它只会往前面统计五个元素啊。好，这就滑动窗口，下面我们来使用Java代码来实现一下。放着window。op加。嗯。嗯嗯。先获取一个环境。get。嗯。嗯。嗯。看window。直滚动方口。每隔五个元素计算一次。前五个元素。加个小碟red map，你有一个red map。注意我们在这呢，还把这个map和map它这个逻辑整合一块，说输入是词频输出是。in。嗯。来。慢点，split。不了。我。在这个。嗯。WORD1。对吧，这样看一下后面一个a。零。嗯。放了window。五。嗯嗯。some。嗯。这个是窗口大小。好，接下来讲第二个把这个注释呢，从这复制一下吧。所以这是每格啊。嗯。好，这个前面啊，其实都一样啊，只有一个地方不一样，对吧。就是把这个放到温度这块，给它改一下就行。和两个参数，嗯。第一个参数窗口大小，第二个参数。滑动间隔。嗯嗯。因为一点。嗯。顺便抛个异常。好，这就可以了，在这我们可以助调一个。验证一下这个滑动窗口。赶紧回来把这个打开。OK。好。没问题吧，没问题啊。这就是Java代码，实现这个count window。</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CountWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CountWindowOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意：由于我们在这里使用keyBy，会先对数据分组</span></span><br><span class="line"><span class="comment">     * 如果某个分组对应的数据窗口内达到了5个元素，这个窗口才会被触发执行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//CountWindow之滚动窗口：每隔5个元素计算一次前5个元素</span></span><br><span class="line">    <span class="comment">/*text.flatMap(_.split(" "))</span></span><br><span class="line"><span class="comment">      .map((_,1))</span></span><br><span class="line"><span class="comment">      .keyBy(0)</span></span><br><span class="line"><span class="comment">      //指定窗口大小</span></span><br><span class="line"><span class="comment">      .countWindow(5)</span></span><br><span class="line"><span class="comment">      .sum(1)</span></span><br><span class="line"><span class="comment">      .print()*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//CountWindow之滑动窗口：每隔1个元素计算一次前5个元素</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">        .countWindow(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print()</span><br><span class="line">    env.execute(<span class="string">"CountWindowOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">滚动窗口</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201756278.png" alt="image-20230420175629114"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201756533.png" alt="image-20230420175643559"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以我在这再输三个，看到没有，到这儿才刚开始执行了一次，他把这个hello打印出来五个。那这个you和me为什么没有打印呢？注意了，所以啊，我们在这啊执行了keyby会对这个数据进行分组，如果某个分组对应的数据窗口内达到了五个元素，这个窗口才会被处罚执行，所以说这个时候相当于是hello对应的那个窗口，它里面够五个元素了，它才会执行。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201758363.png" alt="image-20230420175819357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201758141.png" alt="image-20230420175806865"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">看一下count滑动窗口执行结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801677.png" alt="image-20230420180125565"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801117.png" alt="image-20230420180113979"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801863.png" alt="image-20230420180150478"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201802271.png" alt="image-20230420180220305"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201803247.png" alt="image-20230420180355733"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201805077.png" alt="image-20230420180527334"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201806314.png" alt="image-20230420180609855"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201806384.png" alt="image-20230420180630486"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CountWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowOpJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//CountWindow之滚动窗口：每隔5个元素计算一次前5个元素</span></span><br><span class="line">        <span class="comment">/*text.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">            @Override</span></span><br><span class="line"><span class="comment">            public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span><br><span class="line"><span class="comment">                    throws Exception &#123;</span></span><br><span class="line"><span class="comment">                String[] words = line.split(" ");</span></span><br><span class="line"><span class="comment">                for (String word : words) &#123;</span></span><br><span class="line"><span class="comment">                    out.collect(new Tuple2&lt;String, Integer&gt;(word,1));</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;).keyBy(0)</span></span><br><span class="line"><span class="comment">                //窗口大小</span></span><br><span class="line"><span class="comment">                .countWindow(5)</span></span><br><span class="line"><span class="comment">                .sum(1)</span></span><br><span class="line"><span class="comment">                .print();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//CountWindow之滑动窗口：每隔1个元素计算一次前5个元素</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">                .countWindow(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"CountWindowOpJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自定义Window的使用"><a href="#自定义Window的使用" class="headerlink" title="自定义Window的使用"></a>自定义Window的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下自定义window。其实呢，window还可以再细分一下。可以把它分为呢，一种是基于Key的window。一种是不基于Key的window。其实就是说咱们在使用window之前是否执行了key操作啊，咱们前面演示的都是这种基于Key的window。你看我们在做window之前，前面呢都做了Keyby对吧，那如果呢，需求中不需要根据Key进行分组，你在使用window的时候啊，我们需要对应的去使用那个timeWindowAll和countWindowAll。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201829109.png" alt="image-20230420182825536"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">你使用KeyBy之后的话，它就只能调那个timeWindow,countWindow(这两个也可以用window实现)，这个需要注意一下啊，那如果说是我们自定义的window。如何使用呢？对吧，针对这两种情况。来看一下。针对这个基于Key的window呀，我们需要使用这个window函数</span><br><span class="line"></span><br><span class="line">那针对下面这种不基于Key的window呢，我们可以直接使用这个windowAll就可以了。其实呀，我们前面所说的那个timewindow和timewindowall(这两个也可以用windowAll实现)底层用的就是这个window和windowall，你可以这样理解timewindow是官方封装好的window。所以说呢，timewindow和countwindow呢，都是官方封装好了。</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">val input: DataStream[T] &#x3D; ...</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; tumbling event-time windows</span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(5)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; tumbling processing-time windows</span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(TumblingProcessingTimeWindows.of(Time.seconds(5)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; daily tumbling event-time windows offset by -8 hours.</span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingProcessingTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：自定义MyTimeWindow</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyTimeWindowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//自定义MyTimeWindow滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_,<span class="number">1</span>))</span><br><span class="line">      .keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//窗口大小</span></span><br><span class="line">  .window(<span class="type">TumblingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)))<span class="comment">//注意这里和后面的基于eventtime计算有点不一样</span></span><br><span class="line">      .sum(<span class="number">1</span>)</span><br><span class="line">      .print()</span><br><span class="line">    env.execute(<span class="string">"MyTimeWindowScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个呢，和咱们之前啊使用的什么timewindow那个效果是一样的。这样的话更加灵活一些，我们想怎么定义都可以啊。如果你不使用这个KeyBy的话，那下面你就可以使用windowAll是一样的效果</span><br></pre></td></tr></table></figure><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：自定义MyTimeWindow</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTimeWindowJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自定义MyTimeWindow滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//窗口大小</span></span><br><span class="line">                .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"MyTimeWindowJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Window中的增量聚合和全量聚合"><a href="#Window中的增量聚合和全量聚合" class="headerlink" title="Window中的增量聚合和全量聚合"></a>Window中的增量聚合和全量聚合</h3><h4 id="增量聚合"><a href="#增量聚合" class="headerlink" title="增量聚合"></a>增量聚合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下Window聚合。就是在进行Window聚合操作的时候呢，可以分为两种情况。一种呢是增量聚合，还有一种是全量聚合。</span><br><span class="line"></span><br><span class="line">那下面我们首先来看一下这个增量聚合。增量聚合呢，它表示呀，窗口中每进入一条数据就进行一次计算，常见的一些增量聚合函数如下:</span><br><span class="line">reduce() aggregate() sum() min() max()</span><br><span class="line"></span><br><span class="line">那下面呢，我们来看一个增量聚合的案例啊，就是累加求和</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202150610.png" alt="image-20230420215009349"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">它的具体执行过程是这样的。第一次进来一条数据，则立刻进行累加，求和结果为八，第二次进来一条数据12，则立刻进行累加，求和结果为20。第三次进来一条数据七，则立刻进行累加求和，结果为27。第四次进来一条数据，则立刻进行累加求和，结果为37。这就是这个增量聚合它的一个执行流程。</span><br><span class="line"></span><br><span class="line">那下面呢，我们来看一下reduce函数的一个使用，从这里面我们可以看出来，reduce是每次获取一条数据和上一次的执行结果求和。也就是来一条数据，立刻计算一次，这个就是增量聚合。</span><br></pre></td></tr></table></figure><h4 id="全量聚合"><a href="#全量聚合" class="headerlink" title="全量聚合"></a>全量聚合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们来看一下全量集合。全量集合呀，它就是等属于窗口的数据都到齐了，才开始进行聚合计算，可以实现对窗口内的数据进行排序等需求。常见的一些全量聚合函数为：</span><br><span class="line">apply(windowFunction)，还有这个process(processWindowFunction)</span><br><span class="line">apply呢，它里面接触的是windowfunction,process里面接触是processwindowfunction</span><br><span class="line">注意这个processwindowfunction比windowfunction提供了更多的上下文信息啊。那下面呢，我们来看一个全量聚合的一个案例，求最大值</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202157070.png" alt="image-20230420215701906"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第四次进来调数据10，此时窗口触发，这时候才会对窗口内的数据进行排序，然后获取最大值。</span><br></pre></td></tr></table></figure><h5 id="全量聚合apply"><a href="#全量聚合apply" class="headerlink" title="全量聚合apply"></a>全量聚合apply</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202158547.png" alt="image-20230420215834620"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这个apply函数的一个使用。从这你可以看出来，他接触的是一个iterable，可以认为是一个集合。他可以把这个窗口的数据啊，一次性全都传过来，当这个窗口触发的时候，才会真正执行这个代码。</span><br></pre></td></tr></table></figure><h5 id="全量聚合process"><a href="#全量聚合process" class="headerlink" title="全量聚合process"></a>全量聚合process</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202200468.png" alt="image-20230420220026246"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢是一个process。你看他接触的也是一个iterable，所以说呢，你在这里面就可以获取到这个窗口里面的所有数据了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个呢就是Windows中的全量聚合和增量聚合，后面呢我们就会用到这个apply，还有process它的一个使用，因为有时候我们需要对这个窗口内的所有数据去做一些全量的操作，这样的话就不能用这种增量聚合，而要用这种全量聚合。</span><br></pre></td></tr></table></figure><h3 id="Flink中的Time"><a href="#Flink中的Time" class="headerlink" title="Flink中的Time"></a>Flink中的Time</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对流数据的time可以分为以下三种。第一个Event Time表示事件产生的时间，它通常由事件中的时间戳来描述。第二个ingestion time表示事件进入flink的时间。第三个processing time，它表示事件被处理时当前系统的时间，那这几种时间呀，我们通过这个图可以很清晰的看出来它们之间的关系。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202206362.png" alt="image-20230420220641443"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">首先是even time，这个就是数据产生的时间。第二个是ingestion time表示呢，他进入flink时间，其实就是被那个source把它读取过来那个时间。第三个呢，是这个processing time，它其实呢，就是flink里面具体的算子，在处理的时候它的一个时间，那接下来我们来看一个案例。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202209327.png" alt="image-20230420220902227"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意你看数据呢，是在十点的时候产生的。结果呢，在晚上八点的时候才被flink读取走。那flink真正在处理的时候呢？是8.02秒。</span><br><span class="line"></span><br><span class="line">注意，如果说呀，我们想要统计每分钟内接口调用失败的错误日志个数。那这个时候使用哪个时间才有意义呢？因为数据有可能会出现延迟。如果使用那个数据进入flink的时间或者window处理的时间，其实是没有意义的。这个时候我们需要使用原始日中的时间才是有意义的，这个才是数据产生的时间，我们基于这个时间去统计才有意义。</span><br><span class="line"></span><br><span class="line">那我们在flink流水中默认使用的是哪个时间呢？某种情况下，flink在流处理中使用的时间是这个processingtime。那如果说我们想要修改的话，怎么改呢？可以使用这个env去改env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)可以设置这个time或者是这个IngestionTime。好，这就是flink中的三种time。</span><br></pre></td></tr></table></figure><h3 id="Watermark的分析"><a href="#Watermark的分析" class="headerlink" title="Watermark的分析"></a>Watermark的分析</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211000567.png" alt="image-20230421100013693"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实时计算中，数据时间比较敏感。有eventTime和processTime区分，一般来说eventTime是从原始的消息中提取过来的，processTime是Flink自己提供的，Flink中一个亮点就是可以基于eventTime计算，这个功能很有用，因为实时数据可能会经过比较长的链路，多少会有延时，并且有很大的不确定性，对于一些需要精确体现事件变化趋势的场景中，单纯使用processTime显然是不合理的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面提到了Time的概念，如果我们使用Processing Time，那么在Flink消费数据的时候，它完全不需要关心数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为Processing Time只是代表数据在Flink被处理时的时间，这个时间是顺序的。</span><br><span class="line"></span><br><span class="line">但是如果你使用的是Event Time的话，那么你就不得不面临着这么个问题：事件乱序&amp;事件延迟。</span><br><span class="line"></span><br><span class="line">所以…</span><br><span class="line">为了解决这个问题，Flink中引入了WaterMark机制，即水印的概念。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211008547.png" alt="image-20230421100850143"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">然而在有些场景下，尤其是特别依赖于事件时间而不是处理时间，比如：</span><br><span class="line">错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</span><br><span class="line">设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</span><br><span class="line">比如我做过的充电桩实时报文分析，就必须依赖报文产生的时间，即事件时间</span><br><span class="line">…</span><br><span class="line">针对上面的问题（事件乱序 &amp; 事件延迟），Flink引入了Watermark机制来解决。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">统计8:00 ~ 9:00这个时间段打开淘宝App的用户数量，Flink这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在8:00 ~ 9:00中用户打开App的事件数据，但又不能无限期的等下去？</span><br><span class="line"></span><br><span class="line">当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是Watermark的思想。</span><br><span class="line"></span><br><span class="line">Watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的Watermark。Watermark本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有Watermark大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink也有相应的机制（下文会讲）去处理。</span><br></pre></td></tr></table></figure><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个timestamp.watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现。</span><br><span class="line"></span><br><span class="line">流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（out-of-order或者说late element）。</span><br><span class="line"></span><br><span class="line">但是对于late element，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">比如：</span><br><span class="line">08:00任务开启，设置1分钟的滚动窗口，在08:00:00-08:01:00为第一个窗口，08:01:00-08:02:00为第二个窗口；</span><br><span class="line">现在有一条数据的事件时间是08:00:50，但是这条数据却在08:01:10到达，按照正常的处理，窗口会在结束时间（08:01:00）的时候就触发计算，那么这条数据就会被丢弃；</span><br><span class="line"></span><br><span class="line">但是开启WaterMark后，窗口在08:01:00时不会触发；</span><br><span class="line">因为采用的是EventTime，而数据本身时间是08:00:50，所以该条数据肯定会落到第一个窗口；</span><br><span class="line">假设在08:01:10时的WaterMark为08:01:00（WaterMark可以理解为一个时间戳），发现这个WaterMark和第一个窗口的结束时间相等，此时触发第一个窗口的计算操作，此时这条延迟数据正好参与到计算中；</span><br><span class="line">此时只有水印大于或等于窗口结束时间才会触发窗口的关闭和计算；</span><br><span class="line">此时就不会丢数据。</span><br></pre></td></tr></table></figure><h4 id="WaterMark的传递"><a href="#WaterMark的传递" class="headerlink" title="WaterMark的传递"></a>WaterMark的传递</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Watermark在向下游传递时，是广播到下游所有的子任务中，如果多并行度下有多个watermark传递到下游时，取最小的watermark。</span><br></pre></td></tr></table></figure><h4 id="WaterMark设置"><a href="#WaterMark设置" class="headerlink" title="WaterMark设置"></a>WaterMark设置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">注：如果你采用的是事件时间，即你设置了env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">那么你就必须设置获取事件时间的方法，否则会报错（如果是从kafka消费数据，不设置水印的话，默认采用kafka消息自带的时间戳作为事件时间）</span><br><span class="line"></span><br><span class="line">数据处理中需要通过调用DataStream中的assignTimestampsAndWatermarks方法来分配时间和水印，该方法可以传入两种参数，一个是Assigner With Periodic Watermarks，另一个是Assigner With Punctuated Watermarks。</span><br><span class="line"></span><br><span class="line">所以设置Watermark是有如下两种方式：</span><br><span class="line">Assigner With Punctuated Watermarks：数据流中每一个递增的EventTime都会产生一个Watermark。</span><br><span class="line"></span><br><span class="line">Assigner With Periodic Watermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实际生产中用第二种的比较多，它会周期性产生Watermark的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">通常情况下，在接收到Source的数据后，应该立刻生成Watermark,但是也可以在使用Map或者Filter操作之后，再生成Watermark。</span><br><span class="line"></span><br><span class="line">Watermark的生成方式有两种:</span><br><span class="line">With Periodic Watermarks:周期性触发Watermark的生成和发送,每隔N秒自动向流里面注入一个Watermark，时间间隔由ExecutionConfig.setAutoWatermarkInterval决定，现在新版本的Flink默认是200ms。之前默认是100ms</span><br><span class="line">可以定义一个最大允许乱序的时间，这种比较常用。</span><br><span class="line"></span><br><span class="line">With Punctuated Watermarks:基于某些事件触发Watermark的生成和发送。</span><br><span class="line">基于事件向流里面注入一个Watermark,每个元素都有机会判断是否生成一个watermark值</span><br></pre></td></tr></table></figure><h3 id="开发Watermark代码"><a href="#开发Watermark代码" class="headerlink" title="开发Watermark代码"></a>开发Watermark代码</h3><h4 id="乱序数据处理-数据有序"><a href="#乱序数据处理-数据有序" class="headerlink" title="乱序数据处理(数据有序)"></a>乱序数据处理(数据有序)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了whatermark的一些基本原理，可能大家对它还不够了解，下面我们来通过这个案例加深大家对whatermark的理解。我们来分析一下这个案例。乱序数据处理</span><br><span class="line"></span><br><span class="line">通过socket模拟数据。数据的格式是这样的。前面的话代表的是具体的业务数据，后边的话是一个时间戳，这是一个毫秒的时间戳。中间用逗号分隔。</span><br><span class="line"></span><br><span class="line">其中，时间戳是数据产生的时间。也就是even time。那产生这个数据之后呢？然后使用map函数，把数据转换为tuple2的形式。接着再调用这个函数assignTimestampsAndWatermarks。使用这个方法来抽取timestamp并生成watermark。</span><br><span class="line">接着，再调用window打印信息，来验证window被触发的时机。最后验证乱序数据的处理方式，这是我们一个大致的一个处理流程。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">      env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳(EventTime)和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentMaxTimstamp + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>) </span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样(这里和前面自定义window时，传的参数有点不一样，这里是event)</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>],<span class="type">String</span>,<span class="type">Tuple</span>,<span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup=&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr+<span class="string">","</span>+arr.length+<span class="string">","</span>+sdf.format(arr.head)+<span class="string">","</span>+sdf.format(arr.last)+<span class="string">","</span>+sdf.format(window.getStart)+<span class="string">","</span>+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="根据数据跟踪观察Watermark"><a href="#根据数据跟踪观察Watermark" class="headerlink" title="根据数据跟踪观察Watermark"></a>根据数据跟踪观察Watermark</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211425983.png" alt="image-20230421142543318"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211428308.png" alt="image-20230421142830892"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211435405.png" alt="image-20230421143511791"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211438988.png" alt="image-20230421143817886"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211440736.png" alt="image-20230421144017493"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211441254.png" alt="image-20230421144059007"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211441192.png" alt="image-20230421144132566"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211442566.png" alt="image-20230421144203353"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到这里，window仍然没有被触发，此时watermark的时间已经等于第一条数据的eventtime了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211445272.png" alt="image-20230421144516628"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211445660.png" alt="image-20230421144534532"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">window仍然没有被触发，此时，我们数据已经发送到2026-10-01 10:11:33了，根据eventtime来算，最早的数据已经过去了11s了，window还没开始计算，那到底什么时候会触发window呢？</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211450426.png" alt="image-20230421145025475"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211451804.png" alt="image-20230421145103798"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211452915.png" alt="image-20230421145208979"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到这里，我们做了一个说明。</span><br><span class="line">window的触发机制，是先按照自然时间将window划分，如果window大小是3s，那么1min内会把window划分成如下的形式(左闭右开的区间)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211459275.png" alt="image-20230421145940525"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211500839.png" alt="image-20230421150008692"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">window的设定无关数据本身，而是系统定义好了的。</span><br><span class="line">输入的数据，根据自身的eventtime，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;&#x3D;eventtime时，就符合了window触发的条件了，最终决定window触发，还是由eventtime所属window中的window_end_time决定。</span><br><span class="line"></span><br><span class="line">上面的测试中，最后一条数据到达后，其水位线(watermark)已经上升至10:11:24，正好是最早的一条记录所在window的window_end_time，所以window就被触发了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211511116.png" alt="image-20230421151156542"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211513051.png" alt="image-20230421151309893"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时，watermark时间虽然已经等于第二条数据的时间，但是由于其没有达到第二条数据所在window，但是由于其没有达到第二条数据所在window的结束时间，所以window并没有被触发。那么，第二条数据所在的window时间区间如下。 </span><br><span class="line">[00:00:24,00:00:27)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">也就是说，我们必须输入一个10:11:37的数据，第二条数据所在的window才会被触发，我们继续输入。</span><br><span class="line">0001,1790820697000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211520479.png" alt="image-20230421152050886"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时，我们已经看到，window的触发条件要符合以下几个条件：</span><br><span class="line">1.watermark时间&gt;&#x3D;wind_end_time</span><br><span class="line">2.在[window_start_time,window_end_time)区间中有数据存在(注意是左闭右开的区间)</span><br></pre></td></tr></table></figure><h4 id="Watermark-EventTime-处理乱序数据"><a href="#Watermark-EventTime-处理乱序数据" class="headerlink" title="Watermark+EventTime(处理乱序数据)"></a>Watermark+EventTime(处理乱序数据)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我们前面那个测试啊，数据呢，都是按照这个时间顺序递增的，都是有序的，那现在呢，我们来输入一些的数据，来看看这个whatmark，结合这个一的eventtime机制是如何处理这些乱写数据的。那我们在上面那个基础之上啊，再输入两行数据。</span><br><span class="line">注意这个呢，没有触发对吧</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211904910.png" alt="image-20230421190455535"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211905517.png" alt="image-20230421190548292"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们再输入一条43秒的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211916109.png" alt="image-20230421191608587"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211919959.png" alt="image-20230421191923374"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211920175.png" alt="image-20230421192030363"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大家注意他没有这个33的。我这个窗口呢，是[30,33)，你看我这个一的代表里面数据是有这个33的，为什么这个33没有输出呢。因为这个窗口啊，它是一个左闭右开的。那这个33的话，它其实啊，属于下一个窗口，就是33到36的那个窗口。</span><br><span class="line"></span><br><span class="line">好。所以上面这个结果其实已经表明对迟到的数据了，flink可以通过这个watermark来实现处理一定范围内的乱序数据。因为现在我们允许的最大乱序时间是十秒。就是十秒之内乱序是OK的，那如果超过了这个十秒怎么办？也就是说呢，对于这个迟到(late element)太久的数据，flink是怎么处理的呢？</span><br></pre></td></tr></table></figure><h4 id="延时数据的三种处理方式"><a href="#延时数据的三种处理方式" class="headerlink" title="延时数据的三种处理方式"></a>延时数据的三种处理方式</h4><h5 id="丢弃-默认"><a href="#丢弃-默认" class="headerlink" title="丢弃(默认)"></a>丢弃(默认)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们就来看一下针对迟到太久的数据，它的一些处理方案，现在呢一共有三种。</span><br><span class="line">第一种是丢弃默认的啊。那我们首先呢，来输入一个乱序很多的数据来测试一下(其实只要EventTime&lt;watermark时间)</span><br><span class="line"></span><br><span class="line">它这里是程序重启重新输的数据</span><br><span class="line">0001,1790820690000</span><br><span class="line">0001,1790820703000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221025938.png" alt="image-20230422102536854"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">再输几个EventTime小于Watermark的时间</span><br><span class="line">0001,1790820690000</span><br><span class="line">0001,1790820691000</span><br><span class="line">0001,1790820692000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221027933.png" alt="image-20230422102748626"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">看到没有，这三条他都没有触发这个窗口的执行啊，因为你现在你输入的数据所在的窗口已经执行过了。flink默认对这些迟到的数据的处理方案就是丢弃。这几条数据，30对应的那个窗口数据是不是已经执行过了呀，那这样过来它直接丢弃，这是默认的一个处理方案。</span><br></pre></td></tr></table></figure><h5 id="allowedLateness"><a href="#allowedLateness" class="headerlink" title="allowedLateness"></a>allowedLateness</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那接下来看第二种，你可以通过这个allowedLateness来指定一个允许数据延迟的时间。</span><br><span class="line">本身啊，我们之前通过那个watermark已经设置了一个数据的延迟时间是十秒，对吧。你可以通过这个参数啊，再给他指定一个延迟时间，就类似于我们上班打卡官方延迟对吧，类似于公司统一层面允许大家呢弹性半小时。但是你们这个部门呢，可以再多谈十分钟，有这种效果。</span><br><span class="line"></span><br><span class="line">在某些情况下，我们希望对迟到的数据再提供一个宽容时间。那flink提供了这个方法，可以实现对迟到的数据啊，再给它设置一个延迟时间，在指定延迟时间内到达数据还是可以触发window执行的。所以这时候我们需要去改一下代码了。主要呢，就增加这一行就行。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpForAllowedLatenessScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//允许数据迟到2秒</span></span><br><span class="line">      .allowedLateness(<span class="type">Time</span>.seconds(<span class="number">2</span>))</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>],<span class="type">String</span>,<span class="type">Tuple</span>,<span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup=&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr+<span class="string">","</span>+arr.length+<span class="string">","</span>+sdf.format(arr.head)+<span class="string">","</span>+sdf.format(arr.last)+<span class="string">","</span>+sdf.format(window.getStart)+<span class="string">","</span>+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里又是重启之后了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221044535.png" alt="image-20230422104446863"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221047820.png" alt="image-20230422104738555"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们再输入几条eventtime小于watermark的一个数据来验证一下效果。好，来看一下。注意你会发现，你看。这三条数据过来的时候，窗口同样被触发了，因为之前的话，我们是这个30到33这个窗口对吧。我在这输的这三条数据，一个是30秒了，31、32，它们都属于那个窗口。岁数，你看窗口都被吃光。你看这时候打印的窗口数据是两条，这是三条，这是四条对吧。所以说呢，每条数据都触发了window的执行啊。这三条数据。那下面我们再输一条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221050530.png" alt="image-20230422105039949"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221058268.png" alt="image-20230422105851955"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221100903.png" alt="image-20230422110036594"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这时候啊，我们把这个whatmark呀，给它调到34。往上面调一下。看到没有，这次呢，它是没有触发的啊是34。数据呢是44，这样的话，whatmark变成了34。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221103748.png" alt="image-20230422110328208"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221105374.png" alt="image-20230422110533985"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时呢，把whatmark上升到了34。此时呢，我们再输入几条这种迟到的数据来验证一下效果。因为刚才的话，我们验证了它是可以执行的啊。嗯。结果你会发现，看到没有，这三条又执行了。我们发现数的这三条数据呢，它都触发了window执行。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221107423.png" alt="image-20230422110715967"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221108095.png" alt="image-20230422110824496"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们再输入一行数据，把watermark调到35。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221112669.png" alt="image-20230422111217143"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意下面我们再输入Eventtime&lt;Watermark的数据，还是那个三十三十一三十二啊。嗯。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221114742.png" alt="image-20230422111430693"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221116286.png" alt="image-20230422111654208"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">注意此时这个窗口就不会触发，相当于啊，你这个时候你这个迟到的数据啊，我就不管了。</span><br><span class="line"></span><br><span class="line">来分析一下。你看这几条数据啊，它都没有触发window啊。当这个whatermark等于这个33的时候，它正好呢，是属于这个window_end_time对吧，正好相等，所以说呢，它会触发这个[30到33)窗口执行。</span><br><span class="line"></span><br><span class="line">当这个窗口执行过后啊，我们再输入[30到33)这个窗口内的数据的时候呢，会发现这个窗口是可以被触发的。</span><br><span class="line"></span><br><span class="line">当我们把这个watermark提升到34秒的时候，我们再输入这个窗口内的数据，发现window还是可以被触发的。</span><br><span class="line"></span><br><span class="line">那当我们把这个watermark升到35的时候，再输入这个窗口数据，发现window不会被触发。这是为什么呢？这是因为我们在前面设置了allowedLateness(Time.seconds(2))这个参数。又给它多加了两秒延迟，因此呢，可以允许延迟在两秒内的数据继续触发window执行。所以说当watermark等于34的时候，是可以触发window的，但是35就不行了，这个需要注意一下。</span><br><span class="line"></span><br><span class="line">总结如下，对于这个窗口而言啊，它允许两秒的迟到数据，也就是说呢，你第一次触发是在watermark&gt;&#x3D;window_end_time的时候</span><br><span class="line"></span><br><span class="line">当这个watermark&#x3D;35的时候呢,我们再去输入这个eventtime为三十三十一三十二，这些数据的window_end_time都是33，此时35&lt;33+2为false，所以不会再触发window执行了</span><br></pre></td></tr></table></figure><h6 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h6><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221121053.png" alt="image-20230422112107568"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221121425.png" alt="image-20230422112129357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221603475.png" alt="image-20230422160343061"></p><h5 id="sideOutputLateData"><a href="#sideOutputLateData" class="headerlink" title="sideOutputLateData"></a>sideOutputLateData</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面呢，还有一种处理方案。这个呢，就是收集迟到数据。通过这个函数呢，可以把迟到的数据啊，给它统一收集，统一存储，方便后期排查</span><br><span class="line"></span><br><span class="line">注意咱们刚才讲那个第二种方案，其实可以和第三种结合到一块儿来使用，都是可以的啊，你再给他延迟两秒，如果说他还是没有到达，对吧，那你就把它保存起来，丢了保存起来。当然也可以单独使用，都是可以的啊，这个需要具体根据你们的业务需求来定。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">OutputTag</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpForSideOutputLateDataScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间 10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//保存被丢弃的数据</span></span><br><span class="line">    <span class="keyword">val</span> outputTag = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>]](<span class="string">"late-data"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resStream = waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//保存被丢弃的数据</span></span><br><span class="line">      .sideOutputLateData(outputTag)</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>], <span class="type">String</span>, <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup =&gt; &#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr + <span class="string">","</span> + arr.length + <span class="string">","</span> + sdf.format(arr.head) + <span class="string">","</span> + sdf.format(arr.last) + <span class="string">","</span> + sdf.format(window.getStart) + <span class="string">","</span> + sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;) </span><br><span class="line"></span><br><span class="line">    <span class="comment">//把迟到的数据取出来，暂时打印到控制台，实际工作中可以选择存储到其它存储介质中</span></span><br><span class="line">    <span class="comment">//例如：redis，kafka</span></span><br><span class="line">    <span class="keyword">val</span> sideOutput = resStream.getSideOutput(outputTag)</span><br><span class="line">    sideOutput.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将流中的结果数据也打印到控制台</span></span><br><span class="line">    resStream.print()</span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们来验证一下，先输入这两行数据。第一条。所以你看第一次发了一个30，这是43对吧。此时，这头的mark是33。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221620417.png" alt="image-20230422162002780"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221620154.png" alt="image-20230422162014105"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们再输入几条event time小于watermark的一个时间来测试一下啊。现在你这个窗口已经执行过了，我们再往里添加数据来看一下效果。还这三条数据啊。注意它那个窗口就没有执行了。你看针对这个迟到的数据，我们就可以通过这个sideOutputLateData来保存到这个outputTag中。后期你想在保存的其他存储介质中也是没有任何问题的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221622695.png" alt="image-20230422162202360"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221623442.png" alt="image-20230422162306215"></p><h4 id="在多并行度下的watermark应用"><a href="#在多并行度下的watermark应用" class="headerlink" title="在多并行度下的watermark应用"></a>在多并行度下的watermark应用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面呢，我们演示了在单并行度下whatmark的使用，下面呢我们来看一下在多并行度下面watermark的一个使用。咱们前面的话我们将env.setParallelism(1)。如果不设置的话。那我们在IDE中去执行的时候，默认呢，它会读取我本地的CPU的数量来设置默认并行度。那所以说我在这把这个给它注释掉。在这里加一个系统ID，这样的话我们就知道了是哪条数据被哪个线程所处理。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpMoreParallelismScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间 10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          </span><br><span class="line">          <span class="keyword">val</span> threadId = <span class="type">Thread</span>.currentThread().getId</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"threadId:"</span>+threadId+<span class="string">",key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>],<span class="type">String</span>,<span class="type">Tuple</span>,<span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup=&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr+<span class="string">","</span>+arr.length+<span class="string">","</span>+sdf.format(arr.head)+<span class="string">","</span>+sdf.format(arr.last)+<span class="string">","</span>+sdf.format(window.getStart)+<span class="string">","</span>+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入如下7条数据，发现这个window没有被触发，因为这个时候呢，这七条数据啊，都是被不同的线程处理的，每个线程呢，都有一个watermark。我们前面分析了，在这种多并行度的情况下呢，whatmark呢，它呢有一个对齐机制，它呢会取所有channel中最小的那个watermark。但是呢我们现在默认有8个并行度，你这七条数据呢，都被不同的线路所处理啊。到现在呢，还没有获取到最小的watermark。所以说呢，这个window是无法被触发执行的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221654047.png" alt="image-20230422165413793"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221654240.png" alt="image-20230422165427792"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为这个线路太多了，验证起来了，不太好验证，所以说啊这样。把这个稍微再改一下。我们也不用了一个默认的八个了，我们给它改成2个吧。这个也是多并行度了。好。接下来呢，我们往里面输这么三条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221700312.png" alt="image-20230422170004992"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221702331.png" alt="image-20230422170229823"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一条，没有触发。接下来，第二条，其实理论上如果是单线程的话，这个时候这个窗口已经被触发，但是现在呢，还没有触发。这第三条数据。嗯。好。看到没有，这个时候他就出发。看一下这块的一个总结。此时呢，我们会发现，当第三条数据输入完以后，这个窗口呢，它就被触发了。你前两条数据啊，输入之后呢，它获取到的那个具体的watermark是20。这个时候呢，它对应的window中呢，是没有数据的，所以说呢，什么都没有执行，当你第三条数据输入之后呢，它获取到那个最小的mark呢，就是33了，这个时候呢，它对应的窗口就是它，它里面有数据，所以说呢，这个window就触发了。</span><br></pre></td></tr></table></figure><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来针对这个watermark案例做一个总结。我们在flink中，针对这个watermark，我们该如何设置它的最大乱序时间？注意。最大乱序时间。首先第一点这个要结合我们自己的业务以及呢数据的实际情况去设置，如果OutOfOrderness设置的太小，而我们那个自身数据啊，发送时由于网络等原因导致乱序或者迟到太多，那么呢，最终的结果就是会有很多数据被丢弃。这样的话，对我们数据的正确性影响太大。</span><br><span class="line">那对于这个严重乱序的数据呢？我们需要严格统计数据的最大延迟时间。这样才能最大程度保证计算数据的一个准确度。延时时间设置太小会影响数据准确性。延时时间设置太大，不仅影响数据的一个实时性。更会加重flink作业的一个负担。所以说不是对EventTime要求特别严格的数据，尽量呢不要采用这种Eventtime的方式来处理数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221721954.png" alt="image-20230422172108445"></p><h2 id="Flink并行度"><a href="#Flink并行度" class="headerlink" title="Flink并行度"></a>Flink并行度</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们来分析一下Flink的并行度。一个flink程序由多个组件组成，datasource、transformation、datasink。</span><br><span class="line">一个组件呢，由多个并行的实例(线程)来执行，或者说呢，是由多个线程来执行。一个组件的并行实际数目呢？就被称之为该组件的并行度。其实就是说你这个组件有多少个线程去执行，那么它的并行度(和并行执行能力并不相等)就是多少。</span><br></pre></td></tr></table></figure><h3 id="taskmanager和slot之间的关系"><a href="#taskmanager和slot之间的关系" class="headerlink" title="taskmanager和slot之间的关系"></a>taskmanager和slot之间的关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，在具体分析这个并行度之前，我们先分析一下这个taskmanager和slot之间的关系。flink的每个taskmanager为集群提供的slot的数量通常与每个task manager的可用CPU数量成正比。一般情况下的数量就是每个taskmanager的可用CPU数量。这个task manager节点就是我们集群的一个从节点。那上面这个slot数量就是这个task manager具有的一个并发执行能力。这里面啊，实行的就是具体的一些实例。source、map、keyBy、sink。还有这个图也是一样的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221751734.png" alt="image-20230422175152422"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221755921.png" alt="image-20230422175512133"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Flink中每个TaskManager都是一个JVM进程，它可能会在独立的线程上执行一个或多个subtask。为了控制一个worker能接收多少个task，worker通过task slot来进行控制（一个worker至少有一个task slot）。每个task slot表示TaskManager拥有资源的一个固定大小的子集。默认情况下，Flink允许子任务共享slot，即使它们是不同任务的子任务（前提是它们来自同一个job）。这样的结果是，一个slot可以保存作业的整个管道。</span><br><span class="line"></span><br><span class="line">在您描述的情况中，当并行度为1时，所有算子都在同一个slot中执行。当并行度为2时，算子被分成了三部分，其中两部分由2个slot执行，最后sink由一个单独的slot执行。这可能是因为Flink允许非资源密集型的算子和资源密集型的算子分配到同一个slot中，这样所有的slot之间任务就会平等，不会存在一直空闲一直高负载。</span><br></pre></td></tr></table></figure><h3 id="并行度的设置"><a href="#并行度的设置" class="headerlink" title="并行度的设置"></a>并行度的设置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们就来看一下这个并行度该如何来设置。Flink任务的并行度可以通过4个层面来设置。</span><br><span class="line">Operator Level（算子层面）</span><br><span class="line">Execution Environment Level（执行环境层面）</span><br><span class="line">Client Level（客户端层面）</span><br><span class="line">System Level（系统层面）</span><br><span class="line"></span><br><span class="line">那这四个层面，他们执行的优先级是什么样的？注意这些并行度优先级为Operator Level&gt;Execution Environment Level&gt;Client Level&gt;System Level</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221757173.png" alt="image-20230422175706876"></p><h4 id="Operator-Level"><a href="#Operator-Level" class="headerlink" title="Operator Level"></a>Operator Level</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">算子、数据源和目的地的并行度可以通过调用 setParallelism()方法来指定</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221758256.png" alt="image-20230422175809024"></p><h4 id="Execution-Environment-Level"><a href="#Execution-Environment-Level" class="headerlink" title="Execution Environment Level"></a>Execution Environment Level</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个执行环境层面的。主要呢，是在这个ENV后面来设置一个并行度。这设置的是一个全局的并行度。当然，你也可以选择在下面针对某一个算子再去改它的并行度也是可以的。因为你那个算子层面并行度是大于这个执行环境层面这个并行度的。</span><br><span class="line"></span><br><span class="line">注意：执行环境的并行度可以通过显式设置算子的并行度而被重写。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221758507.png" alt="image-20230422175856304"></p><h4 id="Client-Level"><a href="#Client-Level" class="headerlink" title="Client Level"></a>Client Level</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来是一个客户端层面。这个并行度呢，可以在客户端提交Job的时候来设定。通过那个-P参数来动态指令就可以了。具体呢，是这样的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221802253.png" alt="image-20230422180247995"></p><h4 id="System-Level"><a href="#System-Level" class="headerlink" title="System Level"></a>System Level</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那最后呢，是这个系统层面了。我们在系统层面可以通过配置flink-conf.yaml文件里面parallelism.default属性来指定所有执行环境的默认并行度啊，当然了，你是可以在具体的任务里面再去动态的去改这个并行度。因为他们呢，可以覆盖这个系统层面的并行度。</span><br></pre></td></tr></table></figure><h3 id="并行度案例分析"><a href="#并行度案例分析" class="headerlink" title="并行度案例分析"></a>并行度案例分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来通过一些案例来具体分析一下Flink中的并行度。首先看这个图。这个图里面呢，它表示啊，我们这个集群是有三个从节点。M1，M2，M3，注意每个节点上面具有三个slot。这个表示这个从节点，它具有的3个并发处理能力。那如何实现三个呢？在这个flink-conf.yaml里面来配置了taskmanager.numberOfTaskSlots，把它设置为3。这样话相当于我这个节点上面有三个空闲CPU。那这样的话，我这个集群啊，目前具有的一个处理能力就是9 slot</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221811958.png" alt="image-20230422181155611"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一个案例，它的并行度为1，那如何让它的并行度为1呢？很简单，你在提交这个任务的时候，什么参数都不设置就行。并且我们在开发这个word代码的时候，里面啊，也不设置并行度相关的代码，这样就可以了，这样它就会默认呢，读取这个flink-conf.yaml里面的parallelism的值。这个参数的默认值为1。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221817269.png" alt="image-20230422181742166"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第二个案例，如何实现让它的并行度为2呢？你可以通过这几种方式，首先呢，去改flink-conf.yaml把里面这个默认参数值改为二，或者说我们在动态提交的时候通过-P来指定。或者我们通过这个env来设置都是可以的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221823152.png" alt="image-20230422182315555"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221827716.png" alt="image-20230422182700747"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第三个案例，它的并行度为9，那如何实现呢？你要么在这个配置文件flink-conf.yaml里面，把这个参数设置为9，要么呢-p动态指定。要么呢，通过env来设置都是可以的。这样的话，它就是9份了。这样就占满了，那说我能不能把这个并行度设为10呢？不能，因为你现在最终呢，只有九个slot。这个需要注意啊。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221831842.png" alt="image-20230422183105341"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第四个。我看呀，它这个并行度呢，还是9，但是注意针对这个sink组件的并行度啊，给它设置为1啊。我们在这主要分析一下这个新的组件并行度，全局设置为9，就是根据咱们前面这个案例。这三种你用哪种都可以。但是呢，我们还需要把这个新的组件并行度设置为1，那怎么设置呢？就说你在代码里面啊，通过算式层面来把这个新组件的并行度设置为1，这样的话它就会覆盖那个全局的那个9。当然你其他组件还是按那个九那个并行度去执行，而我这个组件的话，我在这给它覆盖掉，使用一给它覆盖掉。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
</feed>
