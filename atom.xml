<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2023-03-30T17:33:16.070Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库之用户行为数仓3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933.html</id>
    <published>2023-03-30T13:22:30.000Z</published>
    <updated>2023-03-30T17:33:16.070Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="电商数据仓库之用户行为数仓3-数据生成与采集"><a href="#电商数据仓库之用户行为数仓3-数据生成与采集" class="headerlink" title="电商数据仓库之用户行为数仓3-数据生成与采集"></a>电商数据仓库之用户行为数仓3-数据生成与采集</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们就来开发第一个模块：数据采集模块</span><br><span class="line">这一块内容在开发的时候，我们需要先生成测试数据，一份是服务端数据，还有一份是客户端数据</span><br></pre></td></tr></table></figure><h2 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h2><h3 id="【客户端数据】用户行为数据"><a href="#【客户端数据】用户行为数据" class="headerlink" title="【客户端数据】用户行为数据"></a>【客户端数据】用户行为数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先我们模拟生成用户行为数据，也就是客户端数据，主要包含用户打开APP、点击、浏览等行为数据</span><br><span class="line">用户行为数据：通过埋点上报，后端日志服务器(http)负责接收数据</span><br><span class="line">埋点上报数据基本格式：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;uid&quot;：1001, &#x2F;&#x2F;用户ID</span><br><span class="line">&quot;xaid&quot;：&quot;ab25617-c38910-m2991&quot;, &#x2F;&#x2F;手机设备ID</span><br><span class="line">&quot;platform&quot;：2, &#x2F;&#x2F;设备类型, 1:Android-APP, 2:IOS-APP, 3:PC </span><br><span class="line">&quot;ver&quot;：&quot;3.5.10&quot;, &#x2F;&#x2F;大版本号</span><br><span class="line">&quot;vercode&quot;：&quot;35100083&quot;, &#x2F;&#x2F;子版本号</span><br><span class="line">&quot;net&quot;：1, &#x2F;&#x2F;网络类型, 0:未知, 1:WIFI, 2:2G , 3:3G, 4:4G, 5:5G</span><br><span class="line">&quot;brand&quot;：&quot;iPhone&quot;, &#x2F;&#x2F;手机品牌</span><br><span class="line">&quot;model&quot;：&quot;iPhone8&quot;, &#x2F;&#x2F;机型</span><br><span class="line">&quot;display&quot;：&quot;1334x750&quot;, &#x2F;&#x2F;分辨率</span><br><span class="line">&quot;osver&quot;：&quot;ios13.5&quot;, &#x2F;&#x2F;操作系统版本号</span><br><span class="line">&quot;data&quot;：[ &#x2F;&#x2F;用户行为数据</span><br><span class="line">&#123;&quot;act&quot;：1,&quot;acttime&quot;：1592486549819,&quot;ad_status&quot;：1,&quot;loading_time&quot;:100&#125;,</span><br><span class="line">&#123;&quot;act&quot;：2,&quot;acttime&quot;：1592486549819,&quot;goods_id&quot;：&quot;2881992&quot;&#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个json串中的data是一个json数组，它里面包含了多种用户行为数据。</span><br><span class="line">json串中的其它字段属于公共字段</span><br><span class="line"></span><br><span class="line">注意：考虑到性能，一般数据上报都是批量上报，假设间隔10秒上报一次，这种数据延迟是可以接受的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以在每次上报的时候，公共字段只需要报一份就行，把不同的用户行为相关的业务字段放到data数组中，这样可以避免上报大量的重复数据，影响数据上报性能，我们只需要在后期解析的时候，把公共字段和data数组总的每一条业务字段进行拼装，就可以获取到每一个用户行为的所有字段信息。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">act代表具体的用户行为，在这列出来几种</span><br><span class="line">act&#x3D;1：打开APP</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型 </span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">ad_status 开屏广告展示状态, 1:成功 2:失败</span><br><span class="line">loading_time 开屏广告加载耗时(单位毫秒)</span><br><span class="line"></span><br><span class="line">act&#x3D;2：点击商品</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">goods_id 商品ID</span><br><span class="line">location 商品展示顺序：在列表页中排第几位，从0开始</span><br><span class="line"></span><br><span class="line">act&#x3D;3：商品详情页</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">goods_id 商品ID</span><br><span class="line">stay_time 页面停留时长(单位毫秒)</span><br><span class="line">loading_time 页面加载耗时(单位毫秒)</span><br><span class="line"></span><br><span class="line">act&#x3D;4：商品列表页</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">loading_time 页面加载耗时(单位毫秒)</span><br><span class="line">loading_type 加载类型：1:读缓存 2:请求接口</span><br><span class="line">goods_num 列表页加载商品数量</span><br><span class="line"></span><br><span class="line">act&#x3D;5：app崩溃数据</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br></pre></td></tr></table></figure><h4 id="生成用户行为测试数据"><a href="#生成用户行为测试数据" class="headerlink" title="生成用户行为测试数据"></a>生成用户行为测试数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里实现不了，课程提供的用户行为生成接口，需要提供uid、课程订单，然后执行提前编写好的测试数据生成代码。</span><br></pre></td></tr></table></figure><h4 id="部署日志采集服务"><a href="#部署日志采集服务" class="headerlink" title="部署日志采集服务"></a>部署日志采集服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">部署日志采集服务，模拟埋点上报数据的流程，代码在db_data_warehouse中的data_collect这个子项目中，将这个子项目打成jar包，部署到bigdata04服务器中，并且启动此HTTP服务。</span><br><span class="line">对data_collect执行打包操作，在cmd命令下执行 mvn clean package -DskipTests</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">D:\IdeaProjects\db_data_warehouse\data_collect&gt;mvn clean package -DskipTests</span><br><span class="line">[INFO] Scanning for projects...</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br><span class="line">[INFO] Building data_collect 1.0-SNAPSHOT</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br><span class="line">[INFO] Replacing main artifact with repackaged archive</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04的&#x2F;data&#x2F;soft&#x2F;目录下创建data_collect目录</span><br><span class="line"></span><br><span class="line">1 [root@bigdata04 soft]# mkdir data_collect</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">然后把target目录下的 data_collect-1.0-SNAPSHOT.jar 上传到bigdata04的 &#x2F;data&#x2F;soft&#x2F;data_collect</span><br><span class="line">里面</span><br><span class="line">接着就可以启动这个项目了，这个其实就是一个web项目。</span><br><span class="line">为了后面使用方便，我在这里面写一个启动脚本</span><br><span class="line"></span><br><span class="line">[root@bigdata04 data_collect]# vi start.sh</span><br><span class="line">nohup java -jar data_collect-1.0-SNAPSHOT.jar &gt;&gt; nohup.out &amp;</span><br><span class="line">[root@bigdata04 data_collect]# sh start.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">确认是否成功启动</span><br><span class="line">[root@bigdata04 data_collect]# jps -ml</span><br><span class="line">1601 sun.tools.jps.Jps -ml</span><br><span class="line">1563 data_collect-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">获取到的数据格式是这样的：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302339866.png" alt="image-20230330233905222"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302339704.png" alt="image-20230330233917373"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先解析data属性的值，里面包含了多个用户的行为数据</span><br><span class="line">并且每个用户的行为数据中还包含了多种具体的行为操作，因为客户端在上报数据的时候不是产生一条就上报一条，这样效率太低了，一般都会批量上报，所以内层json串中还有一个data参数，data参数的值是一个JSONArray，里面包含一个用户的多种行为数据</span><br><span class="line"></span><br><span class="line">然后通过接口模拟上报数据，data_collect接口接收到数据之后，会对数据进行拆分，将包含了多个用户行为的数据拆开，打平，输出多条日志数据</span><br><span class="line">日志数据格式是这样的：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302343266.png" alt="image-20230330234319650"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">最终的日志数据会保存在data_collect这个日志采集服务所在的机器上，通过log4j记录在&#x2F;data&#x2F;log目录下面。</span><br><span class="line">去确认一下：</span><br><span class="line">[root@bigdata04 log]# ll</span><br><span class="line">total 32</span><br><span class="line">-rw-r--r--. 1 root root 20881 Jun 30 18:00 user_action.log</span><br><span class="line">[root@bigdata04 log]# head -1 user_action.log </span><br><span class="line">&#123;&quot;ver&quot;:&quot;3.4.1&quot;,&quot;display&quot;:&quot;1920x1080&quot;,&quot;osver&quot;:&quot;7.1.1&quot;,&quot;platform&quot;:1,&quot;uid&quot;:&quot;1000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302347512.png" alt="image-20230330234726391"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到这为止，用户行为数据就生成好了。</span><br></pre></td></tr></table></figure><h3 id="【服务端数据】商品订单相关数据"><a href="#【服务端数据】商品订单相关数据" class="headerlink" title="【服务端数据】商品订单相关数据"></a>【服务端数据】商品订单相关数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">接下来需要生成商品订单相关数据，这些数据都是存储在mysql中的</span><br><span class="line">注意：MySQL在这里我使用的版本是8.x</span><br><span class="line"></span><br><span class="line">相关表名为：</span><br><span class="line">订单表：user_order </span><br><span class="line">商品信息表：goods_info</span><br><span class="line">订单商品表：order_item</span><br><span class="line">商品类目码表：category_code</span><br><span class="line">订单收货表：order_delivery </span><br><span class="line">支付流水表：payment_flow</span><br><span class="line">用户收货地址表：user_addr</span><br><span class="line">用户信息表：user</span><br><span class="line">用户扩展表：user_extend</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302357855.png" alt="image-20230330235704680"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">powerdesigner这个软件设计的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先在MySQL中初始化数据库和表。</span><br><span class="line">使用这个脚本进行初始化： init_mysql_tables.sql</span><br><span class="line">初始化成功之后的效果如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310002439.png" alt="image-20230331000219996"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接下来需要向表中初始化数据。</span><br><span class="line">使用generate_data项目中的这个类：GenerateGoodsOrderData</span><br><span class="line"></span><br><span class="line">在具体执行之前需要先修改GenerateGoodsOrderData中的几个参数值</span><br><span class="line">(1)code的值</span><br><span class="line">(2)date的值</span><br><span class="line">(3)user_num的值</span><br><span class="line">(4)order_num的值</span><br><span class="line">(5)修改项目的resources目录下的db.properties文件</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面就可以执行GenerateGoodsOrderData向MySQL中初始化数据了。</span><br></pre></td></tr></table></figure><h2 id="采集数据"><a href="#采集数据" class="headerlink" title="采集数据"></a>采集数据</h2><h3 id="采集用户行为数据"><a href="#采集用户行为数据" class="headerlink" title="采集用户行为数据"></a>采集用户行为数据</h3><h4 id="配置Flume的Agent"><a href="#配置Flume的Agent" class="headerlink" title="配置Flume的Agent"></a>配置Flume的Agent</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据接收到以后，需要使用flume采集数据，按照act值的不同，将数据分目录存储</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">flume Agent配置内容如下：</span><br><span class="line">useraction-to-hdfs.conf</span><br><span class="line"></span><br><span class="line"># agent的名称是a1</span><br><span class="line"># 指定source组件、channel组件和Sink组件的名称</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line"># 配置source组件</span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;user_action.log</span><br><span class="line"># 配置拦截器</span><br><span class="line">a1.sources.r1.interceptors &#x3D; i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type &#x3D; regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i1.regex &#x3D; &quot;act&quot;:(\\d)</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers &#x3D; s1</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.name &#x3D; act</span><br><span class="line"># 配置channel组件</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"># 配置sink组件</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;%Y%m%d&#x2F;%&#123;a</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#增加文件前缀和后缀</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix &#x3D; .log</span><br><span class="line"># 把组件连接起来</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h4 id="开始采集数据"><a href="#开始采集数据" class="headerlink" title="开始采集数据"></a>开始采集数据</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310100507.png" alt="image-20230331010029253"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310101359.png" alt="image-20230331010115320"></p><h3 id="采集商品订单相关数据"><a href="#采集商品订单相关数据" class="headerlink" title="采集商品订单相关数据"></a>采集商品订单相关数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们需要将商品订单数据采集到HDFS里面，咱们前面分析过，在这里针对关系型数据库数据的采集我们使用Sqoop</span><br><span class="line">使用sqoop的导入功能，将MySQL中的数据导入到HDFS上面</span><br><span class="line">那首先我们来看一下Sqoop的使用，因为Sqoop主要是一个工具，所以我们就快速的学习一下。</span><br></pre></td></tr></table></figure><h4 id="数据采集方式"><a href="#数据采集方式" class="headerlink" title="数据采集方式"></a>数据采集方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">全量采集(数据量不大，每天都改变：一天采集一次；数据量不大，几十年都不变：只做一次全量采集)</span><br><span class="line"></span><br><span class="line">增量采集(数据量大，每天采集新增数据) </span><br><span class="line"></span><br><span class="line">hive不能对数据进行修改(比如mysql中的表的订单信息已改变，但hive中不支持修改)-&gt;解决：拉链表</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310122674.png" alt="image-20230331012207048"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">手机号脱名操作</span><br><span class="line">18315138177</span><br><span class="line">183xxxxx177</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库之用户行为数仓2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%932.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%932.html</id>
    <published>2023-03-30T12:27:10.000Z</published>
    <updated>2023-03-30T13:17:21.292Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十二周-综合项目-电商数据仓库之用户行为数仓2"><a href="#第十二周-综合项目-电商数据仓库之用户行为数仓2" class="headerlink" title="第十二周 综合项目:电商数据仓库之用户行为数仓2"></a>第十二周 综合项目:电商数据仓库之用户行为数仓2</h1><h2 id="电商数仓技术选型"><a href="#电商数仓技术选型" class="headerlink" title="电商数仓技术选型"></a>电商数仓技术选型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">咱们前面对项目的需求进行了分析，整体上来说是需要三个大的功能模块，那下面我们就来分析一下，想要实现这些功能模块，具体使用哪些技术框架比较合适</span><br></pre></td></tr></table></figure><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">首先是数据采集：</span><br><span class="line">咱们前面学习了Flume这个数据采集工具</span><br><span class="line">其实还有一些类似的数据采集工具，Logstash、FileBeat，这两个也可以实现数据采集</span><br><span class="line"></span><br><span class="line">那这三个日志采集工具我们需要如何选择呢？</span><br><span class="line">首先从性能消耗上面来说，Flume和Logstash的性能消耗差不多，都是基于JVM执行的，都是重量级的组件，支持多种数据源和目的地。</span><br><span class="line"></span><br><span class="line">FileBeat是一个只支持文件数据采集的工具，是一个轻量级组件，性能消耗比价低，它不是基于JVM执行的，它是使用go语言开发的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们在采集数据的时候可以分为两种情况</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302033047.png" alt="image-20230330203300260"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">第一种是把采集工具部署到产生数据的服务器上面</span><br><span class="line">web项目产生的日志数据直接保存在服务器上面，并且这个服务器的性能比较高，可以允许我在上面部署Flume数据采集工具，这样也不会对上面的web项目的稳定性产生什么影响。</span><br><span class="line"></span><br><span class="line">第二种是把采集工具部署在一个独立的服务器上面</span><br><span class="line">web项目产生的日志数据直接保存在服务器上面，但是这个服务器的性能一般，并且对web项目的稳定性要求非常高，如果让你在上面部署一个其它服务，这样这个服务器的稳定性就没办法保证了，进而也就无法保证web项目的稳定性了，所以这个时候可以选择在产生日志的时候使用埋点上报的方式，通过http接</span><br><span class="line">口把日志数据传输到日志接收服务器中</span><br><span class="line"></span><br><span class="line">那针对第一种情况肯定是要选择一个性能消耗比较低的数据采集工具，优先选择FileBeat</span><br><span class="line">针对第二种情况的话就不需要考虑性能消耗了，因为采集工具是在独立的机器上，不会影响web项目，这个时候我们需要考虑的就是采集工具的功能是否完整，因为在采集数据的时候可能需要对数据进行一些简单的处理，以及后期可能会输出到不同的存储介质中。</span><br><span class="line"></span><br><span class="line">Flume和Logstash都是支持多种输入、多种输出、以及都可以在采集数据的时候对数据做一些处理，那这个时候该如何选择呢？</span><br><span class="line">注意了，这个时候我们就要考虑如果后期采集工具出现了问题，或者我们需要自定义一些功能，维护成本高不高，Flume是使用java开发的，而Logstash是使用ruby开发的，由于我们都是java出身，所以考虑到后期的维护成本，Flume是最优的选择。</span><br><span class="line"></span><br><span class="line">在采集数据的时候，除了日志数据，有时候还需要采集一些业务系统的数据，这些数据一般保存在关系型数据库中，例如：MySQL</span><br><span class="line">我们也需要把这些数据采集过来，如果MySQL开启了binlog，那我们可以使用Flume采集</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">什么是MySQL的binlog呢？</span><br><span class="line">可以这样理解，MySQL对数据库的任何修改都会记录在binlog中，所以如果开启了binlog，那我们就可以使用Flume采集这个日志数据，就可以获取到MySQL中数据的变化了。</span><br><span class="line"></span><br><span class="line">但是目前我们的MySQL没有开启binlog，并且也没有打算开启binlog，那怎么办？</span><br><span class="line">因为我们这个需求不需要实时采集MySQL中的数据，所以不开启binlog也是没有问题的，Flume默认不支持直接采集MySQL中的数据，如果想要实现的话需要自定义Source，这样就比较麻烦了</span><br><span class="line"></span><br><span class="line">其实采集MySQL中的数据有一个比较常用的方式是通过Sqoop实现。</span><br><span class="line">Sqoop中有两大功能，数据导入和数据导出</span><br><span class="line"></span><br><span class="line">数据导入是指把关系型数据库中的数据导入HDFS中</span><br><span class="line">数据导出是指把HDFS中的数据导出到关系型数据库中</span><br><span class="line"></span><br><span class="line">我们后期在做一些报表的时候其实也是需要把数据仓库中的数据(APP层)导出到MySQL中的，所以在这选择Sqoop也是非常实用的。</span><br><span class="line"></span><br><span class="line">所以针对数据采集这块，我们主要选择了Flume和Sqoop</span><br></pre></td></tr></table></figure><h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据采集过来以后，由于我们后面要构建数据仓库，数据仓库是使用Hive实现，Hive的数据是存储在HDFS中的，所以我们把采集到的数据也直接存储到HDFS里面</span><br><span class="line">还有一点是后期在做一些数据报表的时候，是需要把数据仓库中的数据导出到MySQL中的，所以数据存储也需要使用到MySQL。</span><br></pre></td></tr></table></figure><h3 id="数据计算"><a href="#数据计算" class="headerlink" title="数据计算"></a>数据计算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在构建数据仓库的时候，我们前面说了，是使用Hive构建数据仓库，一般的数据处理通过SQL是可以搞定的，如果遇到了比较复杂的处理逻辑，可能还需要和外部的数据进行交互的，这个时候使用SQL就比较麻烦了，内置的函数有时候搞不定，还需要开发自定义函数</span><br><span class="line"></span><br><span class="line">针对复杂的数据清洗任务我们也可以考虑使用Spark进行处理。</span><br></pre></td></tr></table></figure><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在数据可视化层面，我们可以使用Hue(无法出图表)数据查询</span><br><span class="line"></span><br><span class="line">如果想实现写SQL直接出图表(简单的图表)zeppelin</span><br><span class="line">如果想定制开发图表(复杂图表)的话可以使用Echarts(百度开源的)之类的图表库，这个时候是需要我们自己开发数据接口实现的。</span><br></pre></td></tr></table></figure><h2 id="整体架构设计"><a href="#整体架构设计" class="headerlink" title="整体架构设计"></a>整体架构设计</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">技术选型搞定后，下面我们来看一下整体的架构设计</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302116171.png" alt="image-20230330211658080"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">我们采集的数据主要分为服务端数据和客户端数据</span><br><span class="line">什么是服务端数据，就是网站上的商品详情数据以及你下的订单信息之类的数据，这些数据都是在服务端存储着的，一般是存储在类似于MySQL之类的关系型数据库中，这些数据对事务性要求比较严格，所以会存放在关系型数据库中。</span><br><span class="line"></span><br><span class="line">什么是客户端数据呢，就是用户在网站或者app上的一些滑动、点击、浏览、停留时间之类的用户行为数据，这些数据会通过埋点直接上报，这些其实就是一些日志类型的数据了，这种类型的数据没有事务性要求，并且对数据的完整性要求也不是太高，就算丢一些数据，对整体结果影响也不大。</span><br><span class="line"></span><br><span class="line">针对服务端数据，在采集的时候，主要是通过Sqoop进行采集，按天采集，每天凌晨的时候把昨天的数据采集过来，存储到HDFS上面。</span><br><span class="line">针对客户端数据，会通过埋点上报到日志接收服务器中，其实这里面就是一个Http服务，埋点上报就是调用了这个Http服务，把日志数据传输过来，日志接收服务收到数据之后，会把数据落盘，存储到本地，记录为日志文件，然后通过Flume进行采集，将数据采集到HDFS上面，按天分目录存储。</span><br><span class="line"></span><br><span class="line">服务端数据和客户端数据都进入到HDFS之后，就需要对数据进行ETL，构建数据仓库了。</span><br><span class="line">数据仓库构建好了以后可以选择把一些需要报表展现的数据导出到MySQL中，最终在页面进行展现。</span><br></pre></td></tr></table></figure><h2 id="服务器资源规划"><a href="#服务器资源规划" class="headerlink" title="服务器资源规划"></a>服务器资源规划</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">整体架构分析好了，下面我们来分析一下，想要实现这个架构，服务器资源应该如何划分</span><br><span class="line">针对我们的测试环境：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302105852.png" alt="image-20230330210548415"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对生产环境，至少需要这些机器：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302106891.png" alt="image-20230330210649799"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">说明</span><br><span class="line">1：由于NameNode开启了HA，所以SecondaryNameNode进程就不需要了</span><br><span class="line">2：NameNode需要使用单独的机器，并且此机器的内存配置要比较高，建议128G</span><br><span class="line">3：DataNode和NodeManager需要部署在相同的集群上，这样可以实现数据本地化计算</span><br><span class="line">4：Hadoop Client需要部署在需要和Hadoop交互的机器上 </span><br><span class="line">5：数据接口服务器需要使用至少两台，为了实现负载均衡及故障转移，保证数据接收服务的稳定性</span><br><span class="line">6：Flume部署在日志服务器上面，便于采集本机保存的用户行为日志信息；还需要有单独的Flume机器，便于处理其它的日志采集需求</span><br><span class="line">7：Hive需要部署在所有业务机器上</span><br><span class="line">8：MySQL建议单独部署，至少两台，一主一备</span><br><span class="line">9：Sqoop需要部署在所有业务机器上</span><br><span class="line">10：Zeppelin可以单独部署在一台普通配置的机器上即可</span><br><span class="line">11：Azkaban建议至少使用三台，一主两从，这样可以保证一个从节点挂掉之后不影响定时任务的调度</span><br><span class="line">针对Hadoop集群的搭建在线上环境需要使用CDH或者HDP</span><br><span class="line">具体Hadoop集群需要使用多少台集群需要根据当前的数据规模来预估</span><br><span class="line">假设集群中的机器配置为8T，64 Core，128G</span><br><span class="line">1：如果每天会产生1T的日志数据，需要保存半年的历史数据： 1T*180天&#x3D;180T</span><br><span class="line">2：集群中的数据默认是3副本： 180T*3&#x3D;540T</span><br><span class="line">3：预留20%左右的空间： 540T&#x2F;0.8&#x3D;675T</span><br><span class="line">这样计算的话就需要675T&#x2F;8T&#x3D;85台服务器</span><br><span class="line">如果我们在数据仓库中对数据进行分层存储，这样数据会出现冗余，存储空间会再扩容1~2倍</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：没有必要一开始就上线全部的机器，我们可以前期上线30台，后面随着业务数据量的增长再去动态扩容机器即可。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%93.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%93.html</id>
    <published>2023-03-30T09:08:53.000Z</published>
    <updated>2023-03-30T13:08:58.808Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十二周-综合项目-电商数据仓库之用户数据行为数仓"><a href="#第十二周-综合项目-电商数据仓库之用户数据行为数仓" class="headerlink" title="第十二周 综合项目:电商数据仓库之用户数据行为数仓"></a>第十二周 综合项目:电商数据仓库之用户数据行为数仓</h1><h2 id="电商数据仓库效果展示"><a href="#电商数据仓库效果展示" class="headerlink" title="电商数据仓库效果展示"></a>电商数据仓库效果展示</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面我们来学习一个电商行业的数据仓库项目</span><br><span class="line">首先看一下项目效果</span><br><span class="line"></span><br><span class="line">本身我们这个数据仓库项目其实是一个纯后台项目，不过为了让大家能够更加直观的感受项目的效果，我们可以基于数据仓库中的数据统计一些指标进行展现。</span><br><span class="line">我们这个项目要讲的重点不是这个大屏，这个大屏只是一个效果，为了让大家感受更加直观一些而已，我们主要讲的是这些指标对应的底层数据是如何在数据仓库中一层一层构建的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301725400.png" alt="image-20230330172526095"></p><h3 id="项目的由来"><a href="#项目的由来" class="headerlink" title="项目的由来"></a>项目的由来</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下这个项目的由来，我们为什么要做这个数据仓库项目呢？或者说做这个数据仓库项目有什么意义吗？</span><br><span class="line"></span><br><span class="line">在工作中我们经常会遇到这些情况</span><br><span class="line">产品经理过来说，老王啊，你来看一下，为什么这个数据指标在不同的报表中统计的结果不一样呢？</span><br><span class="line">老王听到后，心里拔凉拔凉的，今晚又得加班了，约好的女朋友看电影估计又得泡汤了。</span><br><span class="line"></span><br><span class="line">举个例子：针对平台里面的用户下单数据：我们会在客户端记录一份，就是当用户通过网页或者app下单的时候，会触发一个行为，官方名词叫“埋点”，这个埋点对应的其实就是一个接口，当用户通过网页或者app下单的时候，就会触发这个埋点，然后埋点会上报这个数据，最终会把用户下单行为的数据记录下来，这份数据我们就称之为是客户端记录的数据。</span><br><span class="line"></span><br><span class="line">同时服务端也会在数据库中维护一份用户的下单数据。</span><br><span class="line">最终在做报表统计的时候：</span><br><span class="line">如果想要按照分钟级别实时计算，做一个用户消费金额的曲线图，我们一般会使用客户端实时上报的数据，用起来比较方便，但是通过客户端埋点上报的数据可能会有一些问题，有可能会丢数据，以及用户在点击下单按钮的时候可能由于网络异常导致下单失败了，但是这条行为数据却发送过来了，以及用户下单之后还可能会退款，所以这里面统计的数据指标会有一些偏差，不过偏差倒不是很大，如果只是想看一下当天用户消费总金额的一个实时趋势，其实这样做是没有什么问题的。但是你要是使用这份数据统计每天用户的消费总金额，那肯定是有问题的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">如果想要按照天来汇总每天用户的消费总金额，这种数据指标对数据的实时性没什么要求，但是对数据的准确度有很高要求，所以一般会使用数据库中的数据，因为数据库是有事务的，并且在统计的时候也可以排除掉用户退款的数据，这样统计出来的才是这一天用户真正的消费总金额。</span><br><span class="line">所以刚才产品经理反馈的问题很大概率是这个原因，当然也有可能是由于计算的口径不一样，因为不同的报表可能是由不同的需求人员提出来的，指标的计算公式也是有所差别的，甚至有的指标是上一任产品经理提出来的，每个人想要统计的指标是有一些区别的，所以在使用数据的时候就会遇到各种各样的问题。</span><br><span class="line"></span><br><span class="line">其实，归根结底，就是因为数据不统一，计算流程不统一导致的结果出现偏差。</span><br><span class="line">后来，老王经过一路追踪，发现，原来在统计这个指标的时候，两个报表使用的底层数据不是同一份，所以导致统计的指标有偏差，然后又给产品经理一顿解释，给产品经理解释完又赶紧打电话给女朋友解释，然后就没有然后了。</span><br><span class="line"></span><br><span class="line">经过这件事情之后，老王觉得，数据仓库的构建必须提上日程了</span><br><span class="line">通过构建企业级数据仓库，对企业中的所有数据进行整合，为企业各个部门提供统一的，规范的数据出口</span><br><span class="line">这样大家在使用数据的时候不需要每次都到各种地方去找数据，所有人在使用的时候都是基于相同的基础数据，这样计算出来的指标肯定是相同的。</span><br><span class="line"></span><br><span class="line">一个完善合理的数据仓库对于企业整体的数据管理是意义重大的，而数据仓库也是整个大数据系统中的重要一环，更高层次的数据分析、数据挖掘等工作都会基于数据仓库进行</span><br><span class="line">如果你的底层数据都没有规划好，那么上层的数据分析以及数据挖掘都是会受影响的。</span><br><span class="line">就像是我们盖房子，如果地基没有打牢，盖出来的房子肯定也是摇摇欲坠。</span><br><span class="line">所以说数据仓库对于一个中大型企业而言是至关重要的。</span><br><span class="line"></span><br><span class="line">那话又说回来了，如果你们公司刚起步，产品也是刚上线，这个时候你花大量的时间去搞数据仓库也是没有意义的，这个时候讲究的是快速迭代。</span><br><span class="line"></span><br><span class="line">只有说数据规模上来之后，数据仓库才是有意义的，并且也是必不可少的。</span><br></pre></td></tr></table></figure><h2 id="数据仓库前置技术"><a href="#数据仓库前置技术" class="headerlink" title="数据仓库前置技术"></a>数据仓库前置技术</h2><h3 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">咱们前面说了要构建一个数据仓库，那严格意义上来说，到底什么是数据仓库呢？</span><br><span class="line">咱们前面学习过Hive，说Hive其实就是一个数据仓库，可以这样理解，就是把Hive认为是一种技术，通过Hive这种技术可以实现数据仓库的建设。</span><br><span class="line"></span><br><span class="line">咱们这个项目中的数据仓库就是使用Hive构建的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">来看一下针对数据仓库的官方解释：</span><br><span class="line">数据仓库(Data Warehouse)是一个面向主题的、集成的、稳定的且随时间变化的数据集合，用于支持管理人员的决策</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">注意它里面的这几个特性：</span><br><span class="line"></span><br><span class="line">面向主题</span><br><span class="line">主题就是类型的意思。</span><br><span class="line">传统数据库主要是为应用程序进行数据处理，未必会按照同一主题存储数据；</span><br><span class="line">数据仓库侧重于数据分析工作，是按照主题存储的。</span><br><span class="line"></span><br><span class="line">这一点，类似于传统农贸市场与超市的区别</span><br><span class="line">市场里面，针对一个商贩，他卖的萝卜、白菜这些蔬菜以及水果会在一个摊位上；</span><br><span class="line">而超市里，蔬菜和水果是分开的，并且在蔬菜里面也会进行分类，不同类型的蔬菜放到不同的地方。</span><br><span class="line">也就是说，农贸市场里的菜(数据)是按照商贩(应用程序)去归类(存储)的，而超市里面则是按照蔬菜、水果的类型(同主题)归类的。</span><br><span class="line"></span><br><span class="line">集成</span><br><span class="line">传统数据库通常与某些特定的应用相关，数据库之间相互独立。而数据仓库中的数据是在对原有分散的数据库数据抽取、清理的基础上经过系统加工、汇总和整理得到的，必须消除源数据中的不一致性，以保证数据仓库内的信息是关于整个企业的一致的全局信息。</span><br><span class="line"></span><br><span class="line">稳定</span><br><span class="line">稳定说的是相对稳定</span><br><span class="line">传统数据库中的数据通常实时更新，数据根据需要及时发生变化。数据仓库的数据主要供企业决策分析使用，所涉及的数据操作主要是数据查询，一旦某个数据进入数据仓库以后，一般情况下将被长期保留，也就是数据仓库中一般有大量的查询操作，但修改和删除操作很少，通常只需要定期的加载、刷新。</span><br><span class="line"></span><br><span class="line">变化</span><br><span class="line">这里的变化说的是反映历史变化</span><br><span class="line">传统数据库主要关心当前某一个时间段内的数据，而数据仓库中的数据通常包含历史信息，它里面记录了企业从过去某一时间点(如开始应用数据仓库的时间)到目前的各个阶段的信息，通过这些信息，可以对企业的发展历程和未来趋势做出分析和预测。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">企业数据仓库的建设，是以现有企业业务系统和大量业务数据的积累为基础。数据仓库不是静态的概念，只有把信息及时交给需要这些信息的使用者，供他们做出改善其业务经营的决策，信息才能发挥作用，信息才有意义。而把信息加以整理归纳和重组，并及时提供给相应的管理决策人员，是数据仓库的根本任务。</span><br></pre></td></tr></table></figure><h3 id="数据仓库基础知识"><a href="#数据仓库基础知识" class="headerlink" title="数据仓库基础知识"></a>数据仓库基础知识</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在学习数据仓库之前我们先来看一些数据仓库的基础知识</span><br><span class="line">1：事实表、维度表</span><br><span class="line">2：数据库三范式</span><br><span class="line">3：维度建模模型：雪花模型、星型模型</span><br></pre></td></tr></table></figure><h4 id="事实表、维度表"><a href="#事实表、维度表" class="headerlink" title="事实表、维度表"></a>事实表、维度表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">首先我们来看两个名词：事实表和维度表</span><br><span class="line">什么是事实表呢？</span><br><span class="line">事实表是指保存了大量业务数据的表，或者说保存了一些真实的行为数据的表</span><br><span class="line">例如：销售商品所产生的订单数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301738183.png" alt="image-20230330173811777"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">什么是维度表呢？</span><br><span class="line">首先说一下什么是维度</span><br><span class="line">维度其实指的就是一个对象的属性或者特征，例如：时间维度，地理区域维度，年龄维度</span><br><span class="line">这是维度的概念。</span><br><span class="line">维度表里面存放的其实就是刚才我们所说的那些维度相关的信息。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301739060.png" alt="image-20230330173934481"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是事实表和维度表的特点，大家最起码要能区分出来一个表是事实表还是维度表，否则在工作中别人提到这两个概念你还是一脸懵，那就尴尬了。</span><br></pre></td></tr></table></figure><h4 id="数据库三范式"><a href="#数据库三范式" class="headerlink" title="数据库三范式"></a>数据库三范式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接下来我们需要复习一下数据库中的三范式的特性，不知道大家还有没有印象，这个属于数据库相关的知识，如果大家系统的学习过类似于MySQL之类的关系型数据库的话，这块应该是有一些印象的。</span><br><span class="line">其实严格意义上来说，关系型数据库的范式是有多种的</span><br><span class="line">第一范式(1NF)</span><br><span class="line">第二范式(2NF)</span><br><span class="line">第三范式(3NF)</span><br><span class="line">巴斯-科德范式(BCNF)</span><br><span class="line">第四范式(4NF)</span><br><span class="line">第五范式(5NF)</span><br><span class="line">不过后面那几种不太常见，数据库设计一般满足第三范式就足够了，所以在这我们就分析一下前三种范式</span><br></pre></td></tr></table></figure><h5 id="第一范式-1NF"><a href="#第一范式-1NF" class="headerlink" title="第一范式(1NF)"></a>第一范式(1NF)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">它的意思是说数据库表的每一列都是不可分割的原子数据项</span><br><span class="line">来看下面这个案例：</span><br><span class="line">这里面存储的是学生信息</span><br><span class="line">但是这里面的地址字段显然是不符合第一范式的，因为这里面的地址信息是可以拆分为省份+城市+街道信息的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301743386.png" alt="image-20230330174354885"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以针对这个字段进行拆分，让这个表满足第一范式</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301744439.png" alt="image-20230330174431140"></p><h5 id="第二范式-2NF"><a href="#第二范式-2NF" class="headerlink" title="第二范式(2NF)"></a>第二范式(2NF)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第二范式(2NF)表示在1NF的基础上，数据库表中每一列都和主键相关，不能只和主键的某一部分相关(针对联合主键而言)</span><br><span class="line">也就是说一个表中只能保存一种类型的数据，不可以把多种类型数据保存在同一张表中</span><br><span class="line">来看下面这个案例：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301747252.png" alt="image-20230330174638394"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个表里面除了存储的有学生的班级信息，还有学生的考试成绩信息</span><br><span class="line">根据我们刚才的分析，它是满足第一范式的，但是违背了第二范式，数据库表中的每一列并不是都和主键相关</span><br><span class="line">所以我们为了让这个表满足第二范式，可以这样拆分：</span><br><span class="line">拆成两个表，一个表里面保存学生的班级信息，一个表里面保存学生的考试成绩信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301752515.png" alt="image-20230330175233201"></p><h5 id="第三范式-3NF"><a href="#第三范式-3NF" class="headerlink" title="第三范式(3NF)"></a>第三范式(3NF)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">注意：满足第三范式（3NF）必须先满足第二范式（2NF）。</span><br><span class="line"></span><br><span class="line">第三范式(3NF): 要求一个数据库表中不包含已在其它表中包含的非主键字段</span><br><span class="line">就是说，表中的某些字段信息，如果能够被推导出来，就不应该单独的设计一个字段来存放(能尽量外键join就用外键join)。</span><br><span class="line"></span><br><span class="line">很多时候，我们为了满足第三范式往往会把一张表拆分成多张表</span><br><span class="line"></span><br><span class="line">来看下面这个案例</span><br><span class="line">针对刚才满足了第二范式的表，其实还可以进行拆分</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301750405.png" alt="image-20230330175033146"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这样就满足数据库的第三范式了。</span><br><span class="line">我们在这分析这三种范式有什么意义吗？不要着急，往下面看</span><br></pre></td></tr></table></figure><h4 id="数据仓库建模方式"><a href="#数据仓库建模方式" class="headerlink" title="数据仓库建模方式"></a>数据仓库建模方式</h4><h5 id="ER实体模型"><a href="#ER实体模型" class="headerlink" title="ER实体模型"></a>ER实体模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">数据仓库建模可以使用多种方式</span><br><span class="line">1：ER实体模型，这种模型其实就是满足数据库第三范式的模型，这就是刚才我们为什么要分析数据库中的三范式了。</span><br><span class="line"></span><br><span class="line">ER模型是数据库设计的理论基础，当前几乎所有的OLTP系统设计都采用ER模型建模的方式Bill Inom提出的数仓理论，推荐采用ER关系模型进行建模，不过这种方式在实际工作中不推荐使用。</span><br></pre></td></tr></table></figure><h5 id="维度建模模型"><a href="#维度建模模型" class="headerlink" title="维度建模模型"></a>维度建模模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Ralph Kimball提出的数仓理论中，提出了维度建模，将数据仓库中的表划分为事实表和维度表。</span><br><span class="line">基于事实表和维度表进行维度建模。</span><br><span class="line">维度建模通常又分为星型模型和雪花模型，一会我们详细分析这两种维度建模模型。</span><br><span class="line">维度建模是我们在构建数据仓库中常用的方式。</span><br></pre></td></tr></table></figure><h5 id="Data-Vault模型"><a href="#Data-Vault模型" class="headerlink" title="Data Vault模型"></a>Data Vault模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Data Vault是在ER模型的基础上衍生而来，模型设计的初衷是有效的组织基础数据层，使之易扩展、灵活的应对业务的变化，同时强调历史性、可追溯性和原子性，不要求对数据进行过度的一致性处理；并非针对分析场景所设计。</span><br></pre></td></tr></table></figure><h5 id="Anchor模型"><a href="#Anchor模型" class="headerlink" title="Anchor模型"></a>Anchor模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Anchor是对Data Vault模型做了更近一步的规范化处理，初衷是为了设计高度可扩展的模型，核心思想是所有的扩张只添加而不修改，于是设计出的模型基本变成了k-v结构的模型。</span><br><span class="line">Data Vault模型和Anchor模型，这两种模型大家知道就行了，很少使用，如果大家感兴趣的话可以到网上查阅相关资料了解一下。</span><br></pre></td></tr></table></figure><h4 id="维度建模模型-1"><a href="#维度建模模型-1" class="headerlink" title="维度建模模型"></a>维度建模模型</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面详细分析一下维度建模模型</span><br><span class="line"></span><br><span class="line">星型模型和雪花模型</span><br><span class="line">星型模型和雪花模型主要区别就是对维度表的拆分，</span><br><span class="line">对于雪花模型，维度表的设计更加规范，一般符合3NF；</span><br><span class="line">而星型模型，一般采用降维的操作，利用冗余来避免模型过于复杂，提高易用性和分析效率</span><br><span class="line">先来看一下星型模型</span><br></pre></td></tr></table></figure><h5 id="星型模型"><a href="#星型模型" class="headerlink" title="星型模型"></a>星型模型</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301805131.png" alt="image-20230330180534808"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里面的中间的订单表是事实表，外面的四个是维度表。</span><br><span class="line">这几个维度表，其实严格意义上来说，只能满足第二范式，是不满足第三范式的。</span><br><span class="line">但是这样的好处是查询效率比较高，在查询的时候不需要关联很多张表。</span><br><span class="line">缺点就是数据有冗余。</span><br><span class="line">使用这个五角星代表星型模型还是比较形象的，因为针对事实表周边的这些维度表，外层就没有其它的表了。</span><br></pre></td></tr></table></figure><h5 id="雪花模型"><a href="#雪花模型" class="headerlink" title="雪花模型"></a>雪花模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下雪花模型</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301811839.png" alt="image-20230330181059516"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个里面订单表是一个事实表，其余的都是维度表。</span><br><span class="line">针对商品维度表外层又拆分出来了一个商品类目的维度表，这样拆分之后其实就满足第三范式了，但是这样就变的复杂了，后期在获取商品维度数据的时候，还需要关联这个商品类目维度表。</span><br><span class="line">这里使用这个雪花代表雪花模型也是比较形象的，事实表周边会有一层维度表，这些维度表外层还可能会有多层维度表</span><br><span class="line">那接下里我们针对这两种模型的优缺点进行一个总结</span><br></pre></td></tr></table></figure><h5 id="星型模型-VS-雪花模型"><a href="#星型模型-VS-雪花模型" class="headerlink" title="星型模型 VS 雪花模型"></a>星型模型 VS 雪花模型</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">冗余：雪花模型符合业务逻辑设计，采用3NF设计，有效降低数据冗余；星型模型的维度表设计不符合3NF，反规范化，维度表之间不会直接相关，牺牲部分存储空间</span><br><span class="line"></span><br><span class="line">性能：雪花模型由于存在维度间的关联，采用3NF降低冗余，通常在使用过程中，需要连接更多的维度表，导致性能偏低；星型模型违反三范式，采用降维的操作将维度整合，以存储空间为代价有效降低维度表连接数，性能比雪花模型高</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们在实际工作中一般会选择哪种呢？</span><br><span class="line">在实际工作中我们多采用星型模型，因为数据仓库主要是侧重于做数据分析，对数据的查询性能要求比较高，所以星型模型是比较好的选择，在实际工工作中我们会尽可能的多构建一些宽表，提前把多种有关联的维度整合到一张表中，后期使用时就不需要多表关联了，比较方便，并且性能也高。</span><br></pre></td></tr></table></figure><h3 id="数据仓库分层"><a href="#数据仓库分层" class="headerlink" title="数据仓库分层"></a>数据仓库分层</h3><h4 id="为什么要分层"><a href="#为什么要分层" class="headerlink" title="为什么要分层"></a>为什么要分层</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">数据仓库在构建过程中通常都需要进行分层处理。业务不同，分层的技术处理手段也不同。对数据进行分层的一个主要原因就是希望在管理数据的时候，能对数据有一个更加清晰的掌控</span><br><span class="line">详细来讲，主要有下面几个原因：</span><br><span class="line">1. 清晰的数据结构：每一个分层的数据都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。</span><br><span class="line">2. 数据血缘追踪：简单来讲可以这样理解，我们最终给业务方呈现的是一个能直接使用的业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围，分层之后就很好定位问题，以及可以清晰的知道它的危害范围。</span><br><span class="line">3. 减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少重复计算。</span><br><span class="line">4. 把复杂问题简单化：将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。</span><br><span class="line">5. 屏蔽业务的影响，不必改一次业务就重新接入数据。</span><br></pre></td></tr></table></figure><h4 id="数据仓库分层设计"><a href="#数据仓库分层设计" class="headerlink" title="数据仓库分层设计"></a>数据仓库分层设计</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那针对我们这里的数据仓库该如何分层呢？</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301823028.png" alt="image-20230330182319581"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据仓库一般会分为4层</span><br><span class="line">1. ODS层：原始数据层，数据源中的数据，采集过来之后，原样保存。</span><br><span class="line">2. DWD层：明细数据层：这一层是对ODS层的数据进行清洗，解决一些数据质量问题和数据的完整度问题。</span><br><span class="line">3. DWS层：这一层是对DWD层的数据进行轻度聚合汇总，生成一系列的中间表，提升公共指标的复用性，减少重复加工，并且构建出来一些宽表，用于提供后续的业务查询。</span><br><span class="line">4. APP层：根据业务需要，由前面三层的数据统计而出的结果(一般会使用数据明细层，数据汇总层的数据)，可以直接提供查询展现，一般会把APP层的数据导出到MySQL中供线上系统使用，提供报表展示、数据监控及其它功能。也有公司把这层称为DM层。虽然名字不一样，但是性质是一样的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：针对DWD层在对数据进行清洗的时候，一般需要遵循以下原则</span><br><span class="line">1. 数据唯一性校验(通过数据采集工具采集的数据会存在重复的可能性)</span><br><span class="line">2. 数据完整性校验(采集的数据中可能会出现缺失字段的情况，针对缺失字段的数据建议直接丢掉，如果可以确定是哪一列缺失也可以进行补全，可以用同一列上的前一个数据来填补或者同一列上的后一个数据来填补，或者默认值)</span><br><span class="line">3. 数据合法性校验-1(针对数字列中出现了null、或者-之类的异常值，全部替换为一个特殊值，例如0或者-1，这个需要根据具体的业务场景而定)</span><br><span class="line">4. 数据合法性校验-2(针对部分字段需要校验数据的合法性，例如：用户的年龄，不能是负数)</span><br></pre></td></tr></table></figure><h4 id="数据仓库命名规范"><a href="#数据仓库命名规范" class="headerlink" title="数据仓库命名规范"></a>数据仓库命名规范</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">我们在使用Hive实现数据仓库的时候该如何体现这些层次？</span><br><span class="line">1. 针对数据仓库的每一层都在Hive中创建一个数据库，数据库的命名包含每一层的标识符</span><br><span class="line">例如：针对ODS层可以在Hive中创建数据库 ods_mall，把同一层的表都放到一个数据库里面，方便管理</span><br><span class="line">2. 针对每一层中的表名，在创建的时候可以使用每一层的标识符开头</span><br><span class="line">例如：针对ODS层，创建的表名为：ods_user，这样方便后期使用，只要看到表名就可以知道这个表示哪一层的了。</span><br><span class="line"></span><br><span class="line">针对一些临时表，我们可以在对应的分层中创建表名的时候，以_tmp结尾。</span><br><span class="line">针对一些备份的表，可以在表名后面添加_bak。</span><br></pre></td></tr></table></figure><h3 id="典型的数据仓库系统架构"><a href="#典型的数据仓库系统架构" class="headerlink" title="典型的数据仓库系统架构"></a>典型的数据仓库系统架构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下图是一个典型的企业数据仓库系统，通常包含数据源、数据存储与管理、数据的访问三个部分</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302018736.png" alt="image-20230330201757040"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">数据源部分负责采集各种日志数据、业务数据，以及一些文档资料，将我们需要的这些数据加载到Hive中，构建数据仓库</span><br><span class="line">数据仓库构建好了以后可以为很多服务提供数据支撑</span><br><span class="line"></span><br><span class="line">例如：做数据报表，做OLAP数据分析，以及在做用户画像和数据挖掘的时候都是需要使用到数据仓库中的数据的</span><br><span class="line"></span><br><span class="line">在实际工作中，数据仓库分为离线数据仓库和实时数据仓库</span><br><span class="line">我们这个项目主要分析离线数据仓库，因为到现阶段为止我们主要学习了离线计算相关的技术框架。</span><br></pre></td></tr></table></figure><h3 id="项目需求分析"><a href="#项目需求分析" class="headerlink" title="项目需求分析"></a>项目需求分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">通过刚才对一个典型的企业数据仓库系统架构进行分析，我们发现，想要开发一个完整的数据仓库系统，至少需要以下这几个功能模块</span><br><span class="line">1：数据采集平台，这个模块主要负责采集各种数据源的数据</span><br><span class="line">2：数据仓库，这个模块负责数据存储和管理</span><br><span class="line">3：数据报表，这个模块其实就是数据可视化展示了</span><br><span class="line"></span><br><span class="line">通过这三个模块可以实现数据采集，构建数据仓库，最后基于数据仓库中的数据实现上层应用，体现数据仓库的价值。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-Java程序基础 数组类型</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.html</id>
    <published>2023-03-29T14:30:59.000Z</published>
    <updated>2023-03-29T17:52:24.800Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="数组类型"><a href="#数组类型" class="headerlink" title="数组类型"></a>数组类型</h1><h2 id="法一"><a href="#法一" class="headerlink" title="法一"></a>法一</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">使用数组来表示“一组”int类型。代码如下：</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // <span class="number">5</span>位同学的成绩:</span><br><span class="line">        int[] ns = new int[<span class="number">5</span>];</span><br><span class="line">        ns[<span class="number">0</span>] = <span class="number">68</span>;</span><br><span class="line">        ns[<span class="number">1</span>] = <span class="number">79</span>;</span><br><span class="line">        ns[<span class="number">2</span>] = <span class="number">91</span>;</span><br><span class="line">        ns[<span class="number">3</span>] = <span class="number">85</span>;</span><br><span class="line">        ns[<span class="number">4</span>] = <span class="number">62</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  定义一个数组类型的变量，使用数组类型“类型[]”，例如，int[]。和单个基本类型变量不同，数组变量初始化必须使用new int[<span class="number">5</span>]表示创建一个可容纳<span class="number">5</span>个int元素的数组。</span><br><span class="line">  </span><br><span class="line">Java的数组有几个特点：</span><br><span class="line">  数组所有元素初始化为默认值，整型都是<span class="number">0</span>，浮点型是<span class="number">0.0</span>，布尔型是false；</span><br><span class="line">  数组一旦创建后，大小就不可改变。</span><br><span class="line">  要访问数组中的某一个元素，需要使用索引。数组索引从<span class="number">0</span>开始，例如，<span class="number">5</span>个元素的数组，索引范围是<span class="number">0</span>~<span class="number">4</span>。</span><br><span class="line">  可以修改数组中的某一个元素，使用赋值语句，例如，ns[<span class="number">1</span>] = <span class="number">79</span>;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">可以用数组变量.length获取数组大小</span><br><span class="line">数组是引用类型，在使用索引访问数组元素时，如果索引超出范围，运行时将报错</span><br></pre></td></tr></table></figure><h2 id="法二"><a href="#法二" class="headerlink" title="法二"></a>法二</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">也可以在定义数组时直接指定初始化的元素，这样就不必写出数组大小，而是由编译器自动推算数组大小。例如：</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // <span class="number">5</span>位同学的成绩:</span><br><span class="line">        int[] ns = new int[] &#123; <span class="number">68</span>, <span class="number">79</span>, <span class="number">91</span>, <span class="number">85</span>, <span class="number">62</span> &#125;;</span><br><span class="line">        System.out.println(ns.length); // 编译器自动推算数组大小为<span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">还可以进一步简写为：</span><br><span class="line">int[] ns = &#123; <span class="number">68</span>, <span class="number">79</span>, <span class="number">91</span>, <span class="number">85</span>, <span class="number">62</span> &#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意数组是引用类型，并且数组大小不可变。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HUNHEt" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%5CHUNHEt.md.png" alt="HUNHEt.md.png"></a></p><h2 id="字符串数组"><a href="#字符串数组" class="headerlink" title="字符串数组"></a>字符串数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如果数组元素不是基本类型，而是一个引用类型，那么，修改数组元素会有哪些不同？</span><br><span class="line">字符串是引用类型，因此我们先定义一个字符串数组：</span><br><span class="line"></span><br><span class="line">String[] names = &#123;</span><br><span class="line">    <span class="string">"ABC"</span>, <span class="string">"XYZ"</span>, <span class="string">"zoo"</span></span><br><span class="line">&#125;;xxxxxxxxxx 如果数组元素不是基本类型，而是一个引用类型，那么，修改数组元素会有哪些不同？字符串是引用类型，因此我们先定义一个字符串数组：String[] names = &#123;    <span class="string">"ABC"</span>, <span class="string">"XYZ"</span>, <span class="string">"zoo"</span>&#125;;</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HUNHEt" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%5CHUNHEt.md-16801002864109.png" alt="HUNHEt.md.png"></a></p><p><a href="https://imgtu.com/i/HUUBPf" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E6%95%B0%E7%BB%84%E7%B1%BB%E5%9E%8B%5CHUUBPf.md.png" alt="HUUBPf.md.png"></a></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数组是同一数据类型的集合，数组一旦创建后，大小就不可变；</span><br><span class="line"></span><br><span class="line">可以通过索引访问数组元素，但索引超出范围将报错；</span><br><span class="line"></span><br><span class="line">数组元素可以是值类型（如int）或引用类型（如String），但数组本身是引用类型；</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-Java程序基础 变量和数据类型</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html</id>
    <published>2023-03-29T14:24:52.000Z</published>
    <updated>2023-03-29T17:52:20.297Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="变量和数据类型"><a href="#变量和数据类型" class="headerlink" title="变量和数据类型"></a>变量和数据类型</h1><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Java中，变量分为两种：基本类型的变量和引用类型的变量。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">*先定义再应用*</span><br><span class="line">*可以一次性多个定义和赋值*</span><br><span class="line">*没有赋值，将自动赋默认值(基本数据类型)*</span><br><span class="line">*可以将一个基本数据类型变量赋值给另一个基本类型变量。不是指向同一个地址*</span><br></pre></td></tr></table></figure><h2 id="基本数据类型有"><a href="#基本数据类型有" class="headerlink" title="基本数据类型有"></a>基本数据类型有</h2><p><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5C202303292133989.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">~~~</span><br><span class="line">+ 整型 byte,short,int,long</span><br><span class="line">+ 浮点型 float,double</span><br><span class="line">   float要加上f或F，double可以省略</span><br><span class="line">+ 字符型 char</span><br><span class="line">  用单引号</span><br><span class="line">+ 布尔型 false,true</span><br><span class="line">~~~</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;不同的数据类型占用的字节数不一样。我们看一下Java基本数据类型占用的字节数</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HtXtqe" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5CHtXtqe.png" alt="HtXtqe.png"></a></p><h3 id="整型"><a href="#整型" class="headerlink" title="整型"></a>整型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对于整型类型，Java只定义了带符号的整型，因此，最高位的bit表示符号位（<span class="number">0</span>表示正数，<span class="number">1</span>表示负数）。各种整型能表示的最大范围如下</span><br><span class="line"></span><br><span class="line">byte：<span class="number">-128</span> ~ <span class="number">127</span></span><br><span class="line">short: <span class="number">-32768</span> ~ <span class="number">32767</span></span><br><span class="line">int: <span class="number">-2147483648</span> ~ <span class="number">2147483647</span></span><br><span class="line">long: <span class="number">-9223372036854775808</span> ~ <span class="number">9223372036854775807</span></span><br><span class="line"></span><br><span class="line">对于float类型，需要加上f后缀。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int i = <span class="number">2147483647</span>;</span><br><span class="line">        int i2 = <span class="number">-2147483648</span>;</span><br><span class="line">        int i3 = <span class="number">2</span>_000_000_000; // 加下划线更容易识别</span><br><span class="line">        int i4 = <span class="number">0xff0000</span>; // 十六进制表示的<span class="number">16711680</span></span><br><span class="line">        int i5 = <span class="number">0b1000000000</span>; // 二进制表示的<span class="number">512</span></span><br><span class="line">        long l = <span class="number">9000000000000000000L</span>; // long型的结尾需要加L</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">特别注意：同一个数的不同进制的表示是完全相同的，例如<span class="number">15</span>=<span class="number">0xf</span>＝<span class="number">0b1111</span></span><br></pre></td></tr></table></figure><h3 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a>浮点型</h3><p><code>因为小数用科学计数法表示的时候，小数点是可以“浮动”的,所以称为浮点数</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">float f1 = <span class="number">3.14</span>f;</span><br><span class="line">float f2 = <span class="number">3.14e38</span>f; // 科学计数法表示的<span class="number">3.14</span>x10^<span class="number">38</span></span><br><span class="line">double d = <span class="number">1.79e308</span>;</span><br><span class="line">double d2 = <span class="number">-1.79e308</span>;</span><br><span class="line">double d3 = <span class="number">4.9e-324</span>; // 科学计数法表示的<span class="number">4.9</span>x10^<span class="number">-324</span></span><br></pre></td></tr></table></figure><h3 id="布尔类型"><a href="#布尔类型" class="headerlink" title="布尔类型"></a>布尔类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">布尔类型boolean只有true和false两个值，布尔类型总是关系运算的计算结果</span><br><span class="line"></span><br><span class="line">Java语言对布尔类型的存储并没有做规定，因为理论上存储布尔类型只需要1 bit，但是通常JVM内部会把boolean表示为4字节整数</span><br></pre></td></tr></table></figure><h3 id="字符类型"><a href="#字符类型" class="headerlink" title="字符类型"></a>字符类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">字符类型char表示一个字符。Java的char类型除了可表示标准的ASCII外，还可以表示一个Unicode字符：</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        char a = <span class="string">'A'</span>;</span><br><span class="line">        char zh = <span class="string">'中'</span>;</span><br><span class="line">        System.out.println(a);</span><br><span class="line">        System.out.println(zh);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意char类型使用单引号&#39;，且仅有一个字符，要和双引号&quot;的字符串类型区分开。</span><br></pre></td></tr></table></figure><h2 id="引用类型"><a href="#引用类型" class="headerlink" title="引用类型"></a>引用类型</h2><p><em>除了上述基本类型的变量，剩下的都是引用类型</em></p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">引用类型最常用的就是String字符串：</span><br><span class="line"></span><br><span class="line">String s &#x3D; &quot;hello&quot;;</span><br><span class="line"></span><br><span class="line">引用类型的变量类似于C语言的指针，它内部存储一个“地址”，指向某个对象在内存的位置，后续我们介绍类的概念时会详细讨论。</span><br></pre></td></tr></table></figure><h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">定义变量的时候，如果加上final修饰符，这个变量就变成了常量</span><br><span class="line"></span><br><span class="line">final double PI = <span class="number">3.14</span>; // PI是一个常量</span><br><span class="line">double r = <span class="number">5.0</span>;</span><br><span class="line">double area = PI * r * r;</span><br><span class="line">PI = <span class="number">300</span>; // compile error!</span><br><span class="line"></span><br><span class="line">常量在定义时进行初始化后就不可再次赋值，再次赋值会导致编译错误。</span><br><span class="line">根据习惯，常量名通常全部大写。</span><br></pre></td></tr></table></figure><h3 id="var关键字"><a href="#var关键字" class="headerlink" title="var关键字"></a>var关键字</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">定义变量时，变量类型太长，可以用var</span><br><span class="line">StringBuilder sb = new StringBuilder();</span><br><span class="line"></span><br><span class="line">这个时候，如果想省略变量类型，可以使用var关键字:</span><br><span class="line">var sb = new StringBuilder();</span><br><span class="line">编译器会根据赋值语句自动推断出变量sb的类型是StringBuilder</span><br></pre></td></tr></table></figure><h2 id="变量的作用范围"><a href="#变量的作用范围" class="headerlink" title="变量的作用范围"></a>变量的作用范围</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">定义变量时，要遵循作用域最小化原则，尽量将变量定义在尽可能小的作用域，并且，不要重复使用变量名。</span><br></pre></td></tr></table></figure><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Java提供了两种变量类型：基本类型和引用类型</span><br><span class="line">基本类型包括整型，浮点型，布尔型，字符型。</span><br><span class="line">变量可重新赋值，等号是赋值语句，不是数学意义的等号。</span><br><span class="line">常量在初始化后不可重新赋值，使用常量便于理解程序意图。</span><br></pre></td></tr></table></figure><h2 id="整数运算"><a href="#整数运算" class="headerlink" title="整数运算"></a>整数运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">整数的数值表示不但是精确的，而且整数运算永远是精确的，即使是除法也是精确的，因为两个整数相除只能得到结果的整数部分</span><br><span class="line"></span><br><span class="line">int x = <span class="number">12345</span> / <span class="number">67</span>; // <span class="number">184</span></span><br><span class="line">求余运算使用%：</span><br><span class="line"></span><br><span class="line">int y = <span class="number">12345</span> % <span class="number">67</span>; // <span class="number">12345</span>÷<span class="number">67</span>的余数是<span class="number">17</span></span><br><span class="line">特别注意：整数的除法对于除数为<span class="number">0</span>时运行时将报错，但编译不会报错</span><br></pre></td></tr></table></figure><h3 id="溢出"><a href="#溢出" class="headerlink" title="溢出"></a>溢出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要特别注意，整数由于存在范围限制，如果计算结果超出了范围，就会产生溢出，而溢出不会出错，却会得到一个奇怪的结果</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int x = <span class="number">2147483640</span>;</span><br><span class="line">        int y = <span class="number">15</span>;</span><br><span class="line">        int sum = x + y;</span><br><span class="line">        System.out.println(sum); // <span class="number">-2147483641</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="number">0111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1111</span> <span class="number">1000</span></span><br><span class="line">+ <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">1111</span></span><br><span class="line">-----------------------------------------</span><br><span class="line">  <span class="number">1000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0000</span> <span class="number">0111</span></span><br><span class="line">由于最高位计算结果为<span class="number">1</span>，因此，加法结果变成了一个负数</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">还有一种简写的运算符，即+=，-=，*=，/=，它们的使用方法如下：</span><br><span class="line"></span><br><span class="line">n += <span class="number">100</span>; // <span class="number">3409</span>, 相当于 n = n + <span class="number">100</span>;</span><br><span class="line">n -= <span class="number">100</span>; // <span class="number">3309</span>, 相当于 n = n - <span class="number">100</span>;</span><br></pre></td></tr></table></figure><h3 id="自增-自减"><a href="#自增-自减" class="headerlink" title="自增/自减"></a>自增/自减</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">++</span><br><span class="line"></span><br><span class="line">--</span><br><span class="line"></span><br><span class="line">*写在变量前面和后面是不同的，前面(先加减在运算)，后面(先运算再加减)*</span><br></pre></td></tr></table></figure><h3 id="移位运算符"><a href="#移位运算符" class="headerlink" title="移位运算符"></a>移位运算符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">*数值的最高位是一个符号位*</span><br><span class="line">1.</span><br><span class="line">&gt;&gt;: 右位移</span><br><span class="line">int n &#x3D; 7;       &#x2F;&#x2F; 00000000 00000000 00000000 00000111 &#x3D; 7</span><br><span class="line">int a &#x3D; n &gt;&gt; 1;  &#x2F;&#x2F; 00000000 00000000 00000000 00000011 &#x3D; 3</span><br><span class="line">int b &#x3D; n &gt;&gt; 2;  &#x2F;&#x2F; 00000000 00000000 00000000 00000001 &#x3D; 1</span><br><span class="line">int c &#x3D; n &gt;&gt; 3;  &#x2F;&#x2F; 00000000 00000000 00000000 00000000 &#x3D; 0</span><br><span class="line">如果对一个负数进行右移，最高位的1不动，结果仍然是一个负数：</span><br><span class="line">int n &#x3D; -536870912;</span><br><span class="line">int a &#x3D; n &gt;&gt; 1;  &#x2F;&#x2F; 11110000 00000000 00000000 00000000 &#x3D; -268435456</span><br><span class="line">int b &#x3D; n &gt;&gt; 2;  &#x2F;&#x2F; 11111000 00000000 00000000 00000000 &#x3D; -134217728</span><br><span class="line">int c &#x3D; n &gt;&gt; 28; &#x2F;&#x2F; 11111111 11111111 11111111 11111110 &#x3D; -2</span><br><span class="line">int d &#x3D; n &gt;&gt; 29; &#x2F;&#x2F; 11111111 11111111 11111111 11111111 &#x3D; -1</span><br><span class="line"></span><br><span class="line">2. </span><br><span class="line">&lt;&lt;: 左位移</span><br><span class="line">int n &#x3D; 7;       &#x2F;&#x2F; 00000000 00000000 00000000 00000111 &#x3D; 7</span><br><span class="line">int a &#x3D; n &lt;&lt; 1;  &#x2F;&#x2F; 00000000 00000000 00000000 00001110 &#x3D; 14</span><br><span class="line">int b &#x3D; n &lt;&lt; 2;  &#x2F;&#x2F; 00000000 00000000 00000000 00011100 &#x3D; 28</span><br><span class="line">int c &#x3D; n &lt;&lt; 28; &#x2F;&#x2F; 01110000 00000000 00000000 00000000 &#x3D; 1879048192</span><br><span class="line">int d &#x3D; n &lt;&lt; 29; &#x2F;&#x2F; 11100000 00000000 00000000 00000000 &#x3D; -536870912</span><br><span class="line"></span><br><span class="line">*上面两种不会改变符号位*</span><br><span class="line"></span><br><span class="line">3.无符号的右移运算</span><br><span class="line">使用&gt;&gt;&gt;，它的特点是不管符号位，右移后高位总是补0，因此，对一个负数进行&gt;&gt;&gt;右移，它会变成正数，原因是最高位的1变成了0</span><br><span class="line">int n &#x3D; -536870912;</span><br><span class="line">int a &#x3D; n &gt;&gt;&gt; 1;  &#x2F;&#x2F; 01110000 00000000 00000000 00000000 &#x3D; 1879048192</span><br><span class="line">int b &#x3D; n &gt;&gt;&gt; 2;  &#x2F;&#x2F; 00111000 00000000 00000000 00000000 &#x3D; 939524096</span><br><span class="line">int c &#x3D; n &gt;&gt;&gt; 29; &#x2F;&#x2F; 00000000 00000000 00000000 00000111 &#x3D; 7</span><br><span class="line">int d &#x3D; n &gt;&gt;&gt; 31; &#x2F;&#x2F; 00000000 00000000 00000000 00000001 &#x3D; 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">对byte和short类型进行移位时，会首先转换为int再进行位移。</span><br><span class="line">仔细观察可发现，左移实际上就是不断地×2，右移实际上就是不断地÷2</span><br></pre></td></tr></table></figure><h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">位运算是按位进行与、或、非和异或的运算</span><br><span class="line"></span><br><span class="line">&amp;(与): 同1才为1</span><br><span class="line"></span><br><span class="line">|(或): 有1则为0</span><br><span class="line"></span><br><span class="line">~(非)： 01互换</span><br><span class="line"></span><br><span class="line">^(异或): 不同才为1</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对两个整数进行位运算，实际上就是按位对齐，然后依次对每一位进行运算。</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int i = <span class="number">167776589</span>; // <span class="number">00001010</span> <span class="number">00000000</span> <span class="number">00010001</span> <span class="number">01001101</span></span><br><span class="line">        int n = <span class="number">167776512</span>; // <span class="number">00001010</span> <span class="number">00000000</span> <span class="number">00010001</span> <span class="number">00000000</span></span><br><span class="line">        System.out.println(i &amp; n); // <span class="number">167776512</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运算优先级"><a href="#运算优先级" class="headerlink" title="运算优先级"></a>运算优先级</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在Java的计算表达式中，运算优先级从高到低依次是：</span><br><span class="line"></span><br><span class="line">()</span><br><span class="line">! ~ ++ --</span><br><span class="line">* / %</span><br><span class="line">+ -</span><br><span class="line">&lt;&lt; &gt;&gt; &gt;&gt;&gt;</span><br><span class="line">&amp;</span><br><span class="line">|</span><br><span class="line">+= -= *= /=</span><br><span class="line"></span><br><span class="line">记不住也没关系，只需要加括号就可以保证运算的优先级正确</span><br></pre></td></tr></table></figure><h3 id="类型自动提升-整与整"><a href="#类型自动提升-整与整" class="headerlink" title="类型自动提升(整与整)"></a>类型自动提升(整与整)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在运算过程中，如果参与运算的两个数类型不一致，那么计算结果为较大类型的整型。例如，short和int计算，结果总是int，原因是short首先自动被转型为int</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        short s = <span class="number">1234</span>;</span><br><span class="line">        int i = <span class="number">123456</span>;</span><br><span class="line">        int x = s + i; // s自动转型为int</span><br><span class="line">        short y = s + i; // 编译错误!</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="强制转换-整与整"><a href="#强制转换-整与整" class="headerlink" title="强制转换(整与整)"></a>强制转换(整与整)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">也可以将结果强制转型，即将大范围的整数转型为小范围的整数。强制转型使用(类型)，例如，将int强制转型为short：</span><br><span class="line"></span><br><span class="line">int i &#x3D; 12345;</span><br><span class="line">short s &#x3D; (short) i; &#x2F;&#x2F; 12345</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">要注意，超出范围的强制转型会得到错误的结果，原因是转型时，int的两个高位字节直接被扔掉，仅保留了低位的两个字节</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int i1 = <span class="number">1234567</span>;</span><br><span class="line">        short s1 = (short) i1; // <span class="number">-10617</span></span><br><span class="line">        System.out.println(s1);</span><br><span class="line">        int i2 = <span class="number">12345678</span>; //short <span class="number">32767</span></span><br><span class="line">        short s2 = (short) i2; // <span class="number">24910</span></span><br><span class="line">        System.out.println(s2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="number">-10617</span></span><br><span class="line"><span class="number">24910</span></span><br><span class="line"></span><br><span class="line">// 因此，强制转型的结果很可能是错的</span><br></pre></td></tr></table></figure><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">整数运算的结果永远是精确的；</span><br><span class="line">运算结果会自动提升；</span><br><span class="line">可以强制转型，但超出范围的强制转型会得到错误的结果；</span><br><span class="line">应该选择合适范围的整型（int或long），没有必要为了节省内存而使用byte和short进行整数运算。</span><br></pre></td></tr></table></figure><h2 id="浮点数计算"><a href="#浮点数计算" class="headerlink" title="浮点数计算"></a>浮点数计算</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">*无法精确表示数值，不能做移位和位运算*</span><br><span class="line">由于浮点数存在运算误差，所以比较两个浮点数是否相等常常会出现错误的结果。正确的比较方法是判断两个浮点数之差的绝对值是否小于一个很小的数</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 比较x和y是否相等，先计算其差的绝对值:</span><br><span class="line">double r = Math.abs(x - y);</span><br><span class="line">// 再判断绝对值是否足够小:</span><br><span class="line"><span class="keyword">if</span> (r &lt; <span class="number">0.00001</span>) &#123;</span><br><span class="line">    // 可以认为相等</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    // 不相等</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="类型提升"><a href="#类型提升" class="headerlink" title="类型提升"></a>类型提升</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果参与运算的两个数其中一个是整型，那么整型可以自动提升到浮点型</span><br></pre></td></tr></table></figure><h3 id="溢出-1"><a href="#溢出-1" class="headerlink" title="溢出"></a>溢出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">*整数在除零时编译时不出错，运行时出错*</span><br><span class="line">*浮点数除零不会报错，会返回特殊值：*</span><br><span class="line"></span><br><span class="line">NaN:<span class="keyword">not</span> a number</span><br><span class="line">Infinity:无穷大</span><br><span class="line">-Infinity:负无穷大</span><br><span class="line"></span><br><span class="line">double d1 = <span class="number">0.0</span> / <span class="number">0</span>; // NaN</span><br><span class="line">double d2 = <span class="number">1.0</span> / <span class="number">0</span>; // Infinity</span><br><span class="line">double d3 = <span class="number">-1.0</span> / <span class="number">0</span>; // -Infinity</span><br><span class="line"></span><br><span class="line">这三种特殊值在实际运算中很少碰到，我们只需要了解即可</span><br></pre></td></tr></table></figure><h3 id="强制转换-整与浮"><a href="#强制转换-整与浮" class="headerlink" title="强制转换(整与浮)"></a>强制转换(整与浮)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以将浮点数强制转型为整数。在转型时，浮点数的小数部分会被丢掉。如果转型后超过了整型能表示的最大范围，将返回整型的最大值。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int n1 = (int) <span class="number">12.3</span>; // <span class="number">12</span></span><br><span class="line">int n2 = (int) <span class="number">12.7</span>; // <span class="number">12</span></span><br><span class="line">int n2 = (int) <span class="number">-12.7</span>; // <span class="number">-12</span></span><br><span class="line">int n3 = (int) (<span class="number">12.7</span> + <span class="number">0.5</span>); // <span class="number">13</span></span><br><span class="line">int n4 = (int) <span class="number">1.2e20</span>; // <span class="number">2147483647</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">如果要进行四舍五入，可以对浮点数加上<span class="number">0.5</span>再强制转型</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        double d = <span class="number">2.6</span>;</span><br><span class="line">        int n = (int) (d + <span class="number">0.5</span>);</span><br><span class="line">        System.out.println(n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">浮点数常常无法精确表示，并且浮点数的运算结果可能有误差；</span><br><span class="line">比较两个浮点数通常比较它们的差的绝对值是否小于一个特定值；</span><br><span class="line">整型和浮点型运算时，整型会自动提升为浮点型；</span><br><span class="line">可以将浮点型强制转为整型，但超出范围后将始终返回整型的最大值。</span><br></pre></td></tr></table></figure><h2 id="布尔运算"><a href="#布尔运算" class="headerlink" title="布尔运算"></a>布尔运算</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">对于布尔类型boolean，永远只有true和false两个值。</span><br><span class="line">布尔运算是一种关系运算，包括以下几类：</span><br><span class="line"></span><br><span class="line">比较运算符：&gt;，&gt;&#x3D;，&lt;，&lt;&#x3D;，&#x3D;&#x3D;，!&#x3D;</span><br><span class="line">与运算 &amp;&amp;</span><br><span class="line">或运算 ||</span><br><span class="line">非运算 !</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">boolean isGreater = <span class="number">5</span> &gt; <span class="number">3</span>; // true</span><br><span class="line">int age = <span class="number">12</span>;</span><br><span class="line">boolean isZero = age == <span class="number">0</span>; // false</span><br><span class="line">boolean isNonZero = !isZero; // true</span><br><span class="line">boolean isAdult = age &gt;= <span class="number">18</span>; // false</span><br><span class="line">boolean isTeenager = age &gt;<span class="number">6</span> &amp;&amp; age &lt;<span class="number">18</span>; // true</span><br></pre></td></tr></table></figure><h3 id="关系运算符的优先级"><a href="#关系运算符的优先级" class="headerlink" title="关系运算符的优先级"></a>关系运算符的优先级</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">!</span><br><span class="line">&gt;，&gt;&#x3D;，&lt;，&lt;&#x3D;</span><br><span class="line">&#x3D;&#x3D;，!&#x3D;</span><br><span class="line">&amp;&amp;</span><br><span class="line">||</span><br></pre></td></tr></table></figure><h3 id="短路运算"><a href="#短路运算" class="headerlink" title="短路运算"></a>短路运算</h3><h4 id="true-amp-amp-任意"><a href="#true-amp-amp-任意" class="headerlink" title="true&amp;&amp;任意"></a>true&amp;&amp;任意</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">布尔运算的一个重要特点是短路运算。如果一个布尔运算的表达式能提前确定结果，则后续的计算不再执行，直接返回结果。</span><br><span class="line"></span><br><span class="line">因为false &amp;&amp; x的结果总是false，无论x是true还是false，因此，与运算在确定第一个值为false后，不再继续计算，而是直接返回false</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        boolean b &#x3D; 5 &lt; 3;</span><br><span class="line">        boolean result &#x3D; b &amp;&amp; (5 &#x2F; 0 &gt; 0);</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">false</span><br><span class="line"></span><br><span class="line">如果没有短路运算，&amp;&amp;后面的表达式会由于除数为0而报错，但实际上该语句并未报错，原因在于与运算是短路运算符，提前计算出了结果false</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HNRoOx" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5CHNRoOx.png" alt="HNRoOx.png"></a></p><h4 id="true-任意"><a href="#true-任意" class="headerlink" title="true || 任意"></a>true || 任意</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">类似的，对于||运算，只要能确定第一个值为true，后续计算也不再进行，而是直接返回true：</span><br><span class="line"></span><br><span class="line">boolean result &#x3D; true || (5 &#x2F; 0 &gt; 0); &#x2F;&#x2F; true</span><br></pre></td></tr></table></figure><h4 id="三元运算符"><a href="#三元运算符" class="headerlink" title="三元运算符"></a>三元运算符</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Java还提供一个三元运算符b ? x : y，它根据第一个布尔表达式的结果，分别返回后续两个表达式之一的计算结果</span><br><span class="line"></span><br><span class="line">b为true返回x; b为false返回y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意到三元运算b ? x : y会首先计算b，如果b为true，则只计算x，否则，只计算y。此外，x和y的类型必须相同，因为返回值不是boolean，而是x和y之一。</span><br></pre></td></tr></table></figure><h4 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">与运算和或运算是短路运算；</span><br><span class="line">三元运算b ? x : y后面的类型必须相同，三元运算也是“短路运算”，只计算x或y。</span><br></pre></td></tr></table></figure><h2 id="字符和字符串"><a href="#字符和字符串" class="headerlink" title="字符和字符串"></a>字符和字符串</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Java中，字符和字符串是两个不同的类型</span><br></pre></td></tr></table></figure><h3 id="字符"><a href="#字符" class="headerlink" title="字符"></a>字符</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">字符类型char是基本数据类型，它是character的缩写。一个char保存一个Unicode字符</span><br><span class="line"></span><br><span class="line">因为Java在内存中总是使用Unicode表示字符，所以，一个英文字符和一个中文字符都用一个char类型表示，它们都占用两个字节。要显示一个字符的Unicode编码，只需将char类型直接赋值给int类型即可：</span><br><span class="line"></span><br><span class="line">int n1 &#x3D; &#39;A&#39;; &#x2F;&#x2F; 字母“A”的Unicodde编码是65</span><br><span class="line">int n2 &#x3D; &#39;中&#39;; &#x2F;&#x2F; 汉字“中”的Unicode编码是20013</span><br><span class="line">还可以直接用转义字符\u+Unicode编码来表示一个字符：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 注意是十六进制:</span><br><span class="line">char c3 &#x3D; &#39;\u0041&#39;; &#x2F;&#x2F; &#39;A&#39;，因为十六进制0041 &#x3D; 十进制65</span><br><span class="line">char c4 &#x3D; &#39;\u4e2d&#39;; &#x2F;&#x2F; &#39;中&#39;，因为十六进制4e2d &#x3D; 十进制20013</span><br></pre></td></tr></table></figure><h3 id="字符串-1"><a href="#字符串-1" class="headerlink" title="字符串"></a>字符串</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">和char类型不同，字符串类型String是引用类型，我们用双引号<span class="string">"..."</span>表示字符串。一个字符串可以存储<span class="number">0</span>个到任意个字符：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">""</span>; // 空字符串，包含<span class="number">0</span>个字符</span><br><span class="line">String s1 = <span class="string">"A"</span>; // 包含一个字符</span><br><span class="line">String s2 = <span class="string">"ABC"</span>; // 包含<span class="number">3</span>个字符</span><br><span class="line">String s3 = <span class="string">"中文 ABC"</span>; // 包含<span class="number">6</span>个字符，其中有一个空格</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">因为字符串使用双引号<span class="string">"..."</span>表示开始和结束，那如果字符串本身恰好包含一个<span class="string">"字符怎么表示？例如，"</span>abc<span class="string">"xyz"</span>，编译器就无法判断中间的引号究竟是字符串的一部分还是表示字符串结束。这个时候，我们需要借助转义字符\：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"abc\"xyz"</span>; // 包含<span class="number">7</span>个字符: a, b, c, <span class="string">", x, y, z</span></span><br><span class="line"><span class="string">因为\是转义字符，所以，两个\\表示一个\字符：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">String s = "</span>abc\\xyz<span class="string">"; // 包含7个字符: a, b, c, \, x, y, z</span></span><br><span class="line"><span class="string">常见的转义字符包括：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">\" 表示字符"</span></span><br><span class="line">\<span class="string">' 表示字符'</span></span><br><span class="line">\\ 表示字符\</span><br><span class="line">\n 表示换行符</span><br><span class="line">\r 表示回车符</span><br><span class="line">\t 表示Tab</span><br><span class="line">\u<span class="comment">#### 表示一个Unicode编码的字符</span></span><br><span class="line">例如：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"ABC\n\u4e2d\u6587"</span>; // 包含<span class="number">6</span>个字符: A, B, C, 换行符, 中, 文</span><br></pre></td></tr></table></figure><h3 id="字符串连接"><a href="#字符串连接" class="headerlink" title="字符串连接"></a>字符串连接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Java的编译器对字符串做了特殊照顾，可以使用+连接任意字符串和其他数据类型，这样极大地方便了字符串的处理。例如：</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        String s1 = <span class="string">"Hello"</span>;</span><br><span class="line">        String s2 = <span class="string">"world"</span>;</span><br><span class="line">        String s = s1 + <span class="string">" "</span> + s2 + <span class="string">"!"</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">如果用+连接字符串和其他数据类型，会将其他数据类型先自动转型为字符串，再连接：</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int age = <span class="number">25</span>;</span><br><span class="line">        String s = <span class="string">"age is "</span> + age;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        // 请将下面一组int值视为字符的Unicode码，把它们拼成一个字符串：</span><br><span class="line">        int a = <span class="number">72</span>;</span><br><span class="line">        int b = <span class="number">105</span>;</span><br><span class="line">        int c = <span class="number">65281</span>;</span><br><span class="line">        // FIXME</span><br><span class="line"> String s = <span class="string">""</span>+<span class="string">'\u0048'</span> +<span class="string">'\u0069'</span>  + <span class="string">'\uff01'</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class array &#123;    </span><br><span class="line">  public static void main(String[] args)&#123;        </span><br><span class="line">    int a = <span class="number">72</span>;        </span><br><span class="line">    int b = <span class="number">105</span>;        </span><br><span class="line">    int c = <span class="number">65281</span>;        </span><br><span class="line">    // FIXME:        </span><br><span class="line">    String s = <span class="string">""</span>+(char)a + (char)b + (char)c;        </span><br><span class="line">    System.out.println(s);    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="多行字符串"><a href="#多行字符串" class="headerlink" title="多行字符串"></a>多行字符串</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">如果我们要表示多行字符串，使用+号连接会非常不方便：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"first line \n"</span></span><br><span class="line">         + <span class="string">"second line \n"</span></span><br><span class="line">         + <span class="string">"end"</span>;</span><br><span class="line">从Java <span class="number">13</span>开始，字符串可以用<span class="string">"""..."""</span>表示多行字符串（Text Blocks）了</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        String s = <span class="string">"""</span></span><br><span class="line"><span class="string">                   SELECT * FROM</span></span><br><span class="line"><span class="string">                     users</span></span><br><span class="line"><span class="string">                   WHERE id &gt; 100</span></span><br><span class="line"><span class="string">                   ORDER BY name DESC</span></span><br><span class="line"><span class="string">                   """</span>;</span><br><span class="line">        System.out.println(s);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">上述多行字符串实际上是<span class="number">5</span>行，在最后一个DESC后面还有一个\n。如果我们不想在字符串末尾加一个\n，就需要这么写：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">""" </span></span><br><span class="line"><span class="string">           SELECT * FROM</span></span><br><span class="line"><span class="string">             users</span></span><br><span class="line"><span class="string">           WHERE id &gt; 100</span></span><br><span class="line"><span class="string">           ORDER BY name DESC"""</span>;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">还需要注意到，多行字符串前面共同的空格会被去掉，即：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"""</span></span><br><span class="line"><span class="string">...........SELECT * FROM</span></span><br><span class="line"><span class="string">...........  users</span></span><br><span class="line"><span class="string">...........WHERE id &gt; 100</span></span><br><span class="line"><span class="string">...........ORDER BY name DESC</span></span><br><span class="line"><span class="string">..........."""</span>;</span><br><span class="line">用.标注的空格都会被去掉。</span><br><span class="line"></span><br><span class="line">如果多行字符串的排版不规则，那么，去掉的空格就会变成这样：</span><br><span class="line"></span><br><span class="line">String s = <span class="string">"""</span></span><br><span class="line"><span class="string">.........  SELECT * FROM</span></span><br><span class="line"><span class="string">.........    users</span></span><br><span class="line"><span class="string">.........WHERE id &gt; 100</span></span><br><span class="line"><span class="string">.........  ORDER BY name DESC</span></span><br><span class="line"><span class="string">.........  """</span>;</span><br><span class="line">即总是以最短的行首空格为基准。</span><br></pre></td></tr></table></figure><h3 id="不可变特性"><a href="#不可变特性" class="headerlink" title="不可变特性"></a>不可变特性</h3><p><a href="https://imgtu.com/i/HUnuM8" target="_blank" rel="external nofollow noopener noreferrer"><img src="/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-%E5%8F%98%E9%87%8F%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%5CHUnuM8.md.png" alt="HUnuM8.md.png"></a></p><h3 id="空值"><a href="#空值" class="headerlink" title="空值"></a>空值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">引用类型的变量可以指向一个空值null，它表示不存在，即该变量不指向任何对象。例如：</span><br><span class="line"></span><br><span class="line">String s1 = null; // s1是null</span><br><span class="line">String s2; // 没有赋初值值，s2也是null</span><br><span class="line">String s3 = s1; // s3也是null</span><br><span class="line">String s4 = <span class="string">""</span>; // s4指向空字符串，不是null</span><br><span class="line">注意要区分空值null和空字符串<span class="string">""</span>，空字符串是一个有效的字符串对象，它不等于null</span><br></pre></td></tr></table></figure><h3 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Java的字符类型char是基本类型，字符串类型String是引用类型；</span><br><span class="line">基本类型的变量是“持有”某个数值，引用类型的变量是“指向”某个对象；</span><br><span class="line">引用类型的变量可以是空值null；</span><br><span class="line">要区分空值null和空字符串<span class="string">""</span></span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-Java程序基础 Java程序基本结构</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80-Java%E7%A8%8B%E5%BA%8F%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.html</id>
    <published>2023-03-29T14:19:48.000Z</published>
    <updated>2023-03-29T17:52:28.727Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="java程序基本结构"><a href="#java程序基本结构" class="headerlink" title="java程序基本结构"></a>java程序基本结构</h1><h2 id="类名规范"><a href="#类名规范" class="headerlink" title="类名规范"></a>类名规范</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">+ 首字母大写</span><br><span class="line">+ 字母开头，数字，下划线组合</span><br></pre></td></tr></table></figure><h2 id="方法名规范"><a href="#方法名规范" class="headerlink" title="方法名规范"></a>方法名规范</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命名和class一样，但是首字母小写</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里的方法名是main，返回值是void，表示没有任何返回值。</span><br><span class="line"></span><br><span class="line">我们注意到public除了可以修饰class外，也可以修饰方法。而关键字static是另一个修饰符，它表示静态方法，后面我们会讲解方法的类型，目前，我们只需要知道，Java入口程序规定的方法必须是静态方法，方法名必须为main，括号内的参数必须是String数组。</span><br><span class="line"></span><br><span class="line">每一行语句，分号结尾</span><br></pre></td></tr></table></figure><h2 id="注释方法"><a href="#注释方法" class="headerlink" title="注释方法"></a>注释方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">+ &#x2F;&#x2F;</span><br><span class="line">+ &#x2F;*... *&#x2F;</span><br><span class="line">+ &#x2F;**... *&#x2F;  这是一种特殊注释方法，用在类和方法的定义出，用于自动创建文档</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Java程序对格式没有明确的要求，多几个空格或者回车不影响程序的正确性，但是我们要养成良好的编程习惯</span><br><span class="line">*对于eclipse可以用快捷键ctrl+shift+f，快速格式化代码*</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>廖雪峰Java教程笔记-泛型</title>
    <link href="http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-%E6%B3%9B%E5%9E%8B.html"/>
    <id>http://tianyong.fun/%E5%BB%96%E9%9B%AA%E5%B3%B0Java%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0-%E6%B3%9B%E5%9E%8B.html</id>
    <published>2023-03-29T10:03:11.000Z</published>
    <updated>2023-03-29T17:52:04.932Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h1><h2 id="什么是泛型"><a href="#什么是泛型" class="headerlink" title="什么是泛型"></a>什么是泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ArrayList是一种可变长数组，其内部使用的是Object类型</span><br><span class="line"></span><br><span class="line">public class ArrayList &#123;</span><br><span class="line">    private Object[] array;</span><br><span class="line">    private int size;</span><br><span class="line">    public void add(Object e) &#123;...&#125;</span><br><span class="line">    public void remove(int index) &#123;...&#125;</span><br><span class="line">    public Object get(int index) &#123;...&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果用上述ArrayList存储String类型，会有这么几个缺点：</span><br><span class="line">需要强制转型；</span><br><span class="line">不方便，易出错。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ArrayList list &#x3D; new ArrayList();</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">&#x2F;&#x2F; 获取到Object，必须强制转型为String:</span><br><span class="line">String first &#x3D; (String) list.get(0);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">很容易出现ClassCastException，因为容易“误转型”：</span><br><span class="line">list.add(new Integer(123));</span><br><span class="line">&#x2F;&#x2F; ERROR: ClassCastException:</span><br><span class="line">String second &#x3D; (String) list.get(1);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">要解决上述问题，我们可以为String单独编写一种ArrayList：</span><br><span class="line">public class StringArrayList &#123;</span><br><span class="line">    private String[] array;</span><br><span class="line">    private int size;</span><br><span class="line">    public void add(String e) &#123;...&#125;</span><br><span class="line">    public void remove(int index) &#123;...&#125;</span><br><span class="line">    public String get(int index) &#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这样一来，存入的必须是String，取出的也一定是String，不需要强制转型，因为编译器会强制检查放入的类型：</span><br><span class="line">StringArrayList list &#x3D; new StringArrayList();</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">String first &#x3D; list.get(0);</span><br><span class="line">&#x2F;&#x2F; 编译错误: 不允许放入非String类型:</span><br><span class="line">list.add(new Integer(123));</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">问题暂时解决。</span><br><span class="line"></span><br><span class="line">然而，新的问题是，如果要存储Integer，还需要为Integer单独编写一种ArrayList：</span><br><span class="line">实际上，还需要为其他所有class单独编写一种ArrayList：</span><br><span class="line"></span><br><span class="line">LongArrayList</span><br><span class="line">DoubleArrayList</span><br><span class="line">PersonArrayList</span><br><span class="line">...</span><br><span class="line">这是不可能的，JDK的class就有上千个，而且它还不知道其他人编写的class。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">为了解决新的问题，我们必须把ArrayList变成一种模板：ArrayList&lt;T&gt;，代码如下：</span><br><span class="line">public class ArrayList&lt;T&gt; &#123;</span><br><span class="line">    private T[] array;</span><br><span class="line">    private int size;</span><br><span class="line">    public void add(T e) &#123;...&#125;</span><br><span class="line">    public void remove(int index) &#123;...&#125;</span><br><span class="line">    public T get(int index) &#123;...&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">T可以是任何class。这样一来，我们就实现了：编写一次模版，可以创建任意类型的ArrayList：</span><br><span class="line">&#x2F;&#x2F; 创建可以存储String的ArrayList:</span><br><span class="line">ArrayList&lt;String&gt; strList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">&#x2F;&#x2F; 创建可以存储Float的ArrayList:</span><br><span class="line">ArrayList&lt;Float&gt; floatList &#x3D; new ArrayList&lt;Float&gt;();</span><br><span class="line">&#x2F;&#x2F; 创建可以存储Person的ArrayList:</span><br><span class="line">ArrayList&lt;Person&gt; personList &#x3D; new ArrayList&lt;Person&gt;();</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">因此，泛型就是定义一种模板，例如ArrayList&lt;T&gt;，然后在代码中为用到的类创建对应的ArrayList&lt;类型&gt;：</span><br><span class="line">ArrayList&lt;String&gt; strList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">由编译器针对类型作检查：</span><br><span class="line"></span><br><span class="line">strList.add(&quot;hello&quot;); &#x2F;&#x2F; OK</span><br><span class="line">String s &#x3D; strList.get(0); &#x2F;&#x2F; OK</span><br><span class="line">strList.add(new Integer(123)); &#x2F;&#x2F; compile error!</span><br><span class="line">Integer n &#x3D; strList.get(0); &#x2F;&#x2F; compile error!</span><br><span class="line"></span><br><span class="line">这样一来，既实现了编写一次，万能匹配，又通过编译器保证了类型安全：这就是泛型。</span><br></pre></td></tr></table></figure><h3 id="向上转型"><a href="#向上转型" class="headerlink" title="向上转型"></a>向上转型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在Java标准库中的ArrayList&lt;T&gt;实现了List&lt;T&gt;接口，它可以向上转型为List&lt;T&gt;：</span><br><span class="line"></span><br><span class="line">public class ArrayList&lt;T&gt; implements List&lt;T&gt; &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">即类型ArrayList&lt;T&gt;可以向上转型为List&lt;T&gt;。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">要特别注意：不能把ArrayList&lt;Integer&gt;向上转型为ArrayList&lt;Number&gt;或List&lt;Number&gt;。</span><br><span class="line"></span><br><span class="line">这是为什么呢？假设ArrayList&lt;Integer&gt;可以向上转型为ArrayList&lt;Number&gt;，观察一下代码：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 创建ArrayList&lt;Integer&gt;类型：</span><br><span class="line">ArrayList&lt;Integer&gt; integerList &#x3D; new ArrayList&lt;Integer&gt;();</span><br><span class="line">&#x2F;&#x2F; 添加一个Integer：</span><br><span class="line">integerList.add(new Integer(123));</span><br><span class="line">&#x2F;&#x2F; “向上转型”为ArrayList&lt;Number&gt;：</span><br><span class="line">ArrayList&lt;Number&gt; numberList &#x3D; integerList;</span><br><span class="line">&#x2F;&#x2F; 添加一个Float，因为Float也是Number：</span><br><span class="line">numberList.add(new Float(12.34));</span><br><span class="line">&#x2F;&#x2F; 从ArrayList&lt;Integer&gt;获取索引为1的元素（即添加的Float）：</span><br><span class="line">Integer n &#x3D; integerList.get(1); &#x2F;&#x2F; ClassCastException!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们把一个ArrayList&lt;Integer&gt;转型为ArrayList&lt;Number&gt;类型后，这个ArrayList&lt;Number&gt;就可以接受Float类型，因为Float是Number的子类。但是，ArrayList&lt;Number&gt;实际上和ArrayList&lt;Integer&gt;是同一个对象，也就是ArrayList&lt;Integer&gt;类型，它不可能接受Float类型， 所以在获取Integer的时候将产生ClassCastException。</span><br><span class="line"></span><br><span class="line">实际上，编译器为了避免这种错误，根本就不允许把ArrayList&lt;Integer&gt;转型为ArrayList&lt;Number&gt;。</span><br><span class="line"></span><br><span class="line"> ArrayList&lt;Integer&gt;和ArrayList&lt;Number&gt;两者完全没有继承关系。</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">泛型就是编写模板代码来适应任意类型；</span><br><span class="line"></span><br><span class="line">泛型的好处是使用时不必对类型进行强制转换，它通过编译器对类型进行检查；</span><br><span class="line"></span><br><span class="line">注意泛型的继承关系：可以把&#96;ArrayList&lt;Integer&gt;&#96;向上转型为&#96;List&lt;Integer&gt;&#96;（&#96;T&#96;不能变！），但不能把&#96;ArrayList&lt;Integer&gt;&#96;向上转型为&#96;ArrayList&lt;Number&gt;&#96;（&#96;T&#96;不能变成父类）。</span><br></pre></td></tr></table></figure><h2 id="使用泛型"><a href="#使用泛型" class="headerlink" title="使用泛型"></a>使用泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">使用ArrayList时，如果不定义泛型类型时，泛型类型实际上就是Object：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 编译器警告:</span><br><span class="line">List list &#x3D; new ArrayList(); &#x2F;&#x2F;这里吧ArrayList向上转型为List</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">list.add(&quot;World&quot;);</span><br><span class="line">String first &#x3D; (String) list.get(0);</span><br><span class="line">String second &#x3D; (String) list.get(1);</span><br><span class="line">此时，只能把&lt;T&gt;当作Object使用，没有发挥泛型的优势。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当我们定义泛型类型&lt;String&gt;后，List&lt;T&gt;的泛型接口变为强类型List&lt;String&gt;：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 无编译器警告:</span><br><span class="line">List&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">list.add(&quot;Hello&quot;);</span><br><span class="line">list.add(&quot;World&quot;);</span><br><span class="line">&#x2F;&#x2F; 无强制转型:</span><br><span class="line">String first &#x3D; list.get(0);</span><br><span class="line">String second &#x3D; list.get(1);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当我们定义泛型类型&lt;Number&gt;后，List&lt;T&gt;的泛型接口变为强类型List&lt;Number&gt;：</span><br><span class="line"></span><br><span class="line">List&lt;Number&gt; list &#x3D; new ArrayList&lt;Number&gt;();</span><br><span class="line">list.add(new Integer(123));</span><br><span class="line">list.add(new Double(12.34));</span><br><span class="line">Number first &#x3D; list.get(0);</span><br><span class="line">Number second &#x3D; list.get(1);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">编译器如果能自动推断出泛型类型，就可以省略后面的泛型类型。例如，对于下面的代码：</span><br><span class="line"></span><br><span class="line">List&lt;Number&gt; list &#x3D; new ArrayList&lt;Number&gt;();</span><br><span class="line">编译器看到泛型类型List&lt;Number&gt;就可以自动推断出后面的ArrayList&lt;T&gt;的泛型类型必须是ArrayList&lt;Number&gt;，因此，可以把代码简写为：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 可以省略后面的Number，编译器可以自动推断泛型类型：</span><br><span class="line">List&lt;Number&gt; list &#x3D; new ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><h3 id="泛型接口"><a href="#泛型接口" class="headerlink" title="泛型接口"></a>泛型接口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">除了ArrayList&lt;T&gt;使用了泛型，还可以在接口中使用泛型。例如，Arrays.sort(Object[])可以对任意数组进行排序，但待排序的元素必须实现Comparable&lt;T&gt;这个泛型接口：</span><br><span class="line"></span><br><span class="line">public interface Comparable&lt;T&gt; &#123;</span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 返回负数: 当前实例比参数o小</span><br><span class="line">     * 返回0: 当前实例与参数o相等</span><br><span class="line">     * 返回正数: 当前实例比参数o大</span><br><span class="line">     *&#x2F;</span><br><span class="line">    int compareTo(T o);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">可以直接对String数组进行排序：</span><br><span class="line">&#x2F;&#x2F; sort</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">String[] ss &#x3D; new String[] &#123; &quot;Orange&quot;, &quot;Apple&quot;, &quot;Pear&quot; &#125;;</span><br><span class="line">        Arrays.sort(ss);</span><br><span class="line">        System.out.println(Arrays.toString(ss));</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">这是因为String本身已经实现了Comparable&lt;String&gt;接口。如果换成我们自定义的Person类型试试：</span><br><span class="line">&#x2F;&#x2F; sort</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">    Person[] ps &#x3D; new Person[] &#123;</span><br><span class="line">            new Person(&quot;Bob&quot;, 61),</span><br><span class="line">            new Person(&quot;Alice&quot;, 88),</span><br><span class="line">            new Person(&quot;Lily&quot;, 75),</span><br><span class="line">        &#125;;</span><br><span class="line">        Arrays.sort(ps);</span><br><span class="line">        System.out.println(Arrays.toString(ps));</span><br><span class="line"></span><br><span class="line">class Person &#123;</span><br><span class="line">    String name;</span><br><span class="line">    int score;</span><br><span class="line">    Person(String name, int score) &#123;</span><br><span class="line">        this.name &#x3D; name;</span><br><span class="line">        this.score &#x3D; score;</span><br><span class="line">    &#125;</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return this.name + &quot;,&quot; + this.score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">运行程序，我们会得到ClassCastException，即无法将Person转型为Comparable。我们修改代码，让Person实现Comparable&lt;T&gt;接口：</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; sort</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Person[] ps &#x3D; new Person[] &#123;</span><br><span class="line">            new Person(&quot;Bob&quot;, 61),</span><br><span class="line">            new Person(&quot;Alice&quot;, 88),</span><br><span class="line">            new Person(&quot;Lily&quot;, 75),</span><br><span class="line">        &#125;;</span><br><span class="line">        Arrays.sort(ps);</span><br><span class="line">        System.out.println(Arrays.toString(ps));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Person implements Comparable&lt;Person&gt; &#123;</span><br><span class="line">    String name;</span><br><span class="line">    int score;</span><br><span class="line">    Person(String name, int score) &#123;</span><br><span class="line">        this.name &#x3D; name;</span><br><span class="line">        this.score &#x3D; score;</span><br><span class="line">    &#125;</span><br><span class="line">    public int compareTo(Person other) &#123;</span><br><span class="line">        return this.name.compareTo(other.name);</span><br><span class="line">    &#125;</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return this.name + &quot;,&quot; + this.score;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行上述代码，可以正确实现按name进行排序。</span><br><span class="line"></span><br><span class="line">也可以修改比较逻辑，例如，按score从高到低排序。请自行修改测试。</span><br></pre></td></tr></table></figure><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">使用泛型时，把泛型参数&lt;T&gt;替换为需要的class类型，例如：ArrayList&lt;String&gt;，ArrayList&lt;Number&gt;等；</span><br><span class="line"></span><br><span class="line">可以省略编译器能自动推断出的类型，例如：List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();；</span><br><span class="line"></span><br><span class="line">不指定泛型参数类型时，编译器会给出警告，且只能将&lt;T&gt;视为Object类型；</span><br><span class="line"></span><br><span class="line">可以在接口中定义泛型类型，实现此接口的类必须实现正确的泛型类型。</span><br></pre></td></tr></table></figure><h2 id="编写泛型"><a href="#编写泛型" class="headerlink" title="编写泛型"></a>编写泛型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">编写泛型类比普通类要复杂。通常来说，泛型类一般用在集合类中，例如ArrayList&lt;T&gt;，我们很少需要编写泛型类。</span><br><span class="line"></span><br><span class="line">如果我们确实需要编写一个泛型类，那么，应该如何编写它？</span><br><span class="line">可以按照以下步骤来编写一个泛型类。</span><br><span class="line">首先，按照某种类型，例如：String，来编写类：</span><br><span class="line"></span><br><span class="line">public class Pair &#123;</span><br><span class="line">    private String first;</span><br><span class="line">    private String last;</span><br><span class="line">    public Pair(String first, String last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">然后，标记所有的特定类型，这里是String：</span><br><span class="line"></span><br><span class="line">public class Pair &#123;</span><br><span class="line">    private String first;</span><br><span class="line">    private String last;</span><br><span class="line">    public Pair(String first, String last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">最后，把特定类型String替换为T，并申明&lt;T&gt;：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">熟练后即可直接从T开始编写。</span><br></pre></td></tr></table></figure><h3 id="静态方法"><a href="#静态方法" class="headerlink" title="静态方法"></a>静态方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">编写泛型类时，要特别注意，泛型类型&lt;T&gt;不能用于静态方法。例如：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 对静态方法使用&lt;T&gt;:</span><br><span class="line">    public static Pair&lt;T&gt; create(T first, T last) &#123;</span><br><span class="line">        return new Pair&lt;T&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">上述代码会导致编译错误，我们无法在静态方法create()的方法参数和返回类型上使用泛型类型T。</span><br><span class="line"></span><br><span class="line">有些同学在网上搜索发现，可以在static修饰符后面加一个&lt;T&gt;，编译就能通过：</span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 可以编译通过:</span><br><span class="line">    public static &lt;T&gt; Pair&lt;T&gt; create(T first, T last) &#123;</span><br><span class="line">        return new Pair&lt;T&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">但实际上，这个&lt;T&gt;和Pair&lt;T&gt;类型的&lt;T&gt;已经没有任何关系了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">对于静态方法，我们可以单独改写为“泛型”方法，只需要使用另一个类型即可。对于上面的create()静态方法，我们应该把它改为另一种泛型类型，例如，&lt;K&gt;：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public T getLast() &#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 静态泛型方法应该使用其他类型区分:</span><br><span class="line">    public static &lt;K&gt; Pair&lt;K&gt; create(K first, K last) &#123;</span><br><span class="line">        return new Pair&lt;K&gt;(first, last);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这样才能清楚地将静态方法的泛型类型和实例类型的泛型类型区分开。</span><br></pre></td></tr></table></figure><h3 id="多个泛型类型"><a href="#多个泛型类型" class="headerlink" title="多个泛型类型"></a>多个泛型类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">泛型还可以定义多种类型。例如，我们希望Pair不总是存储两个类型一样的对象，就可以使用类型&lt;T, K&gt;：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T, K&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private K last;</span><br><span class="line">    public Pair(T first, K last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123; ... &#125;</span><br><span class="line">    public K getLast() &#123; ... &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">使用的时候，需要指出两种类型：</span><br><span class="line">Pair&lt;String, Integer&gt; p &#x3D; new Pair&lt;&gt;(&quot;test&quot;, 123);</span><br><span class="line"></span><br><span class="line">Java标准库的Map&lt;K, V&gt;就是使用两种泛型类型的例子。它对Key使用一种类型，对Value使用另一种类型。</span><br></pre></td></tr></table></figure><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">编写泛型时，需要定义泛型类型&lt;T&gt;；</span><br><span class="line"></span><br><span class="line">静态方法不能引用泛型类型&lt;T&gt;，必须定义其他类型（例如&lt;K&gt;）来实现静态泛型方法；</span><br><span class="line"></span><br><span class="line">泛型可以同时定义多种类型，例如Map&lt;K, V&gt;。</span><br></pre></td></tr></table></figure><h2 id="擦拭法"><a href="#擦拭法" class="headerlink" title="擦拭法"></a>擦拭法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">泛型是一种类似”模板代码“的技术，不同语言的泛型实现方式不一定相同。</span><br><span class="line">Java语言的泛型实现方式是擦拭法（Type Erasure）。</span><br><span class="line">所谓擦拭法是指，虚拟机对泛型其实一无所知，所有的工作都是编译器做的。</span><br><span class="line"></span><br><span class="line">例如，我们编写了一个泛型类Pair&lt;T&gt;，这是编译器看到的代码：</span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">而虚拟机根本不知道泛型。这是虚拟机执行的代码：</span><br><span class="line"></span><br><span class="line">public class Pair &#123;</span><br><span class="line">    private Object first;</span><br><span class="line">    private Object last;</span><br><span class="line">    public Pair(Object first, Object last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public Object getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public Object getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">因此，Java使用擦拭法实现泛型，导致了：</span><br><span class="line"></span><br><span class="line">编译器把类型&lt;T&gt;视为Object；</span><br><span class="line">编译器根据&lt;T&gt;实现安全的强制转型。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">使用泛型的时候，我们编写的代码也是编译器看到的代码：</span><br><span class="line"></span><br><span class="line">Pair&lt;String&gt; p &#x3D; new Pair&lt;&gt;(&quot;Hello&quot;, &quot;world&quot;);</span><br><span class="line">String first &#x3D; p.getFirst();</span><br><span class="line">String last &#x3D; p.getLast();</span><br><span class="line">而虚拟机执行的代码并没有泛型：</span><br><span class="line"></span><br><span class="line">Pair p &#x3D; new Pair(&quot;Hello&quot;, &quot;world&quot;);</span><br><span class="line">String first &#x3D; (String) p.getFirst();</span><br><span class="line">String last &#x3D; (String) p.getLast();</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">所以，Java的泛型是由编译器在编译时实行的，编译器内部永远把所有类型T视为Object处理，但是，在需要转型的时候，编译器会根据T的类型自动为我们实行安全地强制转型。</span><br><span class="line"></span><br><span class="line">了解了Java泛型的实现方式——擦拭法，我们就知道了Java泛型的局限：</span><br><span class="line">局限一：&lt;T&gt;不能是基本类型，例如int，因为实际类型是Object，Object类型无法持有基本类型：</span><br><span class="line"></span><br><span class="line">Pair&lt;int&gt; p &#x3D; new Pair&lt;&gt;(1, 2); &#x2F;&#x2F; compile error!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">局限二：无法取得带泛型的Class。观察以下代码：</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">Pair&lt;String&gt; p1 &#x3D; new Pair&lt;&gt;(&quot;Hello&quot;, &quot;world&quot;);</span><br><span class="line">        Pair&lt;Integer&gt; p2 &#x3D; new Pair&lt;&gt;(123, 456);</span><br><span class="line">        Class c1 &#x3D; p1.getClass();</span><br><span class="line">        Class c2 &#x3D; p2.getClass();</span><br><span class="line">        System.out.println(c1&#x3D;&#x3D;c2); &#x2F;&#x2F; true</span><br><span class="line">        System.out.println(c1&#x3D;&#x3D;Pair.class); &#x2F;&#x2F; true</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(T first, T last) &#123;</span><br><span class="line">        this.first &#x3D; first;</span><br><span class="line">        this.last &#x3D; last;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public T getLast() &#123;</span><br><span class="line">        return last;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">因为T是Object，我们对Pair&lt;String&gt;和Pair&lt;Integer&gt;类型获取Class时，获取到的是同一个Class，也就是Pair类的Class。</span><br><span class="line"></span><br><span class="line">换句话说，所有泛型实例，无论T的类型是什么，getClass()返回同一个Class实例，因为编译后它们全部都是Pair&lt;Object&gt;。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">局限三：无法判断带泛型的类型：</span><br><span class="line"></span><br><span class="line">Pair&lt;Integer&gt; p &#x3D; new Pair&lt;&gt;(123, 456);</span><br><span class="line">&#x2F;&#x2F; Compile error:</span><br><span class="line">if (p instanceof Pair&lt;String&gt;) &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">原因和前面一样，并不存在Pair&lt;String&gt;.class，而是只有唯一的Pair.class。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">局限四：不能实例化T类型：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair() &#123;</span><br><span class="line">        &#x2F;&#x2F; Compile error:</span><br><span class="line">        first &#x3D; new T();</span><br><span class="line">        last &#x3D; new T();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">上述代码无法通过编译，因为构造方法的两行语句：</span><br><span class="line"></span><br><span class="line">first &#x3D; new T();</span><br><span class="line">last &#x3D; new T();</span><br><span class="line"></span><br><span class="line">擦拭后实际上变成了：</span><br><span class="line"></span><br><span class="line">first &#x3D; new Object();</span><br><span class="line">last &#x3D; new Object();</span><br><span class="line"></span><br><span class="line">这样一来，创建new Pair&lt;String&gt;()和创建new Pair&lt;Integer&gt;()就全部成了Object，显然编译器要阻止这种类型不对的代码。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">要实例化T类型，我们必须借助额外的Class&lt;T&gt;参数：</span><br><span class="line"></span><br><span class="line">public class Pair&lt;T&gt; &#123;</span><br><span class="line">    private T first;</span><br><span class="line">    private T last;</span><br><span class="line">    public Pair(Class&lt;T&gt; clazz) &#123;</span><br><span class="line">        first &#x3D; clazz.newInstance();</span><br><span class="line">        last &#x3D; clazz.newInstance();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">上述代码借助Class&lt;T&gt;参数并通过反射来实例化T类型，使用的时候，也必须传入Class&lt;T&gt;。例如：</span><br><span class="line"></span><br><span class="line">Pair&lt;String&gt; pair &#x3D; new Pair&lt;&gt;(String.class);</span><br><span class="line">因为传入了Class&lt;String&gt;的实例，所以我们借助String.class就可以实例化String类型。</span><br></pre></td></tr></table></figure><h3 id="不恰当的覆写方法"><a href="#不恰当的覆写方法" class="headerlink" title="不恰当的覆写方法"></a>不恰当的覆写方法</h3><h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="廖雪峰java笔记" scheme="http://tianyong.fun/categories/%E5%BB%96%E9%9B%AA%E5%B3%B0java%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 SparkSql</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-SparkSql.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-SparkSql.html</id>
    <published>2023-03-28T07:23:48.000Z</published>
    <updated>2023-03-30T10:40:41.330Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-SparkSql-极速上手SparkSql"><a href="#第十一周-SparkSql-极速上手SparkSql" class="headerlink" title="第十一周 SparkSql-极速上手SparkSql"></a>第十一周 SparkSql-极速上手SparkSql</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">之前已学完，spark core，离线数据计算</span><br></pre></td></tr></table></figure><h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL和我们之前讲Hive的时候说的hive on spark是不一样的。</span><br><span class="line">hive on spark是表示把底层的mapreduce引擎替换为spark引擎。</span><br><span class="line">而Spark SQL是Spark自己实现的一套SQL处理引擎。</span><br><span class="line"></span><br><span class="line">Spark SQL是Spark中的一个模块，主要用于进行结构化数据的处理。它提供的最核心的编程抽象，就是DataFrame。</span><br><span class="line"></span><br><span class="line">DataFrame&#x3D;RDD+Schema 。</span><br><span class="line">它其实和关系型数据库中的表非常类似，RDD可以认为是表中的数据，Schema是表结构信息。</span><br><span class="line">DataFrame可以通过很多来源进行构建，包括：结构化的数据文件，Hive中的表，外部的关系型数据库，以及RDD</span><br><span class="line">Spark1.3出现的DataFrame ，Spark1.6出现了DataSet，在Spark2.0中两者统一，DataFrame等于DataSet[Row]</span><br></pre></td></tr></table></figure><h2 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">要使用Spark SQL，首先需要创建一个SpakSession对象</span><br><span class="line">SparkSession中包含了SparkContext和SqlContext</span><br><span class="line">所以说想通过SparkSession来操作RDD的话需要先通过它来获取SparkContext</span><br><span class="line"></span><br><span class="line">这个SqlContext是使用sparkSQL操作hive的时候会用到的。</span><br></pre></td></tr></table></figure><h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">使用SparkSession，可以从RDD、HIve表或者其它数据源创建DataFrame</span><br><span class="line">那下面我们来使用JSON文件来创建一个DataFrame</span><br><span class="line">想要使用spark-sql需要先添加spark-sql的依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在项目中添加sql这个包名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281536605.png" alt="image-20230328153653400"></p><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281542225.png" alt="image-20230328154207572"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.sql</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：使用json文件创建DataFrame</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object SqlDemoScala&#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">         val conf &#x3D; new SparkConf()</span><br><span class="line">         .setMaster(&quot;local&quot;)</span><br><span class="line">         &#x2F;&#x2F;创建SparkSession对象，里面包含SparkContext和SqlContext</span><br><span class="line">         val sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">         .appName(&quot;SqlDemoScala&quot;)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         &#x2F;&#x2F;读取json文件，获取DataFrame</span><br><span class="line">         val stuDf &#x3D; sparkSession.read.json(&quot;D:\\student.json&quot;)</span><br><span class="line">         &#x2F;&#x2F;查看DataFrame中的数据</span><br><span class="line">         stuDf.show()</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281546823.png" alt="image-20230328154309744"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用json文件创建DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SqlDemoJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"SqlDemoJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//读取json文件，获取Dataset&lt;Row&gt;</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.read().json(<span class="string">"D:\\student.json"</span>); <span class="comment">// 返回的dataset其实和dataframe一样的，新版本合并了</span></span><br><span class="line">         stuDf.show();</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281546440.png" alt="image-20230328154617041"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于DataFrame等于DataSet[Row]，它们两个可以互相转换，所以创建哪个都是一样的</span><br><span class="line">咱们前面的scala代码默认创建的是DataFrame，java代码默认创建的是DataSet</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">尝试对他们进行转换</span><br><span class="line">在Scala代码中将DataFrame转换为DataSet[Row]，对后面的操作没有影响</span><br><span class="line">&#x2F;&#x2F;将DataFrame转换为DataSet[Row]</span><br><span class="line">val stuDf &#x3D; sparkSession.read.json(&quot;D:\\student.json&quot;).as(&quot;stu&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在Java代码中将DataSet[Row]转换为DataFrame</span><br><span class="line">&#x2F;&#x2F;将Dataset&lt;Row&gt;转换为DataFrame</span><br><span class="line">Dataset&lt;Row&gt; stuDf &#x3D; sparkSession.read().json(&quot;D:\\student.json&quot;).toDF();</span><br></pre></td></tr></table></figure><h3 id="DataFrame常见算子操作"><a href="#DataFrame常见算子操作" class="headerlink" title="DataFrame常见算子操作"></a>DataFrame常见算子操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下Spark sql中针对DataFrame常见的算子操作</span><br><span class="line">先看一下官方文档</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281552610.png" alt="image-20230328155250497"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">printSchema()</span><br><span class="line">show()</span><br><span class="line">select()</span><br><span class="line">filter()、where()</span><br><span class="line">groupBy()</span><br><span class="line">count()</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.sql</span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：DataFrame常见操作</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object DataFrameOpScala &#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">         val conf &#x3D; new SparkConf()</span><br><span class="line">         .setMaster(&quot;local&quot;)</span><br><span class="line">         &#x2F;&#x2F;创建SparkSession对象，里面包含SparkContext和SqlContext</span><br><span class="line">         val sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">         .appName(&quot;DataFrameOpScala&quot;)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">        val stuDf &#x3D; sparkSession.read.json(&quot;D:\\student.json&quot;)</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;打印schema信息</span><br><span class="line">         stuDf.printSchema()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;默认显示所有数据，可以通过参数控制显示多少条</span><br><span class="line">         stuDf.show(2)</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;查询数据中的指定字段信息</span><br><span class="line">         stuDf.select(&quot;name&quot;,&quot;age&quot;).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;在使用select的时候可以对数据做一些操作，需要添加隐式转换函数，否则语法报错</span><br><span class="line">         import sparkSession.implicits._</span><br><span class="line">         stuDf.select($&quot;name&quot;,$&quot;age&quot; + 1).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;对数据进行过滤，需要添加隐式转换函数，否则语法报错</span><br><span class="line">         stuDf.filter($&quot;age&quot;&gt;18).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;where底层调用的就是filter</span><br><span class="line">        stuDf.where($&quot;age&quot;&gt;18).show()</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;对数据进行分组求和</span><br><span class="line">         stuDf.groupBy(&quot;age&quot;).count().show()</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281556810.png" alt="image-20230328155651098"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281557659.png" alt="image-20230328155732191"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281558631.png" alt="image-20230328155839255"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281601278.png" alt="image-20230328160101149"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281602257.png" alt="image-20230328160159987"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281602257.png" alt></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281603534.png" alt="image-20230328160345238"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.java.sql;</span><br><span class="line">import org.apache.spark.SparkConf;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import static org.apache.spark.sql.functions.col;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：DataFrame常见操作</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class DataFrameOpJava &#123;</span><br><span class="line">     public static void main(String[] args) &#123;</span><br><span class="line">         SparkConf conf &#x3D; new SparkConf();</span><br><span class="line">         conf.setMaster(&quot;local&quot;);</span><br><span class="line">         &#x2F;&#x2F;创建SparkSession对象，里面包含SparkContext和SqlContext</span><br><span class="line">         SparkSession sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">         .appName(&quot;DataFrameOpJava&quot;)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         Dataset&lt;Row&gt; stuDf &#x3D; sparkSession.read().json(&quot;D:\\student.json&quot;);</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;打印schema信息</span><br><span class="line">         stuDf.printSchema();</span><br><span class="line">         </span><br><span class="line">         &#x2F;&#x2F;默认显示所有数据，可以通过参数控制显示多少条</span><br><span class="line">         stuDf.show(2);</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;查询数据中的指定字段信息</span><br><span class="line">         stuDf.select(&quot;name&quot;,&quot;age&quot;).show();</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;在select的时候可以对数据做一些操作,需要引入import static org.apache.spa</span><br><span class="line">         stuDf.select(col(&quot;name&quot;),col(&quot;age&quot;).plus(1)).show();</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;对数据进行过滤</span><br><span class="line">         stuDf.filter(col(&quot;age&quot;).gt(18)).show();</span><br><span class="line">         stuDf.where(col(&quot;age&quot;).gt(18)).show();</span><br><span class="line">        </span><br><span class="line">        &#x2F;&#x2F;对数据进行分组求和</span><br><span class="line">         stuDf.groupBy(&quot;age&quot;).count().show();</span><br><span class="line">         sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这些就是针对DataFrame的一些常见的操作。</span><br><span class="line">但是现在这种方式其实用起来还是不方便，只是提供了一些类似于可以操作表的算子，很对一些简单的查询还是可以的，但是针对一些复杂的操作，使用算子写起来就很麻烦了，所以我们希望能够直接支持用sql的方式执行，Spark SQL也是支持的</span><br></pre></td></tr></table></figure><h3 id="DataFrame的sql操作"><a href="#DataFrame的sql操作" class="headerlink" title="DataFrame的sql操作"></a>DataFrame的sql操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">想要实现直接支持sql语句查询DataFrame中的数据</span><br><span class="line">需要两步操作</span><br><span class="line">1. 先将DataFrame注册为一个临时表</span><br><span class="line">2. 使用sparkSession中的sql函数执行sql语句</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用sql操作DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataFrameSqlScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"DataFrameSqlScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="keyword">val</span> stuDf = sparkSession.read.json(<span class="string">"D:\\student.json"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//将DataFrame注册为一个临时表</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">         <span class="comment">//使用sql查询临时表中的数据</span></span><br><span class="line">         sparkSession.sql(<span class="string">"select age,count(*) as num from student group by age"</span>)</span><br><span class="line">         .show()</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303282324292.png" alt="image-20230328232442914"></p><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用sql操作DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataFrameSqlJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"DataFrameSqlJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.read().json(<span class="string">"D:\\student.json"</span>);</span><br><span class="line">         <span class="comment">//将Dataset&lt;Row&gt;注册为一个临时表</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>);</span><br><span class="line">         <span class="comment">//使用sql查询临时表中的数据</span></span><br><span class="line">         sparkSession.sql(<span class="string">"select age,count(*) as num from student group by ag</span></span><br><span class="line"><span class="string">         .show();</span></span><br><span class="line"><span class="string">         sparkSession.stop();</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="RDD转换为DataFrame-包含DataFrame转RDD"><a href="#RDD转换为DataFrame-包含DataFrame转RDD" class="headerlink" title="RDD转换为DataFrame(包含DataFrame转RDD)"></a>RDD转换为DataFrame(包含DataFrame转RDD)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">为什么要将RDD转换为DataFrame?</span><br><span class="line"></span><br><span class="line">在实际工作中我们可能会先把hdfs上的一些日志数据加载进来，然后进行一些处理，最终变成结构化的数据，希望对这些数据做一些统计分析，当然了我们可以使用spark中提供的transformation算子来实现，只不过会有一些麻烦，毕竟是需要写代码的，如果能够使用sql实现，其实是更加方便的。</span><br><span class="line"></span><br><span class="line">所以可以针对我们前面创建的RDD，将它转换为DataFrame，这样就可以使用dataFrame中的一些算子或者直接写sql来操作数据了。</span><br><span class="line">Spark SQL支持这两种方式将RDD转换为DataFrame</span><br><span class="line">1. 反射方式</span><br><span class="line">2. 编程方式</span><br></pre></td></tr></table></figure><h4 id="反射方式"><a href="#反射方式" class="headerlink" title="反射方式"></a>反射方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下反射方式：</span><br><span class="line">这种方式是使用反射来推断RDD中的元数据。</span><br><span class="line">基于反射的方式，代码比较简洁，也就是说当你在写代码的时候，已经知道了RDD中的元数据(已经知道RDD里面的数据长什么样子)，这样的话使用反射这种方式是一种非常不错的选择。</span><br><span class="line">Scala具有隐式转换的特性，所以spark sql的scala接口是支持自动将包含了case class的RDD转换为DataFrame的</span><br><span class="line"></span><br><span class="line">下面来举一个例子</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><h5 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用反射方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RddToDataFrameByReflectScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByReflectScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="keyword">val</span> sc = sparkSession.sparkContext</span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>((<span class="string">"jack"</span>,<span class="number">18</span>),(<span class="string">"tom"</span>,<span class="number">20</span>),(<span class="string">"jessic"</span>,<span class="number">30</span>)))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//基于反射直接将包含Student对象的dataRDD转换为DataFrame</span></span><br><span class="line">         <span class="comment">//需要导入隐式转换</span></span><br><span class="line">         <span class="keyword">import</span> sparkSession.implicits._ <span class="comment">//不导入，toDF()不能用</span></span><br><span class="line">         <span class="keyword">val</span> stuDf = dataRDD.map(tup=&gt;<span class="type">Student</span>(tup._1,tup._2)).toDF()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//下面就可以通过DataFrame的方式操作dataRDD中的数据了</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         <span class="keyword">val</span> resDf = sparkSession.sql(<span class="string">"select name,age from student where age &gt; 18"</span>)</span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD，或者直接在这里show()也行</span></span><br><span class="line">         <span class="keyword">val</span> resRDD = resDf.rdd</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//从row中取数据 ，封装成student，打印到控制台</span></span><br><span class="line">     resRDD.map(row=&gt;<span class="type">Student</span>(row(<span class="number">0</span>).toString,row(<span class="number">1</span>).toString.toInt))</span><br><span class="line">         .collect()</span><br><span class="line">         .foreach(println(_))</span><br><span class="line">         <span class="comment">//使用row的getAs()方法，获取指定列名的值</span></span><br><span class="line">         resRDD.map(row=&gt;<span class="type">Student</span>(row.getAs[<span class="type">String</span>](<span class="string">"name"</span>),row.getAs[<span class="type">Int</span>](<span class="string">"age"</span>)))</span><br><span class="line">         .collect()</span><br><span class="line">         .foreach(println(_))</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//定义一个Student</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">name: <span class="type">String</span>,age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303282345692.png" alt="image-20230328234459220"></p><h5 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.RDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用反射方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RddToDataFrameByReflectJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByReflectJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="comment">//从sparkSession中获取的是scala中的sparkContext，所以需要转换成java中的sparkContext</span></span><br><span class="line">         JavaSparkContext sc = JavaSparkContext.fromSparkContext(sparkSession.sparkContext());</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t1 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jack"</span>, <span class="number">18</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t2 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"tom"</span>, <span class="number">20</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t3 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jessic"</span>, <span class="number">30</span>);</span><br><span class="line">         JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; dataRDD = sc.parallelize(Arrays.asList(t1,t2,t3));</span><br><span class="line">         JavaRDD&lt;Student&gt; stuRDD = dataRDD.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;String, Integer&gt;, Student&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Student <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new <span class="title">Student</span><span class="params">(tup._1, tup._2)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//注意：Student这个类必须声明为public(一个文件里只能有一个public class，所以只能在另一个文件里创建)，并且必须实现序列化</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.createDataFrame(stuRDD, Student<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">         </span><br><span class="line">                                                                      stuDf.createOrReplaceTempView(<span class="string">"student"</span>);</span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         Dataset&lt;Row&gt; resDf = sparkSession.sql(<span class="string">"select name,age from student where age&gt;18"</span>);</span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD，注意：这里需要转为JavaRDD</span></span><br><span class="line">         JavaRDD&lt;Row&gt; resRDD = resDf.javaRDD();</span><br><span class="line">         <span class="comment">//从row中取数据，封装成student，打印到控制台</span></span><br><span class="line">         List&lt;Student&gt; resList = resRDD.map(<span class="keyword">new</span> Function&lt;Row, Student&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Student <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">//return new Student(row.getString(0), row.getInt(1));</span></span><br><span class="line">         <span class="comment">//通过getAs获取数据</span></span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Student(row.getAs(<span class="string">"name"</span>).toString(), Integer.parseInt(row.getAs(<span class="string">"age"</span>).toString());</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).collect();</span><br><span class="line">         <span class="keyword">for</span>(Student stu : resList)&#123;</span><br><span class="line">         System.out.println(stu);</span><br><span class="line">        sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">student</span> <span class="keyword">implements</span> <span class="title">Serializable</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(String name,<span class="keyword">int</span> age)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name=name;</span><br><span class="line">        <span class="keyword">this</span>.age=age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span></span>&#123;</span><br><span class="line"><span class="keyword">this</span>.name=name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getAge</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">this</span>.age=age;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Student&#123;"</span>+</span><br><span class="line">            <span class="string">"name='"</span>+name+<span class="string">'\''</span>+</span><br><span class="line">            <span class="string">",age="</span>+age+</span><br><span class="line">            <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java类中快速为字段生成get,set方法，右键-&gt;generate-&gt;...</span><br><span class="line">    快速生成构造方法 右键-&gt;generate-&gt;constructor</span><br><span class="line">    快速生成toString()方法 右键-&gt;generate-&gt;toString</span><br></pre></td></tr></table></figure><h4 id="编程方式"><a href="#编程方式" class="headerlink" title="编程方式"></a>编程方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来是编程的方式</span><br><span class="line">这种方式是通过编程接口来创建DataFrame，你可以在程序运行时动态构建一份元数据，就是Schema，然后将其应用到已经存在的RDD上。这种方式的代码比较冗长，但是如果在编写程序时，还不知道RDD的元数据，只有在程序运行时，才能动态得知其元数据，那么只能通过这种动态构建元数据的方式。</span><br><span class="line"></span><br><span class="line">也就是说当case calss中的字段无法预先定义的时候，就只能用编程方式动态指定元数据了</span><br></pre></td></tr></table></figure><h5 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">Stru</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用编程方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RddToDataFrameByProgramScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByProgramScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="keyword">val</span> sc = sparkSession.sparkContext</span><br><span class="line">         <span class="comment">// 假设这里不知道RDD的数据结构</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>((<span class="string">"jack"</span>,<span class="number">18</span>),(<span class="string">"tom"</span>,<span class="number">20</span>),(<span class="string">"jessic"</span>,<span class="number">30</span>)))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//组装rowRDD</span></span><br><span class="line">         <span class="keyword">val</span> rowRDD = dataRDD.map(tup=&gt;<span class="type">Row</span>(tup._1,tup._2))</span><br><span class="line">         <span class="comment">// dataframe=rdd+schema</span></span><br><span class="line">         <span class="comment">//指定元数据信息【这个元数据信息就可以动态从外部获取了，比较灵活】</span></span><br><span class="line">         <span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"name"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"age"</span>,<span class="type">IntegerType</span>,<span class="literal">true</span>)</span><br><span class="line">         ))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//组装DataFrame</span></span><br><span class="line">         <span class="keyword">val</span> stuDf = sparkSession.createDataFrame(rowRDD,schema)</span><br><span class="line">         <span class="comment">//下面就可以通过DataFrame的方式操作dataRDD中的数据了</span></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         <span class="keyword">val</span> resDf = sparkSession.sql(<span class="string">"select name,age from student where age &gt; 18"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD</span></span><br><span class="line">         <span class="keyword">val</span> resRDD = resDf.rdd </span><br><span class="line">         resRDD.map(row=&gt;(row(<span class="number">0</span>).toString,row(<span class="number">1</span>).toString.toInt))</span><br><span class="line">         .collect()</span><br><span class="line">         .foreach(println(_))</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IDEA编程时使用一个未知的东西，alt+回车,会提示需要导入的东西</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303290043493.png" alt="image-20230329004355828"></p><h5 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.RowFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructField;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.StructType;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：使用编程方式实现RDD转换为DataFrame</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RddToDataFrameByProgramJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"RddToDataFrameByProgramJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//获取SparkContext</span></span><br><span class="line">         <span class="comment">//从sparkSession中获取的是scala中的sparkContext，所以需要转换成java中的sp</span></span><br><span class="line">         JavaSparkContext sc = JavaSparkContext.fromSparkContext(sparkSession.</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t1 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jack"</span>, <span class="number">18</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t2 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"tom"</span>, <span class="number">20</span>);</span><br><span class="line">         Tuple2&lt;String, Integer&gt; t3 = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(<span class="string">"jessic"</span>, <span class="number">30</span>);</span><br><span class="line">         JavaRDD&lt;Tuple2&lt;String, Integer&gt;&gt; dataRDD = sc.parallelize(Arrays.asList(t1,t2,t3));</span><br><span class="line">         <span class="comment">//组装rowRDD</span></span><br><span class="line">         JavaRDD&lt;Row&gt; rowRDD = dataRDD.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;String, Integer&gt;,Row&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> RowFactory.create(tup._1, tup._2);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//指定元数据信息</span></span><br><span class="line">        ArrayList&lt;StructField&gt; structFieldList = <span class="keyword">new</span> ArrayList&lt;StructField&gt;();</span><br><span class="line">         structFieldList.add(DataTypes.createStructField(<span class="string">"name"</span>, DataTypes.StringType,<span class="keyword">true</span>));</span><br><span class="line">         structFieldList.add(DataTypes.createStructField(<span class="string">"age"</span>, DataTypes.IntegerType,True));</span><br><span class="line">         StructType schema = DataTypes.createStructType(structFieldList);</span><br><span class="line">         <span class="comment">//构建DataFrame</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.createDataFrame(rowRDD, schema);</span><br><span class="line"></span><br><span class="line">         stuDf.createOrReplaceTempView(<span class="string">"student"</span>);</span><br><span class="line">         <span class="comment">//执行sql查询</span></span><br><span class="line">         Dataset&lt;Row&gt; resDf = sparkSession.sql(<span class="string">"select name,age from student where age&gt;18"</span>);</span><br><span class="line">         <span class="comment">//将DataFrame转化为RDD，注意：这里需要转为JavaRDD</span></span><br><span class="line">         JavaRDD&lt;Row&gt; resRDD = resDf.javaRDD();</span><br><span class="line">         List&lt;Tuple2&lt;String, Integer&gt;&gt; resList = resRDD.map(<span class="keyword">new</span> Function&lt;Row, Tuple2&lt;String,Intege&gt;&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(row.getString(<span class="number">0</span>), row.getInt(<span class="number">1</span>));</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).collect();</span><br><span class="line">         </span><br><span class="line">                                                                      <span class="keyword">for</span>(Tuple2&lt;String,Integer&gt; tup : resList)&#123;</span><br><span class="line">         System.out.println(tup);</span><br><span class="line">         &#125;</span><br><span class="line">         sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="load和save操作"><a href="#load和save操作" class="headerlink" title="load和save操作"></a>load和save操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对于Spark SQL的DataFrame来说，无论是从什么数据源创建出来的DataFrame，都有一些共同的load和save操作。(不使用这两个方法，使用TextFile(),再转成DataFrame操作，再转RDD,saveAsText()也可以)</span><br><span class="line"></span><br><span class="line">load操作主要用于加载数据，创建出DataFrame；</span><br><span class="line">save操作，主要用于将DataFrame中的数据保存到文件中。</span><br><span class="line"></span><br><span class="line">我们前面操作json格式的数据的时候好像没有使用load方法，而是直接使用的json方法，这是什么特殊用法吗？</span><br><span class="line">查看json方法的源码会发现，它底层调用的是format和load方法</span><br><span class="line">def json(paths: String*): DataFrame &#x3D; format(&quot;json&quot;).load(paths : _*)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">我们如果使用原始的format和load方法加载数据，</span><br><span class="line">此时如果不指定format，则默认读取的数据源格式是parquet，也可以手动指定数据源格式。Spark SQL内置了一些常见的数据源类型，比如json, parquet, jdbc, orc, csv, text</span><br><span class="line"></span><br><span class="line">通过这个功能，就可以在不同类型的数据源之间进行转换了。</span><br></pre></td></tr></table></figure><h4 id="scala-5"><a href="#scala-5" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：load和save的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LoadAndSaveOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="comment">//读取数据(这种方式与前面讲的创建dataframe是一样的，sparkSession.read.json("xxx"))</span></span><br><span class="line">         <span class="keyword">val</span> stuDf = sparkSession.read</span><br><span class="line">         .format(<span class="string">"json"</span>)</span><br><span class="line">         .load(<span class="string">"D:\\student.json"</span>)</span><br><span class="line">         <span class="comment">//保存数据</span></span><br><span class="line">         stuDf.select(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">         .write</span><br><span class="line">         .format(<span class="string">"csv"</span>)</span><br><span class="line">         .save(<span class="string">"hdfs://bigdata01:9000/out-save001"</span>)</span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291340241.png" alt="image-20230329134033719"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291340871.png" alt="image-20230329134053438"></p><h4 id="java-5"><a href="#java-5" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：load和save的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoadAndSaveOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">         <span class="comment">//读取数据</span></span><br><span class="line">         Dataset&lt;Row&gt; stuDf = sparkSession.read()</span><br><span class="line">         .format(<span class="string">"json"</span>)</span><br><span class="line">         .load(<span class="string">"D:\\student.json"</span>);</span><br><span class="line">         <span class="comment">//保存数据</span></span><br><span class="line">         stuDf.select(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">         .write()</span><br><span class="line">         .format(<span class="string">"csv"</span>)</span><br><span class="line">         .save(<span class="string">"hdfs://bigdata01:9000/out-save002"</span>);</span><br><span class="line">         sparkSession.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="SaveMode"><a href="#SaveMode" class="headerlink" title="SaveMode"></a>SaveMode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL对于save操作，提供了不同的save mode。</span><br><span class="line">主要用来处理，当目标位置已经有数据时应该如何处理。save操作不会执行锁操作，并且也不是原子的，因此是有一定风险出现脏数据的。</span><br><span class="line"></span><br><span class="line">SaveMode 解释</span><br><span class="line">SaveMode.ErrorIfExists (默认) 如果目标位置已经存在数据，那么抛出一个异常</span><br><span class="line">SaveMode.Append 如果目标位置已经存在数据，那么将数据追加进去</span><br><span class="line">SaveMode.Overwrite 如果目标位置已经存在数据，那么就将已经存在的数据删除，用新数据进行覆盖</span><br><span class="line">SaveMode.Ignore 如果目标位置已经存在数据，那么就忽略，不做任何操作</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在LoadAndSaveOpScala中增加SaveMode的设置，重新执行，验证结果</span><br><span class="line">将SaveMode设置为Append，如果目标已存在，则追加</span><br><span class="line">stuDf.select(&quot;name&quot;,&quot;age&quot;)</span><br><span class="line"> .write</span><br><span class="line"> .format(&quot;csv&quot;)</span><br><span class="line"> .mode(SaveMode.Append)&#x2F;&#x2F;追加</span><br><span class="line"> .save(&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;out-save001&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291355158.png" alt="image-20230329135505416"></p><h4 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291356152.png" alt="image-20230329135637838"></p><h3 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Spark中提供了很多内置的函数</span><br><span class="line">种类 函数</span><br><span class="line">聚合函数 avg, count, countDistinct, first, last, max, mean, min, sum, </span><br><span class="line"></span><br><span class="line">集合函数 array_contains, explode, size</span><br><span class="line"></span><br><span class="line">日期&#x2F;时间函数 datediff, date_add, date_sub, add_months, last_day, next_day, </span><br><span class="line"></span><br><span class="line">数学函数 abs, ceil, floor, round</span><br><span class="line"></span><br><span class="line">混合函数 if, isnull, md5, not, rand, when</span><br><span class="line"></span><br><span class="line">字符串函数 concat, get_json_object, length, reverse, split, upper</span><br><span class="line"></span><br><span class="line">窗口函数 denseRank, rank, rowNumber</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">其实这里面的函数和hive中的函数是类似的</span><br><span class="line">注意：SparkSQL中的SQL函数文档不全，其实在使用这些函数的时候，大家完全可以去查看hive中sql的文档，使用的时候都是一样的。</span><br></pre></td></tr></table></figure><h2 id="实战：TopN主播统计"><a href="#实战：TopN主播统计" class="headerlink" title="实战：TopN主播统计"></a>实战：TopN主播统计</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在前面讲Spark core的时候我们讲过一个案例，TopN主播统计，计算每个大区当天金币收入TopN的主播，之前我们使用spark中的transformation算子去计算，实现起来还是比较麻烦的，代码量相对来说比较多，下面我们就使用咱们刚学习的Spark sql去实现一下，你会发现，使用sql之后确实简单多了。</span><br><span class="line"></span><br><span class="line">回顾以下我们的两份原始数据，数据都是json格式的</span><br><span class="line">video_info.log 主播的开播记录，其中包含主播的id：uid、直播间id：vid 、大区：area、视频开播时长：length、增加粉丝数量：follow等信息</span><br><span class="line">gift_record.log 用户送礼记录，其中包含送礼人id：uid，直播间id：vid，礼物id：good_id，金币数量：gold 等信息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最终需要的结果是这样的</span><br><span class="line">US 8407173251015:180,8407173251012:70,8407173251001:60</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">分析一下具体步骤</span><br><span class="line">1. 直接使用SparkSession中的load方式加载json的数据</span><br><span class="line">2. 对这两份数据注册临时表</span><br><span class="line">3. 执行sql计算TopN主播</span><br><span class="line">4. 使用foreach将结果打印到控制台</span><br></pre></td></tr></table></figure><h3 id="scala-6"><a href="#scala-6" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：计算TopN主播</span></span><br><span class="line"><span class="comment"> * 1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line"><span class="comment"> * 2：对这两份数据注册临时表</span></span><br><span class="line"><span class="comment"> * 3：执行sql计算TopN主播</span></span><br><span class="line"><span class="comment"> * 4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopNAnchorScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line">         <span class="keyword">val</span> videoInfoDf = sparkSession.read.json(<span class="string">"D:\\video_info.log"</span>)</span><br><span class="line">         <span class="keyword">val</span> giftRecordDf = sparkSession.read.json(<span class="string">"D:\\gift_record.log"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//2：对这两份数据注册临时表</span></span><br><span class="line">         videoInfoDf.createOrReplaceTempView(<span class="string">"video_info"</span>)</span><br><span class="line">         giftRecordDf.createOrReplaceTempView(<span class="string">"gift_record"</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//3：执行sql计算TopN主播</span></span><br><span class="line">         <span class="keyword">val</span> sql =<span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t4.area, "</span>+</span><br><span class="line">         <span class="string">"concat_ws(',',collect_list(t4.topn)) as topn_list "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t3.area,concat(t3.uid,':',cast(t3.gold_sum_all as int)) as topn "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t2.uid,t2.area,t2.gold_sum_all,row_number() over (partition by area orde</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>t1.uid,max(t1.area) as area,sum(t1.gold_sum) as gold_sum_all <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vi.uid,vi.vid,vi.area,gr.gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>video_info as vi <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>join <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>(select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vid,sum(gold) as gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>gift_record <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as gr <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>on vi.vid = gr.vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t1 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t1.uid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t2 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as t3 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>where t3.num &lt;=<span class="number">3</span> <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t4 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t4.area <span class="string">"</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         val resDf = sparkSession.sql(sql)</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         //4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="string">         resDf.rdd.foreach(row=&gt;println(row.getAs[String]("</span><span class="string">area")+"</span>\<span class="string">t"+row.getAs[String]))</span></span><br><span class="line"><span class="string">         sparkSession.stop()</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">t4.area,</span><br><span class="line"><span class="keyword">concat_ws</span>(<span class="string">','</span>,collect_list(t4.topn)) <span class="keyword">as</span> topn_list</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">t3.area,<span class="keyword">concat</span>(t3.uid,<span class="string">':'</span>,<span class="keyword">cast</span>(t3.gold_sum_all <span class="keyword">as</span> <span class="built_in">int</span>)) <span class="keyword">as</span> topn // <span class="keyword">cast</span>() <span class="keyword">as</span> <span class="built_in">int</span> 是因为结果中有xxx<span class="number">.0</span></span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">t2.uid,t2.area,t2.gold_sum_all,row_number() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> area <span class="keyword">order</span> <span class="keyword">by</span> gold_sum_all <span class="keyword">desc</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">t1.uid,<span class="keyword">max</span>(t1.area) <span class="keyword">as</span> area,<span class="keyword">sum</span>(t1.gold_sum) <span class="keyword">as</span> gold_sum_all</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">vi.uid,vi.vid,vi.area,gr.gold_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">video_info <span class="keyword">as</span> vi</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(<span class="keyword">select</span></span><br><span class="line">vid,<span class="keyword">sum</span>(gold) <span class="keyword">as</span> gold_sum</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">gift_record</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vid</span><br><span class="line">)<span class="keyword">as</span> gr</span><br><span class="line"><span class="keyword">on</span> vi.vid = gr.vid</span><br><span class="line">) <span class="keyword">as</span> t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t1.uid</span><br><span class="line">) <span class="keyword">as</span> t2</span><br><span class="line">)<span class="keyword">as</span> t3</span><br><span class="line"><span class="keyword">where</span> t3.num &lt;=<span class="number">3</span></span><br><span class="line">) <span class="keyword">as</span> t4</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t4.area</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3再select的结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301508636.png" alt="image-20230330150555300"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301509104.png" alt="image-20230330150618987"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t4的prinln(_)是这样时</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301510849.png" alt="image-20230330151021942"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最终结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303301511207.png" alt="image-20230330151135975"></p><h3 id="java-6"><a href="#java-6" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.sql;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：计算TopN主播</span></span><br><span class="line"><span class="comment"> * 1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line"><span class="comment"> * 2：对这两份数据注册临时表</span></span><br><span class="line"><span class="comment"> * 3：执行sql计算TopN主播</span></span><br><span class="line"><span class="comment"> * 4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopNAnchorJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setMaster(<span class="string">"local"</span>);</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">         .appName(<span class="string">"TopNAnchorJava"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate();</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line">         Dataset&lt;Row&gt; videoInfoDf = sparkSession.read().json(<span class="string">"D:\\video_info.log"</span>);</span><br><span class="line">         Dataset&lt;Row&gt; giftRecordDf = sparkSession.read().json(<span class="string">"D:\\gift_record.log"</span>);</span><br><span class="line">         <span class="comment">//2：对这两份数据注册临时表</span></span><br><span class="line">         videoInfoDf.createOrReplaceTempView(<span class="string">"video_info"</span>);</span><br><span class="line">         giftRecordDf.createOrReplaceTempView(<span class="string">"gift_record"</span>);</span><br><span class="line">         <span class="comment">//3：执行sql计算TopN主播</span></span><br><span class="line">         String sql = <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t4.area, "</span>+</span><br><span class="line">         <span class="string">"concat_ws(',',collect_list(t4.topn)) as topn_list "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t3.area,concat(t3.uid,':',cast(t3.gold_sum_all as int)) as t</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>t2.uid,t2.area,t2.gold_sum_all,row_number() over (partition </span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t1.uid,max(t1.area) as area,sum(t1.gold_sum) as gold_sum_all </span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vi.uid,vi.vid,vi.area,gr.gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>video_info as vi <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>join <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>(select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vid,sum(gold) as gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>gift_record <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as gr <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>on vi.vid = gr.vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t1 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t1.uid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t2 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as t3 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>where t3.num &lt;=<span class="number">3</span> <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t4 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t4.area <span class="string">";</span></span><br><span class="line"><span class="string">         Dataset&lt;Row&gt; resDf = sparkSession.sql(sql);</span></span><br><span class="line"><span class="string">         //4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="string">         resDf.javaRDD().foreach(new VoidFunction&lt;Row&gt;() &#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public void call(Row row) throws Exception &#123;</span></span><br><span class="line"><span class="string">         System.out.println(row.getString(0)+"</span>\t<span class="string">"+row.getString(1));</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         sparkSession.stop();</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">代码执行结果如下：</span><br><span class="line">CN 8407173251008:120,8407173251003:60,8407173251014:50</span><br><span class="line">ID 8407173251005:160,8407173251010:140,8407173251002:70</span><br><span class="line">US 8407173251015:180,8407173251012:70,8407173251001:60</span><br></pre></td></tr></table></figure><h3 id="集群执行"><a href="#集群执行" class="headerlink" title="集群执行"></a>集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">新建一个object：TopNAnchorClusterScala，修改代码，将任务的输出数据保存到hdfs上面</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.sql</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：计算TopN主播</span></span><br><span class="line"><span class="comment"> * 1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line"><span class="comment"> * 2：对这两份数据注册临时表</span></span><br><span class="line"><span class="comment"> * 3：执行sql计算TopN主播</span></span><br><span class="line"><span class="comment"> * 4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TopNAnchorClusterScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         <span class="comment">//创建SparkSession对象，里面包含SparkContext和SqlContext</span></span><br><span class="line">         <span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder()</span><br><span class="line">         .appName(<span class="string">"LoadAndSaveOpScala"</span>)</span><br><span class="line">         .config(conf)</span><br><span class="line">         .getOrCreate()</span><br><span class="line">         <span class="comment">//1：直接使用sparkSession中的load方式加载json数据</span></span><br><span class="line">         <span class="keyword">val</span> videoInfoDf = sparkSession.read.json(<span class="string">"hdfs://bigdata01:9000/video_inf</span></span><br><span class="line"><span class="string">         val giftRecordDf = sparkSession.read.json("</span>hdfs:<span class="comment">//bigdata01:9000/gift_rec</span></span><br><span class="line">         <span class="comment">//2：对这两份数据注册临时表</span></span><br><span class="line">         videoInfoDf.createOrReplaceTempView(<span class="string">"video_info"</span>)</span><br><span class="line">         giftRecordDf.createOrReplaceTempView(<span class="string">"gift_record"</span>)</span><br><span class="line">         <span class="comment">//3：执行sql计算TopN主播</span></span><br><span class="line">         <span class="keyword">val</span> sql =<span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t4.area, "</span>+</span><br><span class="line">         <span class="string">"concat_ws(',',collect_list(t4.topn)) as topn_list "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t3.area,concat(t3.uid,':',cast(t3.gold_sum_all as int)) as topn "</span>+</span><br><span class="line">         <span class="string">"from( "</span>+</span><br><span class="line">         <span class="string">"select "</span>+</span><br><span class="line">         <span class="string">"t2.uid,t2.area,t2.gold_sum_all,row_number() over (partition by area orde</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>t1.uid,max(t1.area) as area,sum(t1.gold_sum) as gold_sum_all <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from( <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vi.uid,vi.vid,vi.area,gr.gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>video_info as vi <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>join <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>(select <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>vid,sum(gold) as gold_sum <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>from <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>gift_record <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as gr <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>on vi.vid = gr.vid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t1 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t1.uid <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t2 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>)as t3 <span class="string">"+</span></span><br><span class="line"><span class="string">        "</span>where t3.num &lt;=<span class="number">3</span> <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>) as t4 <span class="string">"+</span></span><br><span class="line"><span class="string">         "</span>group by t4.area <span class="string">"</span></span><br><span class="line"><span class="string">         val resDf = sparkSession.sql(sql)</span></span><br><span class="line"><span class="string">         //4：使用foreach将结果打印到控制台</span></span><br><span class="line"><span class="string">         resDf.rdd</span></span><br><span class="line"><span class="string">         .map(row=&gt;row.getAs[String]("</span><span class="string">area")+"</span>\<span class="string">t"+row.getAs[String]("</span>topn_<span class="string">list")</span></span><br><span class="line"><span class="string">         .saveAsTextFile("</span>hdfs:<span class="comment">//bigdata01:9000/out-topn")</span></span><br><span class="line">         sparkSession.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">修改pom中依赖的配置，全部设置为provided</span><br><span class="line"></span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">         &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">         &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">         &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">         &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">         &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">         &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt;</span><br><span class="line">         &lt;version&gt;1.2.68&lt;&#x2F;version&gt;</span><br><span class="line">         &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">         &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">         &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">         &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line">         &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对spark-core和spark-sql在打包的时候是不需要的，针对fastjson有的spark job是需要的，不过建议</span><br><span class="line">在这设置为provided，打包的时候不要打进去，在具体使用的时候可以在spark-submit脚本中通过–jar</span><br><span class="line">来动态指定这个jar包，最好把这个jar包上传到hdfs上面统一管理和维护。</span><br><span class="line">编译打包，上传到bigdta04上的 &#x2F;data&#x2F;soft&#x2F;sparkjars 目录</span><br><span class="line">创建spark-submit脚本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 sparkjars]# vi topnJob.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.sql.TopNAnchorClusterScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot; \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">提交任务</span><br><span class="line">[root@bigdata04 sparkjars]# sh -x topnJob.sh</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303291557223.png" alt="image-20230329155709778"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E7%AE%97%E5%AD%90%E4%BC%98%E5%8C%96.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E7%AE%97%E5%AD%90%E4%BC%98%E5%8C%96.html</id>
    <published>2023-03-27T17:50:58.000Z</published>
    <updated>2023-03-28T07:20:39.448Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-算子优化"><a href="#第十一周-Spark性能优化的道与术-算子优化" class="headerlink" title="第十一周 Spark性能优化的道与术-算子优化"></a>第十一周 Spark性能优化的道与术-算子优化</h1><h2 id="map-vs-mapPartitions"><a href="#map-vs-mapPartitions" class="headerlink" title="map vs mapPartitions"></a>map vs mapPartitions</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">map操作：对RDD 中的每个元素进行操作，一次处理一条数据</span><br><span class="line"></span><br><span class="line">mapPartitions操作(transformation操作)：对RDD中每个partition进行操作，一次处理一个分区的数据</span><br><span class="line"></span><br><span class="line">所以：</span><br><span class="line">map操作：执行1次map算子只处理1个元素，如果partition中元素较多，假设当前已经处理了1000个元素，在内存不足的情况下，Spark可以通过GC等方法回收内存（比如将已处理掉的</span><br><span class="line">1000个元素从内存中回收）。因此，map操作通常不会导致OOM异常；</span><br><span class="line"></span><br><span class="line">mapPartitions操作：执行1次map算子需要接收该partition 中的所有元素，因此一旦元素很多而内存不足，就容易导致OOM的异常，也不是说一定就会产生OOM异常，只是和map算子对比的话，</span><br><span class="line">相对来说容易产生OOM异常(OOM，java.lang.OutOfMemoryError 错误，也就是java内存溢出错误。)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">不过一般情况下，mapPartitions的性能更高；初始化操作、数据库链接等操作适合使用mapPartitions操作</span><br><span class="line"></span><br><span class="line">这是因为：</span><br><span class="line">假设需要将RDD中的每个元素写入数据库中，这时候就应该把创建数据库链接的操作放置在mapPartitions中，创建数据库链接这个操作本身就是个比较耗时的，如果该操作放在map中执行，将会频繁执行，比较耗时且影响数据库的稳定性。</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：mapPartitons的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapPartitionsOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"MapPartitionsOpScala"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="comment">//设置分区数量为2</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>),<span class="number">2</span>)</span><br><span class="line">         <span class="comment">//map算子一次处理一条数据</span></span><br><span class="line">         <span class="comment">/*val sum = dataRDD.map(item=&gt;&#123;</span></span><br><span class="line"><span class="comment">         println("==============")</span></span><br><span class="line"><span class="comment">         item * 2</span></span><br><span class="line"><span class="comment">         &#125;).reduce( _ + _)*/</span></span><br><span class="line">        <span class="comment">//mapPartitions算子一次处理一个分区的数据</span></span><br><span class="line">         <span class="keyword">val</span> sum = dataRDD.mapPartitions(it=&gt;&#123;</span><br><span class="line">         <span class="comment">//建议针对初始化链接之类的操作，使用mapPartitions，放在mapPartitions内部</span></span><br><span class="line">         <span class="comment">//例如：创建数据库链接，使用mapPartitions可以减少链接创建的次数，提高性能</span></span><br><span class="line">         <span class="comment">//注意：创建数据库链接的代码建议放在次数，不要放在Driver端或者it.foreach内部</span></span><br><span class="line">         <span class="comment">//数据库链接放在Driver端会导致链接无法序列化，无法传递到对应的task中执行，所以</span></span><br><span class="line">         <span class="comment">//数据库链接放在it.foreach()内部还是会创建多个链接，和使用map算子的效果是一样</span></span><br><span class="line">         println(<span class="string">"=================="</span>)</span><br><span class="line">         <span class="keyword">val</span> result = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">         <span class="comment">//这个foreach是调用的scala里面的函数</span></span><br><span class="line">         it.foreach(item=&gt;&#123;</span><br><span class="line">         result.+=(item * <span class="number">2</span>)</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">         result.toIterator</span><br><span class="line">         &#125;).reduce(_ + _)</span><br><span class="line">         println(<span class="string">"sum:"</span>+sum)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：mapPartitons的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapPartitionsOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"MapPartitionsOpJava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>));</span><br><span class="line">         Integer sum = dataRDD.mapPartitions(<span class="keyword">new</span> FlatMapFunction&lt;Iterator&lt;Inte</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterator&lt;Integer&gt; <span class="title">call</span><span class="params">(Iterator&lt;Integer&gt; it)</span> <span class="keyword">throws</span> Except</span></span><br><span class="line"><span class="function">         <span class="comment">//数据库链接的代码需要放在这个位置</span></span></span><br><span class="line"><span class="function">         ArrayList&lt;Integer&gt; list </span>= <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">         list.add(it.next() * <span class="number">2</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">        &#125;</span><br><span class="line">         &#125;).reduce(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> i1 + i2;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         System.out.println(<span class="string">"sum:"</span>+sum);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="foreach-vs-foreachPartition"><a href="#foreach-vs-foreachPartition" class="headerlink" title="foreach vs foreachPartition"></a>foreach vs foreachPartition</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">foreach：一次处理一条数据</span><br><span class="line">foreachPartition：一次处理一个分区的数据</span><br><span class="line">foreachPartition的特性和mapPartitions的特性是一样的，唯一的区别就是mapPartitions是transformation操作（不会立即执行），foreachPartition是action操作（会立即执行）</span><br></pre></td></tr></table></figure><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：foreachPartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ForeachPartitionOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"ForeachPartitionOpScala"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="comment">//设置分区数量为2</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>),<span class="number">2</span>)</span><br><span class="line">         <span class="comment">//foreachPartition：一次处理一个分区的数据，作用和mapPartitions类似</span></span><br><span class="line">         <span class="comment">//唯一的区是mapPartitions是transformation算子，foreachPartition是action算子</span></span><br><span class="line">         dataRDD.foreachPartition(it=&gt;&#123;</span><br><span class="line">         <span class="comment">//在此处获取数据库链接</span></span><br><span class="line">         println(<span class="string">"==============="</span>)</span><br><span class="line">         it.foreach(item=&gt;&#123; <span class="comment">// 和对RDD处理的foreach不一样</span></span><br><span class="line">             <span class="comment">//在这里使用数据库链接</span></span><br><span class="line">             println(item)</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">         &#125;)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281441692.png" alt="image-20230328144113035"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281440127.png" alt="image-20230328144052865"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：foreachPartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ForeachPartitionOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"ForeachPartitionOpJava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span></span><br><span class="line">         dataRDD.foreachPartition(<span class="keyword">new</span> VoidFunction&lt;Iterator&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Iterator&lt;Integer&gt; it)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         System.out.println(<span class="string">"============="</span>);</span><br><span class="line">         <span class="comment">//在此处获取数据库链接</span></span><br><span class="line">         <span class="keyword">while</span> (it.hasNext())&#123;</span><br><span class="line">         <span class="comment">//在这里使用数据库链接</span></span><br><span class="line">         System.out.println(it.next());</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//关闭数据库链接</span></span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="repartition的使用"><a href="#repartition的使用" class="headerlink" title="repartition的使用"></a>repartition的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">对RDD进行重分区，repartition主要有两个应用场景：</span><br><span class="line">1. 可以调整RDD的并行度</span><br><span class="line">针对个别RDD，如果感觉分区数量不合适，想要调整，可以通过repartition进行调整，分区调整了之后，对应的并行度也就可以调整了</span><br><span class="line">2. 可以解决RDD中数据倾斜的问题</span><br><span class="line">如果RDD中不同分区之间的数据出现了数据倾斜，可以通过repartition实现数据重新分发，可以均匀分发到不同分区中</span><br></pre></td></tr></table></figure><h3 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：repartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RepartitionOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"RepartitionOpScala"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="comment">//设置分区数量为2</span></span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>),<span class="number">2</span>)</span><br><span class="line">         <span class="comment">//重新设置RDD的分区数量为3，这个操作会产生shuffle</span></span><br><span class="line">         <span class="comment">//也可以解决RDD中数据倾斜的问题</span></span><br><span class="line">         dataRDD.repartition(<span class="number">3</span>)</span><br><span class="line">         .foreachPartition(it=&gt;&#123;</span><br><span class="line">         println(<span class="string">"========="</span>) <span class="comment">// 运行程序会输出3次</span></span><br><span class="line">         it.foreach(println(_))</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//通过repartition可以控制输出数据产生的文件个数</span></span><br><span class="line">         dataRDD.saveAsTextFile(<span class="string">"hdfs://bigdata01:9000/rep-001"</span>) <span class="comment">// 生成3个文件</span></span><br><span class="line">                dataRDD.repartition(<span class="number">1</span>).saveAsTextFile(<span class="string">"hdfs://bigdata01:9000/rep-002"</span>) <span class="comment">// 生成1个文件 </span></span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：repartition的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RepartitionOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"RepartitionOpJava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>),<span class="number">2</span>);</span><br><span class="line">         dataRDD.repartition(<span class="number">3</span>)</span><br><span class="line">         .foreachPartition(<span class="keyword">new</span> VoidFunction&lt;Iterator&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Iterator&lt;Integer&gt; it)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         System.out.println(<span class="string">"=============="</span>);</span><br><span class="line">         <span class="keyword">while</span> (it.hasNext())&#123;</span><br><span class="line">         System.out.println(it.next());</span><br><span class="line">         &#125;</span><br><span class="line">             &#125;);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="reduceByKey和groupByKey的区别"><a href="#reduceByKey和groupByKey的区别" class="headerlink" title="reduceByKey和groupByKey的区别"></a>reduceByKey和groupByKey的区别</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">在实现分组聚合功能时这两个算子有什么区别？</span><br><span class="line">看这两行代码</span><br><span class="line">val counts &#x3D; wordCountRDD.reduceByKey(_ + _)</span><br><span class="line">val counts &#x3D; wordCountRDD.groupByKey().map(wc &#x3D;&gt; (wc._1, wc._2.sum))</span><br><span class="line"></span><br><span class="line">这两行代码的最终效果是一样的，都是对wordCountRDD中每个单词出现的次数进行聚合统计</span><br><span class="line">那这两种方式在原理层面有什么区别吗？</span><br><span class="line">首先这两个算子在执行的时候都会产生shuffle</span><br><span class="line">但是：</span><br><span class="line">1：当采用reduceByKey时，数据在进行shuffle之前会先进行局部聚合</span><br><span class="line">2：当使用groupByKey时，数据在shuffle之间不会进行局部聚合，会原样进行shuffle</span><br><span class="line"></span><br><span class="line">这样的话reduceByKey就减少了shuffle的数据传送，所以效率会高一些。</span><br><span class="line">如果能用reduceByKey，就用reduceByKey，因为它会在map端，先进行本地combine，可以大大减少要传输到reduce端的数据量，减少网络传输的开销</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面来看这个图，加深一下理解</span><br><span class="line"></span><br><span class="line">从图中可以看出来reduceByKey在shuffle之前会先对数据进行局部聚合，而groupByKey不会，所以在实现分组聚合的需求中，reduceByKey性能略胜一筹。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281513504.png" alt="image-20230328151345043"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html</id>
    <published>2023-03-27T11:31:29.000Z</published>
    <updated>2023-03-29T13:01:48.123Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-企业级最佳实践"><a href="#第十一周-Spark性能优化的道与术-企业级最佳实践" class="headerlink" title="第十一周 Spark性能优化的道与术-企业级最佳实践"></a>第十一周 Spark性能优化的道与术-企业级最佳实践</h1><h2 id="性能优化分析"><a href="#性能优化分析" class="headerlink" title="性能优化分析"></a>性能优化分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一个计算任务的执行主要依赖于CPU、内存、带宽</span><br><span class="line">Spark是一个基于内存的计算引擎，所以对它来说，影响最大的可能就是内存，一般我们的任务遇到了性能瓶颈大概率都是内存的问题，当然了CPU和带宽也可能会影响程序的性能，这个情况也不是没有的，只是比较少。</span><br><span class="line">Spark性能优化，其实主要就是在于对内存的使用进行调优。通常情况下，如果你的Spark程序计算的数据量比较小，并且你的内存足够使用，那么只要网络不至于卡死，一般是不会有大的性能问题的。但是Spark程序的性能问题往往出现在针对大数据量进行计算（比如上亿条数的数据，或者上T规模的数据），这个时候如果内存分配不合理就会比较慢，所以，Spark性能优化，主要是对内存进行优化。</span><br></pre></td></tr></table></figure><h2 id="内存都去哪了"><a href="#内存都去哪了" class="headerlink" title="内存都去哪了"></a>内存都去哪了</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 每个Java对象，都有一个对象头，会占用16个字节，主要是包括了一些对象的元信息，比如指向它的类的指针。如果一个对象本身很小，比如就包括了一个int类型的field，那么它的对象头实际上比对象自身还要大。</span><br><span class="line">2. Java的String对象的对象头，会比它内部的原始数据，要多出40个字节。因为它内部使用char数组来保存内部的字符序列，并且还要保存数组长度之类的信息。</span><br><span class="line">3. Java中的集合类型，比如HashMap和LinkedList，内部使用的是链表数据结构，所以对链表中的每一个数据，都使用了Entry对象来包装。Entry对象不光有对象头，还有指向下一个Entry的指针，通常占用8个字节。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">所以把原始文件中的数据转化为内存中的对象之后，占用的内存会比原始文件中的数据要大</span><br><span class="line"></span><br><span class="line">那我如何预估程序会消耗多少内存呢？</span><br><span class="line">通过cache方法，可以看到RDD中的数据cache到内存中之后占用多少内存，这样就能看出了</span><br><span class="line">代码如下：这个测试代码就只写一个scala版本的了</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：测试内存占用情况</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestMemoryScala</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">     conf.setAppName(<span class="string">"TestMemoryScala"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">     <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_10000000.dat"</span>).cache()</span><br><span class="line">     <span class="keyword">val</span> count = dataRDD.count()</span><br><span class="line">         println(count)</span><br><span class="line">     <span class="comment">//while循环是为了保证程序不结束，方便在本地查看4040页面(在本地运行时的spark web页面)中的storage信息</span></span><br><span class="line">     <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">     ;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271945127.png" alt="image-20230327194520543"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271944394.png" alt="image-20230327194456852"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">执行代码，访问localhost的4040端口界面</span><br><span class="line">这个界面其实就是spark的任务界面，在本地运行任务的话可以直接访问4040界面查看</span><br><span class="line">点击stages可以看到任务的原始输入数据是多大</span><br><span class="line"></span><br><span class="line">点击storage可以看到将数据加载到内存，生成RDD之后的大小</span><br><span class="line"></span><br><span class="line">这样我们就能知道这一份数据在RDD中会占用多少内存了，这样在使用的时候，如果想要把数据全部都加载进内存，就需要给这个任务分配这么多内存了，当然了你分配少一些也可以，只不过这样计算效率会变低，因为RDD中的部分数据内存放不下就会放到磁盘了。</span><br></pre></td></tr></table></figure><h2 id="性能优化方案"><a href="#性能优化方案" class="headerlink" title="性能优化方案"></a>性能优化方案</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">下面我们通过这几个方式来实现对Spark程序的性能优化</span><br><span class="line">    高性能序列化类库</span><br><span class="line">    持久化或者checkpoint</span><br><span class="line">    JVM垃圾回收调优</span><br><span class="line">    提高并行度</span><br><span class="line">    数据本地化</span><br><span class="line">    算子优化</span><br></pre></td></tr></table></figure><h3 id="高性能序列化类库"><a href="#高性能序列化类库" class="headerlink" title="高性能序列化类库"></a>高性能序列化类库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在任何分布式系统中，序列化都是扮演着一个重要的角色的。</span><br><span class="line">如果使用的序列化技术，在执行序列化操作的时候很慢，或者是序列化后的数据还是很大，那么会让分布式应用程序的性能下降很多。所以，进行Spark性能优化的第一步，就是进行序列化的性能优化。</span><br><span class="line"></span><br><span class="line">Spark默认会在一些地方对数据进行序列化，如果我们的算子函数使用到了外部的数据（比如Java中的自定义类型），那么也需要让其可序列化，否则程序在执行的时候是会报错的，提示没有实现序列化，这个一定要注意。</span><br><span class="line"></span><br><span class="line">原因是这样的：</span><br><span class="line">因为Spark的初始化工作是在Driver进程中进行的，但是实际执行是在Worker节点的Executor进程中进行的；当Executor端需要用到Driver端封装的对象时，就需要把Driver端的对象通过序列化传输到Executor端，这个对象就需要实现序列化。</span><br><span class="line">否则会报错，提示对象没有实现序列化</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，其实遇到这种没有实现序列化的对象，解决方法有两种</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 如果此对象可以支持序列化，则将其实现Serializable接口，让它支持序列化</span><br><span class="line">2. 如果此对象不支持序列化，针对一些数据库连接之类的对象，这种对象是不支持序列化的，所以可以把这个代码放到算子内部，这样就不会通过driver端传过去了，它会直接在executor中执行。</span><br><span class="line">Spark对于序列化的便捷性和性能进行了一个取舍和权衡。默认情况下，Spark倾向于序列化的便捷性，使用了Java自身提供的序列化机制——基于ObjectInputStream和 ObjectOutputStream的序列化机制，因为这种方式是Java原生提供的，使用起来比较方便，但是Java序列化机制的性能并不高。序列化的速度相对较慢，而且序列化以后的数据，相对来说还是比较大，比较占空间。所以，如果你的Spark应用程序对内存很敏感，那默认的Java序列化机制并不是最好的选择。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark实际上提供了两种序列化机制：</span><br><span class="line">Java序列化机制和Kryo序列化机制</span><br><span class="line">Spark只是默认使用了java这种序列化机制</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Java序列化机制：默认情况下，Spark使用Java自身的ObjectInputStream和ObjectOutputStream机制进行对象的序列化。只要你的类实现了Serializable接口，那么都是可以序列化的。Java序列化机制的速度比较慢，而且序列化后的数据占用的内存空间比较大，这是它的缺点</span><br><span class="line"></span><br><span class="line">Kryo序列化机制：Spark也支持使用Kryo序列化。Kryo序列化机制比Java序列化机制更快，而且序列化后的数据占用的空间更小，通常比Java序列化的数据占用的空间要小10倍左右。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Kryo序列化机制之所以不是默认序列化机制的原因：</span><br><span class="line"></span><br><span class="line">第一点：因为有些类型虽然实现了Seriralizable接口，但是它也不一定能够被Kryo进行序列化；</span><br><span class="line">第二点：如果你要得到最佳的性能，Kryo还要求你在Spark应用程序中，对所有你需要序列化的类型都进行手工注册，这样就比较麻烦了</span><br><span class="line"></span><br><span class="line">如果要使用Kryo序列化机制</span><br><span class="line">首先要用SparkConf设置spark.serializer的值为 org.apache.spark.serializer.KryoSerializer，就是将Spark的序列化器设置为KryoSerializer。这样，Spark在进行序列化时，就会使用Kryo进行序列化</span><br><span class="line">了。使用Kryo时针对需要序列化的类，需要预先进行注册，这样才能获得最佳性能——如果不注册的话，Kryo也能正常工作，只是Kryo必须时刻保存类型的全类名，反而占用不少内存。</span><br><span class="line">Spark默认对Scala中常用的类型在Kryo中做了注册，但是，如果在自己的算子中，使用了外部的自定义类型的对象，那么还是需要对其进行注册。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注册自定义的数据类型格式：</span><br><span class="line">conf.registerKryoClasses(...)</span><br><span class="line"></span><br><span class="line">注意：如果要序列化的自定义的类型，字段特别多，此时就需要对Kryo本身进行优化，因为Kryo内部的缓存可能不够存放那么大的class对象</span><br><span class="line"></span><br><span class="line">需要调用SparkConf.set()方法，设置spark.kryoserializer.buffer.mb参数的值，将其调大，默认值为2，单位是MB ，也就是说最大能缓存2M的对象，然后进行序列化。可以在必要时将其调大。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">什么场景下适合使用Kryo序列化？</span><br><span class="line"></span><br><span class="line">一般是针对一些自定义的对象，例如我们自己定义了一个对象，这个对象里面包含了几十M，或者上百M的数据，然后在算子函数内部，使用到了这个外部的大对象</span><br><span class="line">如果默认情况下，让Spark用java序列化机制来序列化这种外部的大对象，那么就会导致序列化速度比较慢，并且序列化以后的数据还是比较大。</span><br><span class="line"></span><br><span class="line">所以，在这种情况下，比较适合使用Kryo序列化类库，来对外部的大对象进行序列化，提高序列化速度，减少序列化后的内存空间占用。</span><br><span class="line">用代码实现一个案例：</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><h4 id="使用kryo实现序列化"><a href="#使用kryo实现序列化" class="headerlink" title="使用kryo实现序列化"></a>使用kryo实现序列化</h4><h5 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.<span class="type">Kryo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.<span class="type">KryoRegistrator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Kryo序列化的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KryoSerScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">     conf.setAppName(<span class="string">"KryoSerScala"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     <span class="comment">//指定使用kryo序列化机制，注意：如果使用了registerKryoClasses，其实这一行设置可以省略的</span></span><br><span class="line">     .set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">     .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Person</span>]))<span class="comment">//注册自定义的数据类型(Array里也可以传多个)，注册了性能高一些</span></span><br><span class="line">     <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">     <span class="keyword">val</span> dataRDD = sc.parallelize(<span class="type">Array</span>(<span class="string">"hello you"</span>,<span class="string">"hello me"</span>))</span><br><span class="line">     <span class="keyword">val</span> wordsRDD = dataRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">     <span class="keyword">val</span> personRDD = wordsRDD.map(word=&gt;<span class="type">Person</span>(word,<span class="number">18</span>)).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>)</span><br><span class="line">     personRDD.foreach(println(_))</span><br><span class="line">     <span class="comment">//while循环是为了保证程序不结束，方便在本地查看4040页面中的storage信息</span></span><br><span class="line">     <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">     ;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">                                                                 <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>,age: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span></span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行任务，然后访问localhost的4040界面</span><br><span class="line">在界面中可以看到cache的数据大小是 31 字节。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272100115.png" alt="image-20230327210029655"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那我们把kryo序列化设置去掉，使用默认的java序列化看一下效果</span><br><span class="line">修改代码，注释掉这两行代码即可</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">&#x2F;&#x2F;.registerKryoClasses(Array(classOf[Person]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行任务，再访问4040界面</span><br><span class="line">发现此时占用的内存空间是 138 字节，比使用kryo的方式内存空间多占用了将近5倍。</span><br><span class="line">所以从这可以看出来，使用 kryo 序列化方式对内存的占用会降低很多。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272101635.png" alt="image-20230327210118413"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：如果我们只是将spark的序列化机制改为了kryo序列化，但是没有对使用到的自定义类型手工进行注册，那么此时内存的占用会介于前面两种情况之间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改代码，只注释掉registerKryoClasses这一行代码</span><br><span class="line"></span><br><span class="line">.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">&#x2F;&#x2F;.registerKryoClasses(Array(classOf[Person]))&#x2F;&#x2F;注册自定义的数据类型</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">运行任务，再访问4040界面</span><br><span class="line">发现此时的内存占用为123字节，介于前面的31字节和138字节之间。</span><br><span class="line">所以从这可以看出来，在使用kryo序列化的时候，针对自定义的类型最好是手工注册一下，否则就算开启了kryo序列化，性能的提升也是有限的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272103931.png" alt="image-20230327210309583"></p><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.Kryo;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.serializer.KryoRegistrator;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.StorageLevel;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Kryo序列化的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KryoSerjava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//创建SparkContext：</span></span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"KryoSerjava"</span>)</span><br><span class="line">         .setMaster(<span class="string">"local"</span>)</span><br><span class="line">         .set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSer</span></span><br><span class="line"><span class="string">         .set("</span>spark.kryo.classesToRegister<span class="string">", "</span>com.imooc.java.Person<span class="string">");</span></span><br><span class="line"><span class="string">         JavaSparkContext sc = new JavaSparkContext(conf);</span></span><br><span class="line"><span class="string">         JavaRDD&lt;String&gt; dataRDD = sc.parallelize(Arrays.asList("</span>hello you<span class="string">", "</span>hello me<span class="string">"));</span></span><br><span class="line"><span class="string">         JavaRDD&lt;String&gt; wordsRDD = dataRDD.flatMap(new FlatMapFunction&lt;String,String&gt;()&#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public Iterator&lt;String&gt; call(String line) throws Exception &#123;</span></span><br><span class="line"><span class="string">         return Arrays.asList(line.split("</span> <span class="string">")).iterator();</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         JavaRDD&lt;Person&gt; personRDD = wordsRDD.map(new Function&lt;String, Person&gt;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public Person call(String word) throws Exception &#123;</span></span><br><span class="line"><span class="string">         return new Person(word, 18);</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;).persist(StorageLevel.MEMORY_ONLY_SER());</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">         personRDD.foreach(new VoidFunction&lt;Person&gt;() &#123;</span></span><br><span class="line"><span class="string">         @Override</span></span><br><span class="line"><span class="string">         public void call(Person person) throws Exception &#123;</span></span><br><span class="line"><span class="string">         System.out.println(person);</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;);</span></span><br><span class="line"><span class="string">         while (true)&#123;</span></span><br><span class="line"><span class="string">         ;</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">class Person implements Serializable&#123;</span></span><br><span class="line"><span class="string">    private String name;</span></span><br><span class="line"><span class="string">    private int age;</span></span><br><span class="line"><span class="string">    Person(String name,int age)&#123; // 这里讲可以通过什么自动生成，没听清</span></span><br><span class="line"><span class="string">        this.name = name;</span></span><br><span class="line"><span class="string">        this.age = age;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    @Override</span></span><br><span class="line"><span class="string">    public String toString() &#123; // alt+shift+s,右键generate-&gt;toString</span></span><br><span class="line"><span class="string">        return "</span>Person&#123;<span class="string">" +</span></span><br><span class="line"><span class="string">            "</span>name=<span class="string">'" + name + '</span>\<span class="string">''</span> +</span><br><span class="line">            <span class="string">", age="</span> + age +</span><br><span class="line">            <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="持久化或者checkpoint"><a href="#持久化或者checkpoint" class="headerlink" title="持久化或者checkpoint"></a>持久化或者checkpoint</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对程序中多次被transformation或者action操作的RDD进行持久化操作，避免对一个RDD反复进行计算，再进一步优化，使用Kryo序列化的持久化级别，减少内存占用</span><br><span class="line">为了保证RDD持久化数据在可能丢失的情况下还能实现高可靠，则需要对RDD执行Checkpoint操作</span><br><span class="line">这两个操作我们前面讲过了，在这就不再演示了</span><br></pre></td></tr></table></figure><h3 id="JVM垃圾回收调优"><a href="#JVM垃圾回收调优" class="headerlink" title="JVM垃圾回收调优"></a>JVM垃圾回收调优</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">由于Spark是基于内存的计算引擎，RDD缓存的数据，以及算子执行期间创建的对象都是放在内存中的，所以针对Spark任务如果内存设置不合理会导致大部分时间都消耗在垃圾回收上</span><br><span class="line"></span><br><span class="line">对于垃圾回收来说，最重要的就是调节RDD缓存占用的内存空间，和算子执行时创建的对象占用的内存空间的比例。</span><br><span class="line"></span><br><span class="line">默认情况下，Spark使用每个executor 60%的内存空间来缓存RDD，那么只有40%的内存空间来存放算子执行期间创建的对象</span><br><span class="line">在这种情况下，可能由于内存空间的不足，并且算子对应的task任务在运行时创建的对象过大，那么一旦发现40%的内存空间不够用了，就会触发Java虚拟机的垃圾回收操作。</span><br><span class="line"></span><br><span class="line">因此在极端情况下，垃圾回收操作可能会被频繁触发。</span><br><span class="line">在这种情况下，如果发现垃圾回收频繁发生。那么就需要对这个比例进行调优了， spark.storage.memoryFraction参数的值默认是0.6。</span><br><span class="line">使用SparkConf().set(&quot;spark.storage.memoryFraction&quot;, &quot;0.5&quot;) 可以进行修改，就是将RDD缓存占用内存空间的比例降低为 50% ，从而提供更多的内存空间来保存task运行时创建的对象。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">因此，对于RDD持久化而言，完全可以使用Kryo序列化，加上降低其executor内存占比的方式，来减少其内存消耗。给task提供更多的内存，从而避免task在执行时频繁触发垃圾回收。</span><br><span class="line">我们可以对task的垃圾回收进行监测，在spark的任务执行界面，可以查看每个task执行消耗的时间，以及task gc消耗的时间。</span><br><span class="line"></span><br><span class="line">重新向集群中提交checkpoint的代码，查看spark任务的task指标信息</span><br><span class="line">确保Hadoop集群、yarn的historyserver进程以及spark的historyserver进程是正常运行的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">删除checkpoint任务的输出目录</span><br><span class="line">[root@bigdata04 sparkjars]# hdfs dfs -rm -r &#x2F;out-chk001</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# sh -x checkPointJob.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击生成的第一个job，再点击进去查看这个job的stage，进入第一个stage，查看task的执行情况，看这里面的GC time的数值会不会比较大，最直观的就是如果gc time这里标红了，则说明gc时间过长。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272230037.png" alt="image-20230327223033714"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上面这个是分任务查看，其实还可以查看全局的，看Executor进程中整个任务执行总时间和gc的消耗时间。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272232315.png" alt="image-20230327223148318"></p><h4 id="java-GC"><a href="#java-GC" class="headerlink" title="java GC"></a>java GC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">既然说到了Java中的GC，那我们就需要说道说道了。</span><br><span class="line">Java堆空间被划分成了两块空间：一个是年轻代，一个是老年代。</span><br><span class="line">年轻代放的是短时间存活的对象</span><br><span class="line">老年代放的是长时间存活的对象。</span><br><span class="line">年轻代又被划分了三块空间， Eden、Survivor1、Survivor2</span><br><span class="line"></span><br><span class="line">来看一下这个内存划分比例图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303272234159.png" alt="image-20230327223408841"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">年轻代占堆内存的1&#x2F;3，老年代占堆内存的2&#x2F;3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">其中年轻代又被划分了三块， Eden，Survivor1，Survivor2 的比例为 8:1:1</span><br><span class="line">Eden区域和Survivor1区域用于存放对象，Survivor2区域备用。</span><br><span class="line"></span><br><span class="line">我们创建的对象，首先会放入Eden区域，如果Eden区域满了，那么就会触发一次Minor GC，进行年轻代的垃圾回收(其实就是回收Eden区域内没有人使用的对象)，然后将存活的对象存入Survivor1区域，再创建对象的时候继续放入Eden区域。</span><br><span class="line"></span><br><span class="line">第二次Eden区域满了，那么Eden和Survivor1区域中存活的对象，会一块被移动到Survivor2区域中。然后Survivor1和Survivor2的角色调换，Survivor1变成了备用。</span><br><span class="line"></span><br><span class="line">当第三次Eden区域再满了的时候，Eden和Survivor2区域中存活的对象，会一块被移动到Survivor1区域中，按照这个规律进行循环</span><br><span class="line"></span><br><span class="line">如果一个对象，在年轻代中，撑过了多次垃圾回收(默认是15次)，都没有被回收掉，那么会被认为是长时间存活的，此时就会被移入老年代。此外，如果在将Eden和Survivor1中的存活对象，尝试放入Survivor2中时，发现Survivor2放满了，那么会直接放入老年代。此时就出现了，短时间存活的对象，也会进入老年代的问题。</span><br><span class="line"></span><br><span class="line">如果老年代的空间满了，那么就会触发Full GC，进行老年代的垃圾回收操作，如果执行Full GC也释放不了内存空间，就会报内存溢出的错误了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，Full GC是一个重量级的垃圾回收，Full GC执行的时候，程序是处于暂停状态的，这样会非常影响性能。</span><br></pre></td></tr></table></figure><h4 id="spark-GC调优方案"><a href="#spark-GC调优方案" class="headerlink" title="spark GC调优方案"></a>spark GC调优方案</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Spark中，垃圾回收调优的目标就是，只有真正长时间存活的对象，才能进入老年代，短时间存活的对象，只能呆在年轻代。不能因为某个Survivor区域空间不够，在Minor GC时，就进入了老年代，从而造成短时间存活的对象，长期呆在老年代中占据了空间，这样Full GC时要回收大量的短时间存活的对象，导致Full GC速度缓慢。</span><br><span class="line"></span><br><span class="line">如果发现，在task执行期间，大量full gc发生了，那么说明，年轻代的Eden区域，给的空间不够大。此时可以执行一些操作来优化垃圾回收行为</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1：最直接的就是提高Executor的内存</span><br><span class="line">在spark-submit中通过参数指定executor的内存</span><br><span class="line">--executor-memory 1G </span><br><span class="line"></span><br><span class="line">2：调整Eden与s1和s2的比值【一般情况下不建议调整这块的比值】</span><br><span class="line">-XX:NewRatio&#x3D;4：设置年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代).设置为4,则年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1&#x2F;5</span><br><span class="line">-XX:SurvivorRatio&#x3D;4：设置年轻代中Eden区与Survivor区的大小比值.设置为4,则两个Survivor区与一个Eden区的比值为2:4,一个Survivor区占整个年轻代的1&#x2F;6</span><br><span class="line">具体使用的时候在 spark-submit 脚本中通过 --conf 参数设置即可</span><br><span class="line">--conf &quot;spark.executor.extraJavaOptions&#x3D; -XX:SurvivorRatio&#x3D;4 -XX:NewRatio&#x3D;4&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">其实最直接的就是增加Executor的内存，如果这个内存上不去，其它的修改都是徒劳。</span><br><span class="line">举个例子就是说，一个20岁的成年人和一个3岁的小孩</span><br><span class="line">3岁的小孩掌握再多的格斗技巧都没有用，在绝对的实力面前一切都是花架子。</span><br><span class="line">所以说我们一般很少需要去调整Eden、s1、s2的比值，一般都是直接增加Executor的内存比较靠谱。</span><br></pre></td></tr></table></figure><h3 id="提高并行度"><a href="#提高并行度" class="headerlink" title="提高并行度"></a>提高并行度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实际上Spark集群的资源并不一定会被充分利用到，所以要尽量设置合理的并行度，来充分地利用集群的资源，这样才能提高Spark程序的性能。</span><br><span class="line"></span><br><span class="line">Spark会自动设置以文件作为输入源的RDD的并行度，依据其大小，比如HDFS，就会给每一个block创建一个partition，也依据这个设置并行度。对于reduceByKey等会发生shuffle操作的算子，会使用并行度最大的父RDD的并行度</span><br><span class="line"></span><br><span class="line">可以手动使用textFile()、parallelize()等方法的第二个参数来设置并行度(只针对这一个RDD)；也可以使用spark.default.parallelism参数(全局)，来设置统一的并行度。Spark官方的推荐是，给集群中的每个cpu core设置2~3个task。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">下面来举个例子</span><br><span class="line">我在spark-submit 脚本中给任务设置了5个executor，每个executor，设置了2个cpu core</span><br><span class="line">spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \   &#x2F;&#x2F; 为job设置5个executor</span><br><span class="line">--executor-cores 2 \   &#x2F;&#x2F; 分配两个CPU</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时，如果我在代码中设置了默认并行度为5</span><br><span class="line">conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br><span class="line"></span><br><span class="line">这个参数设置完了以后，也就意味着所有RDD的partition都被设置成了5个，针对RDD的每一个partition，spark会启动一个task来进行计算，所以对于所有的算子操作，都只会创建5个task来处理对应的RDD中的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">但是注意了，我们前面在spark-submit脚本中设置了5个executor，每个executor 2个cpu core，所以这个时候spark其实会向yarn集群申请10个cpu core，但是我们在代码中设置了默认并行度为5，只会产生5个task，一个task使用一个cpu core，那也就意味着有5个cpu core是空闲的，这样申请的资源就浪费了一半。</span><br><span class="line"></span><br><span class="line">其实最好的情况，就是每个cpu core都不闲着，一直在运行，这样可以达到资源的最大使用率，其实让一个cpu core运行一个task都是有点浪费的，官方也建议让每个cpu core运行2~3个task，这样可以充分压榨CPU的性能</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">为什么这样说呢？</span><br><span class="line"></span><br><span class="line">是这样的，因为每个task执行的顺序和执行结束的时间很大概率是不一样的，如果正好有10个cpu，运行10个taks，那么某个task可能很快就执行完了，那么这个CPU就空闲下来了，这样资源就浪费了。</span><br><span class="line">所以说官方推荐，给每个cpu分配2~3个task是比较合理的，可以充分利用CPU资源，发挥它最大的价值。</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">下面我们来实际写个案例看一下效果</span><br><span class="line">Scala代码如下：</span><br><span class="line"></span><br><span class="line">package com.imooc.scala</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 需求：设置并行度</span><br><span class="line"> * 1：可以在textFile或者parallelize等方法的第二个参数中设置并行度</span><br><span class="line"> * 2：或者通过spark.default.parallelism参数统一设置并行度</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object MoreParallelismScala &#123;</span><br><span class="line">     def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">     val conf &#x3D; new SparkConf()</span><br><span class="line">     conf.setAppName(&quot;MoreParallelismScala&quot;)</span><br><span class="line">     &#x2F;&#x2F;.setMaster(&quot;local&quot;)</span><br><span class="line">    &#x2F;&#x2F;设置全局并行度</span><br><span class="line">     conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br><span class="line">     val sc &#x3D; new SparkContext(conf)</span><br><span class="line">     val dataRDD &#x3D; sc.parallelize(Array(&quot;hello&quot;,&quot;you&quot;,&quot;hello&quot;,&quot;me&quot;,&quot;hehe&quot;,&quot;hello&quot;,&quot;you&quot;,&quot;hello&quot;,&quot;me&quot;,&quot;hehe&quot;))</span><br><span class="line">     dataRDD.map((_,1))</span><br><span class="line">     .reduceByKey(_ + _)</span><br><span class="line">     .foreach(println(_))</span><br><span class="line">     sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：设置并行度</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MoreParallelismJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">     conf.setAppName(<span class="string">"MoreParallelismJava"</span>);</span><br><span class="line">     <span class="comment">//设置全局并行度</span></span><br><span class="line">     conf.set(<span class="string">"spark.default.parallelism"</span>,<span class="string">"5"</span>);</span><br><span class="line">     JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">     JavaRDD&lt;String&gt; dataRDD = sc.parallelize(Arrays.asList(<span class="string">"hello"</span>,<span class="string">"you"</span>,<span class="string">"hello"</span>,<span class="string">"me"</span>,<span class="string">"hehe"</span>,<span class="string">"hello"</span>,<span class="string">"you"</span>,<span class="string">"hello"</span>,<span class="string">"me"</span>,<span class="string">"hehe"</span>));</span><br><span class="line">     dataRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">     return new Tuple2&lt;String, Integer&gt;<span class="params">(word,<span class="number">1</span>)</span></span>;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;).reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     <span class="keyword">return</span> i1 + i2;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;).foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">     <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     System.out.println(tup);</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;);</span><br><span class="line">     sc.stop();                                                      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">对代码编译打包</span><br><span class="line">spark-submit脚本内容如下：</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# vi moreParallelismJob.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.MoreParallelismScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280039960.png" alt="image-20230328003937453"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">任务提交到集群运行之后，查看spark的任务界面(job)</span><br><span class="line">先看executors，这里显示了4个executor和1个driver进程，为什么不是5个executor进程呢？</span><br><span class="line"></span><br><span class="line">是因为我们现在使用的是yarn-cluster模式，driver进程运行在集群内部，所以它占了一个executor，如果使用的是yarn-client模式，就会产生5个executor和1个单独的driver进程。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280042154.png" alt="image-20230328004230697"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后去看satges界面，两个Stage都是5个task并行执行，这5个task会使用5个cpu，但是我们给这个任务申请了10个cpu，所以就有5个是空闲的了(这里没考虑driver的占用)。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280057687.png" alt="image-20230328005706629"></p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280054889.png" alt="image-20230328005414569"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当在sparkContext生成对象后，再设置默认并行度会出现问题</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280055569.png" alt="image-20230328005531042"></p><h4 id="提高性能"><a href="#提高性能" class="headerlink" title="提高性能"></a>提高性能</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果想要最大限度利用CPU的性能，至少将spark.default.parallelism的值设置为10，这样可以实现一个cpu运行一个task，其实官方推荐是设置为20或者30。</span><br><span class="line">其实这个参数也可以在spark-submit脚本中动态设置，通过--conf参数设置，这样就比较灵活了。</span><br><span class="line"></span><br><span class="line">注意：此时需要将代码中设置spark.default.parallelism的配置注释掉</span><br><span class="line">&#x2F;&#x2F;conf.set(&quot;spark.default.parallelism&quot;,&quot;5&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">为了看起来更清晰，在这我们使用 yarn-client 模式，这样driver就不会占用我们的分配的executor了</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sparkjars]# vi moreParallelismJob2.sh </span><br><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.scala.MoreParallelismScala \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--num-executors 5 \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot; \</span><br><span class="line">db_spark-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于修改了代码，所以需要重新编译，打包，执行</span><br><span class="line">执行结束后再来查看spark的任务界面，可以看到此时有10个task并行执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280117859.png" alt="image-20230328011738873"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280118186.png" alt="image-20230328011801559"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280116405.png" alt="image-20230328011600636"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是并行度相关的设置</span><br><span class="line">接下来我们来看一个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303280126634.png" alt="image-20230328012605704"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个图中描述的就是刚才我们演示的两种情况下Executor和Task之间的关系</span><br></pre></td></tr></table></figure><h4 id="spark-submit常用参数"><a href="#spark-submit常用参数" class="headerlink" title="spark-submit常用参数"></a>spark-submit常用参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">最后我们来分析总结一下spark-submit脚本中经常配置的一些参数</span><br><span class="line"></span><br><span class="line">--name mySparkJobName：指定任务名称(代码里也可以设置)</span><br><span class="line">--class com.imooc.scala.xxxxx ：指定入口类</span><br><span class="line">--master yarn ：指定集群地址(standalone)，on yarn模式指定yarn</span><br><span class="line">--deploy-mode cluster ：client代表yarn-client，cluster代表yarn-cluster</span><br><span class="line">--executor-memory 1G ：executor进程的内存大小，实际工作中设置2~4G即可</span><br><span class="line">--num-executors 2 ：分配多少个executor进程</span><br><span class="line">--executor-cores 2 : 一个executor进程分配多少个cpu core</span><br><span class="line"></span><br><span class="line">--driver-cores 1 ：(如果使用yarn-cluster模式，这里不用设置也行，直接按executor的配置来)driver进程分配多少cpu core，默认为1即可</span><br><span class="line">--driver-memory 1G：(如果使用yarn-cluster模式，这里不用设置也行，直接按executor的配置来)driver进程的内存，如果需要使用类似于collect之类的action算子向Driver端拉取数据，则这里可以设置大一些</span><br><span class="line">--jars fastjson.jar,abc.jar 在这里可以设置job依赖的第三方jar包(pom里spark-core没提供的)【不建议把第三方依赖打入程序的jar包中，一方面会导致jar变大；另一方面，同一个项目组同事用的第三方依赖版本问题；还有这里可以使用本地路径，或者hdfs路径(建议使用hdfs路径，因为使用本地路径依赖，还是会读取到hdfs上)】</span><br><span class="line">--conf &quot;spark.default.parallelism&#x3D;10&quot;：可以动态指定一些spark任务的参数，指定多个参</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">最后注意一点：针对 --num-executors 和 --executor-cores 的设置</span><br><span class="line">大家看这两种方式设置有什么区别：</span><br><span class="line">第一种方式：</span><br><span class="line">--num-executors 2</span><br><span class="line">--executor-cores 1</span><br><span class="line">第二种方式：</span><br><span class="line">--num-executors 1</span><br><span class="line">--executor-cores 2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这两种设置最终都会向集群申请2个cpu core，可以并行运行两个task，但是这两种设置方式有什么区别呢？</span><br><span class="line"></span><br><span class="line">第一种方法：多executor模式</span><br><span class="line">由于每个executor只分配了一个cpu core，我们将无法利用在同一个JVM中运行多个任务的优点。我们假设这两个executor是在两个节点中启动的，那么针对广播变量这种操作，将在两个节点的中都复制1份，最终会复制两份</span><br><span class="line"></span><br><span class="line">第二种方法：多core模式</span><br><span class="line">此时一个executor中会有2个cpu core，这样可以利用同一个JVM中运行多个任务的优点，并且针对广播变量的这种操作，只会在这个executor对应的节点中复制1份即可。</span><br><span class="line"></span><br><span class="line">那是不是我可以给一个executor分配很多的cpu core，也不是的，因为一个executor的内存大小是固定的，如果在里面运行过多的task可能会导致内存不够用，所以这块一般在工作中我们会给一个executor分配 2~4G 内存，对应的分配 2~4个cpu core。</span><br></pre></td></tr></table></figure><h3 id="数据本地化"><a href="#数据本地化" class="headerlink" title="数据本地化"></a>数据本地化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">数据本地化对于Spark Job性能有着巨大的影响。如果数据以及要计算它的代码是在一起的，那么性能当然会非常高。但是，如果数据和计算它的代码是分开的，那么其中之一必须到另外一方的机器上。通常来说，移动代码到其它节点，会比移动数据到代码所在的节点，速度要得多，因为代码比较小。Spark也正是基于这个数据本地化的原则来构建task调度算法的。</span><br><span class="line"></span><br><span class="line">数据本地化，指的是，数据离计算它的代码有多近。基于数据距离代码的距离，有几种数据本地化级别：</span><br><span class="line"></span><br><span class="line">数据本地化级别 解释</span><br><span class="line">PROCESS_LOCAL 进程本地化，性能最好：数据和计算它的代码在同一个JVM进程中</span><br><span class="line">NODE_LOCAL 节点本地化：数据和计算它的代码在一个节点上，但是不在一个JVM进程</span><br><span class="line">NO_PREF 数据从哪里过来，性能都是一样的，比如从数据库中获取数据，对于task而言在哪个机器上都是一样的</span><br><span class="line">RACK_LOCAL 数据和计算它的代码在一个机架上，数据需要通过网络在节点之间进行传输</span><br><span class="line">ANY 数据可能在任意地方，比如其它网络环境内，或者其它机架上，性能最差</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281058668.png" alt="image-20230328105831703"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Spark倾向使用最好的本地化级别调度task，但这是不现实的</span><br><span class="line">如果目前我们要处理的数据所在的executor上目前没有空闲的CPU，那么Spark就会放低本地化级别。这时有两个选择：</span><br><span class="line">第一，等待，直到executor上的cpu释放出来，那么就分配task过去；</span><br><span class="line">第二，立即在任意一个其它executor上启动一个task。</span><br><span class="line"></span><br><span class="line">Spark默认会等待指定时间，期望task要处理的数据所在的节点上的executor空闲出一个cpu，从而将task分配过去，只要超过了时间，那么Spark就会将task分配到其它任意一个空闲的executor上</span><br><span class="line"></span><br><span class="line">可以设置参数， spark.locality 系列参数，来调节Spark等待task可以进行数据本地化的时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.locality.wait（3000毫秒）：默认等待3秒(通用的所有级别)</span><br><span class="line">spark.locality.wait.process：等待指定的时间看能否达到数据和计算它的代码在同一个JVM</span><br><span class="line">spark.locality.wait.node：等待指定的时间看能否达到数据和计算它的代码在一个节点上执行</span><br><span class="line">spark.locality.wait.rack：等待指定的时间看能否达到数据和计算它的代码在一个机架上</span><br><span class="line"></span><br><span class="line">看这个图里面的task，此时的数据本地化级别是最优的 PROCESS_LOCAL</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303281108354.png" alt="image-20230328110840007"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-checkpoint.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-checkpoint.html</id>
    <published>2023-03-27T06:42:14.000Z</published>
    <updated>2023-03-28T06:52:48.630Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-2"><a href="#第十一周-Spark性能优化的道与术-2" class="headerlink" title="第十一周 Spark性能优化的道与术-2"></a>第十一周 Spark性能优化的道与术-2</h1><h2 id="checkpoint概述"><a href="#checkpoint概述" class="headerlink" title="checkpoint概述"></a>checkpoint概述</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">checkpoint，是Spark提供的一个比较高级的功能。有时候，我们的Spark任务，比较复杂，从初始化RDD开始，到最后整个任务完成，有比较多的步骤，比如超过10个transformation算子。而且，整个任务运行的时间也特别长，比如通常要运行1~2个小时。在这种情况下，就比较适合使用checkpoint功能了。</span><br><span class="line"></span><br><span class="line">因为对于特别复杂的Spark任务，有很高的风险会出现某个要反复使用的RDD因为节点的故障导致丢失，虽然之前持久化过，但是还是导致数据丢失了。那么也就是说，出现失败的时候，没有容错机制，所以当后面的transformation算子，又要使用到该RDD时，就会发现数据丢失了，此时如果没有进行容错处理的话，那么就需要再重新计算一次数据了。</span><br><span class="line">所以针对这种Spark Job，如果我们担心某些关键的，在后面会反复使用的RDD，因为节点故障导致数据丢失，那么可以针对该RDD启动checkpoint机制，实现容错和高可用</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那如何使用checkPoint呢？</span><br><span class="line">首先要调用SparkContext的setCheckpointDir()方法，设置一个容错的文件系统的目录，比如HDFS；然后，对RDD调用checkpoint()方法。</span><br><span class="line">最后，在RDD所在的job运行结束之后，会启动一个单独的job，将checkpoint设置过的RDD的数据写入之前设置的文件系统中。</span><br><span class="line"></span><br><span class="line">这是checkpoint使用的基本步骤，很简单，那我们下面先从理论层面分析一下当我们设置好checkpoint之后，Spark底层都做了哪些事情</span><br></pre></td></tr></table></figure><h2 id="RDD之checkpoint流程"><a href="#RDD之checkpoint流程" class="headerlink" title="RDD之checkpoint流程"></a>RDD之checkpoint流程</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271450477.png" alt="image-20230327144700422"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1：SparkContext设置checkpoint目录，用于存放checkpoint的数据；</span><br><span class="line">对RDD调用checkpoint方法，然后它就会被RDDCheckpointData对象进行管理，此时这个RDD的checkpoint状态会被设置为Initialized</span><br><span class="line">2：待RDD所在的job运行结束，会调用job中最后一个RDD的doCheckpoint方法，该方法沿着RDD的血缘关系向上查找被checkpoint()方法标记过的RDD， 并将其checkpoint状态从Initialized设置为CheckpointingInProgress</span><br><span class="line">3：启动一个单独的job，来将血缘关系中标记为CheckpointInProgress的RDD执行checkpoint操作，也就是将其数据写入checkpoint目录</span><br><span class="line">4：将RDD数据写入checkpoint目录之后，会将RDD状态改变Checkpointed；</span><br><span class="line">并且还会改变RDD的血缘关系，即会清除掉RDD所有依赖的RDD；最后还会设置其父RDD为新创建的CheckpointRDD</span><br></pre></td></tr></table></figure><h2 id="checkpoint与持久化的区别"><a href="#checkpoint与持久化的区别" class="headerlink" title="checkpoint与持久化的区别"></a>checkpoint与持久化的区别</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">那这里所说的checkpoint和我们之前讲的RDD持久化有什么区别吗？</span><br><span class="line">lineage是否发生改变linage（血缘关系）说的就是RDD之间的依赖关系</span><br><span class="line"></span><br><span class="line">持久化，只是将数据保存在内存中或者本地磁盘文件中，RDD的lineage(血缘关系)是不变的；</span><br><span class="line"></span><br><span class="line">Checkpoint执行之后，RDD就没有依赖的RDD了，也就是它的lineage改变了</span><br><span class="line">丢失数据的可能性持久化的数据丢失的可能性较大，如果采用persist把数据存在内存中的话，虽然速度最快但是也是最不可靠的，就算放在磁盘上也不是完全可靠的，因为磁盘也会损坏。Checkpoint的数据通常是保存在高可用文件系统中(HDFS),丢失的可能性很低</span><br><span class="line"></span><br><span class="line">建议：对需要checkpoint的RDD，先执行persist(StorageLevel.DISK_ONLY)</span><br><span class="line">为什么呢？</span><br><span class="line"></span><br><span class="line">因为默认情况下，如果某个RDD没有持久化，但是设置了checkpoint，那么这个时候，本来Spark任务已经执行结束了，但是由于中间的RDD没有持久化，在进行checkpoint的时候想要将这个RDD的数据写入外部存储系统的话，就需要重新计算这个RDD的数据，再将其checkpoint到外部存储系统中。</span><br><span class="line">如果对需要checkpoint的rdd进行了基于磁盘的持久化，那么后面进行checkpoint操作时，就会直接从磁盘上读取rdd的数据了，就不需要重新再计算一次了，这样效率就高了。</span><br><span class="line"></span><br><span class="line">那在这能不能使用基于内存的持久化呢？当然是可以的，不过没那个必要。</span><br></pre></td></tr></table></figure><h2 id="checkPoint的使用"><a href="#checkPoint的使用" class="headerlink" title="checkPoint的使用"></a>checkPoint的使用</h2><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们来演示一下：将一个RDD的数据持久化到HDFS上面</span><br><span class="line">scala代码如下：</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：checkpoint的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CheckPointOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">         conf.setAppName(<span class="string">"CheckPointOpScala"</span>)</span><br><span class="line">         <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">         <span class="keyword">if</span>(args.length==<span class="number">0</span>)&#123;</span><br><span class="line">         <span class="type">System</span>.exit(<span class="number">100</span>)</span><br><span class="line">     &#125;</span><br><span class="line">         <span class="keyword">val</span> outputPath = args(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//1：设置checkpint目录</span></span><br><span class="line">         sc.setCheckpointDir(<span class="string">"hdfs://bigdata01:9000/chk001"</span>)</span><br><span class="line">         <span class="keyword">val</span> dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_10000000.dat"</span>)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//2：对rdd执行checkpoint操作</span></span><br><span class="line">         dataRDD.checkpoint()</span><br><span class="line">         dataRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">         .map((_,<span class="number">1</span>))</span><br><span class="line">         .reduceByKey(_ + _)</span><br><span class="line">         .saveAsTextFile(outputPath)</span><br><span class="line">         sc.stop()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.StorageLevel;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：checkpoint的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CheckPointOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">         conf.setAppName(<span class="string">"CheckPointOpJava"</span>);</span><br><span class="line">         JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">         <span class="keyword">if</span>(args.length==<span class="number">0</span>)&#123;</span><br><span class="line">         System.exit(<span class="number">100</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         String outputPath = args[<span class="number">0</span>];</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//1：设置checkpint目录</span></span><br><span class="line">         sc.setCheckpointDir(<span class="string">"hdfs://bigdata01:9000/chk002"</span>);</span><br><span class="line">         JavaRDD&lt;String&gt; dataRDD = sc.textFile(<span class="string">"hdfs://bigdata01:9000/hello_100000.dat"</span>);</span><br><span class="line">         <span class="comment">//2: 对rdd执行checkpoint操作</span></span><br><span class="line">         dataRDD.checkpoint();</span><br><span class="line">         dataRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">          <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new Tuple2&lt;String, Integer&gt;<span class="params">(word,<span class="number">1</span>)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> i1 + i2;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).saveAsTextFile(outputPath);</span><br><span class="line">         sc.stop();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="提交到集群执行"><a href="#提交到集群执行" class="headerlink" title="提交到集群执行"></a>提交到集群执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下面我们把这个任务打包提交到集群上运行一下，看一下效果。</span><br><span class="line">代码master部分注释掉</span><br><span class="line"></span><br><span class="line">先确保hadoop集群是正常运行的，以及hadoop中的historyserver进程和spark的historyserver进程也是正常运行的。</span><br><span class="line">测试数据之前已经上传到了hdfs上面，如果没有则需要上传</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271517969.png" alt="image-20230327151632675"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将pom.xml中的spark-core的依赖设置为provided，然后编译打包</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.3&lt;&#x2F;version&gt;</span><br><span class="line"> &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">将打包的jar包上传到bigdata04的&#x2F;data&#x2F;soft&#x2F;sparkjars目录，创建一个新的spark-submit脚本</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271519124.png" alt="image-20230327151903625"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行成功之后可以到 setCheckpointDir 指定的目录中查看一下，可以看到目录中会生成对应的文件保存rdd中的数据，只不过生成的文件不是普通文本文件，直接查看文件中的内容显示为乱码。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271523654.png" alt="image-20230327152317696"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271523426.png" alt="image-20230327152338046"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271525661.png" alt="image-20230327152509680"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来进到YARN的8088界面查看</span><br><span class="line">点击Tracking UI进入spark的ui界面看第一个界面jobs</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271526703.png" alt="image-20230327152656935"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在这可以看出来产生了2个job，</span><br><span class="line">第一个job是我们正常的任务执行，执行了39s，一共产生了28个task任务</span><br><span class="line">第二个job是checkpoint启动的job，执行了35s，一共产生了14个task任务</span><br><span class="line"></span><br><span class="line">看第二个界面Stages，这里面的3个Stage是前面2个job产生的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">具体想知道哪些Stage属于哪个job任务的话，可以在任务界面，点击Description中的链接就可以看到job对应的Stage</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271528015.png" alt="image-20230327152845211"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个job其实就是我们实现的单词计数的功能，这个任务产生了两个stage，这两个stage具体是如何划分的呢？</span><br><span class="line">咱们前面讲过，stage的划分是由宽依赖决定的，在这个任务中reduceByKey这个过程会产生宽依赖，所以会产生2个Stage</span><br><span class="line">这里面显示的有这两个stage的一些基本信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271531710.png" alt="image-20230327153130946"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stage id：stage的编号，从0开始</span><br><span class="line">Duration：stage执行消耗的时间</span><br><span class="line">Tasks：Successed&#x2F;Total：task执行成功数量&#x2F;task总量</span><br><span class="line">Input：输入数据量</span><br><span class="line">ouput：输出数据量</span><br><span class="line">shuffle read&#x2F;shuffle read：shuffle过程传输数据量</span><br><span class="line">点击这个界面中的DAG Visualization可以看到当前这个任务stage的划分情况，可以看到每个Stage包含哪些算子</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271534605.png" alt="image-20230327153403130"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">进到Stage内部看一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271541155.png" alt="image-20230327154126374"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271542549.png" alt="image-20230327154248904"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面可以看到每个task的具体执行情况，执行状态，执行消耗的时间，GC消耗的时间，处理的数据量和数据条数、通过shuffle输出的数据量和数据条数</span><br><span class="line">其实从这里也可以看出来文件的每一个block块会产生一个task</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271546781.png" alt="image-20230327154559437"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是这个Stage执行的基本信息了。</span><br><span class="line">加下来看一下第二个Job，这个job是checkpoint启动的任务，查看它的stage的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271546799.png" alt="image-20230327154637511"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个job只会产生一个stage，因为我们只针对textFile的结果设置了checkpoint</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271547533.png" alt="image-20230327154722955"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个stage执行消耗了35s，说明这份数据是重新通过textFile读取过来的。</span><br><span class="line">针对Storage这块，显示的其实就是持久化的数据，如果对RDD做了持久化，那么在任务执行过程中能看到，任务执行结束就看不到了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271548520.png" alt="image-20230327154816099"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来验证一下在开启持久化的情况下执行checkpoint操作时的区别</span><br><span class="line">在代码中针对RDD开启持久化</span><br><span class="line">1：对比此时产生的两个job总的消耗的时间，以及job中的Stage消耗的时间</span><br><span class="line">其实你会发现开启持久化之后，checkpoint的那个job消耗的时间就变少了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271553264.png" alt="image-20230327155305626"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2：查看DAG Visualization，你会发现stage里面也会有有一些不一样的地方</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271556557.png" alt="image-20230327155658220"></p><h2 id="checkpoint源码分析"><a href="#checkpoint源码分析" class="headerlink" title="checkpoint源码分析"></a>checkpoint源码分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们通过理论层面分析了checkpoint的原理，以及演示了checkpoint的使用</span><br><span class="line">下面我们通过源码层面来对我们前面分析的理论进行验证</span><br><span class="line">先下载spark源码，下载流程和下载spark安装包的流程一样</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271953796.png" alt="image-20230327195356368"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">把下载的安装包解压到idea项目目录中</span><br><span class="line"></span><br><span class="line">打开spark-2.4.3源码目录，进入core目录，这个是spark的核心代码，我们要查看的checkpoint的源码就在这个项目中</span><br><span class="line">在idea中打开core这个子项目</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271954921.png" alt="image-20230327195430272"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面我们就来分析一下RDD的checkpoint功能：</span><br><span class="line">checkpoint功能可以分为两块</span><br><span class="line">1：checkpoint的写操作</span><br><span class="line">将指定RDD的数据通过checkpoint存储到指定外部存储中</span><br><span class="line">2：checkpoint的读操作</span><br><span class="line">任务中RDD数据在使用过程中丢失了，正好这个RDD之前做过checkpoint，所以这时就需要通过checkpoint来恢复数据</span><br></pre></td></tr></table></figure><h3 id="checkpoint的写操作"><a href="#checkpoint的写操作" class="headerlink" title="checkpoint的写操作"></a>checkpoint的写操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.1 ：当我们在自己开发的spark任务中先调用 sc.setCheckpointDir时，底层其实就会调用</span><br><span class="line">SparkContext中的setCheckpointDir方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def setCheckpointDir(directory: String) &#123;</span><br><span class="line">     &#x2F;&#x2F; If we are running on a cluster, log a warning if the directory is local.</span><br><span class="line">     &#x2F;&#x2F; Otherwise, the driver may attempt to reconstruct the checkpointed RDD fr</span><br><span class="line">     &#x2F;&#x2F; its own local file system, which is incorrect because the checkpoint fil</span><br><span class="line">     &#x2F;&#x2F; are actually on the executor machines.</span><br><span class="line">     if (!isLocal &amp;&amp; Utils.nonLocalPaths(directory).isEmpty) &#123;</span><br><span class="line">         logWarning(&quot;Spark is not running in local mode, therefore the checkpoint </span><br><span class="line">         s&quot;must not be on the local filesystem. Directory &#39;$directory&#39; &quot; +</span><br><span class="line">         &quot;appears to be on the local filesystem.&quot;)</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;根据我们传过来的目录，后面再拼上一个子目录，子目录使用一个UUID随机字符串</span><br><span class="line">     &#x2F;&#x2F;使用HDFS的javaAPI 在HDFS上创建目录</span><br><span class="line">     checkpointDir &#x3D; Option(directory).map &#123; dir &#x3D;&gt;</span><br><span class="line">     val path &#x3D; new Path(dir, UUID.randomUUID().toString)</span><br><span class="line">     val fs &#x3D; path.getFileSystem(hadoopConfiguration)</span><br><span class="line">     fs.mkdirs(path)</span><br><span class="line">     fs.getFileStatus(path).getPath.toString</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1.2：接着我们会调用RDD.checkpoint方法，此时会执行RDD这个class中的 checkpoint方法</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;这里相当于是checkpoint的一个标记，并没有真正执行checkpoint</span><br><span class="line">def checkpoint(): Unit &#x3D; RDDCheckpointData.synchronized &#123;</span><br><span class="line">     &#x2F;&#x2F; NOTE: we use a global lock here due to complexities downstream with ensu</span><br><span class="line">     &#x2F;&#x2F; children RDD partitions point to the correct parent partitions. In the f</span><br><span class="line">     &#x2F;&#x2F; we should revisit this consideration.</span><br><span class="line">     &#x2F;&#x2F;如果SparkContext没有设置checkpointDir，则抛出异常</span><br><span class="line">     if (context.checkpointDir.isEmpty) &#123;</span><br><span class="line">     throw new SparkException(&quot;Checkpoint directory has not been set in the Sp</span><br><span class="line">     &#125; else if (checkpointData.isEmpty) &#123;</span><br><span class="line">     &#x2F;&#x2F;如果设置了，则创建RDDCheckpointData的子类，这个子类主要负责管理RDD的checkpoi</span><br><span class="line">     &#x2F;&#x2F;并且会初始化checkpoint状态为Initialized</span><br><span class="line">     checkpointData &#x3D; Some(new ReliableRDDCheckpointData(this))</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这个checkpoint方法执行完成之后，这个流程就结束了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.3：剩下的就是在这个设置了checkpint的RDD所在的job执行结束之后，Spark会调用job中最后一个RDD的doCheckpoint方法</span><br><span class="line">这个逻辑是在SparkContext这个class的runJob方法中，当执行到Spark中的action算子时，这个runJob方法会被触发，开始执行任务。</span><br><span class="line">这个runJob的最后一行会调用rdd中的 doCheckpoint 方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;在有action动作时，会触发sparkcontext对runJob的调用</span><br><span class="line">def runJob[T, U: ClassTag](</span><br><span class="line">     rdd: RDD[T],</span><br><span class="line">     func: (TaskContext, Iterator[T]) &#x3D;&gt; U,</span><br><span class="line">     partitions: Seq[Int],</span><br><span class="line">     resultHandler: (Int, U) &#x3D;&gt; Unit): Unit &#x3D; &#123;</span><br><span class="line">     if (stopped.get()) &#123;</span><br><span class="line">     throw new IllegalStageException(&quot;SparkContext has been shutdown&quot;)</span><br><span class="line">     &#125;</span><br><span class="line">     val callSite &#x3D; getCallSite</span><br><span class="line">     val cleanedFunc &#x3D; clean(func)</span><br><span class="line">     logInfo(&quot;Starting job: &quot; + callSite.shortForm)</span><br><span class="line">     if (conf.getBoolean(&quot;spark.logLineage&quot;, false)) &#123;</span><br><span class="line">     logInfo(&quot;RDD&#39;s recursive dependencies:\n&quot; + rdd.toDebugString)</span><br><span class="line">     &#125;</span><br><span class="line">     dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, </span><br><span class="line">     progressBar.foreach(_.finishAll())</span><br><span class="line">     &#x2F;&#x2F;在这里会执行doCheckpoint()</span><br><span class="line">     rdd.doCheckpoint()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.4：接着会进入到RDD中的 doCheckpoint 方法</span><br><span class="line">这里面最终会调用 RDDCheckpointData 的 checkpoint 方法</span><br><span class="line">checkpointData.get.checkpoint()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def doCheckpoint(): Unit &#x3D; &#123;</span><br><span class="line">     RDDOperationScope.withScope(sc, &quot;checkpoint&quot;, allowNesting &#x3D; false, ignoreP</span><br><span class="line">     &#x2F;&#x2F;该rdd是否已经调用doCheckpoint，如果还没有，则开始处理</span><br><span class="line">     if (!doCheckpointCalled) &#123;</span><br><span class="line">     doCheckpointCalled &#x3D; true</span><br><span class="line">     &#x2F;&#x2F;若已经被checkpoint()标记过，则checkpointData.isDefined为true</span><br><span class="line">     if (checkpointData.isDefined) &#123;</span><br><span class="line">     &#x2F;&#x2F;查看是否需要把该rdd的所有依赖全部checkpoint</span><br><span class="line">     &#x2F;&#x2F;checkpointAllMarkedAncestors取自配置&quot;spark.checkpoint.checkpointAllM</span><br><span class="line">     &#x2F;&#x2F;默认不配时值为false</span><br><span class="line">     if (checkpointAllMarkedAncestors) &#123;</span><br><span class="line">     &#x2F;&#x2F; TODO We can collect all the RDDs that needs to be checkpointed, </span><br><span class="line">     &#x2F;&#x2F; them in parallel.</span><br><span class="line">     &#x2F;&#x2F; Checkpoint parents first because our lineage will be truncated af</span><br><span class="line">     &#x2F;&#x2F; checkpoint ourselves</span><br><span class="line">     &#x2F;&#x2F; 血缘上的每一个父rdd递归调用该方法</span><br><span class="line">     dependencies.foreach(_.rdd.doCheckpoint())</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;调用RDDCheckpointData的checkpoint方法</span><br><span class="line">     checkpointData.get.checkpoint()</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">     &#x2F;&#x2F;沿着rdd的血缘关系向上查找被checkpoint()标记过的RDD</span><br><span class="line">     dependencies.foreach(_.rdd.doCheckpoint())</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1.5：接下来进入到 RDDCheckpointData 的 checkpoint 方法中</span><br><span class="line">这里面会调用子类 ReliableCheckpointRDD 中的 doCheckpoint()方法</span><br><span class="line"></span><br><span class="line">final def checkpoint(): Unit &#x3D; &#123;</span><br><span class="line">     &#x2F;&#x2F; Guard against multiple threads checkpointing the same RDD by</span><br><span class="line">     &#x2F;&#x2F; atomically flipping the Stage of this RDDCheckpointData</span><br><span class="line">     &#x2F;&#x2F;&#x2F;&#x2F;将checkpoint的状态从Initialized置为CheckpointingInProgress</span><br><span class="line">     RDDCheckpointData.synchronized &#123;</span><br><span class="line">     if (cpStage &#x3D;&#x3D; Initialized) &#123;</span><br><span class="line">     cpStage &#x3D; CheckpointingInProgress</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">     return</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#x2F;&#x2F;调用子类的doCheckpoint，默认会使用ReliableCheckpointRDD子类，创建一个新的Chec</span><br><span class="line">     val newRDD &#x3D; doCheckpoint()</span><br><span class="line">     &#x2F;&#x2F; Update our Stage and truncate the RDD lineage</span><br><span class="line">     &#x2F;&#x2F;将checkpoint状态置为Checkpointed状态，并且改变rdd之前的依赖，设置父rdd为新创建</span><br><span class="line">     RDDCheckpointData.synchronized &#123;</span><br><span class="line">     cpRDD &#x3D; Some(newRDD)</span><br><span class="line">     cpStage &#x3D; Checkpointed</span><br><span class="line">     rdd.markCheckpointed()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.6：接着来进入 ReliableCheckpointRDD 中的 doCheckpoint() 方法</span><br><span class="line">这里面会调用 ReliableCheckpointRDD 中的 writeRDDToCheckpointDirectory 方法将rdd的数据写入HDFS</span><br><span class="line">中的 checkpoint 目录，并且返回创建的 CheckpointRDD</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">protected override def doCheckpoint(): CheckpointRDD[T] &#x3D; &#123;</span><br><span class="line">     &#x2F;&#x2F;将rdd的数据写入HDFS中的checkpoint目录，并且创建的CheckpointRDD</span><br><span class="line">     val newRDD &#x3D; ReliableCheckpointRDD.writeRDDToCheckpointDirectory(rdd, cpDir</span><br><span class="line">     &#x2F;&#x2F; Optionally clean our checkpoint files if the reference is out of scope</span><br><span class="line">     if (rdd.conf.getBoolean(&quot;spark.cleaner.referenceTracking.cleanCheckpoints&quot;, </span><br><span class="line">     rdd.context.cleaner.foreach &#123; cleaner &#x3D;&gt;</span><br><span class="line">     cleaner.registerRDDCheckpointDataForCleanup(newRDD, rdd.id)</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line">logInfo(s&quot;Done checkpointing RDD $&#123;rdd.id&#125; to $cpDir, new parent is RDD $&#123;n</span><br><span class="line"> newRDD</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.7：接下来进入 ReliableCheckpointRDD 的 writeRDDToCheckpointDirectory 方法</span><br><span class="line">这里面最终会启动一个job，将checkpoint的数据写入到指定的HDFS目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;将rdd的数据写入HDFS中checkpoint目录，并且创建CheckpointRDD</span><br><span class="line">def writeRDDToCheckpointDirectory[T: ClassTag](</span><br><span class="line"> originalRDD: RDD[T],</span><br><span class="line"> checkpointDir: String,</span><br><span class="line"> blockSize: Int &#x3D; -1): ReliableCheckpointRDD[T] &#x3D; &#123;</span><br><span class="line"> val checkpointStartTimeNs &#x3D; System.nanoTime()</span><br><span class="line"> val sc &#x3D; originalRDD.sparkContext</span><br><span class="line"> &#x2F;&#x2F;Create the output path for the checkpoint</span><br><span class="line"> &#x2F;&#x2F;创建checkpoint输出目录</span><br><span class="line"> val checkpointDirPath &#x3D; new Path(checkpointDir)</span><br><span class="line"> &#x2F;&#x2F;获取HDFS文件系统API接口</span><br><span class="line"> val fs &#x3D; checkpointDirPath.getFileSystem(sc.hadoopConfiguration)</span><br><span class="line"> &#x2F;&#x2F;创建目录</span><br><span class="line"> if (!fs.mkdirs(checkpointDirPath)) &#123;</span><br><span class="line"> throw new SparkException(s&quot;Failed to create checkpoint path $checkpointDi</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Save to file, and reload it as an RDD</span><br><span class="line"> &#x2F;&#x2F;将Hadoop配置文件信息广播到所有节点</span><br><span class="line"> val broadcastedConf &#x3D; sc.broadcast(</span><br><span class="line"> new SerializableConfiguration(sc.hadoopConfiguration))</span><br><span class="line"> &#x2F;&#x2F; TODO: This is expensive because it computes the RDD again unnecessarily </span><br><span class="line"> &#x2F;&#x2F;这里强调了checkpoint是一个昂贵的操作，主要是说它昂贵在需要沿着血缘关系重新计算该</span><br><span class="line"> &#x2F;&#x2F;重新启动一个job,将rdd的分区数据写入HDFS</span><br><span class="line"> sc.runJob(originalRDD,</span><br><span class="line"> writePartitionToCheckpointFile[T](checkpointDirPath.toString, broadcasted</span><br><span class="line"> &#x2F;&#x2F;如果rdd的partitioner不为空，则将partitioner写入checkpoint目录</span><br><span class="line"> if (originalRDD.partitioner.nonEmpty) &#123;</span><br><span class="line"> writePartitionerToCheckpointDir(sc, originalRDD.partitioner.get, checkpoi</span><br><span class="line"> &#125;</span><br><span class="line"> val checkpointDurationMs &#x3D;</span><br><span class="line"> TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - checkpointStartTimeNs)</span><br><span class="line"> logInfo(s&quot;Checkpointing took $checkpointDurationMs ms.&quot;)</span><br><span class="line"> &#x2F;&#x2F;创建一个CheckpointRDD,该RDD的分区数目和原始的rdd的分区数是一样的</span><br><span class="line"> val newRDD &#x3D; new ReliableCheckpointRDD[T](</span><br><span class="line"> sc, checkpointDirPath.toString, originalRDD.partitioner)</span><br><span class="line"> if (newRDD.partitions.length !&#x3D; originalRDD.partitions.length) &#123;</span><br><span class="line"> throw new SparkException(</span><br><span class="line"> &quot;Checkpoint RDD has a different number of partitions from original RDD. </span><br><span class="line"> s&quot;RDD [ID: $&#123;originalRDD.id&#125;, num of partitions: $&#123;originalRDD.partit</span><br><span class="line"> s&quot;Checkpoint RDD [ID: $&#123;newRDD.id&#125;, num of partitions: &quot; +</span><br><span class="line"> s&quot;$&#123;newRDD.partitions.length&#125;].&quot;)</span><br><span class="line"> &#125;</span><br><span class="line"> newRDD</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行到这，其实调用过checkpoint方法的RDD就被保存到HDFS上了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：在这里通过checkpoint操作将RDD中的数据写入到HDFS中的时候，会调用RDD中的</span><br><span class="line">iterator方法，遍历RDD中所有分区的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那我们来分析一下这块的代码</span><br><span class="line">此时我们没有对RDD进行持久化，所以走else中的代码</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">进入 computeOrReadCheckpoint(split, context) 中</span><br><span class="line">此时这个RDD是将要进行checkpoint，还没有完成checkpoint，所以走 else ，会执行 compute 方法</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskCont</span><br><span class="line">&#123;</span><br><span class="line"> &#x2F;&#x2F;当前rdd是否已经checkpoint并且物化，如果已经checkpoint并且物化</span><br><span class="line"> &#x2F;&#x2F;则调用firstParent的iterator方法获取</span><br><span class="line"> if (isCheckpointedAndMaterialized) &#123;</span><br><span class="line"> &#x2F;&#x2F;注意：针对checpoint操作，它的血缘关系已经被切断了，那么它的firstParent就是前面</span><br><span class="line"> firstParent[T].iterator(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;如果没有，则表示持久化数据丢失，或者根本就没有持久化，</span><br><span class="line"> &#x2F;&#x2F;需要调用rdd的compute方法开始重新计算，返回一个Iterator对象</span><br><span class="line"> compute(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在这会执行RDD的子类 HadoopRDD 中的 compute 方法</span><br><span class="line">在这里会通过 RecordReader 获取RDD中指定分区的数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">override def compute(theSplit: Partition, context: TaskContext): Interruptibl</span><br><span class="line"> val iter &#x3D; new NextIterator[(K, V)] &#123;</span><br><span class="line"> private val split &#x3D; theSplit.asInstanceOf[HadoopPartition]</span><br><span class="line"> logInfo(&quot;Input split: &quot; + split.inputSplit)</span><br><span class="line"> private val jobConf &#x3D; getJobConf()</span><br><span class="line"> private val inputMetrics &#x3D; context.taskMetrics().inputMetrics</span><br><span class="line"> private val existingBytesRead &#x3D; inputMetrics.bytesRead</span><br><span class="line"> &#x2F;&#x2F; Sets InputFileBlockHolder for the file block&#39;s information</span><br><span class="line"> split.inputSplit.value match &#123;</span><br><span class="line"> case fs: FileSplit &#x3D;&gt;</span><br><span class="line"> InputFileBlockHolder.set(fs.getPath.toString, fs.getStart, fs.getLengt</span><br><span class="line"> case _ &#x3D;&gt;</span><br><span class="line"> InputFileBlockHolder.unset()</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Find a function that will return the FileSystem bytes read by this thr</span><br><span class="line"> &#x2F;&#x2F; creating RecordReader, because RecordReader&#39;s constructor might read s</span><br><span class="line"> private val getBytesReadCallback: Option[() &#x3D;&gt; Long] &#x3D; split.inputSplit.v</span><br><span class="line"> case _: FileSplit | _: CombineFileSplit &#x3D;&gt;</span><br><span class="line"> Some(SparkHadoopUtil.get.getFSBytesReadOnThreadCallback())</span><br><span class="line"> case _ &#x3D;&gt; None</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; We get our input bytes from thread-local Hadoop FileSystem statistics.</span><br><span class="line"> &#x2F;&#x2F; If we do a coalesce, however, we are likely to compute multiple partit</span><br><span class="line"> &#x2F;&#x2F; task and in the same thread, in which case we need to avoid override v</span><br><span class="line"> &#x2F;&#x2F; previous partitions (SPARK-13071).</span><br><span class="line"> private def updateBytesRead(): Unit &#x3D; &#123;</span><br><span class="line"> getBytesReadCallback.foreach &#123; getBytesRead &#x3D;&gt;</span><br><span class="line"> inputMetrics.setBytesRead(existingBytesRead + getBytesRead())</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> private var reader: RecordReader[K, V] &#x3D; null</span><br><span class="line"> private val inputFormat &#x3D; getInputFormat(jobConf)</span><br><span class="line"> HadoopRDD.addLocalConfiguration(</span><br><span class="line"> new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;, Locale.US).format(createTime),</span><br><span class="line"> context.stageId, theSplit.index, context.attemptNumber, jobConf)</span><br><span class="line"> reader &#x3D;</span><br><span class="line"> try &#123;</span><br><span class="line"> inputFormat.getRecordReader(split.inputSplit.value, jobConf, Reporter</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: FileNotFoundException if ignoreMissingFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped missing file: $&#123;split.inputSplit&#125;&quot;, e)</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> null</span><br><span class="line"> &#x2F;&#x2F; Throw FileNotFoundException even if &#96;ignoreCorruptFiles&#96; is true</span><br><span class="line"> case e: FileNotFoundException if !ignoreMissingFiles &#x3D;&gt; throw e</span><br><span class="line"> case e: IOException if ignoreCorruptFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped the rest content in the corrupted file: $&#123;split</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> null</span><br><span class="line"> &#125;</span><br><span class="line"> &#x2F;&#x2F; Register an on-task-completion callback to close the input stream.</span><br><span class="line"> context.addTaskCompletionListener[Unit] &#123; context &#x3D;&gt;</span><br><span class="line"> &#x2F;&#x2F; Update the bytes read before closing is to make sure lingering bytes</span><br><span class="line"> &#x2F;&#x2F; this thread get correctly added.</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> closeIfNeeded()</span><br><span class="line"> &#125;</span><br><span class="line"> private val key: K &#x3D; if (reader &#x3D;&#x3D; null) null.asInstanceOf[K] else reader</span><br><span class="line"> private val value: V &#x3D; if (reader &#x3D;&#x3D; null) null.asInstanceOf[V] else read</span><br><span class="line"> override def getNext(): (K, V) &#x3D; &#123;</span><br><span class="line"> try &#123;</span><br><span class="line"> finished &#x3D; !reader.next(key, value)</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: FileNotFoundException if ignoreMissingFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped missing file: $&#123;split.inputSplit&#125;&quot;, e)</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> &#x2F;&#x2F; Throw FileNotFoundException even if &#96;ignoreCorruptFiles&#96; is true</span><br><span class="line"> case e: FileNotFoundException if !ignoreMissingFiles &#x3D;&gt; throw e</span><br><span class="line"> case e: IOException if ignoreCorruptFiles &#x3D;&gt;</span><br><span class="line"> logWarning(s&quot;Skipped the rest content in the corrupted file: $&#123;split</span><br><span class="line"> finished &#x3D; true</span><br><span class="line"> &#125;</span><br><span class="line"> if (!finished) &#123;</span><br><span class="line"> inputMetrics.incRecordsRead(1)</span><br><span class="line"> &#125;</span><br><span class="line"> if (inputMetrics.recordsRead % SparkHadoopUtil.UPDATE_INPUT_METRICS_INT</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> &#125;</span><br><span class="line"> (key, value)</span><br><span class="line"> &#125;</span><br><span class="line"> override def close(): Unit &#x3D; &#123;</span><br><span class="line"> if (reader !&#x3D; null) &#123;</span><br><span class="line"> InputFileBlockHolder.unset()</span><br><span class="line"> try &#123;</span><br><span class="line"> reader.close()</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: Exception &#x3D;&gt;</span><br><span class="line"> if (!ShutdownHookManager.inShutdown()) &#123;</span><br><span class="line"> logWarning(&quot;Exception in RecordReader.close()&quot;, e)</span><br><span class="line"> &#125;</span><br><span class="line"> &#125; finally &#123;</span><br><span class="line"> reader &#x3D; null</span><br><span class="line"> &#125;</span><br><span class="line"> if (getBytesReadCallback.isDefined) &#123;</span><br><span class="line"> updateBytesRead()</span><br><span class="line"> &#125; else if (split.inputSplit.value.isInstanceOf[FileSplit] ||</span><br><span class="line"> split.inputSplit.value.isInstanceOf[CombineFileSplit]) &#123;</span><br><span class="line"> &#x2F;&#x2F; If we can&#39;t get the bytes read from the FS stats, fall back to t</span><br><span class="line"> &#x2F;&#x2F; which may be inaccurate.</span><br><span class="line"> try &#123;</span><br><span class="line"> inputMetrics.incBytesRead(split.inputSplit.value.getLength)</span><br><span class="line"> &#125; catch &#123;</span><br><span class="line"> case e: java.io.IOException &#x3D;&gt;</span><br><span class="line"> logWarning(&quot;Unable to get input size to set InputMetrics for ta</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> new InterruptibleIterator[(K, V)](context, iter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这样经过几次迭代之后就可以获取到RDD中所有分区的数据了，因为这个compute是一次获取一个分区</span><br><span class="line">的数据。获取到之后checkpoint就可以把这个RDD的数据存储到HDFS上了。</span><br><span class="line">这就是checkpoint的写操作</span><br></pre></td></tr></table></figure><h3 id="checkpoint的读操作"><a href="#checkpoint的读操作" class="headerlink" title="checkpoint的读操作"></a>checkpoint的读操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们来分析一下checkpoint读数据这个操作</span><br><span class="line">当RDD中的数据丢失了以后，需要通过checkpoint读取存储在hdfs上的数据，</span><br><span class="line">2.1：这个时候还是会执行RDD中的iterator方法</span><br><span class="line">由于我们没有做持久化，只做了checkpoint，所以还是会走 else</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">进入 computeOrReadCheckpoint 方法</span><br><span class="line">此时rdd已经 checkpoint 并且物化，所以 if 分支满足</span><br><span class="line">执行 firstParent[T].iterator(split, context) 这行代码</span><br><span class="line"></span><br><span class="line">这行代码的意思是会找到当前这个RDD的父RDD，其实这个RDD执行过checkpoint之后，血缘关系已经</span><br><span class="line">被切断了，它的父RDD就是我们前面创建的那个 ReliableCheckpointRDD</span><br><span class="line">这个 ReliableCheckpointRDD 中没有覆盖 iterator 方法，所以在调用 iterator 的时候还是执行RDD这个</span><br><span class="line">父类中的 iterator ，重新进来之后再判断，这个 ReliableCheckpointRDD 再执行if判断的时候就不满足</span><br><span class="line">了，因为它的 checkpoint 属性不满足，所以会走 else ，执行 compute</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskCont</span><br><span class="line">&#123;</span><br><span class="line"> &#x2F;&#x2F;当前rdd是否已经checkpoint并且物化，如果已经checkpoint并且物化</span><br><span class="line"> &#x2F;&#x2F;则调用firstParent的iterator方法获取</span><br><span class="line"> if (isCheckpointedAndMaterialized) &#123;</span><br><span class="line"> &#x2F;&#x2F;注意：针对checpoint操作，它的血缘关系已经被切断了，那么它的firstParent就是前面</span><br><span class="line"> firstParent[T].iterator(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;如果没有，则表示持久化数据丢失，或者根本就没有持久化，</span><br><span class="line"> &#x2F;&#x2F;需要调用rdd的compute方法开始重新计算，返回一个Iterator对象</span><br><span class="line"> compute(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时会执行 ReliableCheckpointRDD 这个子类中的 compute 方法</span><br><span class="line">这里面就会找到之前checkpoint的文件，从HDFS上恢复RDD中的数据。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">override def compute(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;获取checkpoint文件</span><br><span class="line"> val file &#x3D; new Path(checkpointPath, ReliableCheckpointRDD.checkpointFileNam</span><br><span class="line"> &#x2F;&#x2F;从HDFS上的checkpoint文件中读取checkpoint的数据</span><br><span class="line"> ReliableCheckpointRDD.readCheckpointFile(file, broadcastedConf, context)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这是从checkpoint中读取数据的流程</span><br><span class="line">咱们前面说过，建议对需要做checkpoint的数据先进行持久化，如果我们设置了持久化，针对</span><br><span class="line">checkpoint的写操作，在执行iterator方法的时候会是什么现象呢？</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时在最后将RDD中的数据通过checkpoint存储到HDFS上的时候，会调用RDD的iterator方法，不过此</span><br><span class="line">时 storageLevel 就不为 null 了，因为我们对这个RDD做了基于磁盘的持久化，所以会走 if 分支，执行</span><br><span class="line">getOrCompute</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">final def iterator(split: Partition, context: TaskContext): Iterator[T] &#x3D; &#123;</span><br><span class="line"> &#x2F;&#x2F;如果StorageLevel不为空，表示该RDD已经持久化过了,可能是在内存，也有可能是在磁盘</span><br><span class="line"> if (storageLevel !&#x3D; StorageLevel.NONE) &#123;</span><br><span class="line"> &#x2F;&#x2F;直接从持久化存储中读取或者计算(如果数据丢失)</span><br><span class="line"> getOrCompute(split, context)</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> &#x2F;&#x2F;进行rdd partition的计算或者从checkpoint中读取数据</span><br><span class="line"> computeOrReadCheckpoint(split, context)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">进入 getOrCompute 方法</span><br><span class="line">由于这个RDD的数据已经做了持久化，所以在这就可以从 blockmanager 中读取数据了，就不需要重新从</span><br><span class="line">源头计算或者拉取数据了，所以会提高 checkpoint 的效率</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">private[spark] def getOrCompute(partition: Partition, context: TaskContext): </span><br><span class="line"> val blockId &#x3D; RDDBlockId(id, partition.index)</span><br><span class="line"> var readCachedBlock &#x3D; true</span><br><span class="line"> &#x2F;&#x2F; This method is called on executors, so we need call SparkEnv.get instead </span><br><span class="line"> SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementCla</span><br><span class="line"> readCachedBlock &#x3D; false</span><br><span class="line"> computeOrReadCheckpoint(partition, context)</span><br><span class="line"> &#125;) match &#123;</span><br><span class="line"> case Left(blockResult) &#x3D;&gt;</span><br><span class="line"> if (readCachedBlock) &#123;</span><br><span class="line"> val existingMetrics &#x3D; context.taskMetrics().inputMetrics</span><br><span class="line"> existingMetrics.incBytesRead(blockResult.bytes)</span><br><span class="line"> new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[It</span><br><span class="line"> override def next(): T &#x3D; &#123;</span><br><span class="line"> existingMetrics.incRecordsRead(1)</span><br><span class="line"> delegate.next()</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125; else &#123;</span><br><span class="line"> new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iter</span><br><span class="line"> &#125;</span><br><span class="line"> case Right(iter) &#x3D;&gt;</span><br><span class="line"> new InterruptibleIterator(context, iter.asInstanceOf[Iterator[T]])</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-1.html</id>
    <published>2023-03-27T03:33:38.000Z</published>
    <updated>2023-03-28T06:52:35.476Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术"><a href="#第十一周-Spark性能优化的道与术" class="headerlink" title="第十一周 Spark性能优化的道与术"></a>第十一周 Spark性能优化的道与术</h1><h2 id="宽依赖和窄依赖"><a href="#宽依赖和窄依赖" class="headerlink" title="宽依赖和窄依赖"></a>宽依赖和窄依赖</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">先看一下什么是窄依赖：</span><br><span class="line">窄依赖(Narrow Dependency)：指父RDD的每个分区只被子RDD的一个分区所使用，例如map、filter等这些算子</span><br><span class="line">一个RDD，对它的父RDD只有简单的一对一的关系，也就是说，RDD的每个partition仅仅依赖于父RDD中的一个partition，父RDD和子RDD的partition之间的对应关系，是一对一的。</span><br><span class="line"></span><br><span class="line">宽依赖(Shuffle Dependency)：父RDD的每个分区都可能被子RDD的多个分区使用，例如groupByKey、</span><br><span class="line">reduceByKey，sortBykey等算子，这些算子其实都会产生shuffle操作</span><br><span class="line">也就是说，每一个父RDD的partition中的数据都可能会传输一部分到下一个RDD的每个partition中。此时就会出现，父RDD和子RDD的partition之间，具有错综复杂的关系，那么，这种情况就叫做两个RDD之间是宽依赖，同时，他们之间会发生shuffle操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来看图具体分析一个案例</span><br><span class="line">以单词计数案例来分析</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271142255.png" alt="image-20230327114152987"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">最左侧是linesRDD，这个表示我们通过textFile读取文件中的数据之后获取的RDD</span><br><span class="line">接着是我们使用flatMap算子，对每一行数据按照空格切开，然后可以获取到第二个RDD，这个RDD中包含的是切开的每一个单词</span><br><span class="line">在这里这两个RDD就属于一个窄依赖，因为父RDD的每个分区只被子RDD的一个分区所使用，也就是说他们的分区是一对一的，这样就不需要经过shuffle了。</span><br><span class="line">接着是使用map算子，将每一个单词转换成(单词,1)这种形式，</span><br><span class="line">此时这两个RDD也是一个窄依赖的关系，父RDD的分区和子RDD的分区也是一对一的</span><br><span class="line">最后我们会调用reduceByKey算子，此时会对相同key的数据进行分组，分到一个分区里面，并且进行聚合操作，此时父RDD的每个分区都可能被子RDD的多个分区使用，那这两个RDD就属于宽依赖了。</span><br><span class="line"></span><br><span class="line">这就是宽窄依赖的区别，那我们在这区分宽窄依赖有什么意义吗？</span><br><span class="line">不要着急，往下面看</span><br></pre></td></tr></table></figure><h2 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark job是根据action算子触发的,遇到action算子就会起一个job</span><br><span class="line">Spark Job会被划分为多个Stage，每一个Stage是由一组并行的Task组成的</span><br><span class="line"></span><br><span class="line">注意：stage的划分依据就是看是否产生了shuflle(即宽依赖),遇到一个shuffle操作就划分为前后两个stage</span><br><span class="line">stage是由一组并行的task组成，stage会将一批task用TaskSet来封装，提交给TaskScheduler进行分配，最后发送到Executor执行</span><br><span class="line"></span><br><span class="line">下面来看一张图来具体分析一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271146961.png" alt="image-20230327114640278"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">为什么是从后往前呢？因为RDD之间是有血缘关系的，后面的RDD依赖前面的RDD，也就是说后面的RDD要等前面的RDD执行完,才会执行。</span><br><span class="line">所以从后往前遇到宽依赖就划分为两个stage，shuffle前一个,shuffle后一个。如果整个过程没有产生shuffle那就只会有一个stage</span><br><span class="line"></span><br><span class="line">看这个图</span><br><span class="line">RDD G往前推，到RDD B的时候，是窄依赖，所以不切分Stage，再往前到RDD A，此时产生了宽依赖，所以RDD A属于一个Stage、RDD B 和 G属于一个Stage</span><br><span class="line">再看下面，RDD G到RDD F，产生了宽依赖，所以RDD F属于一个Stage，因为RDD F和RDD C、D、E这几个RDD没有产生宽依赖，都是窄依赖，所以他们属于一个Stage。</span><br><span class="line">所以这个图中,RDD A单独一个stage1,RDD C、D、E、F被划分在stage2中,</span><br><span class="line">最后RDD B和RDD G划分在了stage3 里面。</span><br><span class="line"></span><br><span class="line">注意：Stage划分是从后往前划分，但是stage执行时从前往后的，这就是为什么后面先切割的stage为什么编号是3.</span><br></pre></td></tr></table></figure><h2 id="Spark-Job的三种提交模式"><a href="#Spark-Job的三种提交模式" class="headerlink" title="Spark Job的三种提交模式"></a>Spark Job的三种提交模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1. 第一种，standalone模式，基于Spark自己的standalone集群。</span><br><span class="line">指定spark-submit –master spark:&#x2F;&#x2F;bigdata01:7077</span><br><span class="line"></span><br><span class="line">2. 第二种，是基于YARN的client模式。</span><br><span class="line">指定–master yarn --deploy-mode client</span><br><span class="line">这种方式主要用于测试，查看日志方便一些，部分日志会直接打印到控制台上面，因为driver进程运行在本地客户端，就是提交Spark任务的那个客户端机器，driver负责调度job，会与yarn集群产生大量的通信，一般情况下Spark客户端机器和Hadoop集群的机器是无法内网通信，只能通过外网，这样在大量通信的情况下会影响通信效率，并且当我们执行一些action操作的时候数据也会返回给driver端，driver端机器的配置一般都不高，可能会导致内存溢出等问题。</span><br><span class="line"></span><br><span class="line">3. 第三种，是基于YARN的cluster模式。【推荐】</span><br><span class="line">指定–master yarn --deploy-mode cluster</span><br><span class="line">这种方式driver进程运行在集群中的某一台机器上，这样集群内部节点之间通信是可以通过内网通信的，并且集群内的机器的配置也会比普通的客户端机器配置高，所以就不存在yarn-client模式的一些问题了，只不过这个时候查看日志只能到集群上面看了，这倒没什么影响。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271206035.png" alt="image-20230327120631638"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">左边是standalone模式，现在我们使用的提交方式，driver进程是在客户端机器中的，其实针对standalone模式而言，这个Driver进程也是可以运行在集群中的</span><br><span class="line">来看一下官网文档，standalone模式也是支持的，通过指定deploy-mode 为cluster即可</span><br><span class="line"></span><br><span class="line">中间的值yarn client模式，由于是on yarn模式，所以里面是yarn集群的进程，此时driver进程就在提交spark任务的客户端机器上了</span><br><span class="line">最右边这个是yarn cluster模式，driver进程就会在集群中的某一个节点上面。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271210537.png" alt="image-20230327121044013"></p><h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">在MapReduce框架中，Shuffle是连接Map和Reduce之间的桥梁，Map阶段通过shuffle读取数据并输出到对应的Reduce；而Reduce阶段负责从Map端拉取数据并进行计算。在整个shuffle过程中，往往伴随着大量的磁盘和网络I&#x2F;O。所以shuffle性能的高低也直接决定了整个程序的性能高低。Spark也会有自己</span><br><span class="line">的shuffle实现过程。</span><br><span class="line"></span><br><span class="line">我们首先来看一下</span><br><span class="line">在Spark中，什么情况下，会发生shuffle？</span><br><span class="line">reduceByKey、groupByKey、sortByKey、countByKey、join等操作都会产生shuffle。</span><br><span class="line"></span><br><span class="line">那下面我们来详细分析一下Spark中的shuffle过程。</span><br><span class="line">Spark的shuffle历经了几个过程</span><br><span class="line">1. Spark 0.8及以前使用Hash Based Shuffle</span><br><span class="line">2. Spark 0.8.1 为Hash Based Shuffle引入File Consolidation机制</span><br><span class="line">3. Spark1.6之后使用Sort-Base Shuffle，因为Hash Based Shuffle存在一些不足所以就把它替换掉了。</span><br><span class="line"></span><br><span class="line">所以Spark Shuffle 一共经历了这几个过程：</span><br><span class="line">1. 未优化的 Hash Based Shuffle</span><br><span class="line">2. 优化后的Hash Based Shuffle</span><br><span class="line">3. Sort-Based Shuffle</span><br></pre></td></tr></table></figure><h3 id="未优化的Hash-Based-Shuffle"><a href="#未优化的Hash-Based-Shuffle" class="headerlink" title="未优化的Hash Based Shuffle"></a>未优化的Hash Based Shuffle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">来看一个图，假设我们是在执行一个reduceByKey之类的操作，此时就会产生shuffle</span><br><span class="line">shuffle里面会有两种task，一种是shuffleMapTask，负责拉取前一个RDD中的数据，还有一个ResultTask，负责把拉取到的数据按照规则汇总起来</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271218160.png" alt="image-20230327121800239"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1：假设有1个节点，这个节点上有2个CPU，上面运行了4个ShuffleMapTask，这样的话其实同时只有2个ShuffleMapTask是并行执行的，因为一个cpu core同时只能执行一个ShuffleMapTask。</span><br><span class="line">2：每个ShuffleMapTask都会为每个ResultTask创建一份Bucket缓存，以及对应的ShuffleBlockFile磁盘文件</span><br><span class="line">这样的话，每一个ShuffleMapTask都会产生4份Bucket缓存和对应的4个ShuffleBlockFile文件，分别对应下面的4个ResultTask</span><br><span class="line"></span><br><span class="line">3：假设另一个节点上面运行了4个ResultTask现在等着获取ShuffleMapTask的输出数据，来完成比如ReduceByKey的操作。</span><br><span class="line">这是这个流程，注意了，如果有100个MapTask，100个ResultTask，那么会产生10000个本地磁盘文件，这样需要频繁的磁盘IO，是比较影响性能的。</span><br><span class="line"></span><br><span class="line">注意，那个bucket缓存是非常重要的，ShuffleMapTask会把所有的数据都写入Bucket缓存之后，才会刷写到对应的磁盘文件中，但是这就有一个问题，如果map端数据过多，那么很容易造成内存溢出，所以spark在优化后的Hash Based Shuffle中对这个问题进行了优化，默认这个内存缓存是100kb，当Bucket中的数据达到了阈值之后，就会将数据一点一点地刷写到对应的ShuffleBlockFile磁盘中了。这种操作的优点，是不容易发生内存溢出。缺点在于，如果内存缓存过小的话，那么可能发生过多的磁盘io操作。所以，这里的内存缓存大小，是可以根据实际的业务情况进行优化的。</span><br></pre></td></tr></table></figure><h3 id="优化后的Hash-Based-Shuffle"><a href="#优化后的Hash-Based-Shuffle" class="headerlink" title="优化后的Hash Based Shuffle"></a>优化后的Hash Based Shuffle</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271226135.png" alt="image-20230327122626812"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">看这个优化后的shuffle流程</span><br><span class="line">1：假设机器上有2个cpu，4个shuffleMaptask，这样同时只有2个在并行执行</span><br><span class="line">2：在这个版本中，Spark引入了consolidation机制，一个ShuffleMapTask将数据写入ResultTask数量的本地文件中，这个是不变的，但是当下一个ShuffleMapTask运行的时候，可以直接将数据写入之前产生的本地文件中，相当于对多个ShuffleMapTask的输出进行了合并，从而大大减少了本地磁盘中文件的数量。</span><br><span class="line">此时文件的数量变成了CPU core数量 * ResultTask数量，比如每个节点上有2个CPU，有100个ResultTask，那么每个节点上会产生200个文件</span><br><span class="line">这个时候文件数量就变得少多了。</span><br><span class="line"></span><br><span class="line">但是如果 ResultTask端的并行任务过多的话则 CPU core * Result Task 依旧过大，也会产生很多小文件</span><br></pre></td></tr></table></figure><h3 id="Sort-Based-Shuffle"><a href="#Sort-Based-Shuffle" class="headerlink" title="Sort-Based Shuffle"></a>Sort-Based Shuffle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">引入Consolidation机制虽然在一定程度上减少了磁盘文件数量，但是不足以有效提高Shuffle的性能，这种情况只适合中小型数据规模的数据处理。</span><br><span class="line">为了让Spark能在更大规模的集群上高性能处理大规模的数据，因此Spark引入了 Sort-Based Shuffle。</span><br><span class="line"></span><br><span class="line">该机制针对每一个ShuffleMapTask都只创建一个文件，将所有的 ShuffleMapTask的数据都写入同一个文件，并且对应生成一个索引文件。</span><br><span class="line">以前的数据是放在内存中，等到数据写完了再刷写到磁盘，现在为了减少内存的使用，在内存不够用的时候，可以将内存中的数据溢写到磁盘，结束的时候，再将这些溢写的文件联合内存中的数据一起进行归并，从而减少内存的使用量。一方面文件数量显著减少，另一方面减少缓存所占用的内存大小，而且同时避免GC的风险和频率。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303271434463.png" alt="image-20230327143450475"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://tianyong.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>IDEA使用经验积累</title>
    <link href="http://tianyong.fun/IDEA%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E7%A7%AF%E7%B4%AF.html"/>
    <id>http://tianyong.fun/IDEA%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E7%A7%AF%E7%B4%AF.html</id>
    <published>2023-03-24T15:07:36.000Z</published>
    <updated>2023-03-24T16:04:07.886Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="IDEA使用经验积累"><a href="#IDEA使用经验积累" class="headerlink" title="IDEA使用经验积累"></a>IDEA使用经验积累</h1><h2 id="运行程序报错内存不够"><a href="#运行程序报错内存不够" class="headerlink" title="运行程序报错内存不够"></a>运行程序报错内存不够</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">run-&gt;edit configuration-&gt;vm option</span><br><span class="line">-Xms1024m -Xmx1024m</span><br></pre></td></tr></table></figure><h2 id="Maven创建项目"><a href="#Maven创建项目" class="headerlink" title="Maven创建项目"></a>Maven创建项目</h2><h3 id="编辑spark程序时"><a href="#编辑spark程序时" class="headerlink" title="编辑spark程序时"></a>编辑spark程序时</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">本地运行</span><br><span class="line">&lt;!--&lt;scope&gt;provided&lt;&#x2F;scope&gt;--&gt;</span><br><span class="line">这个表示使用相关spark依赖</span><br><span class="line"></span><br><span class="line">在spark集群运行时</span><br><span class="line">要把注释去掉，表示不使用相关spark依赖</span><br></pre></td></tr></table></figure><h2 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ctrl+alt+v:快速创建返回类型</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第16章 spark</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC16%E7%AB%A0-spark.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC16%E7%AB%A0-spark.html</id>
    <published>2023-02-28T14:35:14.000Z</published>
    <updated>2023-03-03T16:51:35.890Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第15章 hadoop架构再探讨</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC15%E7%AB%A0-hadoop%E6%9E%B6%E6%9E%84%E5%86%8D%E6%8E%A2%E8%AE%A8.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC15%E7%AB%A0-hadoop%E6%9E%B6%E6%9E%84%E5%86%8D%E6%8E%A2%E8%AE%A8.html</id>
    <published>2023-02-28T14:34:54.000Z</published>
    <updated>2023-03-03T16:51:17.421Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第14章 基于hadoop的数据仓库Hive</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC14%E7%AB%A0-%E5%9F%BA%E4%BA%8Ehadoop%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC14%E7%AB%A0-%E5%9F%BA%E4%BA%8Ehadoop%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive.html</id>
    <published>2023-02-28T14:34:22.000Z</published>
    <updated>2023-03-03T16:50:57.288Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第11章 大数据在互联网中的应用</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC11%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9C%A8%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC11%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%9C%A8%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8.html</id>
    <published>2023-02-28T14:33:23.000Z</published>
    <updated>2023-02-28T14:33:23.315Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第10章 数据可视化</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC10%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC10%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html</id>
    <published>2023-02-28T14:32:57.000Z</published>
    <updated>2023-03-03T16:50:39.828Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第9章 图计算</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.html</id>
    <published>2023-02-28T14:32:32.000Z</published>
    <updated>2023-03-09T15:04:20.844Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第9章-图计算"><a href="#第9章-图计算" class="headerlink" title="第9章 图计算"></a>第9章 图计算</h1><h2 id="图计算简介"><a href="#图计算简介" class="headerlink" title="图计算简介"></a>图计算简介</h2><h3 id="图结构数据"><a href="#图结构数据" class="headerlink" title="图结构数据"></a>图结构数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•许多大数据都是以大规模图或网络的形式呈现，如社交网络、传染病传播途径、交通事故对路网的影响</span><br><span class="line">•许多非图结构的大数据，也常常会被转换为图模型后进行分析</span><br><span class="line">•图数据结构很好地表达了数据之间的关联性</span><br><span class="line">•关联性计算是大数据计算的核心——通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用的信息–比如，通过为购物者之间的关系建模，就能很快找到口味相似的用户，并为之推荐商品</span><br><span class="line">–或者在社交网络中，通过传播关系发现意见领袖</span><br></pre></td></tr></table></figure><h3 id="传统图计算解决方案的不足之处"><a href="#传统图计算解决方案的不足之处" class="headerlink" title="传统图计算解决方案的不足之处"></a>传统图计算解决方案的不足之处</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">很多传统的图计算算法都存在以下几个典型问题：</span><br><span class="line">（1）常常表现出比较差的内存访问局部性</span><br><span class="line">（2）针对单个顶点的处理工作过少</span><br><span class="line">（3）计算过程中伴随着并行度的改变</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">针对大型图（比如社交网络和网络图）的计算问题，可能的解决方案及其不足之处具体如下：</span><br><span class="line">•（1）为特定的图应用定制相应的分布式实现：通用性不好</span><br><span class="line">•（2）基于现有的分布式计算平台进行图计算：在性能和易用性方面往往无法达到最优</span><br><span class="line">•现有的并行计算框架像MapReduce还无法满足复杂的关联性计算</span><br><span class="line">•MapReduce作为单输入、两阶段、粗粒度数据并行的分布式计算框架，在表达多迭代、稀疏结构和细粒度数据时，力不从心</span><br><span class="line">•比如，有公司利用MapReduce进行社交用户推荐，对于5000万注册用户，50亿关系对，利用10台机器的集群，需要超过10个小时的计算</span><br><span class="line">•（3）使用单机的图算法库：比如BGL、LEAD、NetworkX、JDSL、Standford GraphBase和FGL等，但是，在可以解决的问题的规模方面具有很大的局限性</span><br><span class="line">•（4）使用已有的并行图计算系统：比如，Parallel BGL和CGM Graph，实现了很多并行图算法，但是，对大规模分布式系统非常重要的一些方面（比如容错），无法提供较好的支持</span><br></pre></td></tr></table></figure><h3 id="图计算通用软件"><a href="#图计算通用软件" class="headerlink" title="图计算通用软件"></a>图计算通用软件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对大型图的计算，目前通用的图计算软件主要包括两种：</span><br><span class="line">第一种主要是基于遍历算法的、实时的图数据库，如Neo4j、OrientDB、DEX和Infinite Graph</span><br><span class="line">第二种是以图顶点为中心的、基于消息传递批处理的并行引擎，如GoldenOrb、Giraph、Pregel和Hama，这些图处理软件主要是基于BSP模型实现的并行图处理系统</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">一次BSP(整体同步并行计算模型、又称大同步模型)计算过程包括一系列全局超步（所谓的超步就是计算中的一次迭代），每个超步主要包括三个组件：</span><br><span class="line">•局部计算：每个参与的处理器都有自身的计算任务，它们只读取存储在本地内存中的值，不同处理器的计算任务都是异步并且独立的</span><br><span class="line"></span><br><span class="line">•通讯：处理器群相互交换数据，交换的形式是，由一方发起推送(put)和获取(get)操作</span><br><span class="line"></span><br><span class="line">•栅栏同步(Barrier Synchronization)：当一个处理器遇到“路障”（或栅栏），会等到其他所有处理器完成它们的计算步骤；每一次同步也是一个超步的完成和下一个超步的开始。图9-1是一个超步的垂直结构图</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308202813506.png" alt="image-20230308202813506" style="zoom:67%;"><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082212630.png" alt="image-20230308221246547"></p><h2 id="Pregel简介"><a href="#Pregel简介" class="headerlink" title="Pregel简介"></a>Pregel简介</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•谷歌公司在2003年到2004年公布了GFS、MapReduce和BigTable，成为后来云计算和Hadoop项目的重要基石</span><br><span class="line"></span><br><span class="line">•谷歌在后Hadoop时代的新“三驾马车”——Caffeine(大规模网页索引的建立)、Dremel(实时交互式分析产品)和Pregel，再一次影响着圈子与大数据技术的发展潮流</span><br><span class="line"></span><br><span class="line">•Pregel是一种基于BSP模型实现的并行图处理系统</span><br><span class="line">•为了解决大型图的分布式计算问题，Pregel搭建了一套可扩展的、有容错机制的平台，该平台提供了一套非常灵活的API，可以描述各种各样的图计算</span><br><span class="line">•Pregel作为分布式图计算的计算框架，主要用于图遍历、最短路径、PageRank计算等等</span><br></pre></td></tr></table></figure><h2 id="Pregel图计算模型"><a href="#Pregel图计算模型" class="headerlink" title="Pregel图计算模型"></a>Pregel图计算模型</h2><h3 id="有向图和顶点"><a href="#有向图和顶点" class="headerlink" title="有向图和顶点"></a>有向图和顶点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">•Pregel计算模型以有向图作为输入，有向图的每个顶点都有一个String类型的顶点ID，每个顶点都有一个可修改的用户自定义值与之关联，每条有向边都和其源顶点关联，并记录了其目标顶点ID，边上有一个可修改的用户自定义值与之关联</span><br><span class="line">•在每个超步S中，图中的所有顶点都会并行执行相同的用户自定义函数。每个顶点可以接收前一个超步(S-1)中发送给它的消息，修改其自身及其出射边的状态，并发送消息给其他顶点，甚至是修改整个图的拓扑结构。需要指出的是，在这种计算模式中，边并不是核心对象，在边上面不会运行相应的计算，只有顶点才会执行用户自定义函数进行相应计算</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091325684.png" alt="image-20230309132510569"></p><h3 id="顶点之间的消息传递"><a href="#顶点之间的消息传递" class="headerlink" title="顶点之间的消息传递"></a>顶点之间的消息传递</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">采用消息传递模型主要基于以下两个原因：</span><br><span class="line">（1）消息传递具有足够的表达能力，没有必要使用远程读取或共享内存的方式</span><br><span class="line">（2）有助于提升系统整体性能。大型图计算通常是由一个集群完成的，集群环境中执行远程数据读取会有较高的延迟；Pregel的消息模式采用异步和</span><br><span class="line">批量的方式传递消息，因此可以缓解远程读取的延迟</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308203709811.png" alt="image-20230308203709811" style="zoom:67%;"><h3 id="Pregel的计算过程"><a href="#Pregel的计算过程" class="headerlink" title="Pregel的计算过程"></a>Pregel的计算过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•Pregel的计算过程是由一系列被称为“超步”的迭代组成的。</span><br><span class="line"></span><br><span class="line">在每个超步中，每个顶点上面都会并行执行用户自定义的函数，该函数描述了一个顶点V在一个超步S中需要执行的操作。</span><br><span class="line"></span><br><span class="line">该函数可以读取前一个超步(S-1)中其他顶点发送给顶点V的消息，执行相应计算后，修改顶点V及其出射边的状态，然后沿着顶点V的出射边发送消息给其他顶点，而且，一个消息可能经过多条边的传递后被发送到任意已知ID的目标顶点上去。</span><br><span class="line"></span><br><span class="line">这些消息将会在下一个超步(S+1)中被目标顶点接收，然后像上述过程一样开始下一个超步(S+1)的迭代过程</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">•在Pregel计算过程中，一个算法什么时候可以结束，是由所有顶点的状态决定的</span><br><span class="line">•在第0个超步，所有顶点处于活跃状态，都会参与该超步的计算过程</span><br><span class="line">•当一个顶点不需要继续执行进一步的计算时，就会把自己的状态设置为“停机”，进入非活跃状态</span><br><span class="line">•一旦一个顶点进入非活跃状态，后续超步中就不会再在该顶点上执行计算，除非其他顶点给该顶点发送消息把它再次激活</span><br><span class="line">•当一个处于非活跃状态的顶点收到来自其他顶点的消息时，Pregel计算框架必须根据条件判断来决定是否将其显式唤醒进入活跃状态</span><br><span class="line">•当图中所有的顶点都已经标识其自身达到“非活跃（inactive）”状态，并且没有消息在传送的时候，算法就可以停止运行</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308204037030.png" alt="image-20230308204037030" style="zoom:67%;"><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308204100764.png" alt="image-20230308204100764" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">发送出去的消息，会在下一个超步执行，每一个顶点都有一个输入队列，队列中的值来源于上一个超步，每一个超步执行时去取自己输入队列中的值</span><br></pre></td></tr></table></figure><h2 id="Pregel的C-API"><a href="#Pregel的C-API" class="headerlink" title="Pregel的C++ API"></a>Pregel的C++ API</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pregel已经预先定义好一个基类——Vertex类：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">template &lt;typename VertexValue, typename EdgeValue, typename MessageValue&gt;</span><br><span class="line">class Vertex &#123;</span><br><span class="line"> public:</span><br><span class="line">    virtual void Compute(MessageIterator* msgs) &#x3D; 0;</span><br><span class="line">    const string&amp; vertex_id() const; &#x2F;&#x2F; 顶点ID</span><br><span class="line">    int64 superstep() const; &#x2F;&#x2F; 超步是第几步</span><br><span class="line">    const VertexValue&amp; GetValue(); &#x2F;&#x2F; 获得顶点的值</span><br><span class="line">    VertexValue* MutableValue(); &#x2F;&#x2F; 修改顶点值</span><br><span class="line">    OutEdgeIterator GetOutEdgeIterator();&#x2F;&#x2F; 获得该顶点出射边</span><br><span class="line">    void SendMessageTo(const string&amp; dest_vertex, const MessageValue&amp; message);</span><br><span class="line">    void VoteToHalt();&#x2F;&#x2F; 修改状态</span><br><span class="line"> &#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">•在Vetex类中，定义了三个值类型参数，分别表示顶点、边和消息。每一个顶点都有一个给定类型的值与之对应</span><br><span class="line">•编写Pregel程序时，需要继承Vertex类，并且覆写Vertex类的虚函数Compute() </span><br><span class="line"></span><br><span class="line">•在Pregel执行计算过程时，在每个超步中都会并行调用每个顶点上定义的Compute()函数</span><br><span class="line">•允许Compute()方法查询当前顶点及其边的信息，以及发送消息到其他的顶点</span><br><span class="line">    –Compute()方法可以调用GetValue()方法来获取当前顶点的值</span><br><span class="line">    –调用MutableValue()方法来修改当前顶点的值</span><br><span class="line">    –通过由出射边的迭代器提供的方法来查看、修改出射边对应的值</span><br><span class="line">•对状态的修改，对于被修改的顶点而言是可以立即被看见的，但是，对于其他顶点而言是不可见的，因此，不同顶点并发进行的数据访问是不存在竞争关系的</span><br><span class="line">整个过程中，唯一需要在超步之间持久化的顶点级状态，是顶点和其对</span><br><span class="line">应的边所关联的值，因而，Pregel计算框架所需要管理的图状态就只包括顶点和边所关联的值，这种做法大大简化了计算流程，同时，也有利于图的分布和故障恢复</span><br></pre></td></tr></table></figure><h3 id="消息传递机制"><a href="#消息传递机制" class="headerlink" title="消息传递机制"></a>消息传递机制</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 顶点之间的通讯是借助于消息传递机制来实现的，每条消息都包含了消息值和需要到达的目标顶点ID。用户可以通过Vertex类的模板参数来设定消息值的数据类型</span><br><span class="line">• 在一个超步S中，一个顶点可以发送任意数量的消息，这些消息将在下一个超步（S+1）中被其他顶点接收</span><br><span class="line">• 一个顶点V通过与之关联的出射边向外发送消息，并且，消息要到达的目标顶点并不一定是与顶点V相邻的顶点，一个消息可以连续经过多条连通的边到达某个与顶点V不相邻的顶点U，U可以从接收的消息中获取到与其不相邻的顶点V的ID</span><br></pre></td></tr></table></figure><h3 id="Combiner"><a href="#Combiner" class="headerlink" title="Combiner"></a>Combiner</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• Pregel计算框架在消息发出去之前，Combiner可以将发往同一个顶点的多个整型值进行求和得到一个值，只需向外发送这个“求和结果”，从而实现了由多个消息合并成一个消息，大大减少了传输和缓存的开销</span><br><span class="line">• 在默认情况下，Pregel计算框架并不会开启Combiner功能，因为，通常很难找到一种对所有顶点的Compute()函数都合适的Combiner</span><br><span class="line">• 当用户打算开启Combiner功能时，可以继承Combiner类并覆写虚函数Combine()</span><br><span class="line">• 此外，通常只对那些满足交换律和结合律的操作才可以去开启Combiner功能，因为，Pregel计算框架无法保证哪些消息会被合并，也无法保证消息传递给 Combine()的顺序和合并操作执行的顺序</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308204959085.png" alt="image-20230308204959085" style="zoom:67%;"><h3 id="Aggregator"><a href="#Aggregator" class="headerlink" title="Aggregator"></a>Aggregator</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• Aggregator提供了一种全局通信、监控和数据查看的机制</span><br><span class="line">• 在一个超步S中，每一个顶点都可以向一个Aggregator提供一个数据，Pregel计算框架会对这些值进行聚合操作产生一个值，在下一个超步（S+1）中，图中的所有顶点都可以看见这个值</span><br><span class="line">• Aggregator的聚合功能，允许在整型和字符串类型上执行最大值、最小值、求和操作，比如，可以定义一个“Sum”Aggregator来统计每个顶点的出射边数量，最后相加可以得到整个图的边的数量</span><br><span class="line">• Aggregator还可以实现全局协同的功能，比如，可以设计“and” Aggregator来决定在某个超步中Compute()函数是否执行某些逻辑分支，只有当“and” Aggregator显示所有顶点都满足了某条件时，才去执行这些逻辑分支</span><br></pre></td></tr></table></figure><h3 id="拓扑改变"><a href="#拓扑改变" class="headerlink" title="拓扑改变"></a>拓扑改变</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Pregel计算框架允许用户在自定义函数Compute()中定义操作，修改图的拓扑结构，比如在图中增加（或删除）边或顶点</span><br><span class="line">• 对于全局拓扑改变，Pregel采用了惰性协调机制，在改变请求发出时，Pregel不会对这些操作进行协调，只有当这些改变请求的消息到达目标顶点并被执行时，Pregel才会对这些操作进行协调，这样，所有针对某个顶点V的拓扑修改操作所引发的冲突，都会由V自己来处理</span><br><span class="line">• 对于本地的局部拓扑改变，是不会引发冲突的，顶点或边的本地增减能够立即生效，很大程度上简化了分布式编程</span><br></pre></td></tr></table></figure><h3 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 在Pregel计算框架中，图的保存格式多种多样，包括文本文件、关系数据库或键值数据库等</span><br><span class="line">• 在Pregel中，“从输入文件生成得到图结构”和“执行图计算”这两个过程是分离的，从而不会限制输入文件的格式</span><br><span class="line">• 对于输出，Pregel也采用了灵活的方式，可以以多种方式进行输出</span><br></pre></td></tr></table></figure><h2 id="Pregel的体系结构"><a href="#Pregel的体系结构" class="headerlink" title="Pregel的体系结构"></a>Pregel的体系结构</h2><h3 id="Pregel的执行过程"><a href="#Pregel的执行过程" class="headerlink" title="Pregel的执行过程"></a>Pregel的执行过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">•在Pregel计算框架中，一个大型图会被划分成许多个分区，每个分区都包含了一部分顶点以及以其为起点的边</span><br><span class="line">•一个顶点应该被分配到哪个分区上，是由一个函数决定的，系统默认函数为hash(ID) mod N，其中，N为所有分区总数，ID是这个顶点的标识符；当然，用户也可以自己定义这个函数</span><br><span class="line">•这样，无论在哪台机器上，都可以简单根据顶点ID判断出该顶点属于哪个分区，即使该顶点可能已经不存在了</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308210115360.png" alt="image-20230308210115360" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在理想的情况下（不发生任何错误），一个Pregel用户程序的执行过程如下：</span><br><span class="line">（1）选择集群中的多台机器执行图计算任务，每台机器上运行用户程序的一个副本，其中，有一台机器会被选为Master，其他机器作为Worker。Master只负责协调多个Worker执行任务，系统不会把图的任何分区分配给它。Worker借助于名称服务系统可以定位到Master的位置，并向Master发送自己的注册信息。</span><br><span class="line">（2）Master把一个图分成多个分区，并把分区分配到多个Worker。一个Worker会领到一个或多个分区，每个</span><br><span class="line">Worker知道所有其他Worker所分配到的分区情况。每个</span><br><span class="line">Worker负责维护分配给自己的那些分区的状态(顶点及边</span><br><span class="line">的增删)，对分配给自己的分区中的顶点执行Compute()函</span><br><span class="line">数，向外发送消息，并管理接收到的消息。</span><br><span class="line">（3）Master会把用户输入划分成多个部分，通常是基于文件边界进行划分。划分后，每个部分都是一系列记录的集合，每条记录都包含一定数量的顶点和边。然后，Master会为每个Worker分配用户输入的一部分。如果一个Worker从输入内容中加载到的顶点，刚好是自己所分配到的分区中的顶点，就会立即更新相应的数据结构。否则，该Worker会根据加载到的顶点的ID，把它发送到其所属的分区所在的Worker上。当所有的输入都被加载后，图中的所有顶点都会被标记为“活跃”状态。</span><br><span class="line">（4）Master向每个Worker发送指令，Worker收到指令后，开始运行一个超步。Worker会为自己管辖的每个分区分配一个线程，对于分区中的每个顶点，Worker会把来自上一个超步的、发给该顶点的消息传递给它，并调用处于“活跃”状态的顶点上的Compute()函数，在执行计算过程中，顶点可以对外发送消息，但是，所有消息的发送工作必须在本超步结束之前完成。当所有这些工作都完成以后，Worker会通知Master，并把自己在下一个超步还处于“活跃”状态的顶点的数量报告给Master。上述步骤会被不断重复，直到所有顶点都不再活跃并且系统中不会有任何消息在传输，这时，执行过程才会结束。</span><br><span class="line">（5）计算过程结束后，Master会给所有的Worker发送指令，通知每个Worker对自己的计算结果进行持久化存储。</span><br></pre></td></tr></table></figure><img src="/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC9%E7%AB%A0-%E5%9B%BE%E8%AE%A1%E7%AE%97.htm/Users\Ty\AppData\Roaming\Typora\typora-user-images\image-20230308210550499.png" alt="image-20230308210550499" style="zoom:67%;"><h3 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Pregel采用检查点机制来实现容错。在每个超步的开始，Master会通知所有的Worker把自己管辖的分区的状态（包括顶点值、边值以及接收到的消息），写入到持久化存储设备</span><br><span class="line">• Master会周期性地向每个Worker发送ping消息，Worker收到ping消息后会给Master发送反馈消息。如果Master在指定时间间隔内没有收到某个Worker的反馈消息，就会把该Worker标记为“失效”。同样地，如果一个Worker在指定的时间间隔内没有收到来自Master的ping消息，该Worker也会停止工作</span><br><span class="line">• 每个Worker上都保存了一个或多个分区的状态信息，当一个Worker发生故障时，它所负责维护的分区的当前状态信息就会丢失。Master监测到一个Worker发生故障“失效”后，会把失效Worker所分配到的分区，重新分配到其他处于正常工作状态的Worker集合上，然后，所有这些分区会从最近的某超步S开始时写出的检查点中，重新加载状态信息。很显然，这个超步S可能会比失效Worker上最后运行的超步S1要早好几个阶段，因此，为了恢复到最新的正确状态，需要重新执行从超步S到超步S1的所有操作</span><br></pre></td></tr></table></figure><h3 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在一个Worker中，它所管辖的分区的状态信息是保存在内存中的。分区中的顶点的状态信息包括：</span><br><span class="line">•顶点的当前值</span><br><span class="line">•以该顶点为起点的出射边列表，每条出射边包含了目标顶点ID和边的值</span><br><span class="line">•消息队列，包含了所有接收到的、发送给该顶点的消息</span><br><span class="line">•标志位，用来标记顶点是否处于活跃状态</span><br><span class="line"></span><br><span class="line">在每个超步中，Worker会对自己所管辖的分区中的每个顶点进行遍历，并调用顶点上的Compute()函数，在调用时，会把以下三个参数传递进去：</span><br><span class="line">•该顶点的当前值</span><br><span class="line">•一个接收到的消息的迭代器</span><br><span class="line">•一个出射边的迭代器</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">•在Pregel中，为了获得更好的性能，“标志位”和输入消息</span><br><span class="line">队列是分开保存的</span><br><span class="line">•对于每个顶点而言，Pregel只保存一份顶点值和边值，但是，会保存两份“标志位”和输入消息队列，分别用于当前超步和下一个超步</span><br><span class="line">•在超步S中，当一个Worker在进行顶点处理时，用于当前超步的消息会被处理，同时，它在处理过程中还会接收到来自其他Worker的消息，这些消息会在下一个超步S+1中被处理，因此，需要两个消息队列用于存放作用于当前超步S的消息和作用于下一个超步S+1的消息</span><br><span class="line">•如果一个顶点V在超步S接收到消息，那么，它表示V将会在下一个超步S+1中（而不是当前超步S中）处于“活跃”状态</span><br><span class="line">•当一个Worker上的一个顶点V需要发送消息到其他顶点U时，该Worker会首先判断目标顶点U是否位于自己机器上</span><br><span class="line">•如果目标顶点U在自己的机器上，就直接把消息放入到与目标顶点U对应的输入消息队列中</span><br><span class="line">•如果发现目标顶点U在远程机器上，这个消息就会被暂时缓存到本地，当缓存中的消息数目达到一个事先设定的阈值时，这些缓存消息会被批量异步发送出去，传输到目标顶点所在的Worker上</span><br><span class="line">•如果存在用户自定义的Combiner操作，那么，当消息被加入到输出队列或者到达输入队列时，就可以对消息执行合并操作，这样可以节省存储空间和网络传输开销</span><br></pre></td></tr></table></figure><h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">•Master主要负责协调各个Worker执行任务，每个Worker会借助于名称服务系统定位到Master的位置，并向Master发送自己的注册信息，Master会为每个Worker分配一个唯一的ID</span><br><span class="line">•Master维护着关于当前处于“有效”状态的所有Worker的各种信息，包括每个Worker的ID和地址信息，以及每个Worker被分配到的分区信息</span><br><span class="line">•虽然在集群中只有一个Master，但是，它仍然能够承担起一个大规模图计算的协调任务，这是因为Master中保存这些信息的数据结构的大小，只与分区的数量有关，而与顶点和边的数量无关</span><br><span class="line">•一个大规模图计算任务会被Master分解到多个Worker去执行，在每个超步开始时，Master都会向所有处于“有效”状态的Worker发送相同的指令，然后等待这些Worker的回应</span><br><span class="line">•如果在指定时间内收不到某个Worker的反馈，Master就认为这个Worker失效</span><br><span class="line">•如果参与任务执行的多个Worker中的任意一个发生了故障失效，Master就会进入恢复模式</span><br><span class="line">•在每个超步中，图计算的各种工作，比如输入、输出、计算、保存和从检查点中恢复，都会在“路障（barrier）”之前结束</span><br><span class="line">•如果路障同步成功，说明一个超步顺利结束，Master就会进入下一个处理阶段，图计算进入下一个超步的执行</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">•Master在内部运行了一个HTTP服务器来显示图计算过程的各种信息</span><br><span class="line">•用户可以通过网页随时监控图计算执行过程各个细节</span><br><span class="line">•图的大小</span><br><span class="line">•关于出度分布的柱状图</span><br><span class="line">•处于活跃状态的顶点数量</span><br><span class="line">•在当前超步的时间信息和消息流量</span><br><span class="line">•所有用户自定义Aggregator的值</span><br></pre></td></tr></table></figure><h3 id="Aggregator-1"><a href="#Aggregator-1" class="headerlink" title="Aggregator"></a>Aggregator</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 每个用户自定义的Aggregator都会采用聚合函数对一个值集合进行聚合计算得到一个全局值</span><br><span class="line">• 每个Worker都保存了一个Aggregator的实例集，其中的每个实例都是由类型名称和实例名称来标识的</span><br><span class="line">• 在执行图计算过程的某个超步S中，每个Worker会利用一个Aggregator对当前本地分区中包含的所有顶点的值进行归约，得到一个本地的局部归约值</span><br><span class="line">• 在超步S结束时，所有Worker会将所有包含局部归约值的Aggregator的值进行最后的汇总，得到全局值，然后提交给Master</span><br><span class="line">• 在下一个超步S+1开始时，Master就会将Aggregator的全局值发送给每个Worker</span><br></pre></td></tr></table></figure><h2 id="Pregel的应用实例"><a href="#Pregel的应用实例" class="headerlink" title="Pregel的应用实例"></a>Pregel的应用实例</h2><h3 id="单源最短路径"><a href="#单源最短路径" class="headerlink" title="单源最短路径"></a>单源最短路径</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303090019737.png" alt="image-20230309001948663"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class ShortestPathVertex</span><br><span class="line"> : public Vertex&lt;int, int, int&gt; &#123;</span><br><span class="line"> void Compute(MessageIterator* msgs) &#123;</span><br><span class="line">     int mindist &#x3D; IsSource(vertex_id()) ? 0 : INF;</span><br><span class="line">     for (; !msgs-&gt;Done(); msgs-&gt;Next())</span><br><span class="line">     mindist &#x3D; min(mindist, msgs-&gt;Value());</span><br><span class="line">     if (mindist &lt; GetValue()) &#123;</span><br><span class="line">     *MutableValue() &#x3D; mindist;</span><br><span class="line">     OutEdgeIterator iter &#x3D; GetOutEdgeIterator();</span><br><span class="line">     for (; !iter.Done(); iter.Next())</span><br><span class="line">     SendMessageTo(iter.Target(),</span><br><span class="line">     mindist + iter.GetValue());</span><br><span class="line"> &#125;</span><br><span class="line"> VoteToHalt();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1 class ShortestPathVertex</span><br><span class="line">2 : public Vertex&lt;int, int, int&gt; &#123;</span><br><span class="line">3 void Compute(MessageIterator* msgs) &#123;</span><br><span class="line">4 int mindist &#x3D; IsSource(vertex_id()) ? 0 : INF;</span><br><span class="line">5 for (; !msgs-&gt;Done(); msgs-&gt;Next())</span><br><span class="line">6 mindist&#x3D; min(mindist, msgs-&gt;Value());</span><br><span class="line">7 if (mindist &lt; GetValue()) &#123;</span><br><span class="line">8 *MutableValue() &#x3D; mindist;</span><br><span class="line">9 OutEdgeIteratoriter &#x3D; GetOutEdgeIterator();</span><br><span class="line">10 for (; !iter.Done(); iter.Next())</span><br><span class="line">11 SendMessageTo(iter.Target(),</span><br><span class="line">12 mindist+ iter.GetValue());</span><br><span class="line">13 &#125;</span><br><span class="line">14 VoteToHalt();</span><br><span class="line">15 &#125;</span><br><span class="line">16 &#125;;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091405728.png" alt="image-20230309140534677"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091405163.png" alt="image-20230309140546109"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303091414778.png" alt="image-20230309141445727"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">超步1：</span><br><span class="line">•顶点0：没有收到消息，依然非活跃</span><br><span class="line">•顶点1：收到消息100（唯一消息），被显式唤醒，执行计算，mindist变为100，小于顶点值INF，顶点值修改为100，没有出射边，不需要发送消息，最后变为非活跃</span><br><span class="line">•顶点2：收到消息30，被显式唤醒，执行计算， mindist变为30，小于顶点值INF，顶点值修改为30，有两条出射边，向顶点3发送消息90（即：30+60），向顶点1发送消息90（即：30+60），最后变为非活跃</span><br><span class="line">•顶点3：没有收到消息，依然非活跃</span><br><span class="line">•顶点4：收到消息10，被显式唤醒，执行计算， mindist变为10，小于顶点值INF，顶点值修改为10，向顶点3发送消息60（即：10+50），最后变为非活跃</span><br><span class="line">剩余超步省略……</span><br><span class="line">当所有顶点非活跃，并且没有消息传递，就结束</span><br></pre></td></tr></table></figure><h3 id="二分匹配"><a href="#二分匹配" class="headerlink" title="二分匹配"></a>二分匹配</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">程序的执行过程是由四个阶段组成的多个循环组成的，当程序执行到超步S时，S mod 4就可以得到当前超步处于循环的哪个阶段。每个循环的四个阶段如下：</span><br><span class="line"> （1）阶段0：对于左集合中的任意顶点V，如果V还没有被匹配，就发送消息给它的每个邻居顶点请求匹配，然后，顶点V会调用VoteToHalt()进入“非活跃”状态。如果顶点V已经找到了匹配，或者V没有找到匹配但是没有出射边，那么，顶点V就不会发送消息。当顶点V没有发送消息，或者顶点V发送了消息但是所有的消息接收者都已经被匹配，那么，该顶点就不会再变为“活跃（active）”状态</span><br><span class="line"> （2）阶段1：对于右集合中的任意顶点U，如果它还没有被匹配，则会随机选择它接收到的消息中的其中一个，并向左集合中的消息发送者发送消息表示接受该匹配请求，然后给左集合中的其他请求者发送拒绝消息；然后，顶点U会调用VoteToHalt()进入“非活跃”状态</span><br><span class="line"> （3）阶段2：左集合中那些还未被匹配的顶点，会从它所收到的、右集合发送过来的接受请求中，选择其中一个给予确认，并发送一个确认消息。对于左集合中已经匹配的顶点而言，因为它们在阶段0不会向右集合发送任何匹配请求消息，因而也不会接收到任何来自右集合的匹配接受消息，因此，是不会执行阶段2的</span><br><span class="line"> （4）阶段3：右集合中还未被匹配的任意顶点U，会收到来自左集合的匹配确认消息，但是，每个未匹配的顶点U，最多会收到一个确认消息。然后，顶点U会调VoteToHalt()进入“非活跃”状态，完成它自身的匹配工作</span><br></pre></td></tr></table></figure><h2 id="Pregel和MapReduce实现PageRank算法的对比"><a href="#Pregel和MapReduce实现PageRank算法的对比" class="headerlink" title="Pregel和MapReduce实现PageRank算法的对比"></a>Pregel和MapReduce实现PageRank算法的对比</h2><h3 id="PageRank算法"><a href="#PageRank算法" class="headerlink" title="PageRank算法"></a>PageRank算法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• PageRank是一个函数，它为网络中每个网页赋一个权值。通过该权值来判断该网页的重要性</span><br><span class="line">• 该权值分配的方法并不是固定的，对PageRank算法的一些简单变形都会改变网页的相对PageRank值（PR值）</span><br><span class="line">• PageRank作为谷歌的网页链接排名算法，基本公式如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082125420.png" alt="image-20230308212003706"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 对于任意一个网页链接，其PR值为链入到该链接的源链接的PR值对该链接的贡献和，其中，N表示该网络中所有网页的数量，Ni为第i个源链接的链出度，PRi表示第i个源链接的PR值</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 网络链接之间的关系可以用一个连通图来表示，下图就是四个网页（A,B,C,D）互相链入链出组成的连通图，从中可以看出，网页A中包含指向网页B、C和D的外链，网页B和D是网页A的源链接</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082125422.png" alt="image-20230308212407285"></p><h3 id="PageRank算法在Pregel中的实现"><a href="#PageRank算法在Pregel中的实现" class="headerlink" title="PageRank算法在Pregel中的实现"></a>PageRank算法在Pregel中的实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 在Pregel计算模型中，图中的每个顶点会对应一个计算单元，每个计算单元包含三个成员变量：</span><br><span class="line">顶点值（Vertex value）：顶点对应的PR值</span><br><span class="line">出射边（Out edge）：只需要表示一条边，可以不取值</span><br><span class="line">消息（Message）：传递的消息，因为需要将本顶点对其它顶点的PR贡献值，传递给目标顶点</span><br><span class="line">• 每个计算单元包含一个成员函数Compute()，该函数定义了顶点上的运算，包括该顶点的PR值计算，以及从该顶点发送消息到其链出顶点</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class PageRankVertex: public Vertex&lt;double, void, double&gt; &#123;</span><br><span class="line">public:</span><br><span class="line">     virtual void Compute(MessageIterator* msgs) &#123;</span><br><span class="line">        if (superstep() &gt;&#x3D; 1) &#123;</span><br><span class="line">            double sum &#x3D; 0;</span><br><span class="line">            for (;!msgs-&gt;Done(); msgs-&gt;Next())</span><br><span class="line">            sum +&#x3D; msgs-&gt;Value();</span><br><span class="line">            *MutableValue() &#x3D;</span><br><span class="line">            0.15 &#x2F; NumVertices() + 0.85 * sum;</span><br><span class="line">        &#125;</span><br><span class="line">        if (superstep() &lt; 30) &#123;</span><br><span class="line">             const int64 n &#x3D; GetOutEdgeIterator().size();</span><br><span class="line">             SendMessageToAllNeighbors(GetValue()&#x2F; n);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">        VoteToHalt();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• PageRankVertex继承自Vertex类，顶点值类型是double，用来保存PageRank中间值，消息类型也是double，用来传输PageRank值，边的value类型是void，因为不需要存储任何信息</span><br><span class="line">• 这里假设在第0个超步时，图中各顶点值被初始化为1&#x2F;NumVertices()，其中，NumVertices()表示顶点数目</span><br><span class="line">• 在前30个超步中，每个顶点都会沿着它的出射边，发送它的PageRank值除以出射边数目以后的结果值。从第1个超步开始，每个顶点会将到达的消息中的值加到sum值中，同时将它的PageRank值设为0.15&#x2F;NumVertices()+0.85*sum</span><br><span class="line">• 到了第30个超步后，就没有需要发送的消息了，同时所有的顶点停止计算，得到最终结果</span><br></pre></td></tr></table></figure><h3 id="PageRank算法在MapReduce中的实现"><a href="#PageRank算法在MapReduce中的实现" class="headerlink" title="PageRank算法在MapReduce中的实现"></a>PageRank算法在MapReduce中的实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• MapReduce也是谷歌公司提出的一种计算模型，它是为全量计算而设计</span><br><span class="line">• 采用MapReduce实现PageRank的计算过程包括三个阶段：</span><br><span class="line">     第一阶段：解析网页</span><br><span class="line">     第二阶段：PageRank分配</span><br><span class="line">     第三阶段：收敛阶段</span><br></pre></td></tr></table></figure><h4 id="阶段1：解析网页"><a href="#阶段1：解析网页" class="headerlink" title="阶段1：解析网页"></a>阶段1：解析网页</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 该阶段的任务就是分析一个页面的链接数并赋初值。</span><br><span class="line">• 一个网页可以表示为由网址和内容构成的键值对&lt; URL，page content&gt;，作为Map任务的输入。阶段1的Map任务把&lt;URL，page content&gt;映射为&lt;URL，&lt;PRinit，url_list&gt;&gt;后进行输出，其中，PRinit是该URL页面对应的PageRank初始值，url_list包含了该URL页面中的外链所指向的所有URL。Reduce任务只是恒等函数，输入和输出相同。</span><br><span class="line">• 对右图，每个网页的初始PageRank值为1&#x2F;4。它在该阶段中:</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082138941.png" alt="image-20230308213807879" style="zoom:67%;"><h4 id="阶段2：PageRank分配"><a href="#阶段2：PageRank分配" class="headerlink" title="阶段2：PageRank分配"></a>阶段2：PageRank分配</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• 该阶段的任务就是多次迭代计算页面的PageRank值。</span><br><span class="line">• 在该阶段中，Map任务的输入是&lt;URL，&lt;cur_rank，url_list&gt;&gt;，其中，cur_rank是该</span><br><span class="line">URL页面对应的PageRank当前值，url_list包含了该URL页面中的外链所指向的所有</span><br><span class="line">URL。</span><br><span class="line">• 对于url_list中的每个元素u，Map任务输出&lt;u，&lt;URL, cur_rank&#x2F;|url_list|&gt;&gt;（其中，|url_list|表示外链的个数），并输出链接关系&lt;URL，url_list&gt;。</span><br><span class="line">• 每个页面的PageRank当前值被平均分配给了它们的每个外链。Map任务的输出会作为下面Reduce任务的输入。对下图第一次迭代Map任务的输入输出如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303082143692.png" alt="image-20230308214310636"></p><h4 id="阶段3：收敛阶段"><a href="#阶段3：收敛阶段" class="headerlink" title="阶段3：收敛阶段"></a>阶段3：收敛阶段</h4><h3 id="PageRank算法在Pregel和MapReduce中实现的比较"><a href="#PageRank算法在Pregel和MapReduce中实现的比较" class="headerlink" title="PageRank算法在Pregel和MapReduce中实现的比较"></a>PageRank算法在Pregel和MapReduce中实现的比较</h3><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
  <entry>
    <title>林子雨 大数据技术原理与应用-第8章 流计算</title>
    <link href="http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC8%E7%AB%A0-%E6%B5%81%E8%AE%A1%E7%AE%97.html"/>
    <id>http://tianyong.fun/%E6%9E%97%E5%AD%90%E9%9B%A8-%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-%E7%AC%AC8%E7%AB%A0-%E6%B5%81%E8%AE%A1%E7%AE%97.html</id>
    <published>2023-02-28T14:32:22.000Z</published>
    <updated>2023-03-08T12:14:57.695Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第8章-流计算"><a href="#第8章-流计算" class="headerlink" title="第8章 流计算"></a>第8章 流计算</h1><h2 id="流计算概述"><a href="#流计算概述" class="headerlink" title="流计算概述"></a>流计算概述</h2><h3 id="静态数据和流数据"><a href="#静态数据和流数据" class="headerlink" title="静态数据和流数据"></a>静态数据和流数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">很多企业为了支持决策分析而构建的数据仓库系统，其中存放的大量历史数据就是静态数据。技术人员可以利用数据挖掘和OLAP（OnLine Analytical Processing）分析工具从静态数据中找到对企业有价值的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081458289.png" alt="image-20230308145838198"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 近年来，在Web应用、网络监控、传感监测等领域，兴起了一种新的数据密集型应用——流数据，即数据以大量、快速、时变的流形式持续到达</span><br><span class="line">• 流数据具有如下特征：</span><br><span class="line">– 数据快速持续到达，潜在大小也许是无穷无尽的</span><br><span class="line">– 数据来源众多，格式复杂</span><br><span class="line">– 数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储</span><br><span class="line">– 注重数据的整体价值，不过分关注个别数据</span><br><span class="line">– 数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序</span><br></pre></td></tr></table></figure><h3 id="批量计算和实时计算"><a href="#批量计算和实时计算" class="headerlink" title="批量计算和实时计算"></a>批量计算和实时计算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 对静态数据和流数据的处理，对应着两种截然不同的计算模式：批量计算和实时计算</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081503278.png" alt="image-20230308150351231"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 批量计算以“静态数据”为对象，可在充裕的时间内对海量数据进行批量处理，计算得到有价值的信息。Hadoop是典型的批处理模型，由HDFS和HBase存放大量的静态数据，由MapReduce负责对海量数据执行批量计算</span><br><span class="line">• 流数据须采用实时计算。实时计算最重要的一个需求是能够实时得到计算结果，一般要求响应时间为秒级。当只需要处理少量数据时，实时计算并不是问题；但是，在大数据时代，数据格式复杂、来源众多、数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计算——流计算，应运而生</span><br></pre></td></tr></table></figure><h3 id="流计算概念"><a href="#流计算概念" class="headerlink" title="流计算概念"></a>流计算概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 流计算：实时获取来自不同数据源的海量数据，经过实时分析处理，获得有价值的信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081512277.png" alt="image-20230308151223180"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">• 流计算秉承一个基本理念，即数据的价值随着时间的流逝而降低。因此，当事件出现时就应该立即进行处理，而不是缓存起来进行批量处理。为了及时处理流数据，就需要一个低延迟、可扩展、高可靠的处理引擎</span><br><span class="line">• 对于一个流计算系统来说，它应达到如下需求：</span><br><span class="line">– 高性能：处理大数据的基本要求，如每秒处理几十万条数据</span><br><span class="line">– 海量式：支持TB级甚至是PB级的数据规模</span><br><span class="line">– 实时性：保证较低的延迟时间，达到秒级别，甚至是毫秒级别</span><br><span class="line">– 分布式：支持大数据的基本架构，必须能够平滑扩展</span><br><span class="line">– 易用性：能够快速进行开发和部署</span><br><span class="line">– 可靠性：能可靠地处理流数据</span><br></pre></td></tr></table></figure><h3 id="流计算与Hadoop"><a href="#流计算与Hadoop" class="headerlink" title="流计算与Hadoop"></a>流计算与Hadoop</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Hadoop设计的初衷是面向大规模数据的批量处理，每台机器并行运行MapReduce任务，最后对结果进行汇总输出</span><br><span class="line">• MapReduce是专门面向静态数据的批量处理的，内部各种实现机制都为批处理做了高度优化，不适合用于处理持续到达的动态数据</span><br><span class="line">• 我们可能会想到一种“变通”的方案来降低批处理的时间延迟——将基于MapReduce的批量处理转为小批量处理，将输入数据切成小的片段，每隔一个周期就启动一次MapReduce作业。但这种方式也无法有效处理流数据</span><br></pre></td></tr></table></figure><h3 id="流计算框架"><a href="#流计算框架" class="headerlink" title="流计算框架"></a>流计算框架</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 当前业界诞生了许多专门的流数据实时计算系统来满足各自需求</span><br><span class="line">• 目前有三类常见的流计算框架和平台：商业级的流计算平台、开源流计算框架、公司为支持自身业务开发的流计算框架</span><br><span class="line">• 较为常见的是开源流计算框架，代表如下：</span><br><span class="line">– Twitter Storm：免费、开源的分布式实时计算系统，可简单、高效、可靠地处理大量的流数据</span><br><span class="line">– Yahoo! S4（Simple Scalable Streaming System）：开源流计算平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的流式系统</span><br></pre></td></tr></table></figure><h2 id="流计算处理流程"><a href="#流计算处理流程" class="headerlink" title="流计算处理流程"></a>流计算处理流程</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 传统的数据处理流程，需要先采集数据并存储在关系数据库等数据管理系统中，之后由用户通过查询操作和数据管理系统进行交互</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081522466.png" alt="image-20230308152245421"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 传统的数据处理流程隐含了两个前提：</span><br><span class="line">– 存储的数据是旧的。存储的静态数据是过去某一时刻的快照，这些数据在查询时可能已不具备时效性了</span><br><span class="line">– 需要用户主动发出查询来获取结果</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 流计算的处理流程一般包含三个阶段：数据实时采集、数据实时计算、实时查询服务</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081524733.png" alt="image-20230308152430685"></p><h3 id="数据实时采集"><a href="#数据实时采集" class="headerlink" title="数据实时采集"></a>数据实时采集</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 数据实时采集阶段通常采集多个数据源的海量数据，需要保证实时性、低延迟与稳定可靠</span><br><span class="line">• 以日志数据为例，由于分布式集群的广泛应用，数据分散存储在不同的机器上，因此需要实时汇总来自不同机器上的日志数据</span><br><span class="line">• 目前有许多互联网公司发布的开源分布式日志采集系统均可满足每秒数百MB的数据采集和传输需求，如：</span><br><span class="line">– Facebook的Scribe</span><br><span class="line">– LinkedIn的Kafka</span><br><span class="line">– 淘宝的Time Tunnel</span><br><span class="line">– 基于Hadoop的Chukwa和Flume</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• 数据采集系统的基本架构一般有以下三个部分：</span><br><span class="line">– Agent：主动采集数据，并把数据推送到Collector部分</span><br><span class="line">– Collector：接收多个Agent的数据，并实现有序、可靠、高性能</span><br><span class="line">的转发</span><br><span class="line">– Store：存储Collector转发过来的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081541896.png" alt="image-20230308154137847"></p><h3 id="数据实时计算"><a href="#数据实时计算" class="headerlink" title="数据实时计算"></a>数据实时计算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 数据实时计算阶段对采集的数据进行实时的分析和计算，并反馈实时结果</span><br><span class="line">• 经流处理系统处理后的数据，可视情况进行存储，以便之后再进行分析计算。在时效性要求较高的场景中，处理之后的数据也可以直接丢弃</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081549997.png" alt="image-20230308154930940" style="zoom:80%;"><h3 id="实时查询服务"><a href="#实时查询服务" class="headerlink" title="实时查询服务"></a>实时查询服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存</span><br><span class="line">• 传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户</span><br><span class="line">• 虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存</span><br><span class="line">• 传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户</span><br><span class="line">• 虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">• 可见，流处理系统与传统的数据处理系统有如下不同：</span><br><span class="line">– 流处理系统处理的是实时的数据，而传统的数据处理系统处理的是预先存储好的静态数据</span><br><span class="line">– 用户通过流处理系统获取的是实时结果，而通过传统的数据处理系统，获取的是过去某一时刻的结果</span><br><span class="line">– 流处理系统无需用户主动发出查询，实时查询服务可以主动将实时结果推送给用户</span><br></pre></td></tr></table></figure><h2 id="流计算的应用"><a href="#流计算的应用" class="headerlink" title="流计算的应用"></a>流计算的应用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 流计算是针对流数据的实时计算，可以应用在多种场景中，如Web服务、机器翻译、广告投放、自然语言处理、气候模拟预测等</span><br><span class="line">• 如百度、淘宝等大型网站中，每天都会产生大量流数据，包括用户的搜索内容、用户的浏览记录等数据。采用流计算进行实时数据分析，可以了解每个时刻的流量变化情况，甚至可以分析用户的实时浏览轨迹，从而进行实时个性化内容推荐</span><br><span class="line">• 但是，并不是每个应用场景都需要用到流计算的。流计算适合于需要处理持续到达的流数据、对数据处理有较高实时性要求的场景</span><br></pre></td></tr></table></figure><h3 id="应用场景1-实时分析"><a href="#应用场景1-实时分析" class="headerlink" title="应用场景1: 实时分析"></a>应用场景1: 实时分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 传统的业务分析一般采用分布式离线计算的方式，即将数据全部保存起来，然后每隔一定的时间进行离线分析来得到结果。但这样会导致一定的延时，难以保证结果的实时性</span><br><span class="line">• 如淘宝网“双十一”、“双十二”的促销活动，商家需要根据广告效果来即使调整广告，这就需要对广告的受访情况进行分析。但以往采用分布式离线分析，需要几小时甚至一天的延时才能得到分析结果。而促销活动只持续一天，因此，隔天才能得到的分析结果便失去了价值</span><br><span class="line">• 虽然分布式离线分析带来的小时级的分析延时可以满足大部分商家的需求，但随着实时性要求越来越高，如何实现秒级别的实时分析响应成为业务分析的一大挑战</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 针对流数据，“量子恒道”开发了海量数据实时流计算框架Super Mario。通过该框架，量子恒道可处理每天TB级的实时流数据，并且从用户发出请求到数据展示，整个延时控制在2-3秒内，达到了实时性的要求</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081604482.png" alt="image-20230308160445437" style="zoom:67%;"><h3 id="应用场景2-实时交通"><a href="#应用场景2-实时交通" class="headerlink" title="应用场景2: 实时交通"></a>应用场景2: 实时交通</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 流计算不仅为互联网带来改变，也能改变我们的生活</span><br><span class="line">• 如提供导航路线，一般的导航路线并没有考虑实时的交通状况，即便在计算路线时有考虑交通状况，往往也只是使用了以往的交通状况数据。要达到根据实时交通状态进行导航的效果，就需要获取海量的实时交通数据并进行实时分析</span><br><span class="line">• 借助于流计算的实时特性，不仅可以根据交通情况制定路线，而且在行驶过程中，也可以根据交通情况的变化实时更新路线，始终为用户提供最佳的行驶路线</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• IBM的流计算平台InfoSphere Streams，广泛应用于制造、零售、交通运输、金融证券以及监管各行各业的解决方案之中，使得实时快速做出决策的理念得以实现</span><br><span class="line">• 以上述的实时交通为例，InfoSphere Streams应用于斯德哥尔摩的交通信息管理，通过结合来自不同源的实时数据，可以生成动态的、多方位的观察交通流量的方式，为城市规划者和乘客提供实时交通状况查询</span><br></pre></td></tr></table></figure><h2 id="流计算开源框架-–-Storm"><a href="#流计算开源框架-–-Storm" class="headerlink" title="流计算开源框架 – Storm"></a>流计算开源框架 – Storm</h2><h3 id="Storm简介"><a href="#Storm简介" class="headerlink" title="Storm简介"></a>Storm简介</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Twitter Storm是一个免费、开源的分布式实时计算系统，Storm对于实时计算的意义类似于Hadoop对于批处理的意义，Storm可以简单、高效、可靠地处理流数据，并支持多种编程语言</span><br><span class="line">• Storm框架可以方便地与数据库系统进行整合，从而开发出强大的实时计算系统</span><br><span class="line">• Twitter是全球访问量最大的社交网站之一，Twitter开发Storm流处理框架也是为了应对其不断增长的流数据实时处理需求</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• Twitter采用了由实时系统和批处理系统组成的分层数据处理架构，一方面由Hadoop和ElephantDB组成批处理系统，另一方面由Storm和Cassandra组成实时系统</span><br><span class="line">• 在计算查询时，该系统会同时查询批处理视图和实时视图，并把它们合并起来以得到最终的结果</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081613772.png" alt="image-20230308161306725" style="zoom:67%;"><h3 id="Storm的特点"><a href="#Storm的特点" class="headerlink" title="Storm的特点"></a>Storm的特点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">• Storm可用于许多领域中，如实时分析、在线机器学习、持续计算、远程RPC、数据提取加载转换等</span><br><span class="line">• Storm具有以下主要特点：</span><br><span class="line">– 整合性：Storm可方便地与队列系统和数据库系统进行整合</span><br><span class="line">– 简易的API：Storm的API在使用上即简单又方便</span><br><span class="line">– 可扩展性：Storm的并行特性使其可以运行在分布式集群中</span><br><span class="line">– 容错性：Storm可自动进行故障节点的重启、任务的重新分配</span><br><span class="line">– 可靠的消息处理：Storm保证每个消息都能完整处理</span><br><span class="line">– 支持各种编程语言：Storm支持使用各种编程语言来定义任务</span><br><span class="line">– 快速部署：Storm可以快速进行部署和使用</span><br><span class="line">– 免费、开源：Storm是一款开源框架，可以免费使用</span><br></pre></td></tr></table></figure><h3 id="Storm设计思想"><a href="#Storm设计思想" class="headerlink" title="Storm设计思想"></a>Storm设计思想</h3><h4 id="Streams"><a href="#Streams" class="headerlink" title="Streams"></a>Streams</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 要了解Storm，首先需要了解Storm的设计思想。Storm对一些设计思想进行了抽象化，其主要术语包括Streams、Spouts、Bolts、Topology和Stream Groupings</span><br><span class="line">• Streams：Storm将流数据Stream描述成一个无限的Tuple序列，这些Tuple序列会以分布式的方式并行地创建和处理</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081637226.png" alt="image-20230308163744184" style="zoom:67%;"><h4 id="Spouts"><a href="#Spouts" class="headerlink" title="Spouts"></a>Spouts</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Spouts：Storm认为每个Stream都有一个源头，并把这个源头抽象为Spouts。Spouts会从外部读取流数据并持续发出Tuple</span><br></pre></td></tr></table></figure><h4 id="Bolts"><a href="#Bolts" class="headerlink" title="Bolts"></a>Bolts</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Bolts：Storm将Streams的状态转换过程抽象为Bolts。Bolts即可以处理Tuple，也可以将处理后的Tuple作为新的Streams发送给其他Bolts。对Tuple的处理逻辑都被封装在Bolts中，可执行过滤、聚合、查询等操作</span><br></pre></td></tr></table></figure><h4 id="Topology"><a href="#Topology" class="headerlink" title="Topology"></a>Topology</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Topology：Storm将Spouts和Bolts组成的网络抽象成Topology，它可以被提交到Storm集群执行。Topology可视为流转换图，图中节点是一个Spout或Bolt，边则表示Bolt订阅了哪个Stream。当Spout或者Bolt发送元组时，它会把元组发送到每个订阅了该Stream的Bolt上进行处理</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081643689.png" alt="image-20230308164303642" style="zoom:67%;"><h4 id="Stream-Groupings"><a href="#Stream-Groupings" class="headerlink" title="Stream Groupings"></a>Stream Groupings</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Stream Groupings：Storm中的Stream Groupings用于告知Topology如何在两个组件间（如Spout和Bolt之间，或者不同的Bolt之间）进行Tuple的传送。每一个Spout和Bolt都可以有多个分布式任务，一个任务在什么时候、以什么方式发送Tuple就是由Stream Groupings来决定的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081645742.png" alt="image-20230308164524685"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">• 目前，Storm中的Stream Groupings有如下几种方式：</span><br><span class="line">– Shuffle Grouping：随机分组，随机分发Tuple</span><br><span class="line">– Fields Grouping：按字段分组，具有相同值的Tuple会被分发到对应的Bolt</span><br><span class="line">– All Grouping：广播分发，每个Tuple都会被分发到所有Bolt中</span><br><span class="line">– Global Grouping：全局分组，Tuple只会分发给一个Bolt</span><br><span class="line">– Non Grouping：不分组，与随机分组效果类似</span><br><span class="line">– Direct Grouping：直接分组，由Tuple的生产者来定义接收者</span><br></pre></td></tr></table></figure><h3 id="Storm框架设计"><a href="#Storm框架设计" class="headerlink" title="Storm框架设计"></a>Storm框架设计</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">• Storm运行任务的方式与Hadoop类似：Hadoop运行的是MapReduce作业，而Storm运行的是“Topology”</span><br><span class="line">• 但两者的任务大不相同，主要的不同是：MapReduce作业最终会完成计算并结束运行，而Topology将持续处理消息（直到人为终止）</span><br><span class="line">• Storm集群采用“Master—Worker”的节点方式：</span><br><span class="line">– Master节点运行名为“Nimbus”的后台程序（类似Hadoop中的“JobTracker”），负责在集群范围内分发代码、为Worker分配任务和监测故障</span><br><span class="line">– Worker节点运行名为“Supervisor”的后台程序，负责监听分配给它所在机器的工作，即根据Nimbus分配的任务来决定启动或停止Worker进程</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Storm使用Zookeeper来作为分布式协调组件，负责Nimbus和多个Supervisor之间的所有协调工作。借助于Zookeeper，若Nimbus进程或Supervisor进程意外终止，重启时也能读取、恢复之前的状态并继续工作，使得Storm极其稳定</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081656820.png" alt="image-20230308165644767" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 基于这样的架构设计，Storm的工作流程如下图所示：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081657604.png" alt="image-20230308165745550"></p><h3 id="Storm实例"><a href="#Storm实例" class="headerlink" title="Storm实例"></a>Storm实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 我们以单词统计的实例来加深对Storm的认识</span><br><span class="line">• Storm的编程模型非常简单，如下代码即定义了整个单词统计</span><br><span class="line">Topology的整体逻辑</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081658383.png" alt="image-20230308165849326"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• Topology中仅定义了整体的计算逻辑，还需要定义具体的处理函数</span><br><span class="line">。具体的处理函数可以使用任一编程语言来定义，甚至也可以结合多种编程语言来实现</span><br><span class="line">• 如SplitSentence()方法虽然是通过Java语言定义的，但具体的操作可通过Python脚本来完成</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081700391.png" alt="image-20230308170042334"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Python脚本splitsentence.py定义了一个简单的单词分割方法，即通过空格来分割单词。分割后的单词通过emit()方法以Tuple的形式发送给订阅了该Stream的Bolt进行接收和处理</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081701390.png" alt="image-20230308170113339"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 单词统计的具体逻辑：首先判断单词是否统计过，若未统计过，需先将count值置为0。若单词已统计过，则每出现一次该单词，count值就加1</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081701542.png" alt="image-20230308170135478"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• 基于Storm的单词统计在形式上与基于MapReduce的单词统计是类似的，MapReduce使用的是Map和Reduce的抽象，而Storm使用的是Soput和Bolt的抽象</span><br><span class="line">• 总结一下Storm进行单词统计的整个流程：</span><br><span class="line">– 从Spout中发送Stream（每个英文句子为一个Tuple）</span><br><span class="line">– 用于分割单词的Bolt将接收的句子分解为独立的单词，将单词作为Tuple的字段名发送出去</span><br><span class="line">– 用于计数的Bolt接收表示单词的Tuple，并对其进行统计</span><br><span class="line">– 输出每个单词以及单词出现过的次数</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081702343.png" alt="image-20230308170211283" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• 上述虽然是一个简单的单词统计，但对其进行扩展，便可应用到许多场景中，如微博中的实时热门话题。Twitter也正是使用了Storm框架实现了实时热门话题</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081702971.png" alt="image-20230308170245909"></p><h3 id="哪些公司在使用Storm"><a href="#哪些公司在使用Storm" class="headerlink" title="哪些公司在使用Storm"></a>哪些公司在使用Storm</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">• Storm自2011年发布以来，凭借其优良的框架设计及开源特性，在流计算领域获得了广泛认可，已应用到许多大型互联网公司的实际项目中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081710751.png" alt="image-20230308171025682"></p><h2 id="Storm安装和运行实例"><a href="#Storm安装和运行实例" class="headerlink" title="Storm安装和运行实例"></a>Storm安装和运行实例</h2><p><a href="http://dblab.xmu.edu.cn/blog/install-storm/" target="_blank" rel="external nofollow noopener noreferrer">Storm安装教程_CentOS6.4/Storm0.9.6</a></p><h3 id="安装Storm的基本过程"><a href="#安装Storm的基本过程" class="headerlink" title="安装Storm的基本过程"></a>安装Storm的基本过程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">本实例中Storm具体运行环境如下：</span><br><span class="line">•CentOS 6.4</span><br><span class="line">•Storm 0.9.6</span><br><span class="line">•Java JDK 1.7</span><br><span class="line">•ZooKeeper 3.4.6</span><br><span class="line">•Python 2.6</span><br><span class="line">备注：CentOS中已默认安装了Python 2.6，我们还需要安装 JDK 环境以及分布式应用程序协调服务 Zookeeper</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">安装Storm的基本过程如下：</span><br><span class="line">•第一步：安装Java环境</span><br><span class="line">•第二步：安装 Zookeeper</span><br><span class="line">•第三步：安装Storm（单机）</span><br><span class="line">•第四步：关闭Storm</span><br></pre></td></tr></table></figure><h4 id="第一步：安装Java环境"><a href="#第一步：安装Java环境" class="headerlink" title="第一步：安装Java环境"></a>第一步：安装Java环境</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">•Storm 运行需要 Java 环境，可选择 Oracle 的 JDK，或是 OpenJDK</span><br><span class="line">，现在一般 Linux 系统默认安装的基本是 OpenJDK，如 CentOS 6.4 </span><br><span class="line">就默认安装了 OpenJDK 1.7。但需要注意的是，CentOS 6.4 中默认安</span><br><span class="line">装的只是 Java JRE，而不是 JDK，为了开发方便，我们还是需要通过</span><br><span class="line">yum 进行安装 JDK</span><br><span class="line"></span><br><span class="line">$ sudo yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel</span><br><span class="line"></span><br><span class="line">•接着需要配置一下 JAVA_HOME 环境变量，为方便，可以在 ~&#x2F;.bashrc</span><br><span class="line">中进行设置</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081716201.png" alt="image-20230308171605152"></p><h4 id="第二步：安装Zookeeper"><a href="#第二步：安装Zookeeper" class="headerlink" title="第二步：安装Zookeeper"></a>第二步：安装Zookeeper</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到官网下载Zookeeper，比如下载 ―zookeeper-3.4.6.tar.gz‖</span><br><span class="line">下载后执行如下命令进行安装 zookeeper（将命令中 3.4.6 改为你下载的版本）：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081716354.png" alt="image-20230308171657313"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown命令让hadoop用户拥有zookeeper目录下的所有文件的权限</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接着执行如下命令进行zookeeper配置：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081719215.png" alt="image-20230308171903171"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将当中的 dataDir&#x3D;&#x2F;tmp&#x2F;zookeeper 更改为</span><br><span class="line">dataDir&#x3D;&#x2F;usr&#x2F;local&#x2F;zookeeper&#x2F;tmp 。接着执行：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081719069.png" alt="image-20230308171920028"></p><h4 id="第三步：安装Storm（单机）"><a href="#第三步：安装Storm（单机）" class="headerlink" title="第三步：安装Storm（单机）"></a>第三步：安装Storm（单机）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到官网下载Storm，比如Storm0.9.6</span><br><span class="line">下载后执行如下命令进行安装Storm：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081720726.png" alt="image-20230308172015682"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接着执行如下命令进行Storm配置:</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081721931.png" alt="image-20230308172103894"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">修改其中的 storm.zookeeper.servers 和 nimbus.host 两个配置项，即取消掉</span><br><span class="line">注释且都修改值为 127.0.0.1（我们只需要在单机上运行），如下图所示。</span><br></pre></td></tr></table></figure><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081721615.png" alt="image-20230308172131564" style="zoom:67%;"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">然后就可以启动 Storm 了。执行如下命令启动 nimbus 后台进程：</span><br><span class="line">$ .&#x2F;bin&#x2F;storm nimbus</span><br><span class="line"></span><br><span class="line">启动 nimbus 后，终端被该进程占用了，不能再继续执行其他命令了。因此</span><br><span class="line">我们需要另外开启一个终端，然后执行如下命令启动 supervisor 后台进程：</span><br><span class="line">$ # 需要另外开启一个终端</span><br><span class="line">$ &#x2F;usr&#x2F;local&#x2F;storm&#x2F;bin&#x2F;storm supervisor</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">同样的，启动 supervisor 后，我们还需要开启另外的终端才能执行其他命令</span><br><span class="line">。另外，我们可以使用 jps 命令 检查是否成功启动，若成功启动会显示</span><br><span class="line">nimbus、supervisor、QuorumPeeMain （QuorumPeeMain 是 zookeeper </span><br><span class="line">的后台进程，若显示 config_value 表明 nimbus 或 supervisor 还在启动中）</span><br><span class="line">，如下图所示。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081723654.png" alt="image-20230308172323611"></p><h4 id="第四步：关闭Storm"><a href="#第四步：关闭Storm" class="headerlink" title="第四步：关闭Storm"></a>第四步：关闭Storm</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">之前启动的 nimbus 和 supervisor 占用了两个终端窗口，切换到这两个终</span><br><span class="line">端窗口，按键盘的 Ctrl+C 可以终止进程，终止后，也就相当于关闭了Storm。</span><br></pre></td></tr></table></figure><h3 id="运行Storm实例"><a href="#运行Storm实例" class="headerlink" title="运行Storm实例"></a>运行Storm实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Storm中自带了一些例子，我们可以执行一下 WordCount 例子来感受一</span><br><span class="line">下 Storm 的执行流程。执行如下命令：</span><br><span class="line">$ &#x2F;usr&#x2F;local&#x2F;storm&#x2F;bin&#x2F;storm jar &#x2F;usr&#x2F;local&#x2F;storm&#x2F;examples&#x2F;stormstarter&#x2F;storm-starter-topologies-0.9.6.jar </span><br><span class="line">storm.starter.WordCountTopology</span><br><span class="line"></span><br><span class="line">该程序是不断地取如下四句英文句子中的一句作为数据源，然后发送给</span><br><span class="line">bolt 来统计单词出现的次数。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303081724098.png" alt="image-20230308172457055"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据技术原理与应用" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
      <category term="林子雨" scheme="http://tianyong.fun/tags/%E6%9E%97%E5%AD%90%E9%9B%A8/"/>
    
  </entry>
  
</feed>
