<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2022-02-21T15:57:06.154Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>maven相关</title>
    <link href="http://tianyong.fun/maven%E7%9B%B8%E5%85%B3.html"/>
    <id>http://tianyong.fun/maven%E7%9B%B8%E5%85%B3.html</id>
    <published>2022-02-21T15:35:12.000Z</published>
    <updated>2022-02-21T15:57:06.154Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="maven相关"><a href="#maven相关" class="headerlink" title="maven相关"></a>maven相关</h1><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.打jar包</span><br><span class="line">在项目根目录下</span><br><span class="line">mvn clean package -DskipTests</span><br><span class="line"></span><br><span class="line">2.为项目下载pom上配置的依赖</span><br><span class="line">在项目根目录下(用cmd或IDEA)</span><br><span class="line">mvn clean compile</span><br><span class="line"></span><br><span class="line">下载后，在项目上右键-&gt;maven-&gt;reload project</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="maven" scheme="http://tianyong.fun/categories/maven/"/>
    
    
  </entry>
  
  <entry>
    <title>mysql安装</title>
    <link href="http://tianyong.fun/mysql%E5%AE%89%E8%A3%85.html"/>
    <id>http://tianyong.fun/mysql%E5%AE%89%E8%A3%85.html</id>
    <published>2022-02-20T10:02:24.000Z</published>
    <updated>2022-02-20T14:59:21.445Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="mysql安装"><a href="#mysql安装" class="headerlink" title="mysql安装"></a>mysql安装</h1><p><a href="https://www.cnblogs.com/ivy-zheng/p/11088644.html" target="_blank" rel="external nofollow noopener noreferrer">url1</a></p><p><a href="https://blog.csdn.net/t15263857960/article/details/83590484" target="_blank" rel="external nofollow noopener noreferrer">url2</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.安装包是目免安装型的</span><br><span class="line">2.在系统环境变量里添加mysql的路径到xxx\bin(也可以用管理员权限打开的cmd，切到这个路径下，再执行命令)</span><br><span class="line">3.用管理员权限打开cmd，输入</span><br><span class="line">mysqld --initialize --console  (注意一定要看之前是否安装过mysql,在环境变量里，一眼就可以看出，不然总是报各种错误; 加上console可以看到初始化后为root创建的临时密码)</span><br><span class="line">4.mysqld -install 将mysql服务安装到win服务</span><br><span class="line">5.net start mysql (启动服务)</span><br><span class="line">6.mysql -uroot -p 再输入之前生成的临时密码</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 如果安装时已有mysql服务，删除：sc delete mysql</span><br></pre></td></tr></table></figure><p><a href="https://www.jianshu.com/p/b70a2cb5d4be" target="_blank" rel="external nofollow noopener noreferrer">url</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改临时密码</span><br><span class="line">ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39;; &#x2F;&#x2F;记得修改自己的账户</span><br><span class="line">flush privileges; &#x2F;&#x2F;修改成功后刷新权限</span><br><span class="line">quit; &#x2F;&#x2F;最后退出</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第6章 Hive技巧与核心复盘</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC6%E7%AB%A0-Hive%E6%8A%80%E5%B7%A7%E4%B8%8E%E6%A0%B8%E5%BF%83%E5%A4%8D%E7%9B%98.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC6%E7%AB%A0-Hive%E6%8A%80%E5%B7%A7%E4%B8%8E%E6%A0%B8%E5%BF%83%E5%A4%8D%E7%9B%98.html</id>
    <published>2022-02-20T04:02:41.000Z</published>
    <updated>2022-02-21T08:37:46.068Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第5章 Hive高级函数实战</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Hive%E9%AB%98%E7%BA%A7%E5%87%BD%E6%95%B0%E5%AE%9E%E6%88%98.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC5%E7%AB%A0-Hive%E9%AB%98%E7%BA%A7%E5%87%BD%E6%95%B0%E5%AE%9E%E6%88%98.html</id>
    <published>2022-02-20T04:02:11.000Z</published>
    <updated>2022-02-21T08:37:42.819Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第4章 Hive核心实战</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98.html</id>
    <published>2022-02-20T04:01:53.000Z</published>
    <updated>2022-02-22T12:45:48.826Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第4章-Hive核心实战"><a href="#第八周-第4章-Hive核心实战" class="headerlink" title="第八周 第4章 Hive核心实战"></a>第八周 第4章 Hive核心实战</h1><a href="/hadoop%E5%AE%9E%E9%AA%8C%E8%AF%BE-hive.html" title="hadoop实验课-hive">hadoop实验课-hive</a><h2 id="Hive中数据库的操作"><a href="#Hive中数据库的操作" class="headerlink" title="Hive中数据库的操作"></a>Hive中数据库的操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">show databases;</span><br><span class="line">use xxx;</span><br><span class="line">create database xxx;</span><br><span class="line"></span><br><span class="line">drop database xxxx;</span><br></pre></td></tr></table></figure><h2 id="Hive中表的操作"><a href="#Hive中表的操作" class="headerlink" title="Hive中表的操作"></a>Hive中表的操作</h2><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table xxx(id int);</span><br></pre></td></tr></table></figure><h3 id="查看创建的表"><a href="#查看创建的表" class="headerlink" title="查看创建的表"></a>查看创建的表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">查看表信息</span><br><span class="line">show tables;###</span><br></pre></td></tr></table></figure><h3 id="查看表结构信息"><a href="#查看表结构信息" class="headerlink" title="查看表结构信息"></a>查看表结构信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc xxx;</span><br></pre></td></tr></table></figure><h3 id="查看表的创建信息"><a href="#查看表的创建信息" class="headerlink" title="查看表的创建信息"></a>查看表的创建信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">show create table xxx;</span><br><span class="line">从这里的location可以看到这个表在hdfs上的位置。</span><br><span class="line">注意了：表中的数据是存储在hdfs中的，但是表的名称、字段信息是存储在metastore中的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">到metastore中看一下</span><br><span class="line">先看tbls表，这个里面中存储的都是在hive中创建的表</span><br><span class="line">可以看到DB_ID 为1</span><br><span class="line">可以到dbs表中看到默认default数据库的id就是1。</span><br><span class="line">TBL_NAME 是这个表的名称。</span><br><span class="line"></span><br><span class="line">在表COLUMNS_V2中存储的是Hive表的字段信息(包含字段注释、字段名称、字段类型、字段顺序)</span><br><span class="line">其中的CD_ID和tbls中的TBL_ID相等</span><br></pre></td></tr></table></figure><h3 id="修改表名"><a href="#修改表名" class="headerlink" title="修改表名"></a>修改表名</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table t2 rename to t2_bak;</span><br><span class="line"></span><br><span class="line">hdfs中对应的目录名称也同步变化了</span><br></pre></td></tr></table></figure><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">咱们前面向表中添加数据是使用的insert命令，其实使用insert向表里面添加数据只是在测试的时候使用，实际中向表里面添加数据很少使用insert命令的</span><br><span class="line">具体原因我们后面再分析，在这大家先带着这个问题。</span><br><span class="line">insert into test2(id, name) values(1,&quot;zhangsan&quot;);</span><br><span class="line"></span><br><span class="line">向表中加载数据可以使用load命令</span><br><span class="line">以t2_bak为例，在bigdata04机器的 &#x2F;data&#x2F;soft&#x2F;hivedata 下有一个 t2.data 文件，将其加载到 t2_bak表中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 hivedata]# pwd</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;hivedata</span><br><span class="line">[root@bigdata04 hivedata]# more t2.data</span><br><span class="line">1 2 3 4 5</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t2.data&#39; into table t2_bak</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from t2_bak;</span><br><span class="line">OK</span><br><span class="line">t2_bak.id</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们到hdfs上去看一下这个表，发现刚才的文件其实就是上传到了t2_bak目录中</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HzRNWV" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/22/HzRNWV.md.png" alt="HzRNWV.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">那我们自己手工通过put命令把数据上传到t2_bak目录中可以吗？</span><br><span class="line">可以的！</span><br><span class="line">hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t2_bak.data &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;t2_bak</span><br><span class="line"></span><br><span class="line">到hdfs上确认一下，可以看到刚才上传的文件</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HzWY0H" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/22/HzWY0H.md.png" alt="HzWY0H.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">再查询一下这个表的数据，可以发现数据多了一份，说明刚才使用hdfs的put命令上传的是可以的。</span><br><span class="line">hive (default)&gt; select * from t2_bak;</span><br><span class="line">OK</span><br><span class="line">t2_bak.id</span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5 </span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5</span><br></pre></td></tr></table></figure><h3 id="表增加字段及注释、删除表"><a href="#表增加字段及注释、删除表" class="headerlink" title="表增加字段及注释、删除表"></a>表增加字段及注释、删除表</h3><h4 id="增加字段"><a href="#增加字段" class="headerlink" title="增加字段"></a>增加字段</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在工作中会有给已存在的表增加字段的需求，需要使用alter命令</span><br><span class="line">在这里我们给t2_bak表增加一个name字段，重新查看表结构信息，再查询一下这个表中的数据，结果发现，第二列为null，这是正常的，因为我们的数据数据文件中就只有一列，第二列查询不到，就显示为null，不会报错，这一点要注意一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table t2_bak add columns (name string);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.175 seconds</span><br><span class="line">hive (default)&gt; desc t2_bak;</span><br><span class="line">OK</span><br><span class="line">col_name data_type comment</span><br><span class="line">id int</span><br><span class="line">name string</span><br><span class="line">Time taken: 0.121 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; select * from t2_bak;</span><br><span class="line">OK</span><br><span class="line">t2_bak.id t2_bak.name</span><br><span class="line">1 NULL</span><br><span class="line">2 NULL</span><br><span class="line">3 NULL</span><br><span class="line">4 NULL</span><br><span class="line">5 NULL</span><br><span class="line">1 NULL</span><br><span class="line">2 NULL</span><br><span class="line">3 NULL</span><br><span class="line">4 NULL</span><br><span class="line">5 NULL</span><br><span class="line">Time taken: 0.199 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure><h4 id="增加注释"><a href="#增加注释" class="headerlink" title="增加注释"></a>增加注释</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">现在我们通过desc查询表中的字段信息发现都没有注释，所以想要给字段加一些注释，以及表本身也可以增加注释，都是使用comment关键字</span><br><span class="line">重新创建一个表t2</span><br><span class="line">注意：在建表语句中，缩进不要使用tab制表符，否则拷贝到hive命令行下执行会提示语句错误，这里的缩进需要使用空格</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create table t2(</span><br><span class="line">age int comment &#39;年龄&#39;</span><br><span class="line">) comment &#39;测试&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">查看这个表的信息，结果发现我们添加的中文注释都是乱码</span><br><span class="line"></span><br><span class="line">hive (default)&gt; desc t2;</span><br><span class="line">OK</span><br><span class="line">col_name data_type comment</span><br><span class="line">age int ??</span><br><span class="line">Time taken: 0.086 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; show create table t2;</span><br><span class="line">OK</span><br><span class="line">createtab_stmt</span><br><span class="line">CREATE TABLE &#96;t2&#96;(</span><br><span class="line">&#96;age&#96; int COMMENT &#39;??&#39;)</span><br><span class="line">COMMENT &#39;??&#39;</span><br><span class="line">ROW FORMAT SERDE</span><br><span class="line">&#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">原因是什么？怎么破？</span><br><span class="line">中文乱码的原因是因为hive数据库里面的表都是latin1编码的，中文本来就会显示乱码，但是又不能修改整个数据库里面所有表的编码，否则在使用hive的时候会出问题，那么只有考虑把存储字段注释和表注释相关的表的编码改为utf8。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">登陆Mysql数据库切换到Hive库：</span><br><span class="line"></span><br><span class="line">C:\Users\yehua&gt;mysql -uroot -padmin</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecu</span><br><span class="line">Welcome to the MySQL monitor. Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 736</span><br><span class="line">Server version: 8.0.16 MySQL Community Server - GPL</span><br><span class="line">Copyright (c) 2000, 2019, Oracle and&#x2F;or its affiliates. All rights reserved.</span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and&#x2F;or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line">Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement</span><br><span class="line">mysql&gt; use hive;</span><br><span class="line">Database changed</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">确认一下现在表COLUMNS_V2和TABLE_PARAMS的编码，都是latin1</span><br><span class="line"></span><br><span class="line">mysql&gt; show create table COLUMNS_V2;</span><br><span class="line">+------------+---------------------------------------------------------------</span><br><span class="line">| Table | Create Table</span><br><span class="line">+------------+---------------------------------------------------------------</span><br><span class="line">| COLUMNS_V2 | CREATE TABLE &#96;columns_v2&#96; (</span><br><span class="line">&#96;CD_ID&#96; bigint(20) NOT NULL,</span><br><span class="line">&#96;COMMENT&#96; varchar(256) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL</span><br><span class="line">&#96;COLUMN_NAME&#96; varchar(767) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL</span><br><span class="line">&#96;TYPE_NAME&#96; mediumtext,</span><br><span class="line">&#96;INTEGER_IDX&#96; int(11) NOT NULL,</span><br><span class="line">PRIMARY KEY (&#96;CD_ID&#96;,&#96;COLUMN_NAME&#96;),</span><br><span class="line">KEY &#96;COLUMNS_V2_N49&#96; (&#96;CD_ID&#96;),</span><br><span class="line">CONSTRAINT &#96;COLUMNS_V2_FK1&#96; FOREIGN KEY (&#96;CD_ID&#96;) REFERENCES &#96;cds&#96; (&#96;CD_ID&#96;</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;latin1 |</span><br><span class="line">+------------+---------------------------------------------------------------</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; show create table TABLE_PARAMS;</span><br><span class="line">+--------------+-------------------------------------------------------------</span><br><span class="line">| Table | Create Table</span><br><span class="line">+--------------+-------------------------------------------------------------</span><br><span class="line">| TABLE_PARAMS | CREATE TABLE &#96;table_params&#96; (</span><br><span class="line">&#96;TBL_ID&#96; bigint(20) NOT NULL,</span><br><span class="line">&#96;PARAM_KEY&#96; varchar(256) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL,</span><br><span class="line">&#96;PARAM_VALUE&#96; mediumtext CHARACTER SET latin1 COLLATE latin1_bin,</span><br><span class="line">PRIMARY KEY (&#96;TBL_ID&#96;,&#96;PARAM_KEY&#96;),</span><br><span class="line">KEY &#96;TABLE_PARAMS_N49&#96; (&#96;TBL_ID&#96;),</span><br><span class="line">CONSTRAINT &#96;TABLE_PARAMS_FK1&#96; FOREIGN KEY (&#96;TBL_ID&#96;) REFERENCES &#96;tbls&#96; (&#96;TB</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;latin1 |</span><br><span class="line">+--------------+-------------------------------------------------------------</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">修改这两张表的编码即可；</span><br><span class="line">alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;</span><br><span class="line">alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果你的表创建了分区的话就要再执行两条命令：</span><br><span class="line">alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br><span class="line">alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这样修改之后以后就可以看到中文注释了。</span><br><span class="line">注意：需要先把之前创建的t2表删除掉，因为之前存储的中文已经是乱码了，无法恢复，删除之后重新创建就可以了</span><br></pre></td></tr></table></figure><h3 id="指定列和行分隔符的指定"><a href="#指定列和行分隔符的指定" class="headerlink" title="指定列和行分隔符的指定"></a>指定列和行分隔符的指定</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">在我们实际工作中，肯定不会像上面一样创建一张非常简单的表，实际中表的字段会比较多，下面我们就来创建一个多字段的表t3</span><br><span class="line">create table t3(</span><br><span class="line">id int comment &#39;ID&#39;,</span><br><span class="line">stu_name string comment &#39;name&#39;,</span><br><span class="line">stu_birthday date comment &#39;birthday&#39;,</span><br><span class="line">online boolean comment &#39;is online&#39;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这样创建没有问题，我们来加载对应的数据文件 &#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t3.data ，表中的多列内容之间是使用制表符分割的</span><br><span class="line">看一下表中的数据，会不会有问题呢？</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# pwd</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;hivedata</span><br><span class="line">[root@bigdata04 hivedata]# more t3.data</span><br><span class="line">1 张三 2020-01-01 true</span><br><span class="line">2 李四 2020-02-01 false</span><br><span class="line">3 王五 2020-03-01 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t3.data&#39; into table t3;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from t3;</span><br><span class="line">OK</span><br><span class="line">t3.id t3.stu_name t3.stu_birthday t3.online</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">Time taken: 0.229 seconds, Fetched: 3 row(s)</span><br><span class="line"></span><br><span class="line">在这里发现不是我们想要的结果，都是 NULL ，说明数据没有被识别，这是为什么？</span><br><span class="line">注意了，hive在创建表的时候，需要我们指定相应的行分隔符，列分隔符。而我们在创建mysql表的时候，这些都是不需要的，因为它在组织数据的时候，已经规定好了数据的表现形式。</span><br><span class="line">我们刚才在创建t3的时候没有指定相应的分隔符，所以导致使用制表符分割的数据无法被解析。</span><br><span class="line">实际上呢，hive是有默认的分隔符的，默认的行分隔符是 &#39;\n&#39; ，就是换行符，而默认的列分隔符呢，是\001 。</span><br><span class="line">\001 这个是ASCII码中的特殊不常使用的不可见字符，在文本中我们可以通过 ctrl+v 和 ctrl+a 来输入\001 ，这里我们在将 t3.data 改一下，重新上传，再查看表t3。</span><br><span class="line">修改t3.data</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# vi t3.data</span><br><span class="line">1^A张三^A2020-01-01^Atrue</span><br><span class="line">2 李四 2020-02-01 false</span><br><span class="line">3 王五 2020-03-01 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">重新加载数据，查询表数据，这个时候发现刚才修改的那条数据被成功解析了</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t3.data&#39; into table t3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Loading data to table default.t3</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.367 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from t3;</span><br><span class="line">OK</span><br><span class="line">t3.id t3.stu_name t3.stu_birthday t3.online</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">1 张三 2020-01-01 true</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">NULL NULL NULL NULL</span><br><span class="line">Time taken: 0.149 seconds, Fetched: 6 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">那么问题来了，为了我们能够在上传数据之后让hive表正确识别数据，那我们该如何修改hive表的默认分隔符呢？</span><br><span class="line">其实也是非常的简单，只要我们在创建表的时候指定一下分隔符就可以了，我们把建表语句修改一下，重新创建一个表t3_new</span><br><span class="line"></span><br><span class="line">create table t3_new(</span><br><span class="line">id int comment &#39;ID&#39;,</span><br><span class="line">stu_name string comment &#39;name&#39;,</span><br><span class="line">stu_birthday date comment &#39;birthday&#39;,</span><br><span class="line">online boolean comment &#39;is online&#39;</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br><span class="line"></span><br><span class="line">在这需要注意的是， lines terminated by 行分隔符可以忽略不写，但是如果要写的话，只能写到最后面！</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">把t3.data文件中的字段分隔符都恢复为制表符，然后重新把数据加载到t3_new表中。</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;t3.data&#39; into table t3_new</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">查看t3_new表中的数据，注意，针对无法识别的数据显示为NULL，因为最后一列为boolean类型，但是在数据中我故意指定了一个数字，所以导致无法解析，但是不会导致数据加载失败，也不会导致查询失败，这就是hive的特性，他不会提前检查数据，只有在使用的时候才会检查数据，如果数据有问题就显示为null，也不报错。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from t3_new;</span><br><span class="line">OK</span><br><span class="line">t3_new.id t3_new.stu_name t3_new.stu_birthday t3_new.online</span><br><span class="line">1 张三 2020-01-01 true</span><br><span class="line">2 李四 2020-02-01 false</span><br><span class="line">3 王五 2020-03-01 NULL</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第4章 Hive核心实战2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%202.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%202.html</id>
    <published>2022-02-20T04:01:53.000Z</published>
    <updated>2022-02-23T05:05:43.370Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第4章-Hive核心实战"><a href="#第八周-第4章-Hive核心实战" class="headerlink" title="第八周 第4章 Hive核心实战"></a>第八周 第4章 Hive核心实战</h1><a href="/hadoop%E5%AE%9E%E9%AA%8C%E8%AF%BE-hive.html" title="hadoop实验课-hive">hadoop实验课-hive</a><h2 id="Hive中的数据类型"><a href="#Hive中的数据类型" class="headerlink" title="Hive中的数据类型"></a>Hive中的数据类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive作为一个类似数据库的框架，也有自己的数据类型，便于存储、统计、分析。</span><br><span class="line">Hive中主要包含两大数据类型</span><br><span class="line">一类是基本数据类型</span><br><span class="line">一类是复合数据类型</span><br><span class="line">基本数据类型：常用的有INT,STRING,BOOLEAN,DOUBLE等</span><br><span class="line">复合数据类型：常用的有ARRAY,MAP,STRUCT等</span><br></pre></td></tr></table></figure><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">看这个表，一般数字类型我们可以试验int，小数可以使用double，日期可以使用date类型、还有就是</span><br><span class="line">boolean类型，这些算是比较常见的了，前面我们在建表的时候基本都用过了。</span><br><span class="line">这些基本数据类型倒没有什么特殊之处</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据类型 开始支持版本 数据类型 开始支持版本</span><br><span class="line">TINYINT ~ TIMESTAMP 0.8.0</span><br><span class="line">SMALLINT ~ DATE 0.12.0</span><br><span class="line">INT&#x2F;INTEGER ~ STRING ~</span><br><span class="line">BIGINT ~ VARCHAR 0.12.0</span><br><span class="line">FLOAT ~ CHAR 0.13.0</span><br><span class="line">DOUBLE ~ BOOLEAN ~</span><br><span class="line">DECIMAL 0.11.0</span><br></pre></td></tr></table></figure><h3 id="复合数据类型"><a href="#复合数据类型" class="headerlink" title="复合数据类型"></a>复合数据类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面主要看一下复合数据类型，在这里我们主要分析这三个， array，map和struct</span><br><span class="line"></span><br><span class="line">数据类型 开始支持版本 格式</span><br><span class="line">ARRAY 0.14.0 ARRAY&lt;data_type&gt;</span><br><span class="line">MAP 0.14.0 MAP&lt;primitive_type, data_type&gt;</span><br><span class="line">STRUCT ~ STRUCT&lt;col_name : data_type, ...&gt;</span><br></pre></td></tr></table></figure><h4 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先来看Array，这个表示是一个数组结构</span><br><span class="line">在这里举一个例子：学生有多个爱好，有两个学生，zhangsan、lisi，</span><br><span class="line">zhangsan的爱好是swing、sing、coding</span><br><span class="line">lisi的爱好是music、football</span><br><span class="line">每个学生的爱好都是不固定的，有多有少，如果根据学生的每一个爱好都在表里面增加一列，这样就不合适了，后期可能要经常增加列存储不同的爱好</span><br><span class="line">如果我们如果把每个学生的爱好都拼接为一个字符串保存到一个字段中，这样针对存储层面来说是没有问题的，但是后期需要根据爱好的增加而修改字段，这样操作起来很不方便，如果想获取每个学生的1个爱好，这样是没办法直接获取的，因为这些爱好是以字符串的形式保存在一个字段中的</span><br><span class="line">为了方便存储和使用，我们针对学生的爱好这种数据个数不固定的场景，可以使用数组的形式来存储</span><br><span class="line">测试数据是这样的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 hivedata]# more stu.data</span><br><span class="line">1 zhangsan swing,sing,coding</span><br><span class="line">2 lisi music,football</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">来建一张表，指定了一个array数组类型的字段叫favors，数组中存储字符串，数组中的元素怎么分割呢？通过 collection items terminated by &#39;,&#39; 指定的</span><br><span class="line"></span><br><span class="line">create table stu(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">favors array&lt;string&gt;</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;stu.data&#39; into ta</span><br><span class="line">Loading data to table default.stu</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.478 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">查询数组中的某一个元素，使用 arrayName[index]</span><br><span class="line">hive (default)&gt; select * from stu;</span><br><span class="line">OK</span><br><span class="line">stu.id stu.name stu.favors</span><br><span class="line">1 zhangsan [&quot;swing&quot;,&quot;sing&quot;,&quot;coding&quot;]</span><br><span class="line">2 lisi [&quot;music&quot;,&quot;football&quot;]</span><br><span class="line">Time taken: 1.547 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; select id,name,favors[1] from stu;</span><br><span class="line">OK</span><br><span class="line">id name _c2</span><br><span class="line"></span><br><span class="line">1 zhangsan sing</span><br><span class="line">2 lisi football</span><br><span class="line">Time taken: 0.631 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">角标是从0开始的，如果获取到了不存在的角标则返回null</span><br><span class="line">这就是Array类型的使用了</span><br></pre></td></tr></table></figure><h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面来说一下另外一种常见的集合——map，我们知道map集合里面存储的是键值对，每一个键值对属于Map集合的一个item，</span><br><span class="line">这里给大家举个例子，有两个学生zhangsan、lisi，每个学生有语文、数学、英语，成绩如下：</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# more stu2.data</span><br><span class="line">1 zhangsan chinese:80,math:90,english:100</span><br><span class="line">2 lisi chinese:89,english:70,math:88</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">针对学生的成绩信息最好也是存储到一个字段中，方便管理和使用，发现学生的成绩都是key-value类型的，所以非常适合使用map类型</span><br><span class="line">建表语句如下：指定scores字段类型为map格式</span><br><span class="line">通过 collection items terminated by &#39;,&#39; 指定了map中元素之间的分隔符</span><br><span class="line">通过 map keys terminated by &#39;:&#39; 指定了key和value之间的分隔符</span><br><span class="line"></span><br><span class="line">create table stu2(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">scores map&lt;string,int&gt;</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;stu2.data&#39; into t</span><br><span class="line">Loading data to table default.stu2</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.521 seconds</span><br><span class="line"></span><br><span class="line">查看表中的数据</span><br><span class="line">hive (default)&gt; select * from stu2;</span><br><span class="line">OK</span><br><span class="line">stu2.id stu2.name stu2.scores</span><br><span class="line">1 zhangsan &#123;&quot;chinese&quot;:80,&quot;math&quot;:90,&quot;english&quot;:100&#125;</span><br><span class="line">2 lisi &#123;&quot;chinese&quot;:89,&quot;english&quot;:70,&quot;math&quot;:88&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">查询所有学生的语文和数学成绩</span><br><span class="line">hive (default)&gt; select id,name,scores[&#39;chinese&#39;] as ch_score ,scores[&#39;math&#39;] as math_score from stu2;</span><br><span class="line"></span><br><span class="line">id name ch_score math_score</span><br><span class="line">1  zhangsan  80    90</span><br><span class="line">2 lisi       89    88</span><br><span class="line">Time taken: 0.232 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在这注意一下，我们取数据是根据元素中的key获取的，和map结构中元素的位置没有关系</span><br></pre></td></tr></table></figure><h4 id="Struct"><a href="#Struct" class="headerlink" title="Struct"></a>Struct</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">再来介绍最后一种复合类型struct，有点像java中的对象，举个例子说明一下，</span><br><span class="line">某学校有2个实习生，zhangsan、lisi，每个实习生都有地址信息，一个是户籍地所在的城市，一个是公司所在的城市，</span><br><span class="line">我们来组织一下数据</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# more stu3.data</span><br><span class="line">1 zhangsan bj,sh</span><br><span class="line">2 lisi gz,sz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">针对这里面的地址信息，不能懒省事使用字符串，否则后期想要获取他们对应的户籍地城市或者公司所在</span><br><span class="line">的城市信息时就比较麻烦了</span><br><span class="line">所以在这我们可以考虑使用Struct类型</span><br><span class="line">建表语句如下：</span><br><span class="line"></span><br><span class="line">create table stu3(</span><br><span class="line">id int,</span><br><span class="line">name string,</span><br><span class="line">address struct&lt;home_addr:string,office_addr:string&gt;</span><br><span class="line">)row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;stu3.data&#39; into t</span><br><span class="line">Loading data to table default.stu3</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.447 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from stu3;</span><br><span class="line">OK</span><br><span class="line">stu3.id stu3.name stu3.address</span><br><span class="line">1 zhangsan &#123;&quot;home_addr&quot;:&quot;bj&quot;,&quot;office_addr&quot;:&quot;sh&quot;&#125;</span><br><span class="line">2 lisi &#123;&quot;home_addr&quot;:&quot;gz&quot;,&quot;office_addr&quot;:&quot;sz&quot;&#125;</span><br><span class="line">Time taken: 0.189 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; select id,name,address.home_addr from stu3;</span><br><span class="line">OK</span><br><span class="line">id name home_addr</span><br><span class="line">1 zhangsan bj</span><br><span class="line">2 lisi gz</span><br><span class="line">Time taken: 0.201 seconds, Fetched: 2 row(s)</span><br><span class="line"></span><br><span class="line">在这里大家会发现其实这个需求，我们使用Array也是可以搞定的吧，只不过是在查询的时候只能通过角</span><br><span class="line">标访问，不太方便而已。</span><br></pre></td></tr></table></figure><h4 id="Struct和Map的区别"><a href="#Struct和Map的区别" class="headerlink" title="Struct和Map的区别"></a>Struct和Map的区别</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">如果从建表语句上来分析，其实这个Struct和Map还是有一些相似之处的</span><br><span class="line">来总结一下：</span><br><span class="line">map中可以随意增加k-v对的个数</span><br><span class="line">struct中的k-v个数是固定的</span><br><span class="line">map在建表语句中需要指定k-v的类型</span><br><span class="line">struct在建表语句中需要指定好所有的属性名称和类型</span><br><span class="line">map中通过[]取值</span><br><span class="line">struct中通过.取值，类似java中的对象属性引用</span><br><span class="line">map的源数据中需要带有k-v</span><br><span class="line">struct的源数据中只需要有v即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">总体而言还是map比较灵活，但是会额外占用磁盘空间，因为他比struct多存储了数据的key</span><br><span class="line">struct只需要存储value，比较节省空间，但是灵活性有限，后期无法动态增加k-v</span><br></pre></td></tr></table></figure><h2 id="案例：复合数据类型的使用"><a href="#案例：复合数据类型的使用" class="headerlink" title="案例：复合数据类型的使用"></a>案例：复合数据类型的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在这里我们把前面学到的这三种复合数据类型结合到一块来使用一下。</span><br><span class="line">有一份数据是这样的</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# more student.data</span><br><span class="line">1 zhangsan english,sing,swing chinese:80,math:90,english:10</span><br><span class="line">2 lisi games,coding chinese:89,english:70,math:88 gz,sz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">根据这份数据建表，根据咱们前面的学习，这里面这几个字段分别是int类型、string类型，array类型，map类型，struct类型</span><br><span class="line">其实也不一定非要使用这些复合类型，主要是需要根据具体业务分析，使用复合数据类型可以更方便的操作数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table student (</span><br><span class="line">id int comment &#39;id&#39;,</span><br><span class="line">name string comment &#39;name&#39;,</span><br><span class="line">favors array&lt;string&gt; ,</span><br><span class="line">scores map&lt;string, int&gt;,</span><br><span class="line">address struct&lt;home_addr:string,office_addr:string&gt;</span><br><span class="line">) row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;</span><br><span class="line">lines terminated by &#39;\n&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;student.data&#39; into table student;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from student;</span><br><span class="line">OK</span><br><span class="line">student.id student.name student.favors student.scores student.addre</span><br><span class="line">1 zhangsan [&quot;english&quot;,&quot;sing&quot;,&quot;swing&quot;] &#123;&quot;chinese&quot;:80,&quot;math&quot;:</span><br><span class="line">2 lisi [&quot;games&quot;,&quot;coding&quot;] &#123;&quot;chinese&quot;:89,&quot;english&quot;:70,&quot;math&quot;:88&#125;</span><br><span class="line">Time taken: 0.168 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><h2 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">问：在mysql中有一张表student(id,name)，还有一张表address(stu_id,</span><br><span class="line">home,school)，还有联系方式表contact(stu_id,mine,parents,others)。如果把这三张表迁移到hive中，</span><br><span class="line">如何迁移？</span><br><span class="line">答：</span><br><span class="line">可以一一对应迁移，优点是迁移成本非常低，包括DDL和业务逻辑，几乎不需要修改，可以直接使用。缺点是产生大量的表连接，造成查询慢。</span><br><span class="line">可以一对多，mysql中的多张关联表可以创建为hive中的一张表。优点是减少表连接操作。缺点是迁移成本高，需要修改原有的业务逻辑。</span><br><span class="line">实际上，在我们日常的开发过程中遇到这样的问题，要想比较完美、顺利的解决，一般都分为两个阶段，</span><br><span class="line">第一个阶段，现在快捷迁移，就是上面说的一一对应，让我们的系统能跑起来，在此基础之上呢，再做一张大表，尽量包含以上所有字段，例如：</span><br><span class="line">stu(id, name, address struct&lt;home,school&gt;, contact struct&lt;…&gt;);</span><br><span class="line">等第二个阶段完工之后了，就可以跑在新的系统里面了。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第4章 Hive核心实战2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%203.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC4%E7%AB%A0-Hive%E6%A0%B8%E5%BF%83%E5%AE%9E%E6%88%98%203.html</id>
    <published>2022-02-20T04:01:53.000Z</published>
    <updated>2022-02-23T15:19:11.365Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第4章-Hive核心实战"><a href="#第八周-第4章-Hive核心实战" class="headerlink" title="第八周 第4章 Hive核心实战"></a>第八周 第4章 Hive核心实战</h1><a href="/hadoop%E5%AE%9E%E9%AA%8C%E8%AF%BE-hive.html" title="hadoop实验课-hive">hadoop实验课-hive</a><h2 id="Hive中的表类型"><a href="#Hive中的表类型" class="headerlink" title="Hive中的表类型"></a>Hive中的表类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在Mysql中没有表类型这个概念，因为它就只有一种表。</span><br><span class="line">但是Hive中是有多种表类型的，我们可以分为四种，内部表、外部表、分区表、桶表</span><br><span class="line">下面来一个一个学习一下这些类型的表</span><br></pre></td></tr></table></figure><h3 id="内部表"><a href="#内部表" class="headerlink" title="内部表"></a>内部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">首先看内部表</span><br><span class="line">内部表也可以称为受控表</span><br><span class="line">它是Hive中的默认表类型，表数据默认存储在 warehouse 目录中</span><br><span class="line">在加载数据的过程中，实际数据会被移动到warehouse目录中，就是咱们前面在使用load加载数据的时候，数据就会被加载到warehouse中表对应的目录中</span><br><span class="line">当我们删除表时，表中的数据和元数据将会被同时删除</span><br><span class="line">实际上，我们前面创建的表都属于受控表，前面我们已经演示了，创建一张表，其对应就，在metastore中存储表的元数据信息，当我们一旦从hive中删除一张表之后，表中的数据会被删除，在metastore中存储的元数据信息也会被删除。</span><br><span class="line">这就是内部表的特性。</span><br></pre></td></tr></table></figure><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">建表语句中包含 External 的表叫外部表</span><br><span class="line">外部表在加载数据的时候，实际数据并不会移动到warehouse目录中，只是与外部数据建立一个链接(映射关系)</span><br><span class="line">表的定义和数据的生命周期互相不约束，数据只是表对hdfs上的某一个目录的引用而已，当删除表定义的时候，数据依然是存在的。仅删除表和数据之间引用关系，所以这种表是比较安全的，就算是我们误删表了，数据还是没丢的</span><br><span class="line">我们来创建一张外部表，看一下外部表的建表语句该如何来写</span><br><span class="line">看一下官方文档</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">那根据这个格式我们自己来创建一个外部表</span><br><span class="line">create external table external_table (</span><br><span class="line">key string</span><br><span class="line">) location &#39;&#x2F;data&#x2F;external&#39;;</span><br><span class="line"></span><br><span class="line">表创建完以后到hdfs上查询，如果指定的目录不存在会自动创建</span><br><span class="line">此时到hdfs的 &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F; 目录下查看，是看不到这个表的目录的，因为这个表的目录是我们刚才通过location指定的目录</span><br><span class="line"></span><br><span class="line">我们再来看一下metastore中的tbls表，这里看到external_table的类型是外部表。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bCVdy9" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bCVdy9.md.png" alt="bCVdy9.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 hivedata]# more external_table.data</span><br><span class="line">a </span><br><span class="line">b </span><br><span class="line">c </span><br><span class="line">d </span><br><span class="line">e</span><br><span class="line"></span><br><span class="line">加载数据</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;external_table.dat</span><br><span class="line">Loading data to table default.external_table</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.364 seconds</span><br><span class="line"></span><br><span class="line">此时加载的数据会存储到hdfs的 &#x2F;data&#x2F;external 目录下</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bCZgA0" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bCZgA0.md.png" alt="bCZgA0.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来尝试删除这个表，看看会发生什么现象</span><br><span class="line">到hdfs上查看数据，发现之前上传上去的数据还在</span><br><span class="line"></span><br><span class="line">这个其实就是前面我们所的外部表的特性，外部表被删除时，只会删除表的元数据，表中的数据不会被删除。</span><br></pre></td></tr></table></figure><h4 id="内部表和外部表相互转化"><a href="#内部表和外部表相互转化" class="headerlink" title="内部表和外部表相互转化"></a>内部表和外部表相互转化</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">注意：实际上内外部表是可以互相转化的，需要我们做一下简单的设置即可。</span><br><span class="line">内部表转外部表</span><br><span class="line">alter table tblName set tblproperties (‘external’&#x3D;‘true’);</span><br><span class="line">外部表转内部表</span><br><span class="line">alter table tblName set tblproperties (‘external’&#x3D;‘false’);</span><br><span class="line">(不要根据数据是否在warehouse中，来判断是否是内外部表；外部表不指定location时，删除外部表时，warehouse里的数据也不会被删除；内部表也可指指定数据不放到warehouse里，但删除表时，数据还是会被删除)</span><br><span class="line">在实际工作中，我们在hive中创建的表95%以上的都是外部表</span><br><span class="line">因为大致流程是这样的，我们先通过flume采集数据，把数据上传到hdfs中，然后在hive中创建外部表和hdfs上的数据绑定关系，就可以使用sql查询数据了，所以连load数据那一步都可以省略了，因为是先有数据，才创建的表。</span><br><span class="line">画图分析一下。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bCn2Nt" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bCn2Nt.md.png" alt="bCn2Nt.md.png"></a></p><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><h4 id="单个分区字段"><a href="#单个分区字段" class="headerlink" title="单个分区字段"></a>单个分区字段</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  假设我们的web服务器每天都产生一个日志数据文件，Flume把数据采集到HDFS中，每一天的数据存储到一个日期目录中。我们如果想查询某一天的数据的话，hive执行的时候默认会对所有文件都扫描一遍，然后再过滤出来我们想要查询的那一天的数据</span><br><span class="line">如果你已经采集了一年的数据，这样每次计算都需要把一年的数据取出来，再过滤出来某一天的数据，效率就太低了，会非常浪费资源，所以我们可以让hive在查询的时候，根据你要查询的日期，直接定位到对应的日期目录。这样就可以直接查询满足条件的数据了，效率提升可不止一点点啊，是质的提升。</span><br><span class="line">想要实现这个功能，就需要使用分区表了</span><br><span class="line"></span><br><span class="line">  分区可以理解为分类，通过分区把不同类型的数据放到不同目录中</span><br><span class="line">  分区的标准就是指定分区字段，分区字段可以有一个或多个，根据咱们刚才举的例子，分区字段就是日期</span><br><span class="line">  分区表的意义在于优化查询，查询时尽量利用分区字段，如果不使用分区字段，就会全表扫描，最典型的一个场景就是把天作为分区字段，查询的时候指定天</span><br><span class="line">按照上面的分析，我们来创建一个分区表，使用partitioned by指定区分字段，分区字段的名称为dt，类型为string</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">create table partition_1 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) partitioned by (dt string)</span><br><span class="line">row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;;</span><br><span class="line"></span><br><span class="line">查看表的信息，可以看到分区信息</span><br><span class="line">hive (default)&gt; desc partition_1;</span><br><span class="line">OK</span><br><span class="line">col_name data_type comment</span><br><span class="line">id int</span><br><span class="line">name string</span><br><span class="line">dt string</span><br><span class="line"># Partition Information</span><br><span class="line"># col_name data_type comment</span><br><span class="line">dt string</span><br><span class="line">Time taken: 0.745 seconds, Fetched: 7 row(s)</span><br></pre></td></tr></table></figure><h5 id="加载数据时自动创建分区"><a href="#加载数据时自动创建分区" class="headerlink" title="加载数据时自动创建分区"></a>加载数据时自动创建分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">数据格式是这样的</span><br><span class="line">[root@bigdata04 hivedata]# more partition_1.data</span><br><span class="line">1 zhangsan</span><br><span class="line">2 lisi</span><br><span class="line"></span><br><span class="line">向分区表中加载数据【注意，在这里添加数据的同时需要指定分区信息】</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_1.data&#39; into table partition_1 partition(dt&#x3D;&#39;20200101&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">来查看一下hdfs中的信息，刚才创建的分区信息在hdfs中的体现是一个目录。</span><br><span class="line">由于这个分区表属于内部表， 所以目录还在warehouse这个目录中</span><br></pre></td></tr></table></figure><p><img src="C:%5CUsers%5CTy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220223165814509.png" alt="image-20220223165814509"></p><h5 id="手动创建分区"><a href="#手动创建分区" class="headerlink" title="手动创建分区"></a>手动创建分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">当然我也可以手动在表中只创建分区：</span><br><span class="line">hive (default)&gt; alter table partition_1 add partition (dt&#x3D;&#39;2020-01-02&#39;);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.295 seconds</span><br><span class="line"></span><br><span class="line">此时会发现hdfs中又多了一个目录，只不过这个分区目录中是没有数据的</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bClx0g" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bClx0g.md.png" alt="bClx0g.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">向这个分区中添加数据，可以使用刚才的load命令或者hdfs的put命令都可以</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_1.data&#39;</span><br><span class="line">Loading data to table default.partition_1 partition (dt&#x3D;2020-01-02)</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bC1YHe" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bC1YHe.md.png" alt="bC1YHe.md.png"></a></p><h5 id="查看表的分区"><a href="#查看表的分区" class="headerlink" title="查看表的分区"></a>查看表的分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如何查看我的表中目前有哪些分区呢，语法为： show partitions tblName</span><br><span class="line"></span><br><span class="line">hive (default)&gt; show partitions partition_1;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">dt&#x3D;2020-01-01</span><br><span class="line">dt&#x3D;2020-01-02</span><br><span class="line">Time taken: 0.246 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><h5 id="删除分区"><a href="#删除分区" class="headerlink" title="删除分区"></a>删除分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">那问题来了，刚才增加了一个分区，那我能删除一个分区吗？</span><br><span class="line">必须是可以的</span><br><span class="line"></span><br><span class="line">hive (default)&gt; alter table partition_1 drop partition(dt&#x3D;&#39;2020-01-02&#39;);</span><br><span class="line">Dropped the partition dt&#x3D;2020-01-02</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.771 seconds</span><br><span class="line">hive (default)&gt; show partitions partition_1;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">dt&#x3D;2020-01-01</span><br><span class="line">Time taken: 0.174 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，此时分区删除之后，分区中对应的数据也就没有了，因为是内部表，所以分区的数据是会被删掉的</span><br></pre></td></tr></table></figure><h4 id="多个分区字段"><a href="#多个分区字段" class="headerlink" title="多个分区字段"></a>多个分区字段</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">刚才呢，我们创建了一个分区，但是有的业务需求，需要创建多个分区，可以吗？</span><br><span class="line">当然是可以的！</span><br><span class="line">这里再举一个例子。某学校，有若干二级学院，每年都招很多学生，学校的统计需求大部分会根据年份和学院名称作为条件</span><br><span class="line">所以为了提高后期的统计效率，我们最好是使用年份和学院名称作为分区字段</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table partition_2 (</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">) partitioned by (year int, school string)</span><br><span class="line">row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc partition_2;</span><br><span class="line">OK</span><br><span class="line">col_name data_type comment</span><br><span class="line">id int</span><br><span class="line">name string</span><br><span class="line">year int</span><br><span class="line">school string</span><br><span class="line"># Partition Information</span><br><span class="line"># col_name data_type comment</span><br><span class="line">year int</span><br><span class="line">school string</span><br><span class="line">Time taken: 0.097 seconds, Fetched: 9 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据文件内容</span><br><span class="line">[root@bigdata04 hivedata]# more partition_2.data</span><br><span class="line">1 zhangsan</span><br><span class="line">2 lisi</span><br><span class="line">3 wangwu</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：数据文件中只需要有id和name这两个字段的值就可以了，具体year和school这两个分区字段是在加载分区的时候指定的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_2.data&#39; into partition_2 patition(year&#x3D;2020, school&#x3D;&quot;xk&quot;)</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_2.data&#39; into partition_2 patition(year&#x3D;2020, school&#x3D;&quot;english&quot;)</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_2.data&#39; into partition_2 patition(year&#x3D;2019, school&#x3D;&quot;xk&quot;)</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;partition_2.data&#39; into partition_2 patition(year&#x3D;2019, school&#x3D;&quot;english&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">查看分区信息</span><br><span class="line">hive (default)&gt; show partitions partition_2;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">year&#x3D;2019&#x2F;school&#x3D;english</span><br><span class="line">year&#x3D;2019&#x2F;school&#x3D;xk</span><br><span class="line">year&#x3D;2020&#x2F;school&#x3D;english</span><br><span class="line">year&#x3D;2020&#x2F;school&#x3D;xk</span><br></pre></td></tr></table></figure><h4 id="查询分区表"><a href="#查询分区表" class="headerlink" title="查询分区表"></a>查询分区表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">前面我们讲了如何创建、增加和删除分区</span><br><span class="line">还有一个比较重要的是我们该如何查询分区中的数据呢？其实非常简单，分区相当于我们的一个查询条</span><br><span class="line">件，直接跟在where后面就可以了。</span><br><span class="line"></span><br><span class="line">select * from partition_2; 【全表扫描，没有用到分区的特性】</span><br><span class="line">select * from partition_2 where year &#x3D; 2019;【用到了一个分区字段进行过滤】</span><br><span class="line">select * from partition_2 where year &#x3D; 2019 and school &#x3D; &#39;xk&#39;;【用到了两个分区字段进行过滤】</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这就是分区表的主要操作</span><br><span class="line">其实我们在这使用的分区表可以认为是内部分区表，内部分区表的应用场景也不多，外部分区表的应用场景才多，外部分区表就是在外部表的基础上又增加了分区。</span><br></pre></td></tr></table></figure><h4 id="外部分区表"><a href="#外部分区表" class="headerlink" title="外部分区表"></a>外部分区表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">外部分区表示工作中最常用的表</span><br><span class="line">我们先来创建一个外部分区表</span><br><span class="line"></span><br><span class="line">create external table ex_par(</span><br><span class="line">id int,</span><br><span class="line">name string</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line">row format delimited</span><br><span class="line">fields terminated by &#39;\t&#39;</span><br><span class="line">location &#39;&#x2F;data&#x2F;ex_par&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">其它的操作和前面操作普通分区表是一样的，我们主要演示一下添加分区数据和删除分区的操作添加分区数据</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;ex_par.data&#39; into table ex_par partition(dt&#x3D;20200101)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive (default)&gt; show partitions ex_par;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">dt&#x3D;2020-01-01</span><br><span class="line">Time taken: 0.415 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">删除分区(删除后，此时hdfs上的分区目录还在)</span><br><span class="line">hive (default)&gt; alter table ex_par drop partition(dt&#x3D;&#39;2020-01-01&#39;);</span><br><span class="line">Dropped the partition dt&#x3D;2020-01-01</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.608 seconds</span><br><span class="line">hive (default)&gt; show partitions ex_par;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">Time taken: 0.229 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：此时分区目录的数据还是在的，因为这个是外部表，所以删除分区也只是删除分区的定义，分区中的数据还是在的，这个和内部分区表就不一样了</span><br><span class="line"></span><br><span class="line">虽然这个分区目录还在，但是刚才我们通过，show partitions 已经查不到分区信息了，所以查询表数据是查不出来的，虽然这个目录确实在这个表对应的hdfs目录中，但是由于这个是一个分区表，这份数据没有和任何分区绑定，所以就查询不出来</span><br><span class="line">这个一定要注意，在实际工作中新手最容易遇到的一个问题就是，针对分区表，通过hdfs的put命令把数据上传上去了，但是却查不到数据，就是因为没有在表中添加分区信息，也就是说你们现在虽然在一起了，但是还没有领结婚证，国家还不承认。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果数据已经上传上去了，如何给他们绑定关系呢？就是使用前面咱们讲的alter add partition命令，注</span><br><span class="line">意在这里需要通过location指定分区目录</span><br><span class="line">hive (default)&gt; alter table ex_par add partition(dt&#x3D;&#39;2020-01-01&#39;) location &#39;&#x2F;data&#x2F;ex_par&#x2F;dt&#x3D;20200101&#39;;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">此时再查询分区数据和表数据，就正常了。</span><br><span class="line">hive (default)&gt; show partitions ex_par;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">dt&#x3D;2020-01-01</span><br><span class="line">Time taken: 0.19 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; select * from ex_par;</span><br><span class="line">OK</span><br><span class="line">ex_par.id ex_par.name ex_par.dt</span><br><span class="line">1 zhangsan 2020-01-01</span><br><span class="line">2 lisi 2020-01-01</span><br><span class="line">3 wangwu 2020-01-01</span><br><span class="line">Time taken: 0.432 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;ex_par.data&#39; into table ex_par  partition(dt&#x3D;&#39;20200101&#39;);</span><br><span class="line">load data .... partition 这条命令做了两个事情，1：上传数据，2：添加分区(绑定数据和分区之间的关系)</span><br><span class="line"></span><br><span class="line">hdfs dfs -mkdir &#x2F;data&#x2F;ex_par&#x2F;20200101</span><br><span class="line">hdfs dfs -put &#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;ex_par.data &#x2F;data&#x2F;ex_par&#x2F;20200101</span><br><span class="line"></span><br><span class="line">alter table ex_par add partition(dt&#x3D;&#39;20200101&#39;) location &#39;&#x2F;data&#x2F;ex_par&#x2F;dt&#x3D;20200101&#39;;</span><br><span class="line">上面这三条命令做了两件事情，1：上传数据 2：添加分区(绑定数据和分区之间的关系)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">外部分区表是工作中最常见的表</span><br></pre></td></tr></table></figure><h3 id="桶表"><a href="#桶表" class="headerlink" title="桶表"></a>桶表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">桶表是对数据进行哈希取值，然后放到不同文件中存储</span><br><span class="line">物理上，每个桶就是表(或分区）里的一个文件</span><br><span class="line">什么时候会用到桶表呢？</span><br><span class="line">举个例子，针对中国的人口，主要集中河南、江苏、山东、广东、四川，其他省份就少的多了，你像西藏</span><br><span class="line">就三四百万，海南也挺少的，如果使用分区表，我们把省份作为分区字段，数据会集中在某几个分区，其</span><br><span class="line">他分区数据就不会很多，那这样对数据存储以及查询不太友好，在计算的时候会出现数据倾斜的问题，计</span><br><span class="line">算效率也不高，我们应该相对均匀的存放数据，从源头上解决，这个时候我们就可以采用分桶的概念，也</span><br><span class="line">就是使用桶表</span><br></pre></td></tr></table></figure><h4 id="创建桶表"><a href="#创建桶表" class="headerlink" title="创建桶表"></a>创建桶表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下面来建立一个桶表：</span><br><span class="line">这个表的意思是按照id进行分桶，分成4个桶。</span><br><span class="line">create table bucket_tb(</span><br><span class="line">id int</span><br><span class="line">) clustered by (id) into 4 buckets;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">create table bucket_tb(</span><br><span class="line">id int</span><br><span class="line">) clustered by (id) into 4 buckets;</span><br><span class="line"></span><br><span class="line">这个时候往桶中加载数据的时候，就不能使用load data的方式了，而是需要使用其它表中的数据，那么给桶表加载数据的写法就有新的变化了。</span><br><span class="line">类似这样的写法</span><br><span class="line">insert into table … select … from …;</span><br><span class="line">注意，在插入数据之前需要先设置开启桶操作，不然数据无法分到不同的桶里面</span><br><span class="line">其实这里的分桶就是设置reduce任务的数量，因为你分了多少个桶，最终结果就会产生多少个文件，最终结果中文件的数量就和reduce任务的数量是挂钩的</span><br><span class="line">设置完 set hive.enforce.bucketing &#x3D; true 可以自动控制reduce的数量从而适配bucket的个数</span><br><span class="line"></span><br><span class="line">hive (default)&gt; set hive.enforce.bucketing&#x3D;true;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">初始化一个表，用于向桶表中加载数据</span><br><span class="line">原始数据文件是这样的</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hivedata]# more b_source.data</span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5 </span><br><span class="line">6 </span><br><span class="line">7 </span><br><span class="line">8 </span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table b_source(id int);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.262 seconds</span><br><span class="line">hive (default)&gt; load data local inpath &#39;&#x2F;data&#x2F;soft&#x2F;hivedata&#x2F;b_source.data&#39; into table b_source;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from b_source;</span><br><span class="line">OK</span><br><span class="line">b_source.id</span><br><span class="line">1 </span><br><span class="line">2 </span><br><span class="line">3 </span><br><span class="line">4 </span><br><span class="line">5 </span><br><span class="line">6 </span><br><span class="line">7 </span><br><span class="line">8 </span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">Time taken: 0.187 seconds, Fetched: 12 row(s)</span><br></pre></td></tr></table></figure><h4 id="向桶表加载数据"><a href="#向桶表加载数据" class="headerlink" title="向桶表加载数据"></a>向桶表加载数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">向桶表中加载数据</span><br><span class="line">hive (default)&gt; insert into table bucket_tb select id from b_source where id !&#x3D; NULL;</span><br></pre></td></tr></table></figure><h4 id="查看结果"><a href="#查看结果" class="headerlink" title="查看结果"></a>查看结果</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from bucket_tb;</span><br><span class="line">OK</span><br><span class="line">bucket_tb.id</span><br><span class="line">12</span><br><span class="line">8 </span><br><span class="line">4 </span><br><span class="line">9 </span><br><span class="line">5 </span><br><span class="line">1</span><br><span class="line">10</span><br><span class="line">6 </span><br><span class="line">2</span><br><span class="line">11</span><br><span class="line">7 </span><br><span class="line">3</span><br><span class="line">Time taken: 0.183 seconds, Fetched: 12 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">按照我们设置的桶的数量为4，这样在hdfs中会存在4个对应的文件，每个文件的大小是相似的</span><br></pre></td></tr></table></figure><p><img src="C:%5CUsers%5CTy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220223230539806.png" alt="image-20220223230539806"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到hdfs上查看桶表中的文件内容，可以看出是通过对buckets取模确定的</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/bPt0SJ" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/23/bPt0SJ.md.png" alt="bPt0SJ.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这样就实现了数据分桶存储。</span><br><span class="line">桶表的主要作用：</span><br></pre></td></tr></table></figure><h4 id="桶表的作用"><a href="#桶表的作用" class="headerlink" title="桶表的作用"></a>桶表的作用</h4><h5 id="数据抽样"><a href="#数据抽样" class="headerlink" title="数据抽样"></a>数据抽样</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">数据抽样</span><br><span class="line">假如我们使用的是一个大规模的数据集,我们只想去抽取部分数据进行查看.使用bucket表可以变得更加的高效</span><br><span class="line">select * from bucket_tb tablesample(bucket 1 out of 4 on id);</span><br><span class="line">tablesample是抽样语句</span><br><span class="line">语法解析：TABLESAMPLE(BUCKET x OUT OF y ON column)</span><br><span class="line">y尽可能是桶表的bucket数的倍数或者因子，而且y必须要大于等于x</span><br><span class="line">y表示是把桶表中的数据随机分为多少桶</span><br><span class="line">x表示取出第几桶的数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bucket 1 out of 4 on id：根据id对桶表中的数据重新分桶，分成4桶，取出第1桶的数据</span><br><span class="line">bucket 2 out of 4 on id：根据id对桶表中的数据重新分桶，分成4桶，取出第2桶的数据</span><br><span class="line">bucket 3 out of 4 on id：根据id对桶表中的数据重新分桶，分成4桶，取出第3桶的数据</span><br><span class="line">bucket 4 out of 4 on id：根据id对桶表中的数据重新分桶，分成4桶，取出第4桶的数据</span><br><span class="line">验证一下效果，这里面四个SQL语句，每个SQL语句取出一个桶的数据，最终的总和就是表中的所有数据</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from bucket_tb tablesample(bucket 1 out of 4 on id);</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from bucket_tb tablesample(bucket 2 out of 4 on id);</span><br></pre></td></tr></table></figure><h5 id="提高某些查询效率"><a href="#提高某些查询效率" class="headerlink" title="提高某些查询效率"></a>提高某些查询效率</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">例如：join查询,可以避免产生笛卡尔积的操作</span><br><span class="line">select a.id,a.name,b.addr from a join b on a.id &#x3D; b.id;</span><br><span class="line">如果a表和b表已经是分桶表，而且分桶的字段是id字段，那么做这个操作的时候就不需要再进行全表笛卡尔积了，因为分桶之后相同规则的id已经在相同的文件里面了。</span><br></pre></td></tr></table></figure><h2 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h2><h2 id="综合案例"><a href="#综合案例" class="headerlink" title="综合案例"></a>综合案例</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第3章 Hive基础使用</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Hive%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Hive%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8.html</id>
    <published>2022-02-20T04:01:26.000Z</published>
    <updated>2022-02-22T04:12:08.754Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第3章-Hive基础使用"><a href="#第八周-第3章-Hive基础使用" class="headerlink" title="第八周 第3章 Hive基础使用"></a>第八周 第3章 Hive基础使用</h1><h2 id="Hive的使用方式"><a href="#Hive的使用方式" class="headerlink" title="Hive的使用方式"></a>Hive的使用方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">操作Hive可以在Shell命令行下操作，或者是使用JDBC代码的方式操作</span><br><span class="line">下面先来看一下在命令行中操作的方式</span><br></pre></td></tr></table></figure><h3 id="命令行方式"><a href="#命令行方式" class="headerlink" title="命令行方式"></a>命令行方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">针对命令行这种方式，其实还有两种使用</span><br><span class="line">第一个是使用bin目录下的hive命令，这个是从hive一开始就支持的使用方式</span><br><span class="line">后来又出现一个beeline命令，它是通过HiveServer2服务连接hive，它是一个轻量级的客户端工具，所以</span><br><span class="line">后来官方开始推荐使用这个。</span><br><span class="line">具体使用哪个我觉得属于个人的一个习惯问题，特别是一些做了很多年大数据开发的人，已经习惯了使用</span><br><span class="line">hive命令，如果让我使用beeline会感觉有点别扭</span><br><span class="line">针对我们写的hive sql通过哪一种客户端去执行结果都是一样的，没有任何区别，所以在这里我们使用哪</span><br><span class="line">个就无所谓了。</span><br></pre></td></tr></table></figure><h4 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">先看第一种，这种直接就可以连进去</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;hive</span><br><span class="line"></span><br><span class="line">这里有一行信息提示，从Hive2开始Hive-on-MR就过时了，并且在以后的版本中可能就不维护了，建议</span><br><span class="line">使用其它的计算引擎，例如：spark或者tez</span><br><span class="line">如果你确实想使用MapReduce引擎，那建议你使用Hive1.x的版本。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">hive&gt; create table t1(id int,name string);</span><br><span class="line"></span><br><span class="line">hive&gt; insert into t1(id,name) values(1,&quot;zs&quot;);</span><br><span class="line">此时会产生mapreduce任务</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from t1;</span><br><span class="line"></span><br><span class="line">查询数据，为什么这时没有产生mapreduce任务呢？因为这个计算太简单了，不需要经过mapreduce任</span><br><span class="line">务就可以获取到结果，直接读取表对应的数据文件就可以了。</span><br><span class="line"></span><br><span class="line">hive&gt; drop table t1;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">退出</span><br><span class="line">exit;</span><br><span class="line">quit;</span><br><span class="line">ctrl C</span><br></pre></td></tr></table></figure><h4 id="beeline"><a href="#beeline" class="headerlink" title="beeline"></a>beeline</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.启动hiveserver2服务</span><br><span class="line">&#x2F;bin&#x2F;hiveserver2</span><br><span class="line">2.bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">注意了，启动hiveserver2服务之后，最下面会输出几行Hive Session ID的信息，一定要等到输出4行以后再使用beeline去连接，否则会提示连接拒绝</span><br><span class="line"></span><br><span class="line">当hiveserver2服务没有真正启动成功之前连接会提示这样的信息</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost</span><br><span class="line">Connecting to jdbc:hive2:&#x2F;&#x2F;localhost:10000</span><br><span class="line">20&#x2F;05&#x2F;06 16:44:21 [main]: WARN jdbc.HiveConnection: Failed to connect to loca</span><br><span class="line">Could not open connection to the HS2 server. Please check the server URI and</span><br><span class="line">Error: Could not open client transport with JDBC Uri: jdbc:hive2:&#x2F;&#x2F;localhost:</span><br><span class="line">Beeline version 3.1.2 by Apache Hive</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">等待hiveserver2服务真正启动之后再连接，此时就可以连接进去了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2:&#x2F;&#x2F;localhost:10000&gt; create table t1(id int,name string);</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2:&#x2F;&#x2F;localhost:10000&gt; insert into t1(id,name) values(1,&quot;zs&quot;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">发现添加数据报错，提示匿名用户对&#x2F;tmp&#x2F;hadoop-yarn没有写权限</span><br><span class="line">解决方法有两个</span><br><span class="line">1. 给hdfs中的&#x2F;tmp&#x2F;hadoop-yarn设置777权限，让匿名用户具备权限</span><br><span class="line">可以直接给tmp及下面的所有目录设置777权限</span><br><span class="line">hdfs dfs -chmod -R 777 &#x2F;tmp</span><br><span class="line">2. 在启动beeline的时候指定一个对这个目录有操作权限的用户</span><br><span class="line">bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000 -n root</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时使用bin&#x2F;hive命令行查看也是可以的，这两种方式维护的是同一份Metastore</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在beeline后面指定hiveserver2的地址的时候，可以指定当前机器的内网ip也是可以的。(其它机器也可连)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">后面我们使用的时候我还是使用hive命令，已经习惯用这个了，还有一个就是大家如果也用这个的话，别</span><br><span class="line">人是不是感觉你也是老司机了，但是你要知道官方目前是推荐使用beeline命令的</span><br><span class="line">在工作中我们如果遇到了每天都需要执行的命令，那我肯定想要把具体的执行sql写到脚本中去执行，但</span><br><span class="line">是现在这种用法每次都需要开启一个会话，好像还没办法把命令写到脚本中。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意了，hive后面可以使用 -e 命令，这样这条hive命令就可以放到脚本中定时调度执行了因为这样每次hive都会开启一个新的会话，执行完毕以后再关闭这个会话。</span><br><span class="line"></span><br><span class="line">当然了beeline也可以，后面也是跟一个-e参数</span><br></pre></td></tr></table></figure><h3 id="JDBC方式"><a href="#JDBC方式" class="headerlink" title="JDBC方式"></a>JDBC方式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">JDBC这种方式也需要连接hiveserver2服务，前面我们已经启动了hiveserver2服务，在这里直接使用就可以了</span><br><span class="line">创建maven项目 db_hive</span><br><span class="line">在pom中添加hive-jdbc的依赖(百度mvn可以下载依赖)</span><br><span class="line"></span><br><span class="line">&lt;!-- hive-jdbc驱动 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-jdbc&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;version&gt;3.1.2&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开发代码，创建包名： com.imooc.hive</span><br><span class="line">创建类名： HiveJdbcDemo</span><br></pre></td></tr></table></figure><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.hive;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.Statement;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * JDBC代码操作 Hive</span></span><br><span class="line"><span class="comment"> * 注意：需要先启动hiveserver2服务</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveJdbcDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="comment">//指定hiveserver2的连接</span></span><br><span class="line">        String jdbcUrl = <span class="string">"jdbc:hive2://192.168.206.132:10000"</span>;</span><br><span class="line">    <span class="comment">//获取jdbc连接，这里的user使用root，就是linux中的用户名，password随便指定即</span></span><br><span class="line">        Connection conn = DriverManager.getConnection(jdbcUrl, <span class="string">"root"</span>, <span class="string">"any"</span>)</span><br><span class="line">    <span class="comment">//获取Statement</span></span><br><span class="line">        Statement stmt = conn.createStatement();</span><br><span class="line">    <span class="comment">//指定查询的sql</span></span><br><span class="line">        String sql = <span class="string">"select * from t1"</span>;</span><br><span class="line">    <span class="comment">//执行sql</span></span><br><span class="line">        ResultSet res = stmt.executeQuery(sql);</span><br><span class="line">    <span class="comment">//循环读取结果</span></span><br><span class="line">        <span class="keyword">while</span> (res.next())&#123;</span><br><span class="line">            System.out.println(res.getInt(<span class="string">"id"</span>)+<span class="string">"\t"</span>+res.getString(<span class="string">"name"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;D:&#x2F;.m2&#x2F;org&#x2F;apache&#x2F;logging&#x2F;log4j&#x2F;log4j-slf4</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;D:&#x2F;.m2&#x2F;org&#x2F;slf4j&#x2F;slf4j-log4j12&#x2F;1.6.1&#x2F;slf4j</span><br><span class="line">SLF4J: See http:&#x2F;&#x2F;www.slf4j.org&#x2F;codes.html#multiple_bindings for an explanati</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory</span><br><span class="line"></span><br><span class="line">ERROR StatusLogger No log4j2 configuration file found. Using default configur</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">分析上面的警告信息，发现现在是有两个log4j的实现类，需要去掉一个，还有就是缺少log4j2的配置文</span><br><span class="line">件，注意log4j2的配置文件是xml格式的，不是properties格式的</span><br><span class="line">1: 去掉多余的log4j依赖，从日志中可以看到日志的路径</span><br><span class="line">这两个去掉哪个都可以，这两个都是hive-jdbc这个依赖带过来的，所以需要修改pom文件中hive-jdbc中</span><br><span class="line">的依赖</span><br><span class="line">&lt;!-- hive-jdbc驱动 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">&lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class="line">&lt;artifactId&gt;hive-jdbc&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;version&gt;3.1.2&lt;&#x2F;version&gt;</span><br><span class="line">&lt;exclusions&gt;</span><br><span class="line">&lt;!-- 去掉 log4j依赖 --&gt;</span><br><span class="line">&lt;exclusion&gt;</span><br><span class="line">&lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">&lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;&#x2F;exclusion&gt;</span><br><span class="line">&lt;&#x2F;exclusions&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">2：在项目的resources目录中增加log4j2.xml配置文件</span><br><span class="line"></span><br><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;Configuration status&#x3D;&quot;INFO&quot;&gt;</span><br><span class="line">&lt;Appenders&gt;</span><br><span class="line">&lt;Console name&#x3D;&quot;Console&quot; target&#x3D;&quot;SYSTEM_OUT&quot;&gt;</span><br><span class="line">&lt;PatternLayout pattern&#x3D;&quot;%d&#123;YYYY-MM-dd HH:mm:ss&#125; [%t] %-5p %c&#123;1&#125;:%</span><br><span class="line">&lt;&#x2F;Console&gt;</span><br><span class="line">&lt;&#x2F;Appenders&gt;</span><br><span class="line">&lt;Loggers&gt;</span><br><span class="line">&lt;Root level&#x3D;&quot;info&quot;&gt;</span><br><span class="line">&lt;AppenderRef ref&#x3D;&quot;Console&quot; &#x2F;&gt;</span><br><span class="line">&lt;&#x2F;Root&gt;</span><br><span class="line">&lt;&#x2F;Loggers&gt;</span><br><span class="line">&lt;&#x2F;Configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="Set命令的使用"><a href="#Set命令的使用" class="headerlink" title="Set命令的使用"></a>Set命令的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在hive命令行中可以使用set命令临时设置一些参数的值，</span><br><span class="line">其实就是临时修改hive-site.xml中参数的值</span><br><span class="line">不过通过set命令设置的参数只在当前会话有效，退出重新打开就无效了</span><br><span class="line">如果想要对当前机器上的当前用户有效的话可以把命令配置在 ~&#x2F;.hiverc文件中</span><br><span class="line">所以总结一下，使用set命令配置的参数是当前会话有效，在~&#x2F;.hiverc文件中配置的是当前机器中的当前用户有效，而在hive-site.xml中配置的则是永久有效了，</span><br><span class="line">在hive-site.xml中有一个参数是 hive.cli.print.current.db ，这个参数可以显示当前所在的数据库名称，默认值为 false 。</span><br></pre></td></tr></table></figure><h4 id="显示数据库名"><a href="#显示数据库名" class="headerlink" title="显示数据库名"></a>显示数据库名</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.cli.print.current.db &#x3D; true;</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure><h4 id="显示列属性"><a href="#显示列属性" class="headerlink" title="显示列属性"></a>显示列属性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from t1;</span><br><span class="line">OK</span><br><span class="line">1 zs</span><br><span class="line">Time taken: 0.184 seconds, Fetched: 1 row(s)</span><br><span class="line">hive (default)&gt; set hive.cli.print.header &#x3D; true;</span><br><span class="line">hive (default)&gt; select * from t1;</span><br><span class="line">OK</span><br><span class="line">t1.id t1.name</span><br><span class="line">1 zs</span><br><span class="line">Time taken: 0.202 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">些参数属于我自己的个人习惯，所以我希望把这个配置放到我个人用户下面</span><br><span class="line">修改 ~&#x2F;.hiverc ，我们每次在进入hive命令行的时候都会加载当前用户目录下的 .hiverc 文件中的内容</span><br></pre></td></tr></table></figure><h4 id="历史命令"><a href="#历史命令" class="headerlink" title="历史命令"></a>历史命令</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如果我们想查看一下hive的历史操作命令如何查看呢？</span><br><span class="line">linux中有一个history命令可以查看历史操作命令hive中也有类似的功能，hive中的历史命令会存储在当前用户目录下的 .hivehistory 目录中</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# more ~&#x2F;.hivehistory</span><br><span class="line">show tables;</span><br><span class="line">exit</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><h2 id="Hive的日志配置"><a href="#Hive的日志配置" class="headerlink" title="Hive的日志配置"></a>Hive的日志配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;lib&#x2F;log4jSLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0&#x2F;share&#x2F;hadoop&#x2F;common</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">我们每次进入hive命令行的时候都会出现这么一坨日志，看着很恶心，想要去掉，怎么办呢？</span><br><span class="line">通过分析日志可知，现在也是有重复的日志依赖，所以需要删除一个，</span><br><span class="line">这里是hive中的一个日志依赖包和hadoop中的日志依赖包冲入了，那我们只能去掉Hive的了，因为hadoop是共用的，尽量不要删它里面的东西。</span><br><span class="line">为了保险起见，我们可以使用mv给这个日志依赖包重命名，这样它就不生效了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">还有就是当我们遇到Hive执行发生错误的时候，我们要学会去查看Hive的日志信息，通过日志的提示来分析，找到错误的根源，帮助我们及时解决错误。</span><br><span class="line">那我们在哪里查看Hive日志呢，我们可以通过配置文件来找到默认日志文件所在的位置。</span><br><span class="line">在hive的conf目录下有一些log4j的模板配置文件，我们需要去修改一下，让它生效。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">首先是 hive-log4j.properties.template 这个文件，去掉 .template 后缀，修改里面的 property.hive.log.level 和 property.hive.log.dir 参数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# mv hive-log4j2.properties.template hive-log4j2.propert</span><br><span class="line">[root@bigdata04 conf]# vi hive-log4j2.properties</span><br><span class="line">property.hive.log.level &#x3D; WARN</span><br><span class="line">property.hive.root.logger &#x3D; DRFA</span><br><span class="line">property.hive.log.dir &#x3D; &#x2F;data&#x2F;hive_repo&#x2F;log</span><br><span class="line">property.hive.log.file &#x3D; hive.log</span><br><span class="line">property.hive.perflogger.log.level &#x3D; INFO</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">然后修改 hive-exec-log4j2.properties.template 这个文件，去掉 .template 后缀修改里面的</span><br><span class="line"></span><br><span class="line">[root@bigdata04 conf]# mv hive-exec-log4j2.properties.template hive-exec-log4</span><br><span class="line">[root@bigdata04 conf]# vi hive-exec-log4j2.properties</span><br><span class="line">property.hive.log.level &#x3D; WARN</span><br><span class="line">property.hive.root.logger &#x3D; FA</span><br><span class="line">property.hive.query.id &#x3D; hadoop</span><br><span class="line">property.hive.log.dir &#x3D; &#x2F;data&#x2F;hive_repo&#x2F;log</span><br><span class="line">property.hive.log.file &#x3D; $&#123;sys:hive.query.id&#125;.log</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这样后期分析日志就可以到 &#x2F;data&#x2F;hive_repo&#x2F;log 目录下去查看了。</span><br><span class="line"></span><br><span class="line">提交任务后，在mapreduce产生的一些日志，还是要去yarn的weibu界面看</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第2章 数据库与数据仓库的区别</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB.html</id>
    <published>2022-02-20T04:01:04.000Z</published>
    <updated>2022-02-21T08:47:05.460Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第2章-数据库与数据仓库的区别"><a href="#第八周-第2章-数据库与数据仓库的区别" class="headerlink" title="第八周 第2章 数据库与数据仓库的区别"></a>第八周 第2章 数据库与数据仓库的区别</h1><h2 id="Hive-VS-Mysql"><a href="#Hive-VS-Mysql" class="headerlink" title="Hive VS Mysql"></a>Hive VS Mysql</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">为了加深对Hive的理解，下面我们拿Hive和我们经常使用的Mysql做一个对比</span><br><span class="line"></span><br><span class="line">           HIVE MySQL</span><br><span class="line">数据存储位置 HDFS 本地磁盘</span><br><span class="line">数据格式 用户定义 系统决定</span><br><span class="line">数据更新 不支持(不支持修改和删除,新增) 支持(支持增删</span><br><span class="line">索引   有，但较弱，一般很少用 有，经常使用</span><br><span class="line">执行   MapReduce Executor</span><br><span class="line">执行延迟 高 低</span><br><span class="line">可扩展性 高 低</span><br><span class="line">数据规模 大 小</span><br></pre></td></tr></table></figure><h2 id="数据库-VS-数据仓库"><a href="#数据库-VS-数据仓库" class="headerlink" title="数据库 VS 数据仓库"></a>数据库 VS 数据仓库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">前面我们说了Hive是一个数据仓库，咱们平时经常使用的mysql属于数据库，那数据库和数据仓库到底有什么区别呢？</span><br><span class="line">下面我们来分析一下</span><br><span class="line">数据库：传统的关系型数据库主要应用在基本的事务处理，例如银行交易之类的场景</span><br><span class="line">数据库支持增删改查这些常见的操作。</span><br><span class="line">数据仓库：主要做一些复杂的分析操作，侧重决策支持，相对数据库而言，数据仓库分析的数据规模要大得多。但是数据仓库只支持查询操作，不支持修改和删除</span><br><span class="line">这些都是明面上的一些区别</span><br><span class="line">其实数据库与数据仓库的本质区别就是 OLTP与OLAP 的区别</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive和关系数据库存储文件的系统不同，hive使用的是hadoop的HDFS（hadoop的分布式文件系统），关系数据库则是服务器本地的文件 系统</span><br></pre></td></tr></table></figure><h3 id="OLTP-VS-OLAP"><a href="#OLTP-VS-OLAP" class="headerlink" title="OLTP VS OLAP"></a>OLTP VS OLAP</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那这里的OLTO和OLAP又是什么意思呢？</span><br><span class="line">OLTP(On-Line Transaction Processing)：操作型处理，称为联机事务处理，也可以称为面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性等问题</span><br><span class="line">OLAP(On-Line Analytical Processing)：分析型处理，称为联机分析处理，一般针对某些主题历史数据进行分析，支持管理决策。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">其实从字面上来对比，OLTP 和 OLAP 只有一个单词不一样</span><br><span class="line">OLTP侧重于事务，OLAP侧重于分析</span><br><span class="line">所以数据库和数据仓库的特性是不一样的，不过我们平时在使用的时候，可以把Hive作为一个数据库来操作，但是你要知道他们两个是不一样的。数据仓库的概念是比数据库要大的</span><br></pre></td></tr></table></figure><h2 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">现在我们对Hive有了基本的了解，就想使用Hive分析一下HDFS中的数据，在分析数据之前，我们需要先把Hive安装部署起来</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">想要安装Hive，那首先要下载Hive的安装包，进入Hive的官网，找到download下载链接</span><br><span class="line"></span><br><span class="line">发现目前hive主要有三大版本，Hive1.x、Hive2.x、Hive3.x</span><br><span class="line">Hive1.x已经2年没有更新了，所以这个版本后续基本不会再维护了，不过这个版本已经迭代了很多年了，也是比较稳定的</span><br><span class="line">Hive2.x最近一直在更新</span><br><span class="line">Hive3.x上次是19年8月份更新的，也算是一直在维护</span><br><span class="line">那我们到底选择哪个版本呢？注意了，在选择Hive版本的时候我们需要注意已有的Hadoop集群的版本。</span><br><span class="line">因为Hive会依赖于Hadoop，所以版本需要兼容才可以。</span><br><span class="line">具体Hive和Hadoop的版本对应关系可以在download页面下的news列表里面看到。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">按照这里面说的hive2.x的需要在hadoop2.x版本中运行，hive3.x的需要在hadoop3.x版本中运行。</span><br><span class="line">所以在这里我们最好是使用Hive3.x的版本</span><br><span class="line">那我们就下载hive-3.1.2这个版本，如果想要下载其它历史版本的话这里面还找不到，不过可以使用</span><br><span class="line">apache的一个通用archive地址</span><br><span class="line">https:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;hive&#x2F;</span><br><span class="line">在这里面就可以找到hive的所有历史版本了</span><br></pre></td></tr></table></figure><h3 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面开始安装Hive</span><br><span class="line">Hive相当于Hadoop的客户端工具，安装时不一定非要放在集群的节点中，可以放在任意一个集群客户端节点上都可以</span><br><span class="line">1.下载，上传，解压</span><br><span class="line">2.修改配置文件</span><br><span class="line">需要修改配置文件，进入hive的conf目录中，先对这两个模板文件重命名</span><br><span class="line"></span><br><span class="line">[root@bigdata04 conf]# mv hive-env.sh.template hive-env.sh</span><br><span class="line">[root@bigdata04 conf]# mv hive-default.xml.template hive-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">3：然后再修改这两个文件的内容</span><br><span class="line">注意：在 hive-env.sh 文件的末尾直接增加下面三行内容，【根据实际的路径配置】</span><br><span class="line"></span><br><span class="line">[root@bigdata04 conf]# vi hive-env.sh</span><br><span class="line">.....</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注意：在hive-site.xml文件中根据下面property中的name属性的值修改对应value的值，这些属</span><br><span class="line">性默认里面都是有的，所以都是修改对应的value的值即可</span><br><span class="line">由于这里面需要指定Metastore的地址，Metastore我们使用Mysql，所以需要大家提前安装好</span><br><span class="line">Mysql，我这里使用的是Mysql8.0.16版本，Mysql安装包会提供给大家，建议大家直接在自己的</span><br><span class="line">windows机器中安装Mysql即可，当然了，你在Linux中安装也可以。</span><br><span class="line">我这里Mysql的用户名是root、密码是admin，在下面的配置中会使用到这些信息，大家可以根据</span><br><span class="line">自己实际的用户名和密码修改这里面的value的值</span><br></pre></td></tr></table></figure><h3 id="mysql安装"><a href="#mysql安装" class="headerlink" title="mysql安装"></a>mysql安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于这里面需要指定Metastore的地址，Metastore我们使用Mysql，所以需要大家提前安装好Mysql，我这里使用的是Mysql8.0.16版本，Mysql安装包会提供给大家，建议大家直接在自己的windows机器中安装Mysql即可，当然了，你在Linux中安装也可以。</span><br></pre></td></tr></table></figure><a href="/mysql%E5%AE%89%E8%A3%85.html" title="mysql安装">mysql安装</a><h3 id="hive配置修改"><a href="#hive配置修改" class="headerlink" title="hive配置修改"></a>hive配置修改</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我这里Mysql的用户名是root、密码是admin，在下面的配置中会使用到这些信息，大家可以根据自己实际的用户名和密码修改这里面的value的值</span><br></pre></td></tr></table></figure><h4 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面这个ip应该错了，应该填自己主机ip；并在mysql中创建hive数据库</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# vi hive-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;192.168.0.10:3306&#x2F;hive?serverTimezone&#x3D;Asia&#x2F;Shanghai&lt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;com.mysql.cj.jdbc.Driver&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;root&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;admin&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.querylog.location&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;data&#x2F;hive_repo&#x2F;querylog&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.exec.local.scratchdir&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;data&#x2F;hive_repo&#x2F;scratchdir&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.downloaded.resources.dir&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;&#x2F;data&#x2F;hive_repo&#x2F;resources&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h4 id="导入mysql驱动"><a href="#导入mysql驱动" class="headerlink" title="导入mysql驱动"></a>导入mysql驱动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意mysql驱动包的版本，要和我们安装的版本保持一致：mysql-connector-java-8.0.16.jar</span><br><span class="line"></span><br><span class="line">[root@bigdata04 lib]# ll</span><br><span class="line">........</span><br><span class="line">-rw-r--r--. 1 root root 2293144 Mar 20 2019 mysql-connector-java-8.0.16.jar</span><br></pre></td></tr></table></figure><h4 id="修改core-site-xml"><a href="#修改core-site-xml" class="headerlink" title="修改core-site.xml"></a>修改core-site.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改bigdata01中的core-site.xml，然后同步到集群中的另外两个节点上(客户端节点不用修改)</span><br><span class="line">如果不增加这个配置，使用beeline连接hive的时候会报错</span><br><span class="line"></span><br><span class="line">注意：bigdata04这个客户端节点上不需要修改这个配置就可以了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 hadoop]# vi core-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;*&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><h3 id="初始化Hive的Metastore"><a href="#初始化Hive的Metastore" class="headerlink" title="初始化Hive的Metastore"></a>初始化Hive的Metastore</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure><h4 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava&#x2F;lang&#x2F;String;Ljava&#x2F;lang&#x2F;Object;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.hadoop和hive的两个guava.jar版本不一致</span><br><span class="line">两个位置分别位于下面两个目录：</span><br><span class="line">- &#x2F;usr&#x2F;local&#x2F;hive&#x2F;lib&#x2F;guava-19.0.jar</span><br><span class="line">- &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;guava-27.0-jre.jar</span><br><span class="line"></span><br><span class="line">直接把hive里的移走，把hadoop里的复制过来，完美解决</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2.SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;lib&#x2F;log4j-slf4j-impl-2.10.0.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.2&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;slf4j-log4j12-1.7.25.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这两个jar不同，但方法有相同的，不用管</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">3.</span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;schematool -dbType mysql -initSch</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.RuntimeException: com.ctc.wstx.exc.WstxP</span><br><span class="line">at [row,col,system-id]: [3215,96,&quot;file:&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;conf</span><br><span class="line"></span><br><span class="line">但是执行之后发现报错了，提示hive-site.xml文件中的第3215行内容有问题</span><br><span class="line">其实这个是原始配置文件本身就有的问题，最直接的就是把这一行直接删掉，删除之后的效果如下：其实就是把hive.txn.xlock.iow对应的description标签内容删掉，这样就可以了</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.txn.xlock.iow&lt;&#x2F;name&gt;</span><br><span class="line">&lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;description&gt;</span><br><span class="line">&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">4.</span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;schematool -dbType mysql -initSchema</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;lib&#x2F;log4j-slf4j-impl-2.10.0.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.2&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;slf4j-log4j12-1.7.25.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http:&#x2F;&#x2F;www.slf4j.org&#x2F;codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Metastore connection URL:        jdbc:mysql:&#x2F;&#x2F;192.168.206.1:3306&#x2F;hive?serverTimezone&#x3D;Asia&#x2F;Shanghai</span><br><span class="line">Metastore Connection Driver :    com.mysql.cj.jdbc.Driver</span><br><span class="line">Metastore connection User:       root</span><br><span class="line">org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.</span><br><span class="line">Underlying cause: com.mysql.cj.jdbc.exceptions.CommunicationsException : Communications link failure</span><br><span class="line"></span><br><span class="line">The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.</span><br><span class="line">SQL Error code: 0</span><br><span class="line">Use --verbose for detailed stacktrace.</span><br><span class="line">*** schemaTool failed ***</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.打开&#x2F;usr&#x2F;local&#x2F;hive&#x2F;conf&#x2F;hive-site.xml</span><br><span class="line"></span><br><span class="line">hive配置文件hive-site.xml中ConnectionURL中加上serverTimezone&#x3D;GMT就可以了。</span><br><span class="line"></span><br><span class="line">2.&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;192.168.52.26:3306&#x2F;hive&lt;&#x2F;value&gt;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第八周 第1章 快速了解Hive</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC1%E7%AB%A0-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3Hive.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%85%AB%E5%91%A8-%E7%AC%AC1%E7%AB%A0-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3Hive.html</id>
    <published>2022-02-20T04:00:28.000Z</published>
    <updated>2022-02-20T05:09:02.153Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第八周-第1章-快速了解Hive"><a href="#第八周-第1章-快速了解Hive" class="headerlink" title="第八周 第1章 快速了解Hive"></a>第八周 第1章 快速了解Hive</h1><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Hive是建立在Hadoop上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载，可以简称为ETL。</span><br><span class="line">Hive 定义了简单的类SQL查询语言，称为HQL，它允许熟悉SQL的用户直接查询Hadoop中的数据，同时，这个语言也允许熟悉MapReduce的开发者开发自定义的mapreduce任务来处理内建的SQL函数无法完成的复杂的分析任务。</span><br><span class="line">Hive中包含的有SQL解析引擎，它会将SQL语句转译成M&#x2F;R Job,然后在Hadoop中执行。</span><br><span class="line">通过这里的分析我们可以了解到Hive可以通过sql查询Hadoop中的数据，并且sql底层也会转化成mapreduce任务，所以hive是基于hadoop的。</span><br></pre></td></tr></table></figure><h2 id="Hive的数据存储"><a href="#Hive的数据存储" class="headerlink" title="Hive的数据存储"></a>Hive的数据存储</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Hive的数据存储基于Hadoop的 HDFS</span><br><span class="line">Hive没有专门的数据存储格式</span><br><span class="line">Hive默认可以直接加载文本文件（TextFile），还支持SequenceFile、RCFile等文件格式</span><br><span class="line">针对普通文本数据，我们在创建表时，只需要指定数据的列分隔符与行分隔符，Hive即可解析里面的数据</span><br></pre></td></tr></table></figure><h2 id="Hive的系统架构"><a href="#Hive的系统架构" class="headerlink" title="Hive的系统架构"></a>Hive的系统架构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面我们来分析一下Hive的系统架构</span><br><span class="line">看这个图，下面表示是Hadoop集群，上面是Hive，从这也可以看出来Hive是基于Hadoop的。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HLJa4O" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/20/HLJa4O.md.png" alt="HLJa4O.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">用户接口，包括 CLI、JDBC&#x2F;ODBC、WebGUI</span><br><span class="line">CLI，即Shell命令行，表示我们可以通过shell命令行操作Hive</span><br><span class="line">JDBC&#x2F;ODBC 是 Hive 的Java操作方式，与使用传统数据库JDBC的方式类似</span><br><span class="line">WebUI是通过浏览器访问 Hive</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">元数据存储(Metastore)，注意：这里的存储是名词，Metastore表示是一个存储系统</span><br><span class="line">Hive中的元数据包括表的相关信息，Hive会将这些元数据存储在Metastore中，目前Metastore只支持 mysql、derby</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Driver：包含：编译器、优化器、执行器</span><br><span class="line">编译器、优化器、执行器可以完成 Hive的 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划最终存储在 HDFS 中，并在随后由MapReduce 调用执行</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Hadoop：Hive会使用 HDFS 进行存储，利用 MapReduce 进行计算</span><br><span class="line">Hive 的数据存储在 HDFS 中，大部分的查询由 MapReduce 完成（特例 select * from table 不会生</span><br><span class="line">成 MapRedcue 任务，如果在SQL语句后面再增加where过滤条件就会生成MapReduce任务了。）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  在这有一点需要注意的，就是从Hive2开始，其实官方就不建议默认使用MapReduce引擎了，而是建议使用Tez引擎或者是Spark引擎，不过目前一直到最新的3.x版本中mapreduce还是默认的执行引擎</span><br><span class="line">  其实大数据计算引擎是有几个发展阶段的，</span><br><span class="line">  首先是第一代大数据计算引擎：MapReduce</span><br><span class="line">  接着是第二代大数据计算引擎：Tez，Tez的存在感比较低，它是源于MapReduce，主要和Hive结合在一起使用，它的核心思想是将Map和Reduce两个操作进一步拆分，这些分解后的元操作可以灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可以形成一个大的作业，这样可以提高计算效率，我们在实际工作中Hive使用的就是 Tez引擎，替换Hive的执行引擎也很简单，只需要把Tez安装好（Tez也是支持在YARN上执行的），然后到Hive中配置一下就可以了，不管使用什么引擎，不会对我们使用hive造</span><br><span class="line">成什么影响，也就说对上层的使用没有影响</span><br><span class="line">  接着是第三代大数据计算引擎：Spark，Spark在当时属于一个划时代的产品，改变了之前基于磁盘的计算思路，而是采用内存计算，就是说Spark把数据读取过来以后，中间的计算结果是不会进磁盘的，一直到出来最终结果，才会写磁盘，这样就大大提高了计算效率，而MapReduce的中间结果是会写磁盘的，所以效率没有Spark高。Spark的执行效率号称比MapReduce 快100倍，当然这需要在一定数据规模下才会差这么多，如果我们就计算几十兆或者几百兆的文件，你去对比发现其实也不会差多少，后面我们也会学到Spark这个基于内存的大数据计算引擎</span><br><span class="line"></span><br><span class="line">注意：spark也是支持在YARN上执行的</span><br><span class="line"></span><br><span class="line">其实目前还有第四代大数据计算引擎，：Flink，Flink是一个可以支持纯实时数据计算的计算引擎，在实时计算领域要优于Saprk，Flink和Spark其实是有很多相似之处，在某些方面他们两个属于互相参考，互相借鉴，互相成长，Flink后面我们也会学到，等后面我们讲到这个计算引擎的时候再详细分析。</span><br><span class="line"></span><br><span class="line">注意：Flink也是支持在YARN上执行的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">所以发现没有，MapReduce、Tez、Spark、Flink这些计算引擎都是支持在yarn上执行的，所以说Hdoop2中对架构的拆分是非常明智的。</span><br><span class="line">解释完这些名词之后其实我们就对这个架构有了一个基本理解，</span><br><span class="line">再看来这个图</span><br><span class="line">用户通过接口传递Hive SQL，然后经过Driver对SQL进行分析、编译，生成查询计划，查询计划会存储在HDFS中，然后再通过MapReduce进行计算出结果，这就是整个大的流程。</span><br><span class="line">其实在这里我们可以发现，Hive这个哥们是既不存储数据，也不计算数据，这些活都给了Hadoop来干，Hive底层最核心的东西其实就是Driver这一块，将SQL语句解析为最终的查询计划。</span><br></pre></td></tr></table></figure><h3 id="Metastore"><a href="#Metastore" class="headerlink" title="Metastore"></a>Metastore</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">接着来看一下Hive中的元数据存储，Metastore</span><br><span class="line">Metastore是Hive元数据的集中存放地。</span><br><span class="line">Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在的hdfs目录等</span><br><span class="line">Metastore默认使用内嵌的derby数据库</span><br><span class="line">Derby数据库的缺点：在同一个目录下一次只能打开一个会话</span><br><span class="line">使 用 derby 存 储 方 式 时 ， Hive 会 在 当 前 目 录 生 成 一 个 derby.log 文 件 和 一 个 metastore_db 目 录 ，metastore_db里面会存储具体的元数据信息</span><br><span class="line">如果下次切换到一个另一个新目录访问Hive，则会重新生成derby.log文件metastore_db目录，这样就没有办法使用之前的元数据信息了。</span><br><span class="line">推荐使用MySQL作为外置存储引擎，可以支持多用户同时访问以及元数据共享。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="http://tianyong.fun/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>写linux的shell脚本方法积累</title>
    <link href="http://tianyong.fun/%E5%86%99linux%E7%9A%84shell%E8%84%9A%E6%9C%AC%E6%96%B9%E6%B3%95%E7%A7%AF%E7%B4%AF.html"/>
    <id>http://tianyong.fun/%E5%86%99linux%E7%9A%84shell%E8%84%9A%E6%9C%AC%E6%96%B9%E6%B3%95%E7%A7%AF%E7%B4%AF.html</id>
    <published>2022-02-18T06:58:58.000Z</published>
    <updated>2022-02-20T03:00:27.913Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="写linux的shell脚本方法积累"><a href="#写linux的shell脚本方法积累" class="headerlink" title="写linux的shell脚本方法积累"></a>写linux的shell脚本方法积累</h1><h1 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 循环向文件中生成数据</span><br><span class="line"># 下面括号里没空格会报错</span><br><span class="line">while [ &quot;1&quot;&#x3D;&quot;1&quot; ]</span><br><span class="line">do</span><br><span class="line">        # 获取当前时间戳</span><br><span class="line">        curr_time&#x3D;&#96;date +%s&#96;</span><br><span class="line">        # 获取当前主机名</span><br><span class="line">        name&#x3D;&#96;hostname&#96;</span><br><span class="line">        echo $&#123;name&#125;_$&#123;curr_time&#125; &gt;&gt; &#x2F;data&#x2F;log&#x2F;networkLogUploadToHdfsExample&#x2F;access.l</span><br><span class="line">og</span><br><span class="line">        sleep 1</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h2 id="案例二-写Flume进程的监控脚本"><a href="#案例二-写Flume进程的监控脚本" class="headerlink" title="案例二 写Flume进程的监控脚本"></a>案例二 写Flume进程的监控脚本</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">monlist &#x3D; &#96;cat monlist.conf&#96;</span><br><span class="line">echo &quot;start check&quot;</span><br><span class="line">for item in $&#123;monlist&#125;</span><br><span class="line">do</span><br><span class="line">    # 设置字段分隔符</span><br><span class="line">    OLD_IFS&#x3D;$IFS</span><br><span class="line">    IFS&#x3D;&quot;&#x3D;&quot;</span><br><span class="line">    # 把一行内容转成多列[数组] </span><br><span class="line">    arr&#x3D;($item) </span><br><span class="line">    # 获取等号左边的内容</span><br><span class="line">    name&#x3D;$&#123;arr[0]&#125;</span><br><span class="line">    # 获取等号右边的内容</span><br><span class="line">    script&#x3D;$&#123;arr[1]&#125;</span><br><span class="line">    echo &quot;time is:&quot;&#96;date +&quot;%Y-%m-%d %H:%M:%S&quot;&#96;&quot; check &quot;$name</span><br><span class="line">    if [ &#96;jps -m|grep $name | wc -l&#96; -eq 0 ]</span><br><span class="line">    then</span><br><span class="line">    # 发短信或者邮件告警</span><br><span class="line">    echo &#96;date +&quot;%Y-%m-%d %H:%M:%S&quot;&#96;$name &quot;is none&quot;</span><br><span class="line">    sh -x .&#x2F;$&#123;script&#125;</span><br><span class="line">    fi</span><br><span class="line">    done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 myconfFile]# vim monlist.conf </span><br><span class="line">load-failover.conf &#x3D; startExample.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 myconfFile]# vim startExample.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">flume_path &#x3D; &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin</span><br><span class="line">nohup $&#123;flume_path&#125;&#x2F;bin&#x2F;flume-ng --name a1 --conf $&#123;flume_path&#125;&#x2F;conf --conf-file $&#123;flume_</span><br><span class="line">path&#125;&#x2F;conf&#x2F;myconfFile&#x2F;load-failover.conf &amp;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="linux" scheme="http://tianyong.fun/categories/linux/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第七周 第2章 极速上手Flume使用 采集网络日志上传到HDFS</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8-%E9%87%87%E9%9B%86%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97%E4%B8%8A%E4%BC%A0%E5%88%B0HDFS.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8-%E9%87%87%E9%9B%86%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97%E4%B8%8A%E4%BC%A0%E5%88%B0HDFS.html</id>
    <published>2022-02-18T04:47:46.000Z</published>
    <updated>2022-02-20T04:05:22.671Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第七周-第2章-极速上手Flume使用-采集网络日志上传到HDFS"><a href="#第七周-第2章-极速上手Flume使用-采集网络日志上传到HDFS" class="headerlink" title="第七周 第2章 极速上手Flume使用 采集网络日志上传到HDFS"></a>第七周 第2章 极速上手Flume使用 采集网络日志上传到HDFS</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面我们讲了两个案例的使用，接下来看一个稍微复杂一点的案例：</span><br><span class="line">需求是这样的，</span><br><span class="line">1. 将A和B两台机器实时产生的日志数据汇总到机器C中</span><br><span class="line">2. 通过机器C将数据统一上传至HDFS的指定目录中</span><br><span class="line">注意：HDFS中的目录是按天生成的，每天一个目录</span><br><span class="line">看下面这个图，来详细分析一下</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HTp8o9" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTp8o9.md.png" alt="HTp8o9.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">根据刚才的需求分析可知，我们一共需要三台机器</span><br><span class="line">这里使用bigdata02和bigdata03采集当前机器上产生的实时日志数据，统一汇总到bigdata04机器上。</span><br><span class="line">其中bigdata02和bigdata03中的source使用基于file的source，ExecSource，因为要实时读取文件中的新增数据channel在这里我们使用基于内存的channel，因为这里是采集网站的访问日志，就算丢一两条数据对整体结果影响也不大，我们只希望采集到的数据可以快读进入hdfs中，所以就选择了基于内存的channel。</span><br><span class="line">由于bigdata02和bigdata03的数据需要快速发送到bigdata04中，为了快速发送我们可以通过网络直接传输，sink建议使用avrosink，avro是一种数据序列化系统，经过它序列化的数据传输起来效率更高，并且它对应的还有一个avrosource，avrosink的数据可以直接发送给avrosource，所以他们可以无缝衔接。</span><br><span class="line">这样bigdata04的source就确定了 使用avrosource、channel还是基于内存的channel，sink就使用hdfssink，因为是要向hdfs中写数据的。</span><br><span class="line">这里面的组件，只有execsource、avrosource、avrosink我们还没有使用过，其他的组件都使用过了。</span><br><span class="line">最终需要在每台机器上启动一个agent，启动的时候需要注意先后顺序，先启动bigdata04上面的，再启动bigdata02和bigdata03上面的。</span><br></pre></td></tr></table></figure><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><h3 id="bigdata02"><a href="#bigdata02" class="headerlink" title="bigdata02"></a>bigdata02</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h3 id="bigdata03"><a href="#bigdata03" class="headerlink" title="bigdata03"></a>bigdata03</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：bigdata02和bigdata03中配置的a1.sinks.k1.port 的值45454需要和bigdata04中配置的一致</span><br></pre></td></tr></table></figure><h3 id="bigdata02-conf"><a href="#bigdata02-conf" class="headerlink" title="bigdata02 conf"></a>bigdata02 conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata02 conf]# vim file-to-avro-101.conf</span><br><span class="line"></span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;networkLogUploadToHdfsExample&#x2F;access.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type &#x3D; avro</span><br><span class="line"># ip也行</span><br><span class="line">a1.sinks.k1.hostname &#x3D; bigdata04</span><br><span class="line"># 端口没用过的就行</span><br><span class="line">a1.sinks.k1.port &#x3D; 45454</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels&#x3D;c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h3 id="bigdata03-conf"><a href="#bigdata03-conf" class="headerlink" title="bigdata03 conf"></a>bigdata03 conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata03 conf]# vim file-to-avro-102.conf</span><br><span class="line"></span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;networkLogUploadToHdfsExample&#x2F;access.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type &#x3D; avro</span><br><span class="line"># ip也行</span><br><span class="line">a1.sinks.k1.hostname &#x3D; bigdata04</span><br><span class="line"># 端口没用过的就行</span><br><span class="line">a1.sinks.k1.port &#x3D; 45454</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels&#x3D;c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h3 id="bigdata04-conf"><a href="#bigdata04-conf" class="headerlink" title="bigdata04 conf"></a>bigdata04 conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这台机器我们已经安装过Flume了，所以直接配置Agent即可</span><br><span class="line">在指定Agent中sink配置的时候注意，我们的需求是需要按天在hdfs中创建目录，并把当天的数据上传到当天的日期目录中，这也就意味着hdfssink中的path不能写死，需要使用变量，动态获取时间，查看官方文档可知，在hdfs的目录中需要使用%Y%m%d</span><br><span class="line">在这还有一点需要注意的，因为我们这里需要抽取时间，这个时间其实是需要从数据里面抽取，咱们前面说过数据的基本单位是Event，Event是一个对象，后面我们会详细分析，在这里大家先知道它里面包含的既有我们采集到的原始的数据，还有一个header属性，这个header属性是一个key-value结构的，我们现在抽取时间就需要到event的header中抽取，但是默认情况下event的header中是没有日期的，强行抽取是会报错的，会提示抽取不到，返回空指针异常。</span><br><span class="line">java.lang.NullPointerException: Expected timestamp in the Flume event headers, but it was n</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那如何向header中添加日期呢？ 其实官方文档中也说了，可以使用hdfs.useLocalTimeStamp或者时间拦截器，时间拦截器我们后面会讲，暂时最简单直接的方式就是使用hdfs.useLocalTimeStamp，这个属性的值默认为false，需要改为true。</span><br></pre></td></tr></table></figure><p><img src="C:%5CUsers%5CTy%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20220218163730584.png" alt="image-20220218163730584"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# vim avro-to-hdfs.conf </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sources.r1.type &#x3D; avro</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 45454</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flume&#x2F;networkLogUploadToHdfsExaple&#x2F;%Y%m%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; access-</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix&#x3D;.log</span><br><span class="line">a1.sinks.k1.hdfs.fileType&#x3D;DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat&#x3D;Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval&#x3D;3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize&#x3D;134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount&#x3D;0</span><br><span class="line">#(不设置前面使用时间变量会报错)使用event header里的timestamp要麻烦些</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp&#x3D;true</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels&#x3D;c1</span><br><span class="line">a1.sinks.k1.channel&#x3D;c1</span><br></pre></td></tr></table></figure><h2 id="模拟数据源"><a href="#模拟数据源" class="headerlink" title="模拟数据源"></a>模拟数据源</h2><h3 id="bigdata02-1"><a href="#bigdata02-1" class="headerlink" title="bigdata02"></a>bigdata02</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata02 log]# vim SimulateData.sh </span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 循环向文件中生成数据</span><br><span class="line">while [ &quot;1&quot;&#x3D;&quot;1&quot; ]</span><br><span class="line">do</span><br><span class="line">        # 获取当前时间戳</span><br><span class="line">        curr_time&#x3D;&#96;date +%s&#96;</span><br><span class="line">        # 获取当前主机名</span><br><span class="line">        name&#x3D;&#96;hostname&#96;</span><br><span class="line">        echo $&#123;name&#125;_$&#123;curr_time&#125; &gt;&gt; &#x2F;data&#x2F;log&#x2F;networkLogUploadToHdfsExample&#x2F;access.log</span><br><span class="line">        sleep 1</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h3 id="bigdata03-1"><a href="#bigdata03-1" class="headerlink" title="bigdata03"></a>bigdata03</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">同上</span><br></pre></td></tr></table></figure><h2 id="启动进程"><a href="#启动进程" class="headerlink" title="启动进程"></a>启动进程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来开始启动相关的服务进程</span><br><span class="line">首先启动bigdata04上的agent服务</span><br><span class="line">接下来启动bigdata-02上的agent服务和shell脚本</span><br><span class="line">最后启动bigdata-03上的agent服务和shell脚本</span><br></pre></td></tr></table></figure><h3 id="启动bigdata04"><a href="#启动bigdata04" class="headerlink" title="启动bigdata04"></a>启动bigdata04</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h3 id="启动bigdata02"><a href="#启动bigdata02" class="headerlink" title="启动bigdata02"></a>启动bigdata02</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h3 id="启动bigdata03"><a href="#启动bigdata03" class="headerlink" title="启动bigdata03"></a>启动bigdata03</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.shell脚本</span><br><span class="line">while [ 1&#x3D;1 ] </span><br><span class="line"># 两个空格不能少</span><br><span class="line"></span><br><span class="line">2.flume配置</span><br><span class="line">注释不要写在语句的同一行后面</span><br><span class="line"></span><br><span class="line">3.启动agent时</span><br><span class="line">先启动bigdata02,bigdata03和先关bigdata04都会造成数据流失</span><br></pre></td></tr></table></figure><h2 id="结果查看"><a href="#结果查看" class="headerlink" title="结果查看"></a>结果查看</h2><p><a href="https://imgtu.com/i/HTfiPs" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTfiPs.md.png" alt="HTfiPs.md.png"></a></p><p><a href="https://imgtu.com/i/HTfGM6" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTfGM6.md.png" alt="HTfGM6.md.png"></a><br><a href="https://imgtu.com/i/HTf3xx" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTf3xx.md.png" alt="HTf3xx.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：启动之后稍等一会就可以看到数据了，我们观察数据的变化，会发现hdfs中数据增长的不是很快，它会每隔一段时间添加一批数据，实时性好像没那么高？</span><br><span class="line">这是因为avrosink中有一个配置batch-size，它的默认值是100，也就是每次发送100条数据，如果数据不够100条，则不发送。</span><br><span class="line">具体这个值设置多少合适，要看你source数据源大致每秒产生多少数据，以及你希望的延迟要达到什么程度，如果这个值设置太小的话，会造成sink频繁向外面写数据，这样也会影响性能。</span><br><span class="line">最终，依次停止bigdata02、bigdata03中的服务，最后停止bigdata04中的服务</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HTIFTU" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/HTIFTU.md.png" alt="HTIFTU.md.png"></a></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第七周 第2章 极速上手Flume使用 采集文件内容到HDFS</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8-%E9%87%87%E9%9B%86%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E5%88%B0HDFS.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8-%E9%87%87%E9%9B%86%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E5%88%B0HDFS.html</id>
    <published>2022-02-18T04:18:46.000Z</published>
    <updated>2022-02-22T04:18:01.302Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第2章-极速上手Flume使用-采集文件内容到HDFS"><a href="#第2章-极速上手Flume使用-采集文件内容到HDFS" class="headerlink" title="第2章 极速上手Flume使用 采集文件内容到HDFS"></a>第2章 极速上手Flume使用 采集文件内容到HDFS</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一个工作中的典型案例：</span><br><span class="line">采集文件内容上传至HDFS</span><br><span class="line">需求：采集目录中已有的文件内容，存储到HDFS</span><br><span class="line">分析：source是要基于目录的，channel建议使用file，可以保证不丢数据，sink使用hdfs</span><br><span class="line">下面要做的就是配置Agent了，可以把example.conf拿过来修改一下，新的文件名为file-to-hdfs.conf</span><br><span class="line">首先是基于目录的source，咱们前面说过，Spooling Directory Source可以实现目录监控来看一下这个Spooling Directory Source</span><br></pre></td></tr></table></figure><h2 id="source"><a href="#source" class="headerlink" title="source"></a>source</h2><h3 id="Spooling-Directory-Source"><a href="#Spooling-Directory-Source" class="headerlink" title="Spooling Directory Source"></a>Spooling Directory Source</h3><p><a href="https://imgtu.com/i/HI4GDS" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HI4GDS.md.png" alt="HI4GDS.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">channels和type肯定是必填的，还有一个是spoolDir，就是指定一个监控的目录</span><br><span class="line">看他下面的案例，里面还多指定了一个fileHeader，这个我们暂时也用不到，后面等我们讲了Event之后</span><br><span class="line">大家就知道这个fileHeader可以干什么了，先记着有这个事把。</span><br></pre></td></tr></table></figure><h2 id="channel"><a href="#channel" class="headerlink" title="channel"></a>channel</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来是channel了</span><br><span class="line">channel在这里使用基于文件的，可以保证数据的安全性</span><br><span class="line">如果针对采集的数据，丢个一两条对整体结果影响不大，只要求采集效率，那么这个时候完全可以使用基于内存的channel</span><br><span class="line">咱们前面的例子中使用的是基于内存的channel，下面我们到文档中找一下基于文件的channel</span><br></pre></td></tr></table></figure><h3 id="File-Channel"><a href="#File-Channel" class="headerlink" title="File Channel"></a>File Channel</h3><p><a href="https://imgtu.com/i/HI5YM6" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HI5YM6.md.png" alt="HI5YM6.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">根据这里的例子可知，主要配置checkpointDir和dataDir，因为这两个目录默认会在用户家目录下生成，建议修改到其他地方</span><br><span class="line">checkpointDir是存放检查点目录</span><br><span class="line">data是存放数据的目录</span><br></pre></td></tr></table></figure><h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最后是sink</span><br><span class="line">因为要向hdfs中输出数据，所以可以使用hdfssink</span><br></pre></td></tr></table></figure><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p><a href="https://imgtu.com/i/HIozKx" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HIozKx.md.png" alt="HIozKx.md.png"></a></p><p><a href="https://imgtu.com/i/HITpqK" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HITpqK.md.png" alt="HITpqK.md.png"></a></p><p><a href="https://imgtu.com/i/HITCVO" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/HITCVO.md.png" alt="HITCVO.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs.path是必填项，指定hdfs上的存储目录</span><br><span class="line">看这里例子中还指定了filePrefix参数，这个是一个文件前缀，会在hdfs上生成的文件前面加上这个前缀，这个属于可选项，有需求的话可以加上</span><br><span class="line">一般在这我们需要设置writeFormat和fileType这两个参数</span><br><span class="line">默认情况下writeFormat的值是Writable，建议改为Text，看后面的解释，如果后期想使用hive或者impala操作这份数据的话，必须在生成数据之前设置为Text，Text表示是普通文本数据</span><br><span class="line">fileType默认是SequenceFile，还支持DataStream 和 CompressedStream ，DataStream 不会对输出数据进行压缩，CompressedStream 会对输出数据进行压缩，在这里我们先不使用压缩格式的，所以选择DataStream</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">除了这些参数以外，还有三个也比较重要</span><br><span class="line">hdfs.rollInterval、hdfs.rollSize和hdfs.rollCount</span><br><span class="line">hdfs.rollInterval默认值是30，单位是秒，表示hdfs多长时间切分一个文件，因为这个采集程序是一直运行的，只要有新数据，就会被采集到hdfs上面，hdfs默认30秒钟切分出来一个文件，如果设置为0表示不按时间切文件</span><br><span class="line">hdfs.rollSize默认是1024，单位是字节，最终hdfs上切出来的文件大小都是1024字节，如果设置为0表示不按大小切文件</span><br><span class="line">hdfs.rollCount默认设置为10，表示每隔10条数据切出来一个文件，如果设置为0表示不按数据条数切文件</span><br><span class="line">这三个参数，如果都设置的有值，哪个条件先满足就按照哪个条件都会执行。</span><br><span class="line">在实际工作中一般会根据时间或者文件大小来切分文件，我们之前在工作中是设置的时间和文件大小相结合，时间设置的是一小时，文件大小设置的128M，这两个哪个满足执行哪个</span><br><span class="line">所以针对hdfssink的配置最终是这样的</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; spooldir</span><br><span class="line">a1.sources.r1.spoolDir &#x3D; &#x2F;data&#x2F;log&#x2F;studentDir</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; file</span><br><span class="line">a1.channels.c1.checkpointDir &#x3D; &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;studentDir&#x2F;chec</span><br><span class="line">kpoint</span><br><span class="line">a1.channels.c1.dataDirs &#x3D; &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;studentDir&#x2F;data</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;192.168.206.129:9000&#x2F;flume&#x2F;studentDir</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; stu-</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h2 id="测试文件"><a href="#测试文件" class="headerlink" title="测试文件"></a>测试文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">下面就可以启动agent了，在启动agent之前，先初始化一下测试数据</span><br><span class="line">创建&#x2F;data&#x2F;log&#x2F;studentDir目录，然后在里面添加一个文件，class1.dat</span><br><span class="line">class1.dat中存储的是学生信息，学生姓名、年龄、性别</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ~]# mkdir -p &#x2F;data&#x2F;log&#x2F;studentDir</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;log&#x2F;studentDir</span><br><span class="line">[root@bigdata04 studentDir]# more class1.dat</span><br><span class="line">jack 18 male</span><br><span class="line">jessic 20 female</span><br><span class="line">tom 17 male</span><br></pre></td></tr></table></figure><h2 id="启动Agent"><a href="#启动Agent" class="headerlink" title="启动Agent"></a>启动Agent</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">启动之前，先启动hadoop</span><br><span class="line">启动Agent，使用在前台启动的方式，方便观察现象</span><br><span class="line"></span><br><span class="line">apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf --conf-file conf&#x2F;file-to-hdfs.conf -Dflume.root.logger&#x3D;INFO,console</span><br></pre></td></tr></table></figure><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2020-05-02 15:36:58,283 (conf-file-poller-0) [ERROR - org.apache.flume.node.P</span><br><span class="line">java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;io&#x2F;SequenceFile$Compression</span><br><span class="line">at org.apache.flume.sink.hdfs.HDFSEventSink.configure(HDFSEventSink.j</span><br><span class="line">at org.apache.flume.conf.Configurables.configure(Configurables.java:4</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.SequenceFil</span><br><span class="line">at java.net.URLClassLoader.findClass(URLClassLoader.java:382)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)</span><br><span class="line">at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">... 12 more</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">但是发现在启动的时候报错，提示找不到SequenceFile，但是我们已经把fileType改为了DataStream，</span><br><span class="line">但是Flume默认还是会加载这个类</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.SequenceFile$CompressionT</span><br><span class="line">ype</span><br><span class="line">就算你把SequenceFile相关的jar包都拷贝到flume的lib目录下解决了这个问题，但是还是会遇到找不到</span><br><span class="line">找不到HDFS这种文件类型，还是缺少hdfs相关的jar包</span><br><span class="line">No FileSystem for scheme: hdfs</span><br><span class="line">当然这个问题也可以通过拷贝jar包来解决这个问题，但是这样其实太费劲了，并且后期我们有很大可能需要在这个节点上操作HDFS，所以其实最简单直接的方法就是把这个节点设置为hadoop集群的一个客户端节点，这样操作hdfs就没有任何问题了。</span><br><span class="line">咱们之前在讲Hadoop的时候讲了客户端节点的特性，其实很简单，我们直接把集群中修改好配置的hadoop目录远程拷贝到bigdata04上就可以了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# scp -rq hadoop-3.2.0 192.168.182.103:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line"></span><br><span class="line">由于bigdata01和bigdata04没有做免密码登录，也不认识它的主机名，所以就使用ip，并且输入密码了。</span><br><span class="line">拷贝完成之后到bigdata04节点上验证一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意：还需要修改环境变量，配置HADOOP_HOME，否则启动Agent的时候还是会提示找不到SequenceFile</span><br><span class="line"></span><br><span class="line">[root@bigdata04 hadoop-3.2.0]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">.....</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br><span class="line">export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$PATH</span><br><span class="line">[root@bigdata04 hadoop-3.2.0]# source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><h2 id="再次启动Agent"><a href="#再次启动Agent" class="headerlink" title="再次启动Agent"></a>再次启动Agent</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">此时可以看到Agent正常启动</span><br><span class="line"></span><br><span class="line">到hdfs上验证结果</span><br><span class="line">[root@bigdata01 lib]# hdfs dfs -ls &#x2F;flume&#x2F;studentDir</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 root supergroup         47 2022-02-18 11:46 &#x2F;flume&#x2F;studentDir&#x2F;stu-.1645155976762.tmp</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">此时发现文件已经生成了，只不过默认情况下现在的文件是 .tmp 结尾的，表示它在被使用，因为Flume只要采集到数据就会向里面写，这个后缀默认是由 hdfs.inUseSuffix 参数来控制的。</span><br><span class="line">文件名上还拼接了一个当前时间戳，这个是默认文件名的格式，当达到文件切割时机的时候会给文件改名字，去掉.tmp</span><br><span class="line">这个文件现在也是可以查看的，里面的内容其实就是class1.dat文件中的内容</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -cat hdfs:&#x2F;&#x2F;192.168.182.100:9000&#x2F;flume&#x2F;studentDi</span><br><span class="line">jack 18 male</span><br><span class="line">jessic 20 female</span><br><span class="line">tom 17 male</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">所以此时Flume就会监控linux中的&#x2F;data&#x2F;log&#x2F;studentDir目录，当发现里面有新文件的时候就会把数据采集过来。</span><br><span class="line">那Flume怎么知道哪些文件是新文件呢？它会不会重复读取同一个文件的数据呢？</span><br><span class="line">不会的，我们到&#x2F;data&#x2F;log&#x2F;studentDir目录看一下你就知道了</span><br><span class="line"></span><br><span class="line">[root@bigdata04 studentDir]# cd checkpoint&#x2F;</span><br><span class="line">[root@bigdata04 checkpoint]# ls</span><br><span class="line">checkpoint  checkpoint.meta  inflightputs  inflighttakes  in_use.lock  queueset</span><br><span class="line">[root@bigdata04 checkpoint]# pwd</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;studentDir&#x2F;checkpoint</span><br><span class="line">You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 studentDir]# cd data&#x2F;</span><br><span class="line">[root@bigdata04 data]# ls</span><br><span class="line">in_use.lock  log-2       log-3.meta  log-5       log-6.meta</span><br><span class="line">log-1        log-2.meta  log-4       log-5.meta  log-7</span><br><span class="line">log-1.meta   log-3       log-4.meta  log-6       log-7.meta</span><br><span class="line">[root@bigdata04 data]# pwd</span><br><span class="line">&#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;studentDir&#x2F;data</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">发现里面有一个 log-1 的文件，这个文件中存储的其实就是读取到的内容，只不过在这无法直接查看。</span><br><span class="line">现在我们想看一下Flume最终生成的文件是什么样子的，难道要根据配置等待1个小时或者弄一个128M的文件过来吗，</span><br><span class="line">其实也没必要，我们可以暴力操作一下</span><br><span class="line">停止Agent就可以看到了，当Agent停止的时候就会去掉 .tmp 标志了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那我再重启Agent之后，会不会再给加上.tmp呢，不会了，每次停止之前都会把所有的文件解除占用状态，下次启动的时候如果有新数据，则会产生新的文件，这其实就模拟了一下自动切文件之后的效果。</span><br><span class="line">但是这个文件看起来比较别扭，连个后缀都没有，没有后缀倒不影响使用，就是看起来不好看</span><br><span class="line">在这给大家留一个作业，下一次再生成新文件的时候我希望文件名有一个后缀是.log，大家下去之后自己查看官网文档资料，修改Agent配置，添加测试数据，验证效果。</span><br><span class="line">答案：其实就是使用hdfs sink中的hdfs.fileSuffix参数</span><br></pre></td></tr></table></figure><h2 id="异常-1"><a href="#异常-1" class="headerlink" title="异常"></a>异常</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Flume v1.9.0启动报错ERROR - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:459)</span><br><span class="line"></span><br><span class="line">Hadoop 3.3.0 中的 guava 版本和 Flume 1.9.0 中的版本不一致</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/llwy1428/article/details/112169028" target="_blank" rel="external nofollow noopener noreferrer">解决</a></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flume" scheme="http://tianyong.fun/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第七周 第3章 Flume核心复盘篇</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Flume%E6%A0%B8%E5%BF%83%E5%A4%8D%E7%9B%98%E7%AF%87.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Flume%E6%A0%B8%E5%BF%83%E5%A4%8D%E7%9B%98%E7%AF%87.html</id>
    <published>2022-02-16T14:47:50.000Z</published>
    <updated>2022-02-20T04:05:51.636Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第七周 第3章 Flume出神入化篇</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Flume%E5%87%BA%E7%A5%9E%E5%85%A5%E5%8C%96%E7%AF%87.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC3%E7%AB%A0-Flume%E5%87%BA%E7%A5%9E%E5%85%A5%E5%8C%96%E7%AF%87.html</id>
    <published>2022-02-16T14:47:37.000Z</published>
    <updated>2022-02-20T04:05:44.323Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第七周-第3章-Flume出神入化篇"><a href="#第七周-第3章-Flume出神入化篇" class="headerlink" title="第七周 第3章 Flume出神入化篇"></a>第七周 第3章 Flume出神入化篇</h1><h2 id="各种自定义组件"><a href="#各种自定义组件" class="headerlink" title="各种自定义组件"></a>各种自定义组件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">咱们前面讲了很多组件，有核心组件和高级组件</span><br><span class="line">source、channel、sink以及Source Interceptors，Channel Selectors、Sink Processors</span><br><span class="line">针对这些组件，Flume都内置提供了组件的很多具体实现，在实际工作中，95%以上的数据采集需求都是</span><br><span class="line">可以满足的，但是谁也不敢保证100%都能满足，因为什么奇葩的需求都会有，那针对系统内没有提供的</span><br><span class="line">一些组件怎么办呢？</span><br><span class="line">假设我们想把flume采集到的数据输出到mysql中，那这个时候就需要有针对mysql的sink组件了，但是</span><br><span class="line">Flume中并没有，因为这种需求不常见，往mysql中写的都是结构化数据，数据的格式是固定的，但是</span><br><span class="line">flume采集的一般都是日志数据，这种属于非结构化数据，不支持也是正常的，但是我们在这里就是需要</span><br><span class="line">使用Flume往mysql中写数据，那怎么办？</span><br><span class="line">要不我们考虑换一个采集工具把，当然这也是一种解决方案，如果有其他采集工具支持向mysql中写数据</span><br><span class="line">的话那可以考虑换一个采集工具，如果所有的采集工具都不支持向mysql中写数据呢，也就是说你这个需</span><br><span class="line">求就是前无古人后无来者的，怎么破？</span><br><span class="line">不用担心，天无绝人之路，其实咱们使用的Flume提供的那些内置组件也都是作者一行代码一行代码写出</span><br><span class="line">来的，那我们是不是也可以自己写一个自定义的组件呢？可以的，并且flume也很欢迎你这样去做，它把</span><br><span class="line">开发文档什么的东西都给你准备好了。</span><br><span class="line">注意了，就算没有文档，我们也要想办法去自定义，没有文档的话就需要去抠Flume的源码了。</span><br><span class="line">在这里Flume针对自定义组件提供了详细的文档说明，我们来看一下</span><br><span class="line">通过Flume User Guide可以看到，针对source、channle、sink、Source Interceptors，Channel</span><br><span class="line">Selectors、都是可以的，这里面都显示了针对自定义的组件如何配置使用</span><br><span class="line">Sink Processors目前暂时不支持自定义。</span><br><span class="line">那这些支持自定义的组件具体开发步骤是什么样的呢？代码该写成什么样的呢？</span><br><span class="line">大家还记得Flume有两个文档链接吗？</span><br><span class="line">Flume Developer Guide</span><br><span class="line">只不过开发者文档里面目前还不算太完善，但是基本source、sink组件的自定义过程在这里都是有的</span><br><span class="line">索</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">自定义channel的内容目前还没完善，如果你确实想自定义这个组件，就需要到Flume源码中找到目前支持的那些channel的代码，参考着实现我们自定义的channel组件。</span><br><span class="line">大家在这里知道可以自定义，并且知道自定义组件的文档在哪里就可以了，目前来说，需要我们自定义组件的场景实在是太少了，几乎和买彩票中奖的概率差不多。</span><br><span class="line">前面我们掌握了Flume的基本使用和高级使用场景，下面我们来看一下针对Flume的一些企业级优化和监控手段</span><br></pre></td></tr></table></figure><h2 id="Flume优化"><a href="#Flume优化" class="headerlink" title="Flume优化"></a>Flume优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 调整Flume进程的内存大小，建议设置1G~2G，太小的话会导致频繁GC</span><br><span class="line">因为Flume进程也是基于Java的，所以就涉及到进程的内存设置，一般建议启动的单个Flume进程(或者说单个Agent)内存设置为1G~2G，内存太小的话会频繁GC，影响Agent的执行效率。</span><br><span class="line">那具体设置多少合适呢？</span><br><span class="line">这个需求需要根据Agent读取的数据量的大小和速度有关系，所以需要具体情况具体分析，当Flume的Agent启动之后，对应就会启动一个进程，我们可以通过jstat -gcutil PID 1000来看看这个进程GC的信息，每一秒钟刷新一次，如果GC次数增长过快，说明内存不够用。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">使用jps查看目前启动flume进程</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ~]# jps</span><br><span class="line">2957 Jps</span><br><span class="line">2799 Application</span><br><span class="line"></span><br><span class="line">执行 jstat -gcutil PID 1000</span><br><span class="line">[root@bigdata04 ~]# jstat -gcutil 2799 1000</span><br><span class="line">S0 S1 E O M CCS YGC YGCT FGC FGCT GCT</span><br><span class="line">100.00 0.00 17.54 42.80 96.46 92.38 8 0.029 0 0.000 0</span><br><span class="line">100.00 0.00 17.54 42.80 96.46 92.38 8 0.029 0 0.000 0</span><br><span class="line">100.00 0.00 17.54 42.80 96.46 92.38 8 0.029 0 0.000 0</span><br><span class="line">100.00 0.00 17.54 42.80 96.46 92.38 8 0.029 0 0.000 0</span><br><span class="line">100.00 0.00 17.54 42.80 96.46 92.38 8 0.029 0 0.000 0</span><br><span class="line">100.00 0.00 17.54 42.80 96.46 92.38 8 0.029 0 0.000 0</span><br><span class="line">100.00 0.00 17.54 42.80 96.46 92.38 8 0.029 0 0.000 0</span><br><span class="line">100.00 0.00 17.54 42.80 96.46 92.38 8 0.029 0 0.000 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在这里主要看YGC YGCT FGC FGCT GCT</span><br><span class="line">YGC：表示新生代堆内存GC的次数，如果每隔几十秒产生一次，也还可以接受，如果每秒都会发生一次YGC，那说明需要增加内存了</span><br><span class="line">YGCT：表示新生代堆内存GC消耗的总时间</span><br><span class="line">FGC：FULL GC发生的次数，注意，如果发生FUCC GC，则Flume进程会进入暂停状态，FUCC GC执行完以后</span><br><span class="line">Flume才会继续工作，所以FUCC GC是非常影响效率的，这个指标的值越低越好，没有更好。</span><br><span class="line">GCT：所有类型的GC消耗的总时间</span><br></pre></td></tr></table></figure><h3 id="修改Flume进程内存"><a href="#修改Flume进程内存" class="headerlink" title="修改Flume进程内存"></a>修改Flume进程内存</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如果需要调整Flume进程内存的话，需要调整 flume-env.s h脚本中的 JAVA_OPTS 参数</span><br><span class="line">把 export JAVA_OPTS 参数前面的#号去掉才会生效。</span><br><span class="line"></span><br><span class="line">export JAVA_OPTS&#x3D;&quot;-Xms1024m -Xmx1024m -Dcom.sun.management.jmxremote&quot;</span><br><span class="line"></span><br><span class="line">建议这里的 Xms 和 Xmx 设置为一样大，避免进行内存交换，内存交换也比较消耗性能。</span><br></pre></td></tr></table></figure><h3 id="一台机器多个agent时"><a href="#一台机器多个agent时" class="headerlink" title="一台机器多个agent时"></a>一台机器多个agent时</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在一台服务器启动多个agent的时候，建议修改配置区分日志文件</span><br><span class="line">因为在conf目录下有log4j.properties，在这里面指定了日志文件的名称和位置，所有使用conf目录下面</span><br><span class="line">配置启动的Agent产生的日志都会记录到同一个日志文件中，如果我们在一台机器上启动了10几个Agent，后期发现某一个Agent挂了，想要查看日志分析问题，这个时候就疯了，因为所有Agent产生的日志都混到一块了，压根都没法分析日志了。</span><br><span class="line">所以建议拷贝多个conf目录，然后修改对应conf目录中log4j.properties日志的文件名称(可以保证多个agent的日志分别存储)，并且把日志级别调整为warn(减少垃圾日志的产生)，默认info级别会记录很多日志信息。</span><br><span class="line">这样在启动Agent的时候分别通过–conf参数指定不同的conf目录，后期分析日志就方便了，每一个Agent都有一个单独的日志文件。</span><br></pre></td></tr></table></figure><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">以bigdata04机器为例：</span><br><span class="line">复制conf-failover目录，以后启动sink的failover任务的时候使用这个目录</span><br><span class="line">修改 log4j.properties中的日志记录级别和日志文件名称，日志文件目录可以不用修改，统一使用logs目录即可。</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cp -r conf&#x2F; conf-failover</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-failover&#x2F;</span><br><span class="line">[root@bigdata04 conf-failover]# vi log4j.properties</span><br><span class="line">.....</span><br><span class="line">flume.root.logger&#x3D;WARN,LOGFILE</span><br><span class="line">flume.log.dir&#x3D;.&#x2F;logs</span><br><span class="line">flume.log.file&#x3D;flume-failover.log</span><br></pre></td></tr></table></figure></code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# nohup bin&#x2F;flume-ng agent --name a1 --conf conf-failover --conf-file xxxx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这样就会在flume的logs目录中产生 flume-failover.log 文件，并且文件中只记录WARN和ERROR级别</span><br><span class="line">的日志，这样后期排查日志就很清晰了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd logs&#x2F;</span><br><span class="line">[root@bigdata04 logs]# ll</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--. 1 root root 478 May 3 16:25 flume-failover.log</span><br></pre></td></tr></table></figure><h2 id="Flume进程监控"><a href="#Flume进程监控" class="headerlink" title="Flume进程监控"></a>Flume进程监控</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Flume的Agent服务是一个独立的进程，假设我们使用source-&gt;channel-&gt;sink实现了一个数据采集落盘</span><br><span class="line">的功能，如果这个采集进程被误操作干掉了，这个时候我们是发现不了的，什么时候会发现呢？</span><br><span class="line">可能第二天，产品经理找到你了，说昨天的这个指标值有点偏低啊，你来看下怎么回事，然后你就一顿操</span><br><span class="line">作猛如虎，结果发现原始数据少了一半多，那是因为Flume的采集程序在昨天下午的时候被误操作干掉</span><br><span class="line">了。</span><br><span class="line">找到问题之后，你就苦巴巴的手工去补数据，重跑计算程序，最后再找产品经理确认数据的准确性。</span><br><span class="line">类似的问题会有很多，这说明你现在是无法掌控你手下的这些程序，他们都是不受控的状态，说不定哪天</span><br><span class="line">哪个程序不高兴，他就自杀了，不干活了，过了好几天，需要用到这个数据的时候你才发现，发现的早的</span><br><span class="line">话还能补数据，发现晚的话数据可能都补不回来了，这样对公司来说就是属于比较严重的数据故障问题，</span><br><span class="line">这样你年终奖想拿18薪就不太现实了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">所以针对这些存在单点故障的进程，我们都需要添加监控告警机制，最起码出问题能及时知道，再好一点的呢，可以尝试自动修复重启。</span><br><span class="line">那针对Flume中的Agent我们就来实现一个监控功能，并且尝试自动重启</span><br><span class="line">大致思路是这样的，</span><br><span class="line">1. 首先需要有一个配置文件，配置文件中指定你现在需要监控哪些Agent</span><br><span class="line">2. 有一个脚本负责读取配置文件中的内容，定时挨个检查Agent对应的进程还在不在，如果发现对应的进程不在，则记录错误信息，然后告警(发短信或者发邮件) 并尝试重启</span><br></pre></td></tr></table></figure><h3 id="编写监控相关程序"><a href="#编写监控相关程序" class="headerlink" title="编写监控相关程序"></a>编写监控相关程序</h3><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 myconfFile]# vim monlist.conf </span><br><span class="line">load-failover.conf&#x3D;startExample.sh # 等号处空格不能要，monlist.sh要出错</span><br></pre></td></tr></table></figure><h4 id="进程启动脚本"><a href="#进程启动脚本" class="headerlink" title="进程启动脚本"></a>进程启动脚本</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 myconfFile]# vim startExample.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">flume_path&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin # 等号处空格不能要</span><br><span class="line">nohup $&#123;flume_path&#125;&#x2F;bin&#x2F;flume-ng --name a1 --conf $&#123;flume_path&#125;&#x2F;conf --conf-file $&#123;flume_path&#125;&#x2F;conf&#x2F;myconfFile&#x2F;load-failover.conf &amp;</span><br></pre></td></tr></table></figure><h4 id="监控脚本"><a href="#监控脚本" class="headerlink" title="监控脚本"></a>监控脚本</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 myconfFile]# vim monlist.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">monlist&#x3D;&#96;cat monlist.conf&#96; #等号那里不能有空格</span><br><span class="line">echo &quot;start check&quot;</span><br><span class="line">for item in $&#123;monlist&#125;</span><br><span class="line">do</span><br><span class="line">    # 设置字段分隔符</span><br><span class="line">    OLD_IFS&#x3D;$IFS</span><br><span class="line">    IFS&#x3D;&quot;&#x3D;&quot;</span><br><span class="line">    # 把一行内容转成多列[数组] </span><br><span class="line">    arr&#x3D;($item) </span><br><span class="line">    # 获取等号左边的内容</span><br><span class="line">    name&#x3D;$&#123;arr[0]&#125;</span><br><span class="line">    # 获取等号右边的内容</span><br><span class="line">    script&#x3D;$&#123;arr[1]&#125;</span><br><span class="line">    echo &quot;time is:&quot;&#96;date +&quot;%Y-%m-%d %H:%M:%S&quot;&#96;&quot; check &quot;$name</span><br><span class="line">    if [ &#96;jps -m|grep $name | wc -l&#96; -eq 0 ]</span><br><span class="line">    then</span><br><span class="line">    # 发短信或者邮件告警</span><br><span class="line">    echo &#96;date +&quot;%Y-%m-%d %H:%M:%S&quot;&#96;$name &quot;is none&quot;</span><br><span class="line">    sh -x .&#x2F;$&#123;script&#125;</span><br><span class="line">    fi</span><br><span class="line">    done</span><br></pre></td></tr></table></figure><h4 id="设置定时检查"><a href="#设置定时检查" class="headerlink" title="设置定时检查"></a>设置定时检查</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：这个需要定时执行，所以可以使用crontab定时调度</span><br><span class="line"></span><br><span class="line">* * * * * root &#x2F;bin&#x2F;bash &#x2F;data&#x2F;soft&#x2F;monlist.sh</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第七周 第3章 精讲Flume高级组件</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC3%E7%AB%A0-%E7%B2%BE%E8%AE%B2Flume%E9%AB%98%E7%BA%A7%E7%BB%84%E4%BB%B6.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC3%E7%AB%A0-%E7%B2%BE%E8%AE%B2Flume%E9%AB%98%E7%BA%A7%E7%BB%84%E4%BB%B6.html</id>
    <published>2022-02-16T14:47:16.000Z</published>
    <updated>2022-02-20T04:05:37.566Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第七周-第3章-精讲Flume高级组件"><a href="#第七周-第3章-精讲Flume高级组件" class="headerlink" title="第七周 第3章 精讲Flume高级组件"></a>第七周 第3章 精讲Flume高级组件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">前面我们掌握了Flume中的核心组件 source、channel、sink的使用，下面我们来学习一下Flume中的一</span><br><span class="line">些高级组件的使用</span><br></pre></td></tr></table></figure><h2 id="高级组件"><a href="#高级组件" class="headerlink" title="高级组件"></a>高级组件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Source Interceptors：Source可以指定一个或者多个拦截器按先后顺序依次对采集到的数据进行处</span><br><span class="line">理。</span><br><span class="line">Channel Selectors：Source发往多个Channel的策略设置，如果source后面接了多个channel，到</span><br><span class="line">底是给所有的channel都发，还是根据规则发送到不同channel，这些是由Channel Selectors来控制</span><br><span class="line">的</span><br><span class="line">Sink Processors：Sink 发送数据的策略设置，一个channel后面可以接多个sink，channel中的数据</span><br><span class="line">是被哪个sink获取，这个是由Sink Processors控制的</span><br></pre></td></tr></table></figure><h3 id="Event"><a href="#Event" class="headerlink" title="Event"></a>Event</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在具体分析这些高级组件之前，我们先插播一个小知识点，这个知识点在高级组件中会用到</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Event是Flume传输数据的基本单位，也是事务的基本单位，在文本文件中，通常一行记录就是一个Event</span><br><span class="line">Event中包含header和body；</span><br><span class="line">body是采集到的那一行记录的原始内容</span><br><span class="line">header类型为Map&lt;String, String&gt;，里面可以存储一些属性信息，方便后面使用</span><br><span class="line">我们可以在Source中给每一条数据的header中增加key-value，在Channel和Sink中使用header中的值了。</span><br></pre></td></tr></table></figure><h3 id="Source-Interceptors"><a href="#Source-Interceptors" class="headerlink" title="Source Interceptors"></a>Source Interceptors</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">接下里我们看一下第一个高级组件，Source Interceptors</span><br><span class="line">系统中已经内置提供了很多Source Interceptors</span><br><span class="line"></span><br><span class="line">常见的Source Interceptors类型：Timestamp Interceptor、Host Interceptor、Search and Replace</span><br><span class="line">Interceptor 、Static Interceptor、Regex Extractor Interceptor 等</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  Timestamp Interceptor：向event中的header里面添加timestamp 时间戳信息</span><br><span class="line">Host Interceptor：向event中的header里面添加host属性，host的值为当前机器的主机名或者ip</span><br><span class="line">  Search and Replace Interceptor：根据指定的规则查询Event中body里面的数据，然后进行替换，这个拦截器会修改event中body的值，也就是会修改原始采集到的数据内容</span><br><span class="line">  Static Interceptor：向event中的header里面添加固定的key和value</span><br><span class="line">  Regex Extractor Interceptor：根据指定的规则从Event中的body里面抽取数据，生成key和value，再把key和value添加到header中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">根据刚才的分析，总结一下：</span><br><span class="line">  Timestamp Interceptor、Host Interceptor、Static Interceptor、Regex Extractor Interceptor是向event中的header里面添加key-value类型的数据，方便后面的channel和sink组件使用，对采集到的原始数据内容没有任何影响</span><br><span class="line"></span><br><span class="line">  Search and Replace Interceptor是会根据规则修改event中body里面的原始数据内容，对header没有任何影响，使用这个拦截器需要特别小心，因为他会修改原始数据内容。</span><br><span class="line">  这里面这几个拦截器其中Search and Replace Interceptor和Regex Extractor Interceptor 我们在工作中使用的比较多一些</span><br></pre></td></tr></table></figure><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">对采集到的数据按天按类型分目录存储</span><br><span class="line">我们的原始数据是这样的，看这个文件，Flume测试数据格式.txt</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">video_info</span><br><span class="line"></span><br><span class="line">&#123;&quot;id&quot;:&quot;14943445328940974601&quot;,&quot;uid&quot;:&quot;840717325115457536&quot;,&quot;lat&quot;:&quot;53.530598&quot;,&quot;lnt&quot;:&quot;-2.5620373&quot;,&quot;hots&quot;:0,&quot;title&quot;:&quot;0&quot;,&quot;status&quot;:&quot;1&quot;,&quot;topicId&quot;:&quot;0&quot;,&quot;end_time&quot;:&quot;1494344570&quot;,&quot;watch_num&quot;:0,&quot;share_num&quot;:&quot;1&quot;,&quot;replay_url&quot;:null,&quot;replay_num&quot;:0,&quot;start_time&quot;:&quot;1494344544&quot;,&quot;timestamp&quot;:1494344571,&quot;type&quot;:&quot;video_info&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">user_info</span><br><span class="line"></span><br><span class="line">&#123;&quot;uid&quot;:&quot;861848974414839801&quot;,&quot;nickname&quot;:&quot;mick&quot;,&quot;usign&quot;:&quot;&quot;,&quot;sex&quot;:1,&quot;birthday&quot;:&quot;&quot;,&quot;face&quot;:&quot;&quot;,&quot;big_face&quot;:&quot;&quot;,&quot;email&quot;:&quot;abc@qq.com&quot;,&quot;mobile&quot;:&quot;&quot;,&quot;reg_type&quot;:&quot;102&quot;,&quot;last_login_time&quot;:&quot;1494344580&quot;,&quot;reg_time&quot;:&quot;1494344580&quot;,&quot;last_update_time&quot;:&quot;1494344580&quot;,&quot;status&quot;:&quot;5&quot;,&quot;is_verified&quot;:&quot;0&quot;,&quot;verified_info&quot;:&quot;&quot;,&quot;is_seller&quot;:&quot;0&quot;,&quot;level&quot;:1,&quot;exp&quot;:0,&quot;anchor_level&quot;:0,&quot;anchor_exp&quot;:0,&quot;os&quot;:&quot;android&quot;,&quot;timestamp&quot;:1494344580,&quot;type&quot;:&quot;user_info&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gift_record</span><br><span class="line"></span><br><span class="line">&#123;&quot;send_id&quot;:&quot;834688818270961664&quot;,&quot;good_id&quot;:&quot;223&quot;,&quot;video_id&quot;:&quot;14943443045138661356&quot;,&quot;gold&quot;:&quot;10&quot;,&quot;timestamp&quot;:1494344574,&quot;type&quot;:&quot;gift_record&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  这份数据中有三种类型的数据，视频信息、用户信息、送礼信息，数据都是json格式的，这些数据还有一个共性就是里面都有一个type字段，type字段的值代表数据类型</span><br><span class="line">当我们的直播平台正常运行的时候，会实时产生这些日志数据，我们希望把这些数据采集到hdfs上进行存储，并且要按照数据类型进行分目录存储，视频数据放一块、用户数据放一块、送礼数据放一块</span><br><span class="line">  针对这个需求配置agent的话，source使用基于文件的execsource、channle使用基于文件的channle，我们希望保证数据的完整性和准确性，sink使用hdfssink</span><br><span class="line">  但是注意了，hdfssink中的path不能写死，首先是按天 就是需要动态获取日期，然后是因为不同类型的数据要存储到不同的目录中</span><br><span class="line">  那也就意味着path路径中肯定要是有变量，除了日期变量还要有数据类型变量，</span><br><span class="line">这里的数据类型的格式都是单词中间有一个下划线，但是我们的要求是目录中的单词不要出现下划线，使用驼峰的命名格式。</span><br><span class="line">  所以最终在hdfs中需要生成的目录大致是这样的</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H7Ry2q" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/H7Ry2q.png" alt="H7Ry2q.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里的日期变量好获取，但是数据类型如何获取呢？</span><br><span class="line">注意了，咱们前面分析了，通过source的拦截器可以向event的header中添加key-value，然后在后面的channle或者sink中获取key-value的值</span><br><span class="line">那我们在这就可以通过Regex Extractor Interceptor获取原始数据中的type字段的值，获取出来以后存储到header中，这样在sink阶段就可以获取到了。</span><br><span class="line">但是这个时候直接获取到的type的值是不满足要求的，需要对type的值进行转换，去掉下划线，转化为驼峰形式</span><br><span class="line">所以可以先使用Search and Replace Interceptor对原始数据中type的值进行转换，然后使用Regex Extractor Interceptor指定规则获取type字段的值，添加到header中。</span><br></pre></td></tr></table></figure><h5 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以整体的流程是这样的</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H7WlLT" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/H7WlLT.md.png" alt="H7WlLT.md.png"></a></p><h5 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">#给三个组件起名</span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置sources</span><br><span class="line">a1.sources.r1.type&#x3D;exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;moreTypeData&#x2F;moreType.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置拦截器</span><br><span class="line">a1.sources.r1.interceptors &#x3D; i1 i2 i3 i4</span><br><span class="line">a1.sources.r1.interceptors.i1.type &#x3D; search_replace</span><br><span class="line">a1.sources.r1.interceptors.i1.searchPattern &#x3D; &quot;type&quot;:&quot;video_info&quot;</span><br><span class="line">a1.sources.r1.interceptors.i1.replaceString &#x3D; &quot;type&quot;:&quot;videoInfo&quot;</span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors.i2.type &#x3D; search_replace</span><br><span class="line">a1.sources.r1.interceptors.i2.searchPattern &#x3D; &quot;type&quot;:&quot;user_info&quot;</span><br><span class="line">a1.sources.r1.interceptors.i2.replaceString &#x3D; &quot;type&quot;:&quot;userInfo&quot;</span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors.i3.type &#x3D; search_replace</span><br><span class="line">a1.sources.r1.interceptors.i3.searchPattern &#x3D; &quot;type&quot;:&quot;gift_record&quot;</span><br><span class="line">a1.sources.r1.interceptors.i3.replaceString &#x3D; &quot;type&quot;:&quot;giftRecord&quot;</span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors.i4.type&#x3D;regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i4.regex &#x3D; &quot;type&quot;:&quot;(\\w+)&quot;</span><br><span class="line">a1.sources.r1.interceptors.i4.serializers &#x3D; s1</span><br><span class="line">a1.sources.r1.interceptors.i4.serializers.s1.name &#x3D; logType</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置channels</span><br><span class="line">a1.channels.c1.type &#x3D; file</span><br><span class="line">a1.channels.c1.checkpointDir &#x3D; &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;moreType&#x2F;checkpoint</span><br><span class="line">a1.channels.c1.dataDirs &#x3D; &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin&#x2F;data&#x2F;moreType&#x2F;data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#配置sinks</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flume&#x2F;moreTypeData&#x2F;%Y%m%d&#x2F;%&#123;logType&#125;</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data-</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix&#x3D; .log</span><br><span class="line">a1.sinks.k1.hdfs.fileType&#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval&#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize&#x3D;134217728</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat&#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp&#x3D;true</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.channels&#x3D;c1</span><br><span class="line">a1.sinks.k1.channel&#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：这里面的拦截器，拦截器可以设置一个或者多个，source采集的每一条数据都会经过所有</span><br><span class="line">的拦截器进行处理，多个拦截器按照顺序执行。</span><br></pre></td></tr></table></figure><h5 id="结果查看"><a href="#结果查看" class="headerlink" title="结果查看"></a>结果查看</h5><p><a href="https://imgtu.com/i/H7LO1O" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/18/H7LO1O.md.png" alt="H7LO1O.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">看一下HDFS中的文件内容，发现type字段的值确实被拦截器修改了</span><br><span class="line">这就实现了按天，按类型分目录存储。</span><br></pre></td></tr></table></figure><h3 id="Channel-Selectors"><a href="#Channel-Selectors" class="headerlink" title="Channel Selectors"></a>Channel Selectors</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下Channel Selectors</span><br><span class="line">Channel Selectors类型包括：Replicating Channel Selector 和Multiplexing Channel Selector</span><br><span class="line">其中Replicating Channel Selector是默认的channel 选择器，它会将Source采集过来的Event发往所有</span><br><span class="line">Channel</span><br><span class="line">查看官方文档中针对这个默认channel选择器的解释</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HHUhBq" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHUhBq.md.png" alt="HHUhBq.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在这个例子的配置中，c3是可选channel。对c3的写入失败将被忽略。由于c1和c2未标记为可选，因此未</span><br><span class="line">能写入这些channel将导致事务失败</span><br><span class="line">针对这个配置，通俗一点来说就是，source的数据会发往c1、c2、c3这三个channle中，可以保证c1、</span><br><span class="line">c2一定能接收到所有数据，但是c3就无法保证了</span><br><span class="line">这个selector.optional参数是一个可选项，可以不用配置就行。</span><br><span class="line">如果是多个channel的话，直接在channels参数后面指定多个channel的名称就可以了，多个channel名</span><br><span class="line">称中间使用空格隔开，</span><br><span class="line">其实你看这个名称是channels 带有s，从名字上看就能看出来他支持多个channel</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">还有一个 channel选择器是Multiplexing Channel Selector，它表示会根据Event中header里面的值将</span><br><span class="line">Event发往不同的Channel</span><br><span class="line">看下官网中的介绍</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HHaEDI" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHaEDI.md.png" alt="HHaEDI.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在这个例子的配置中，指定了4个channel,c1、c2、c3、c4</span><br><span class="line">source采集到的数据具体会发送到哪个channel中，会根据event中header里面的state属性的值，这个</span><br><span class="line">是通过selector.header控制的</span><br><span class="line">如果state属性的值是CZ，则发送给c1</span><br><span class="line">如果state属性的值是US，则发送给c2 c3</span><br><span class="line">如果state属性的值是其它值，则发送给c4</span><br><span class="line">这些规则是通过selector.mapping和selector.default控制的</span><br><span class="line">这样就可以实现根据一定规则把数据分发给不同的channel了。</span><br></pre></td></tr></table></figure><h4 id="Replicating-Channel-Selector-案例"><a href="#Replicating-Channel-Selector-案例" class="headerlink" title="Replicating Channel Selector  案例"></a>Replicating Channel Selector  案例</h4><p><a href="https://imgtu.com/i/HHdMo6" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHdMo6.md.png" alt="HHdMo6.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在这个案例中我们使用Replicating选择器，将source采集到的数据重复发送给两个channle，最后每个</span><br><span class="line">channel后面接一个sink，负责把数据存储到不同存储介质中，方便后期使用。</span><br><span class="line">在实际工作中这种需求还是比较常见的，就是我们希望把一份数据采集过来以后，分别存储到不同的存储</span><br><span class="line">介质中，不同存储介质的特点和应用场景是不一样的，典型的就是hdfssink 和kafkasink，</span><br><span class="line">通过hdfssink实现离线数据落盘存储，方便后面进行离线数据计算</span><br><span class="line">通过kafkasink实现实时数据存储，方便后面进行实时计算，</span><br><span class="line">由于我们还没有学kafka，所以在这里先使用loggersink代理。</span><br></pre></td></tr></table></figure><h5 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# vim tcp-to-replicatingchannel.conf </span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1 c2</span><br><span class="line">a1.sinks&#x3D;k1 k2</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 55555</span><br><span class="line"></span><br><span class="line"># 配置channel选择器[默认就是replicating,可以省略]</span><br><span class="line">a1.sources.r1.selector.type &#x3D; replicating</span><br><span class="line">#a1.sources.r1.channels &#x3D; c1 c2</span><br><span class="line">#a1.sources.r1.selector.optional &#x3D; c2</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type &#x3D; memory</span><br><span class="line">a1.channels.c2.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c2.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># </span><br><span class="line">a1.sinks.k1.type&#x3D;logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type&#x3D;hdfs</span><br><span class="line">a1.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flume&#x2F;replicatingSelector</span><br><span class="line">a1.sinks.k2.hdfs.filePrefix &#x3D; data-</span><br><span class="line">a1.sinks.k2.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k2.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k2.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k2.hdfs.fileSuffiix &#x3D; .log</span><br><span class="line">a1.sinks.k2.hdfs.writeFormat &#x3D; Text</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.channels &#x3D; c1 c2</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line">a1.sinks.k2.channel &#x3D; c2</span><br></pre></td></tr></table></figure><h5 id="启动，输入数据，查看"><a href="#启动，输入数据，查看" class="headerlink" title="启动，输入数据，查看"></a>启动，输入数据，查看</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">生成测试数据，通过telnet连接到socket</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HHyp1f" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHyp1f.png" alt="HHyp1f.png"></a></p><p><a href="https://imgtu.com/i/HHy9c8" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHy9c8.md.png" alt="HHy9c8.md.png"></a></p><p><a href="https://imgtu.com/i/HHyucT" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHyucT.md.png" alt="HHyucT.md.png"></a></p><h4 id="Multiplexing-Channel-Selector-案例"><a href="#Multiplexing-Channel-Selector-案例" class="headerlink" title="Multiplexing Channel Selector  案例"></a>Multiplexing Channel Selector  案例</h4><p><a href="https://imgtu.com/i/HHyq2V" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHyq2V.md.png" alt="HHyq2V.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">最终再把不同channel中的数据存储到不同介质中。</span><br><span class="line">在这里面我们需要用到正则抽取拦截器在Event的header中生成key-value</span><br><span class="line">作为Multiplexing选择器的规则</span><br></pre></td></tr></table></figure><h5 id="测试数据格式"><a href="#测试数据格式" class="headerlink" title="测试数据格式"></a>测试数据格式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:19,&quot;city&quot;:&quot;bj&quot;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:26,&quot;city&quot;:&quot;sh&quot;&#125;</span><br></pre></td></tr></table></figure><h5 id="配置-2"><a href="#配置-2" class="headerlink" title="配置"></a>配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来配置Agent，复制tcp-to-replicatingchannel.conf的内容，</span><br><span class="line">主要增加source拦截器和修改channel选择器，以及hdfsink中的path路径</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 conf]# vim tcp-to-multiplexingchannel.conf </span><br><span class="line">#</span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1 c2</span><br><span class="line">a1.sinks&#x3D;k1 k2</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 55555</span><br><span class="line"></span><br><span class="line">  配置正则抽取拦截器</span><br><span class="line">a1.sources.r1.interceptors&#x3D;i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type&#x3D;regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i1.regex &#x3D; &quot;city&quot;:&quot;(\\w+)&quot;</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers &#x3D; s1</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.name &#x3D; city</span><br><span class="line"></span><br><span class="line"># 配置channel选择器</span><br><span class="line">a1.sources.r1.selector.type &#x3D; multiplexing</span><br><span class="line">a1.sources.r1.selector.header &#x3D; city</span><br><span class="line">a1.sources.r1.selector.mapping.bj &#x3D; c1</span><br><span class="line">#a1.sources.r1.selector.mapping.sh &#x3D; c2    </span><br><span class="line">a1.sources.r1.selector.default &#x3D; c2</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type &#x3D; memory</span><br><span class="line">a1.channels.c2.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c2.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># </span><br><span class="line">a1.sinks.k1.type&#x3D;logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type&#x3D;hdfs</span><br><span class="line">a1.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flume&#x2F;multiplexingSelector</span><br><span class="line">a1.sinks.k2.hdfs.filePrefix &#x3D; data-</span><br><span class="line">a1.sinks.k2.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k2.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k2.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k2.hdfs.fileSuffiix &#x3D; .log</span><br><span class="line">a1.sinks.k2.hdfs.writeFormat &#x3D; Text</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.channels &#x3D; c1 c2</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line">a1.sinks.k2.channel &#x3D; c2</span><br><span class="line">You have new mail in &#x2F;var&#x2F;spool&#x2F;mail&#x2F;root</span><br><span class="line">[root@bigdata04 conf]# vim tcp-to-multiplexingchannel.conf </span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 55555</span><br><span class="line"></span><br><span class="line"># 配置正则抽取拦截器</span><br><span class="line">a1.sources.r1.interceptors&#x3D;i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type&#x3D;regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i1.regex &#x3D; &quot;city&quot;:&quot;(\\w+)&quot;</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers &#x3D; s1</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.name &#x3D; city</span><br><span class="line"></span><br><span class="line"># 配置channel选择器</span><br><span class="line">a1.sources.r1.selector.type &#x3D; multiplexing</span><br><span class="line">a1.sources.r1.selector.header &#x3D; city</span><br><span class="line">a1.sources.r1.selector.mapping.bj &#x3D; c1</span><br><span class="line">#a1.sources.r1.selector.mapping.sh &#x3D; c2    </span><br><span class="line">a1.sources.r1.selector.default &#x3D; c2</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type &#x3D; memory</span><br><span class="line">a1.channels.c2.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c2.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># </span><br><span class="line">a1.sinks.k1.type&#x3D;logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type&#x3D;hdfs</span><br><span class="line">a1.sinks.k2.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flume&#x2F;multiplexingSelector</span><br><span class="line">a1.sinks.k2.hdfs.filePrefix &#x3D; data-</span><br><span class="line">a1.sinks.k2.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k2.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k2.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k2.hdfs.fileSuffiix &#x3D; .log</span><br><span class="line">a1.sinks.k2.hdfs.writeFormat &#x3D; Text</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.channels &#x3D; c1 c2</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line">a1.sinks.k2.channel &#x3D; c2</span><br></pre></td></tr></table></figure><h5 id="启动，输入测试数据，查看"><a href="#启动，输入测试数据，查看" class="headerlink" title="启动，输入测试数据，查看"></a>启动，输入测试数据，查看</h5><p><a href="https://imgtu.com/i/HHWN8A" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHWN8A.png" alt="HHWN8A.png"></a><br><a href="https://imgtu.com/i/HHWavt" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHWavt.md.png" alt="HHWavt.md.png"></a><br><a href="https://imgtu.com/i/HHWUgI" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HHWUgI.md.png" alt="HHWUgI.md.png"></a></p><h3 id="Sink-Processors"><a href="#Sink-Processors" class="headerlink" title="Sink Processors"></a>Sink Processors</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下Sink处理器</span><br><span class="line">  Sink Processors类型包括这三种：Default Sink Processor、Load balancing Sink Processor和Failover Sink Processor</span><br><span class="line">  DefaultSink Processor是默认的，不用配置sinkgroup，就是咱们现在使用的这种最普通的形式，一个channel后面接一个sink的形式</span><br><span class="line">  Load balancing Sink Processor是负载均衡处理器，一个channle后面可以接多个sink，这多个sink属于一个sink group，根据指定的算法进行轮询或者随机发送，减轻单个sink的压力</span><br><span class="line">  Failover Sink Processor是故障转移处理器，一个channle后面可以接多个sink，这多个sink属于一个sink group，按照sink的优先级，默认先让优先级高的sink来处理数据，如果这个sink出现了故障，则用优先级低一点的sink处理数据，可以保证数据不丢失。</span><br></pre></td></tr></table></figure><h4 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h4><h4 id="Load-balancing-Sink-Processor-案例"><a href="#Load-balancing-Sink-Processor-案例" class="headerlink" title="Load balancing Sink Processor  案例"></a>Load balancing Sink Processor  案例</h4><p><a href="https://imgtu.com/i/HbsWad" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HbsWad.md.png" alt="HbsWad.md.png"></a></p><p><a href="https://imgtu.com/i/HbsfIA" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HbsfIA.md.png" alt="HbsfIA.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">看中间的参数信息，</span><br><span class="line">processor.sinks：指定这个sink groups中有哪些sink，指定sink的名称，多个的话中间使用空格隔开即可【注意，这里写的是processor.sinks，但是在下面的example中使用的是sinks，实际上就是sinks，所以文档也是有一些瑕疵的，不过Flume的文档已经算是写的非常好的了】</span><br><span class="line">processor.type：针对负载均衡的sink处理器，这里需要指定load_balance</span><br><span class="line">processor.selector：此参数的值内置支持两个，round_robin和random，round_robin表示轮询，按照sink的顺序，轮流处理数据，random表示随机。</span><br><span class="line">processor.backoff：默认为false，设置为true后，故障的节点会列入黑名单，过一定时间才会再次发送数据，如果还失败，则等待时间是指数级增长；一直到达到最大的时间。</span><br><span class="line">如果不开启，故障的节点每次还会被重试发送，如果真有故障节点的话就会影响效率。</span><br><span class="line">processor.selector.maxTimeOut：最大的黑名单时间，默认是30秒</span><br></pre></td></tr></table></figure><h5 id="配置-3"><a href="#配置-3" class="headerlink" title="配置"></a>配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个负载均衡案例可以解决之前单节点输出能力有限的问题，可以通过多个sink后面连接多个Agent实现负载均衡，如果后面的Agent挂掉1个，也不会影响整体流程，只是处理效率又恢复到了之前的状态。</span><br></pre></td></tr></table></figure><h6 id="bigdata04"><a href="#bigdata04" class="headerlink" title="bigdata04"></a>bigdata04</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1 k2</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># 配置sink组件,[为了方便演示效果，把batch-size设置为1]</span><br><span class="line">a1.sinks.k1.type &#x3D; avro</span><br><span class="line">a1.sinks.k1.hostname &#x3D; bigdata02</span><br><span class="line">a1.sinks.k1.port &#x3D; 45454</span><br><span class="line">a1.sinks.k1.batch-size&#x3D;1</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type &#x3D; avro</span><br><span class="line">a1.sinks.k2.hostname &#x3D; bigdata03</span><br><span class="line">a1.sinks.k2.port &#x3D; 41414</span><br><span class="line">a1.sinks.k1.batch-size&#x3D;1</span><br><span class="line"></span><br><span class="line"># 配置sink策略</span><br><span class="line">a1.sinkgroups &#x3D; g1</span><br><span class="line">a1.sinkgroups.g1.sinks &#x3D; k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type &#x3D; load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff &#x3D; true</span><br><span class="line">a1.sinkgroups.g1.processor.selector &#x3D; round_robin</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel&#x3D;c1</span><br><span class="line">a1.sinks.k2.channel&#x3D;c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.port &#x3D; 45454和a1.sinks.k2.port &#x3D; 41414的端口一致也没事，因为是在不同的节点上，只要bigdata02,bigdata03上写的一致就行</span><br></pre></td></tr></table></figure><h6 id="bigdata02"><a href="#bigdata02" class="headerlink" title="bigdata02"></a>bigdata02</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.type &#x3D; avro</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 45454</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flume&#x2F;sinkProcessor_load_balance</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data130</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat&#x3D;Text</span><br><span class="line">a1.sinks.k1.hdfs.fileType&#x3D;DataStream</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval&#x3D;3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize&#x3D;134217728</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix&#x3D;.log</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.channels&#x3D;c1</span><br><span class="line">a1.sinks.k1.channel&#x3D;c1</span><br></pre></td></tr></table></figure><h6 id="bigdata03"><a href="#bigdata03" class="headerlink" title="bigdata03"></a>bigdata03</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.type &#x3D; avro</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 41414</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;flume&#x2F;sinkProcessor_load_balance</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data131</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat&#x3D;Text</span><br><span class="line">a1.sinks.k1.hdfs.fileType&#x3D;DataStream</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval&#x3D;3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize&#x3D;134217728</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix&#x3D;.log</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.channels&#x3D;c1</span><br><span class="line">a1.sinks.k1.channel&#x3D;c1</span><br></pre></td></tr></table></figure><h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">还是先启动后面，再启动前面</span><br><span class="line"></span><br><span class="line">注意：02,03在启动之前需要到&#x2F;etc&#x2F;profile中先配置HADOOP_HOME环境变量，因为这个Agent中使用到了hdfs</span><br></pre></td></tr></table></figure><h5 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError:com.google.common.base.Preconditions.checkArgument(...)(已解决)</span><br><span class="line"></span><br><span class="line">hadoop和flume的guava版本不同</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/lkx99661014/article/details/104586955" target="_blank" rel="external nofollow noopener noreferrer">URL</a></p><h5 id="结果查看-1"><a href="#结果查看-1" class="headerlink" title="结果查看"></a>结果查看</h5><p><a href="https://imgtu.com/i/HbsWad" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HbsWad.png" alt="HbsWad.png"></a><br><a href="https://imgtu.com/i/HbsfIA" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HbsfIA.png" alt="HbsfIA.png"></a><a href="https://imgtu.com/i/HbvJbD" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HbvJbD.png" alt="HbvJbD.png"></a><br><a href="https://imgtu.com/i/HbvGDO" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HbvGDO.png" alt="HbvGDO.png"></a><br><a href="https://imgtu.com/i/HbvtVe" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HbvtVe.png" alt="HbvtVe.png"></a></p><h4 id="Failover-Sink-Processor"><a href="#Failover-Sink-Processor" class="headerlink" title="Failover Sink Processor"></a>Failover Sink Processor</h4><h4 id="Failover-Sink-Processor-案例"><a href="#Failover-Sink-Processor-案例" class="headerlink" title="Failover Sink Processor  案例"></a>Failover Sink Processor  案例</h4><p><a href="https://imgtu.com/i/Hbz23n" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/Hbz23n.png" alt="Hbz23n.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下故障转移</span><br><span class="line">在这个图中，也是一个channel后面接了两个sink，但是这里和负载均衡架构不一样的是，这两个sink正常情况下只有一个干活，另一个是不干活的</span><br><span class="line"></span><br><span class="line">这个故障转移案例可以解决sink组件单点故障的问题，如果某一个sink输出功能失效，另一个还可以顶上</span><br><span class="line">来，同时只会存在一个真正输出数据的sink。</span><br><span class="line"></span><br><span class="line">来看一下Failover Sink Processor的文档介绍</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HbzWj0" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HbzWj0.png" alt="HbzWj0.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">processor.type：针对故障转移的sink处理器，使用failover</span><br><span class="line">processor.priority.：指定sink group中每一个sink组件的优先级，默认情况下channel中的数据会被优先级比较高的sink取走</span><br><span class="line">processor.maxpenalty：sink发生故障之后，最大等待时间</span><br></pre></td></tr></table></figure><h5 id="配置-4"><a href="#配置-4" class="headerlink" title="配置"></a>配置</h5><h6 id="bigdata04-1"><a href="#bigdata04-1" class="headerlink" title="bigdata04"></a>bigdata04</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">就sink配置策略改变</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">a1.sources&#x3D;r1</span><br><span class="line">a1.channels&#x3D;c1</span><br><span class="line">a1.sinks&#x3D;k1 k2</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># 配置sink组件,[为了方便演示效果，把batch-size设置为1]</span><br><span class="line">a1.sinks.k1.type &#x3D; avro</span><br><span class="line">a1.sinks.k1.hostname &#x3D; bigdata02</span><br><span class="line">a1.sinks.k1.port &#x3D; 45454</span><br><span class="line">a1.sinks.k1.batch-size&#x3D;1</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type &#x3D; avro</span><br><span class="line">a1.sinks.k2.hostname &#x3D; bigdata03</span><br><span class="line">a1.sinks.k2.port &#x3D; 41414</span><br><span class="line">a1.sinks.k1.batch-size&#x3D;1</span><br><span class="line"></span><br><span class="line">#配置sink策略</span><br><span class="line">a1.sinkgroups &#x3D; g1</span><br><span class="line">a1.sinkgroups.g1.sinks &#x3D; k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type &#x3D; failover</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 &#x3D; 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 &#x3D; 10</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty &#x3D; 10000</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel&#x3D;c1</span><br><span class="line">a1.sinks.k2.channel&#x3D;c1</span><br></pre></td></tr></table></figure><h6 id="bigdata02-1"><a href="#bigdata02-1" class="headerlink" title="bigdata02"></a>bigdata02</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与上个案例一样，就hdfs地址改变</span><br></pre></td></tr></table></figure><h6 id="bigdata03-1"><a href="#bigdata03-1" class="headerlink" title="bigdata03"></a>bigdata03</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与上个案例一样，就hdfs地址改变</span><br></pre></td></tr></table></figure><h6 id="结果查看-2"><a href="#结果查看-2" class="headerlink" title="结果查看"></a>结果查看</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[![HqC2IU.png](https:&#x2F;&#x2F;s4.ax1x.com&#x2F;2022&#x2F;02&#x2F;19&#x2F;HqC2IU.png)](https:&#x2F;&#x2F;imgtu.com&#x2F;i&#x2F;HqC2IU)</span><br><span class="line">[![HqCWiF.png](https:&#x2F;&#x2F;s4.ax1x.com&#x2F;2022&#x2F;02&#x2F;19&#x2F;HqCWiF.png)](https:&#x2F;&#x2F;imgtu.com&#x2F;i&#x2F;HqCWiF)</span><br><span class="line">[![HqCgaT.png](https:&#x2F;&#x2F;s4.ax1x.com&#x2F;2022&#x2F;02&#x2F;19&#x2F;HqCgaT.png)](https:&#x2F;&#x2F;imgtu.com&#x2F;i&#x2F;HqCgaT)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">然后到hdfs上验证数据，发现现在数据是通过bigdata03这台机器写出去的，因为对应bigdata03这台机</span><br><span class="line">器的sink组件的优先级比较高</span><br></pre></td></tr></table></figure><h4 id="模拟Agent挂掉"><a href="#模拟Agent挂掉" class="headerlink" title="模拟Agent挂掉"></a>模拟Agent挂掉</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来模拟bigdata03这台机器上的的Agent挂掉，也就意味着k2这个sink写不出去数据了，此时，我们再通过socket发送一条数据，看看会怎么样</span><br><span class="line">直接在bigdata03窗口中按 ctrl+c 停止agent即可</span><br><span class="line">然后再生成一条数据</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/HqACcQ" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HqACcQ.png" alt="HqACcQ.png"></a><br><a href="https://imgtu.com/i/HqA91g" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/19/HqA91g.png" alt="HqA91g.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时如果把bigdata03上的Agent再启动的话，会发现新采集的数据会通过bigdata03上的Agent写出去，这是因为它的优先级比较高。</span><br><span class="line">这就是Sink故障转移的应用</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第七周 第2章 极速上手Flume使用</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8%20%20Flume%E7%9A%84hello%20world.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC2%E7%AB%A0-%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8BFlume%E4%BD%BF%E7%94%A8%20%20Flume%E7%9A%84hello%20world.html</id>
    <published>2022-02-16T14:46:42.000Z</published>
    <updated>2022-02-20T04:05:18.022Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第七周-第2章-极速上手Flume使用"><a href="#第七周-第2章-极速上手Flume使用" class="headerlink" title="第七周 第2章 极速上手Flume使用"></a>第七周 第2章 极速上手Flume使用</h1><h2 id="Flume的Hello-World！"><a href="#Flume的Hello-World！" class="headerlink" title="Flume的Hello World！"></a>Flume的Hello World！</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">下面我们就想上手操作Flume，具体该怎么做呢？</span><br><span class="line">先来看一个入门级别的Hello World案例。</span><br><span class="line">我们前面说了，启动Flume任务其实就是启动一个Agent，Agent是由source、channel、sink组成的，这</span><br><span class="line">些组件在使用的时候只需要写几行配置就可以了</span><br><span class="line">那下面我们就看一下source、channel、sink该如何配置呢？</span><br><span class="line">接下来带着大家看一下官网</span><br><span class="line">找到左边的documentation，查看文档信息</span><br><span class="line"></span><br><span class="line">其实Flume的操作文档是非常良心的，整理的非常详细，给flume的维护者们点个赞。</span><br><span class="line">进入Flume User Guide</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H5YXR0" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/H5YXR0.md.png" alt="H5YXR0.md.png"></a></p><h3 id="flume配置"><a href="#flume配置" class="headerlink" title="flume配置"></a>flume配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">下面有一个Agent配置的例子：</span><br><span class="line"></span><br><span class="line"># example.conf: A single-node Flume configuration</span><br><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; localhost</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">这个例子中首先定义了source的名字、sink的名字还有channel的名字</span><br><span class="line">下面配置source的相关参数</span><br><span class="line">下面配置了sink的相关参数</span><br><span class="line">接着配置了channel的相关参数</span><br><span class="line">最后把这三个组件连接到了一起，就是告诉source需要向哪个channel写入数据，告诉sink需要从哪个</span><br><span class="line">channel读取数据，这样source、channel、sink这三个组件就联通了。</span><br><span class="line">总结下来，配置Flume agent的主要流程是这样的</span><br><span class="line">1. 给每个组件起名字</span><br><span class="line">2. 配置每个组件的相关参数</span><br><span class="line">3. 把它们联通起来</span><br><span class="line">注意了，在Agent中配置的三大组件为什么要这样写呢？如果我是第一次使用我也不会写啊。</span><br><span class="line">三大组件的配置在文档中是有详细说明的，来看一下，在Flume Sources下面显示的都是已经内置支持的</span><br><span class="line">Source组件</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H5YOGq" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/H5YOGq.png" alt="H5YOGq.png"></a></p><h4 id="source"><a href="#source" class="headerlink" title="source"></a>source</h4><h5 id="NetCat-Tcp"><a href="#NetCat-Tcp" class="headerlink" title="NetCat Tcp"></a>NetCat Tcp</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">咱们刚才看的案例中使用的是source类型是netcat，其实就是NetCat TCP Source，看一下详细内容</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H5YLin" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/H5YLin.png" alt="H5YLin.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这里面的粗体字体是必选的参数</span><br><span class="line">第一个参数是为了指定source需要向哪个channel写数据，这个其实是通用的参数，</span><br><span class="line">主要看下面这三个，type、bind、port</span><br><span class="line">type：类型需要指定为natcat</span><br><span class="line">bind：指定当前机器的ip，使用hostname也可以</span><br><span class="line">port：指定当前机器中一个没有被使用的端口</span><br><span class="line">指定bind和port表示开启监听模式，监听指定ip和端口中的数据，其实就是开启了一个socket的服务端，</span><br><span class="line">等待客户端连接进来写入数据</span><br><span class="line">在这里给agent起名为a1,所以netcat类型的配置如下，这里面还指定了source、channel的名字，并且把</span><br><span class="line">source和channel连接到一起了，刨除这几个配置之外就剩下了三行配置，就是刚才我们分析的那三个必</span><br><span class="line">填参数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 6666</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line">注意了，bind参数后面指定的ip是四个0，这个当前机器的通用ip，因为一台机器可以有多个ip，例如：</span><br><span class="line">内网ip、外网ip，如果通过bind参数指定某一个ip的话，表示就只监听通过这个ip发送过来的数据了，这</span><br><span class="line">样会有局限性，所以可以指定0.0.0.0。</span><br><span class="line">下面几个参数都是可选配置，默认可以不配置。</span><br><span class="line">接着是channel，案例中channel使用的是memory</span><br></pre></td></tr></table></figure><h4 id="channel"><a href="#channel" class="headerlink" title="channel"></a>channel</h4><h5 id="memory-channel"><a href="#memory-channel" class="headerlink" title="memory channel"></a>memory channel</h5><p><a href="https://imgtu.com/i/H5t5f1" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/H5t5f1.png" alt="H5t5f1.png"></a><br><a href="https://imgtu.com/i/H5t76K" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/H5t76K.md.png" alt="H5t76K.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里面只有type是必填项，其他都是可选的</span><br></pre></td></tr></table></figure><h4 id="sink"><a href="#sink" class="headerlink" title="sink"></a>sink</h4><h5 id="logger-sink"><a href="#logger-sink" class="headerlink" title="logger sink"></a>logger sink</h5> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">最后看一下sink，在案例中sink使用的是logger，对应的就是Logger Sink</span><br><span class="line"></span><br><span class="line">logger sink中默认也只需要指定type即可</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H5NajK" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/H5NajK.png" alt="H5NajK.png"></a><br><a href="https://imgtu.com/i/H5NUc6" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/17/H5NUc6.md.png" alt="H5NUc6.md.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">后期我们如果想要使用其他的内置组件，直接到官网文档这里查找即可，这里面的配置有很多，没有必要去记，肯定记不住，只要知道到哪里去找就可以，工作的时候又不是闭卷考试，官网是可以随便使用的，</span><br><span class="line">所以建议大家到官网找到配置之后直接拷贝，要不然自己手写很容易出错。</span><br><span class="line">配置文件分析完了，可以把这些配置放到一个配置文件中，起名叫example.conf，把这个配置文件放到conf&#x2F; 目录下。</span><br></pre></td></tr></table></figure><h3 id="Flume配置文件"><a href="#Flume配置文件" class="headerlink" title="Flume配置文件"></a>Flume配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># example.conf: A single-node Flume configuration</span><br><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; localhost</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1 注意：这里没有s，真他妈操蛋</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意了，这个配置文件中的a1表示是agent的名称，还有就是port指定的端口必须是未被使用的，可以先查询一下当前机器使用了哪些端口，端口的可用范围是1-65535，如果懒得去查的话，就尽量使用偏大一些的端口，这样被占用的概率就非常低了</span><br></pre></td></tr></table></figure><h3 id="启动Agent"><a href="#启动Agent" class="headerlink" title="启动Agent"></a>启动Agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;flume-ng agent --name a1 --conf conf --conf-file example.conf -Dflume.root.logger&#x3D;INFO,console</span><br><span class="line">(经过实验，必须在apche安装目录下执行; 在bin下执行有问题)</span><br><span class="line"></span><br><span class="line">这里面使用flume-ng命令</span><br><span class="line"></span><br><span class="line">后面指定agent，表示启动一个Flume的agent代理</span><br><span class="line">--name：指定agent的名字</span><br><span class="line">--conf：指定flume配置文件的根目录</span><br><span class="line">--conf-file：指定Agent对应的配置文件(包含source、channel、sink配置的文件)</span><br><span class="line">-D：动态添加一些参数，在这里是指定了flume的日志输出级别和输出位置，INFO表示日志级别，</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意了，其实agent的启动命令还可以这样写</span><br><span class="line">bin&#x2F;flume-ng agent -n $agent_name -c conf -f conf&#x2F;flume-conf.properties.template</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这里面的-n属于简写，完整的写法就是–name</span><br><span class="line">-c完整写法的–conf</span><br><span class="line">-f完整写法是–conf-file</span><br><span class="line">所以以后看到这两种写法要知道他们都是正确的写法。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动Agent</span><br><span class="line">在这里我们使用完整的写法，看起来清晰一些</span><br><span class="line">注意了，由于配置文件里面指定了agent的名称为a1,所以在–name后面也需要指定a1，还有就是通过–conf-file指定配置文件的时候需要指定conf目录下的example.conf配置文件</span><br></pre></td></tr></table></figure> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动之后会看到如下信息，表示启动成功，启动成功之后，这个窗口会被一直占用，因为Agent服务一直在运行，现在属于一个前台进程</span><br><span class="line"></span><br><span class="line">2020-05-02 10:14:56,464 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.so</span><br></pre></td></tr></table></figure><h4 id="开启的socket服务端"><a href="#开启的socket服务端" class="headerlink" title="开启的socket服务端"></a>开启的socket服务端</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果看到提示的有ERROR级别的日志信息，就需要具体问题具体分析了，一般都是配置文件配置错误了。</span><br><span class="line">接下来我们需要连接到source中通过netcat开启的socket服务端</span><br><span class="line">克隆一个bigdata04的会话，因为前面启动Agent之后，窗口就被占用了</span><br><span class="line">使用telnet命令可以连接到指定socket服务，telnet后面的主机名和端口是根据example.conf配置文件中配置的</span><br><span class="line">注意：如果提示找不到telnet命令，则需要使用yum在线安装</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 ~]# yum install -y telnet</span><br><span class="line">[root@bigdata04 ~]# telnet localhost 44444</span><br><span class="line">Trying ::1...</span><br><span class="line">telnet: connect to address ::1: Connection refused</span><br><span class="line">Trying 127.0.0.1...</span><br><span class="line">Connected to localhost.</span><br><span class="line">Escape character is &#39;^]&#39;.</span><br><span class="line">hello world!</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">回到Agent所在的窗口，可以看到下面多了一行日志，就是我们在telnet中输入的内容</span><br></pre></td></tr></table></figure><h4 id="修改ip"><a href="#修改ip" class="headerlink" title="修改ip"></a>修改ip</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">按 ctrl+c 断开telnet连接</span><br><span class="line">重新使用telnet连接，此时不使用localhost，使用本机的内网ip可以吗？ 192.168.182.103</span><br><span class="line"></span><br><span class="line">[root@bigdata04 ~]# telnet 192.168.182.103 44444</span><br><span class="line">Trying 192.168.182.103...</span><br><span class="line">telnet: connect to address 192.168.182.103: Connection refused</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">所以此时Agent中source的配置在使用的时候就受限制了，在开启telnet客户端的时候就只能在本地开启了，无法在其他机器上使用，因为source中绑定的ip是localhost。</span><br><span class="line"></span><br><span class="line">如果想要支持一个网络内其它机器上也可以使用telnet链接的话就需要修改bind参数指定的值了最直接的就是指定192.168.182.103这个内网ip，其实还有一种更加通用的方式是指定0.0.0.0，此时表示会监听每一个可用的ip地址，所以在绑定ip端口时，ip通常都会使用0.0.0.0</span><br><span class="line">那在这里我们把example.conf中的localhost改为0.0.0.0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">按ctrl+c停止刚才启动的agent</span><br><span class="line"></span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">启动Agent  </span><br><span class="line"></span><br><span class="line">在另一个会话窗口中使用telnet连接</span><br><span class="line">[root@bigdata04 ~]# telnet 192.168.182.103 44444</span><br><span class="line">Trying 192.168.182.103...</span><br><span class="line">Connected to 192.168.182.103.</span><br><span class="line">Escape character is &#39;^]&#39;.</span><br><span class="line">hi</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">此时可以在其他机器上使用telnet连接也可以，在bigdata01机器上</span><br></pre></td></tr></table></figure><h3 id="Flume-Agent后台运行"><a href="#Flume-Agent后台运行" class="headerlink" title="Flume Agent后台运行"></a>Flume Agent后台运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">但是注意了，此时Flume中Agent服务是在前台运行，这个服务实际工作中需要一直运行，所以需要放到后台运行。</span><br><span class="line">Flume自身没有提供直接把进程放到后台执行的参数，所以就需要使用咱们前面学习的nohup和&amp;了。</span><br><span class="line">此时就不需要指定-Dflume.root.logger&#x3D;INFO,console参数了，默认情况下flume的日志会记录到日志文件中。</span><br><span class="line">停掉之前的Agent，重新执行</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# nohup bin&#x2F;flume-ng agent --name a1 --conf conf --conf-file xxx &amp;</span><br><span class="line"></span><br><span class="line">启动之后，通过jps命令可以查看到一个application进程，这个就是启动的Agent</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# jps</span><br><span class="line">9619 Jps</span><br><span class="line">9581 Application</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">这样看起来不清晰，如果后期启动了多个Agent，都分不出来哪个是哪个了</span><br><span class="line">可以在jps后面加上参数 -ml，这样可以看到启动时指定的一些参数信息</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# jps -m</span><br><span class="line">9659 Jps -m</span><br><span class="line">9581 Application --name a1 --conf-file conf&#x2F;example.conf</span><br><span class="line"></span><br><span class="line">或者使用ps命令也可以</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# ps -ef|grep flume</span><br><span class="line">root 9581 1500 0 10:54 pts&#x2F;0 00:00:00 &#x2F;data&#x2F;soft&#x2F;jdk1.8&#x2F;bin&#x2F;java</span><br><span class="line">root 9672 1500 0 10:57 pts&#x2F;0 00:00:00 grep --color&#x3D;auto flume</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时需要想要停止这个Agent的话就需要使用kill命令了</span><br></pre></td></tr></table></figure><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">哪个都可以，条条道路通罗马，具体就看你个人喜好了，ps命令显示的内容更为详细。</span><br><span class="line">这个Agent中的sink组件把数据以日志的方式写出去了，所以这个数据默认就会进入到flume的日志文件中，那我们来看一下flume的日志文件在flume的logs目录中有一个flume.log</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 logs]# tail -2 flume.log</span><br><span class="line">02 May 2020 10:54:28,211 INFO [lifecycleSupervisor-1-4] (org.apache.flume.so</span><br><span class="line">02 May 2020 10:54:28,215 INFO [lifecycleSupervisor-1-4] (org.apache.flume.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">再使用telnet向里面输入一些数据</span><br><span class="line">[root@bigdata04 ~]# telnet 192.168.182.103 44444</span><br><span class="line">Trying 192.168.182.103...</span><br><span class="line">Connected to 192.168.182.103.</span><br><span class="line">Escape character is &#39;^]&#39;.</span><br><span class="line">daemon</span><br><span class="line">OK</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">再回agent看</span><br><span class="line">[root@bigdata04 logs]# tail -2 flume.log</span><br><span class="line">02 May 2020 10:54:28,215 INFO [lifecycleSupervisor-1-4] (org.apache.flume.so</span><br><span class="line">02 May 2020 11:00:26,293 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果配置文件里不是配置的logger，在这个日志文件里就看不见了</span><br></pre></td></tr></table></figure><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Error downloading packages:</span><br><span class="line">  1:telnet-0.17-66.el7.x86_64: [Errno 256] No more mirrors to try.(ping www.baidu.com也ping不通)</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第七周 第1章 极速入门Flume</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC1%E7%AB%A0-%E6%9E%81%E9%80%9F%E5%85%A5%E9%97%A8Flume.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E4%B8%83%E5%91%A8-%E7%AC%AC1%E7%AB%A0-%E6%9E%81%E9%80%9F%E5%85%A5%E9%97%A8Flume.html</id>
    <published>2022-02-16T14:45:18.000Z</published>
    <updated>2022-02-20T04:05:14.532Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第七周-第1章-极速入门"><a href="#第七周-第1章-极速入门" class="headerlink" title="第七周 第1章 极速入门"></a>第七周 第1章 极速入门</h1><h2 id="什么是Flume"><a href="#什么是Flume" class="headerlink" title="什么是Flume"></a>什么是Flume</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">先来看一下官方解释</span><br><span class="line">Flume是一个高可用，高可靠，分布式的海量日志采集、聚合和传输的系统，能够有效的收集、聚合、移</span><br><span class="line">动大量的日志数据。</span><br><span class="line">其实通俗一点来说就是Flume是一个很靠谱，很方便、很强的日志采集工具。</span><br><span class="line">他是目前大数据领域数据采集最常用的一个框架</span><br><span class="line">为什么它这么香呢？</span><br><span class="line">主要是因为使用Flume采集数据不需要写一行代码，注意是一行代码都不需要，只需要在配置文件中随便</span><br><span class="line">写几行配置Flume就会死心塌地的给你干活了，是不是很香？</span><br><span class="line">看这个图，这个属于Flume的一个非常典型的应用场景，使用Flume采集数据，最终存储到HDFS上。</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H4pUiV" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/16/H4pUiV.png" alt="H4pUiV.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">左边的web server表示是一个web项目，web项目会产生日志数据，通过中间的Agent把日志数据采集到</span><br><span class="line">HDFS中。</span><br><span class="line">其中这个Agent就是我们使用Flume启动的一个代理，它是一个持续传输数据的服务，数据在Agent内部</span><br><span class="line">的这些组件之间传输的基本单位是Event</span><br><span class="line">从图中可以看到，Agent是由Source、Channel、Sink这三大组件组成的，这就是Flume中的三大核心组</span><br><span class="line">件，</span><br><span class="line">其中source是数据源，负责读取数据</span><br><span class="line">channel是临时存储数据的，source会把读取到的数据临时存储到channel中</span><br><span class="line">sink是负责从channel中读取数据的，最终将数据写出去，写到指定的目的地中</span><br><span class="line">后面我们会详细分析这三大组件。</span><br></pre></td></tr></table></figure><h2 id="Flume的特性"><a href="#Flume的特性" class="headerlink" title="Flume的特性"></a>Flume的特性</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 它有一个简单、灵活的基于流的数据流结构，这个其实就是刚才说的Agent内部有三大组件，数据通</span><br><span class="line">过这三大组件流动的</span><br><span class="line">2. 具有负载均衡机制和故障转移机制，这个后面我们会详细分析</span><br><span class="line">3. 一个简单可扩展的数据模型(Source、Channel、Sink)，这几个组件是可灵活组合的</span><br></pre></td></tr></table></figure><h2 id="Flume高级应用场景"><a href="#Flume高级应用场景" class="headerlink" title="Flume高级应用场景"></a>Flume高级应用场景</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们分析了Flume的典型常见应用场景，下面来看一下Flume的高级应用场景</span><br><span class="line">看这个图，这个图里面主要演示了Flume的多路输出，就是可以将采集到的一份数据输出到多个目的地</span><br><span class="line">中，不同目的地的数据对应不同的业务场景。</span><br></pre></td></tr></table></figure><h3 id="一对多的输出"><a href="#一对多的输出" class="headerlink" title="一对多的输出"></a>一对多的输出</h3><p><a href="https://imgtu.com/i/H49eOJ" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/16/H49eOJ.png" alt="H49eOJ.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">下面来详细分析一下</span><br><span class="line">这个图里面一共有两个Agent，表示我们启动了2个Flume的代理，或者可以理解为了启动了2个flume的</span><br><span class="line">进程。</span><br><span class="line">首先看左边这个agent，给他起个名字叫 foo</span><br><span class="line">这里面有一个source，source后面接了3个channel，表示source读取到的数据会重复发送给每个</span><br><span class="line">channel，每个channel中的数据都是一样的</span><br><span class="line">针对每个channel都接了一个sink，这三个sink负责读取对应channel中的数据，并且把数据输出到不同</span><br><span class="line">的目的地，</span><br><span class="line">sink1负责把数据写到hdfs中</span><br><span class="line">sink2负责把数据写到一个Java消息服务数据队列中</span><br><span class="line">sink3负责把数据写给另一个Agent</span><br><span class="line">注意了，Flume中多个Agent之间是可以连通的，只需要让前面Agent的sink组件把数据写到下一</span><br><span class="line">个Agent的source组件中即可。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">所以sink3就把数据输出到了Agent bar中</span><br><span class="line">在Agent bar中同样有三个组件，source组件其实就获取到了sink3发送过来的数据，然后把数据临时存</span><br><span class="line">储到自己的channel4中，最终再通过sink组件把数据写到其他地方。</span><br><span class="line">这就是这个场景的应用，把采集到的一份数据重复输出到不同的目的地中</span><br></pre></td></tr></table></figure><h3 id="flume的汇聚功能"><a href="#flume的汇聚功能" class="headerlink" title="flume的汇聚功能"></a>flume的汇聚功能</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接着再看下面这张图，这张图主要表示了flume的汇聚功能，就是多个Agent采集到的数据统一汇聚到一</span><br><span class="line">个Agent</span><br></pre></td></tr></table></figure><p><a href="https://imgtu.com/i/H4ifAA" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://s4.ax1x.com/2022/02/16/H4ifAA.png" alt="H4ifAA.png"></a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">下面来详细分析一下，</span><br><span class="line">这个图里面一共启动了四个agent，左边的三个agent都是负责采集对应web服务器中的日志数据，数据</span><br><span class="line">采集过来之后统一发送给agent4，最后agent4进行统一汇总，最终写入hdfs。</span><br><span class="line">这种架构的好处是后期如果要修改最终数据的输出目的地，只需要修改agent4中的sink即可，不需要修</span><br><span class="line">改agent1、2、3。</span><br><span class="line">但是这种架构也有弊端，</span><br><span class="line">1. 如果有很多个agent同时向agent4写数据，那么agent4会出现性能瓶颈，导致数据处理过慢</span><br><span class="line">2. 这种架构还存在单点故障问题，如果agent4挂了，那么所有的数据都断了。</span><br><span class="line">不过这些问题可以通过flume中的负载均衡和故障转移机制解决，后面我们会详细分析</span><br></pre></td></tr></table></figure><h2 id="Flume的三大核心组件"><a href="#Flume的三大核心组件" class="headerlink" title="Flume的三大核心组件"></a>Flume的三大核心组件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Flume的三大核心组件：</span><br><span class="line">Source：数据源</span><br><span class="line">Channel：临时存储数据的管道</span><br><span class="line">Sink：目的地</span><br><span class="line">接下来具体看一下这三大核心组件都是干什么的</span><br></pre></td></tr></table></figure><h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Source：数据源：通过source组件可以指定让Flume读取哪里的数据，然后将数据传递给后面的</span><br><span class="line">channel</span><br><span class="line">Flume内置支持读取很多种数据源，基于文件、基于目录、基于TCP\UDP端口、基于HTTP、Kafka的</span><br><span class="line">等等、当然了，如果这里面没有你喜欢的，他也是支持自定义的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">在这我们挑几个常用的看一下：</span><br><span class="line">Exec Source：实现文件监控，可以实时监控文件中的新增内容，类似于linux中的tail -f 效果。</span><br><span class="line">在这需要注意 tail -F 和 tail -f 的区别</span><br><span class="line">tail -F</span><br><span class="line">等同于–follow&#x3D;name --retry，根据文件名进行追踪，并保持重试，即该文件被删除或改名后，如果</span><br><span class="line">再次创建相同的文件名，会继续追踪</span><br><span class="line">tail -f</span><br><span class="line">等同于–follow&#x3D;descriptor，根据文件描述符进行追踪，当文件改名或被删除，追踪停止</span><br><span class="line">在实际工作中我们的日志数据一般都会通过log4j记录，log4j产生的日志文件名称是固定的，每天定</span><br><span class="line">时给文件重命名</span><br><span class="line">假设默认log4j会向access.log文件中写日志，每当凌晨0点的时候，log4j都会对文件进行重命名，在</span><br><span class="line">access后面添加昨天的日期，然后再创建新的access.log记录当天的新增日志数据。</span><br><span class="line">这个时候如果想要一直监控access.log文件中的新增日志数据的话，就需要使用tail -F</span><br></pre></td></tr></table></figure><h4 id="Exec-Source"><a href="#Exec-Source" class="headerlink" title="Exec Source"></a>Exec Source</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">NetCat TCP&#x2F;UDP Source： 采集指定端口(tcp、udp)的数据，可以读取流经端口的每一行数据</span><br><span class="line">Spooling Directory Source：采集文件夹里新增的文件</span><br><span class="line">Kafka Source：从Kafka消息队列中采集数据</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意了，前面我们分析的这几个source组件，其中execsource 和 kafkasource在实际工作中是最</span><br><span class="line">常见的，可以满足大部分的数据采集需求。</span><br></pre></td></tr></table></figure><h4 id="NetCat-TCP-UDP-Source"><a href="#NetCat-TCP-UDP-Source" class="headerlink" title="NetCat TCP/UDP Source"></a>NetCat TCP/UDP Source</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">采集指定端口(tcp、udp)的数据，可以读取流经端口的每一行数据</span><br></pre></td></tr></table></figure><h4 id="Spooling-Directory-Source"><a href="#Spooling-Directory-Source" class="headerlink" title="Spooling Directory Source"></a>Spooling Directory Source</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">采集文件夹里新增的文件</span><br></pre></td></tr></table></figure><h4 id="Kafka-Source"><a href="#Kafka-Source" class="headerlink" title="Kafka Source"></a>Kafka Source</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从Kafka消息队列中采集数据</span><br></pre></td></tr></table></figure><h3 id="channel"><a href="#channel" class="headerlink" title="channel"></a>channel</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Channel：接受Source发出的数据，可以把channel理解为一个临时存储数据的管道</span><br><span class="line">Channel的类型有很多：内存、文件，内存+文件、JDBC等</span><br><span class="line"></span><br><span class="line">接下来我们来分析一下</span><br></pre></td></tr></table></figure><h4 id="Memory-Channel"><a href="#Memory-Channel" class="headerlink" title="Memory Channel"></a>Memory Channel</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Memory Channel：使用内存作为数据的存储</span><br><span class="line">优点是效率高，因为就不涉及磁盘IO</span><br><span class="line">缺点有两个</span><br><span class="line">1：可能会丢数据，如果Flume的agent挂了，那么channel中的数据就丢失了。</span><br><span class="line">2：内存是有限的，会存在内存不够用的情况</span><br></pre></td></tr></table></figure><h4 id="File-Channel"><a href="#File-Channel" class="headerlink" title="File Channel"></a>File Channel</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">File Channel：使用文件来作为数据的存储</span><br><span class="line">优点是数据不会丢失</span><br><span class="line">缺点是效率相对内存来说会有点慢，但是这个慢并没有我们想象中的那么慢，</span><br><span class="line">所以这个也是比较常用的一种channel。</span><br></pre></td></tr></table></figure><h4 id="Spillable-Memory-Channel"><a href="#Spillable-Memory-Channel" class="headerlink" title="Spillable Memory Channel"></a>Spillable Memory Channel</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spillable Memory Channel：使用内存和文件作为数据存储，即先把数据存到内存中，如果内存中</span><br><span class="line">数据达到阈值再flush到文件中</span><br><span class="line">优点：解决了内存不够用的问题。</span><br><span class="line">缺点：还是存在数据丢失的风险</span><br></pre></td></tr></table></figure><h3 id="sink"><a href="#sink" class="headerlink" title="sink"></a>sink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Sink：从Channel中读取数据并存储到指定目的地</span><br><span class="line">Sink的表现形式有很多：打印到控制台、HDFS、Kafka等，</span><br><span class="line">注意：Channel中的数据直到进入目的地才会被删除，当Sink写入目的地失败后，可以自动重写，</span><br><span class="line">不会造成数据丢失，这块是有一个事务保证的。</span><br></pre></td></tr></table></figure><h4 id="常用的sink组件有"><a href="#常用的sink组件有" class="headerlink" title="常用的sink组件有"></a>常用的sink组件有</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Logger Sink：将数据作为日志处理，可以选择打印到控制台或者写到文件中，这个主要在测试的时</span><br><span class="line">候使用</span><br><span class="line">HDFS Sink：将数据传输到HDFS中，这个是比较常见的，主要针对离线计算的场景</span><br><span class="line">Kafka Sink：将数据发送到kafka消息队列中，这个也是比较常见的，主要针对实时计算场景，数据</span><br><span class="line">不落盘，实时传输，最后使用实时计算框架直接处理。</span><br></pre></td></tr></table></figure><h2 id="Flume安装部署"><a href="#Flume安装部署" class="headerlink" title="Flume安装部署"></a>Flume安装部署</h2><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><h3 id="Flume安装部署-1"><a href="#Flume安装部署-1" class="headerlink" title="Flume安装部署"></a>Flume安装部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">想要使用Flume采集数据，那肯定要先安装Flume</span><br><span class="line">在这里我重新克隆了一台Linux机器，主机名设置为bigdata04，ip设置为192.168.206.132</span><br><span class="line">屏蔽防火墙，安装jdk并配置环境变量，因为Flume是java开发，所以需要依赖jdk环境</span><br><span class="line"></span><br><span class="line">直接克隆的bigdata01虚拟机的当前状态：ip和hostname需要改，防火墙和java环境已经配置好了，ip映射需要添加</span><br><span class="line">这些工作已经提前做好了，继续往下面分析</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">安装包下载好以后上传到linux机器的&#x2F;data&#x2F;soft目录下，并且解压</span><br><span class="line"></span><br><span class="line">修改盘flume的env环境变量配置文件</span><br><span class="line">在flume的conf目录下，修改flume-env.sh.template的名字，去掉后缀template</span><br><span class="line"></span><br><span class="line">这样就好了，Flume的安装是不是很简单，这个时候我们不需要启动任何进程，只有在配置好采集任务之</span><br><span class="line">后才需要启动Flume。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>java使用技巧积累</title>
    <link href="http://tianyong.fun/java%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E7%A7%AF%E7%B4%AF.html"/>
    <id>http://tianyong.fun/java%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E7%A7%AF%E7%B4%AF.html</id>
    <published>2022-02-14T13:59:41.000Z</published>
    <updated>2022-02-15T15:04:26.832Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="java使用技巧积累"><a href="#java使用技巧积累" class="headerlink" title="java使用技巧积累"></a>java使用技巧积累</h1><h2 id="休眠固定时间"><a href="#休眠固定时间" class="headerlink" title="休眠固定时间"></a>休眠固定时间</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Thread.sleep((<span class="number">1</span>));</span><br></pre></td></tr></table></figure><h2 id="java生成文件"><a href="#java生成文件" class="headerlink" title="java生成文件"></a>java生成文件</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">generate_141M</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        String fileName = <span class="string">"D:\\s_name_141.dat"</span>;</span><br><span class="line">        System.out.println(<span class="string">"start: 开始生成141M文件-&gt;"</span> + fileName);</span><br><span class="line">        BufferedWriter bfw = <span class="keyword">new</span> BufferedWriter(<span class="keyword">new</span> FileWriter(fileName));</span><br><span class="line">        <span class="keyword">int</span> num = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (num &lt; <span class="number">8221592</span>) &#123;</span><br><span class="line">            bfw.write(<span class="string">"zhangsan beijing"</span>);</span><br><span class="line">            bfw.newLine();</span><br><span class="line">            num++;</span><br><span class="line">            <span class="keyword">if</span> (num % <span class="number">10000</span> == <span class="number">0</span>) &#123;</span><br><span class="line">                bfw.flush();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="字符串变整数"><a href="#字符串变整数" class="headerlink" title="字符串变整数"></a>字符串变整数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Integer.parseInt(&quot;2&quot;)</span><br><span class="line">生成long型的Long.parseLong(&quot;2&quot;)</span><br></pre></td></tr></table></figure><h2 id="制表符"><a href="#制表符" class="headerlink" title="制表符"></a>制表符</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;t</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="java" scheme="http://tianyong.fun/categories/java/"/>
    
    
  </entry>
  
  <entry>
    <title>java写文件和读文件</title>
    <link href="http://tianyong.fun/java%E5%86%99%E6%96%87%E4%BB%B6%E5%92%8C%E8%AF%BB%E6%96%87%E4%BB%B6.html"/>
    <id>http://tianyong.fun/java%E5%86%99%E6%96%87%E4%BB%B6%E5%92%8C%E8%AF%BB%E6%96%87%E4%BB%B6.html</id>
    <published>2022-02-14T13:02:15.000Z</published>
    <updated>2022-02-14T13:03:11.573Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="java写文件和读文件"><a href="#java写文件和读文件" class="headerlink" title="java写文件和读文件"></a>java写文件和读文件</h1><p><a href="https://www.cnblogs.com/rinack/p/14173936.html" target="_blank" rel="external nofollow noopener noreferrer">url</a></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="java" scheme="http://tianyong.fun/categories/java/"/>
    
    
  </entry>
  
</feed>
