<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2023-04-23T15:01:17.171Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大数据开发工程师-第十一周 Spark性能优化的道与术-Spark Streaming-6</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-6.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9A%84%E9%81%93%E4%B8%8E%E6%9C%AF-6.html</id>
    <published>2023-04-23T14:22:49.000Z</published>
    <updated>2023-04-23T15:01:17.171Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十一周-Spark性能优化的道与术-Spark-Streaming-6"><a href="#第十一周-Spark性能优化的道与术-Spark-Streaming-6" class="headerlink" title="第十一周 Spark性能优化的道与术-Spark Streaming-6"></a>第十一周 Spark性能优化的道与术-Spark Streaming-6</h1><h2 id="SparkStreaming-wordcount程序开发"><a href="#SparkStreaming-wordcount程序开发" class="headerlink" title="SparkStreaming wordcount程序开发"></a>SparkStreaming wordcount程序开发</h2><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面我们来学习一下Spark中的Spark streaming。针对Spark Streaming，我们主要讲一些基本的用法，因为目前在实时计算领域，flink的应用场景会更多。Spark streaming啊，它是Spark Core API的一种扩展。它可以用于进行大规模、高吞吐量、容错的实时数据流的处理。大家注意这个实时啊，属于近实时。最小可以支持秒级别的实时处理。</span><br><span class="line"></span><br><span class="line">那spark streaming的工作原理呢？是这样的。它呢会接收实时输入的数据流，然后呢，将数据啊拆分成多个Batch。比如呢，每收集一秒数据给它封装为一个batch，然后将每个batch呢交给这个spark计算引擎进行处理。最后呢，会产生出一个结果数据流。这个结果数据流里面数据呢，也是由一个一个的batch所组成的，所以说呢，spark streaming的实时处理，其实呢，就是一小批一小批的处理。那下面呢，我们就来开发一个spark streaming的实时wordcount程序来感受一下。</span><br><span class="line"></span><br><span class="line">现在我们来创建一个项目。it。点击这个auto。把它那个基本环境再配置一下，因为我们在这呢，也要写了一个SC，所以说在这点右键。到这儿。在这下面创建一个GALA。对吧，然后注意右键把它视为S对吧。接下来到这个depends里面，注意添加那个scholar的SDK，注意针对这个我们需要添加02:11的。因为我们使用的那个卡夫卡集群，它那个是02:11编译的啊。那个SC的版本，所以说呢，在这儿使用02:11。好，这样就行了。那下面呢，我们需要找一下它对应的一个依赖，SPA，人命的依赖。现在我们来说一下。就那个。用02:11的，注意我们之前用的是2.4.3这个版本。S。把这个除掉是吧。好。那这样基本环境就OK了。下面呢，我们来开发这个word的程序，在这呢，先建一个package。I1克点。八。the stream count。需求呢，是这样的。通过socket。模拟产生数据。实时计算数据中。单词出现的次数。这个没方法。好，那在这注意，我们需要先创建一个streaming。context。然后呢，指定数据处理。间隔。所以五秒吧。因为我们前面说了，你SPA死命，他这个实时处理，其实还是一小批一小批的处理，所以说你需要指定它这个一小批这个间隔是多少秒。那现在我们直接利用一个streaming。这边呢，首先传一个。配一样康复。注意第二个呢，才是这个距离的时间叫。五秒。SSC吧。注意，那我们在上面来创建这个。mark。康复配置对象。嗯。嗯。了，我们现在本意来执行。注意咱们之前啊，开发这个发个离线代码的时候，我们呢，穿的都是logo对吧，注意这时候呢。你需要这样来写LOGO2。什么意思呢？所以。止住了。LOCAL2。表示启动两个进程。一个进程。否则读取。数据源的数据一个进程。负责处理数据。ABB name。好，这样就行了。嗯。好，接下来我们来通过socket。获取实时产生的数据。B04端口9001。这个可以叫SRD。是吧，这里面也是RDD啊。下面我们就对接收到的数据使用。空格进行切割。转换成单个单词。改个e lines RD。第二，find map。加你的SP。按空格切就行啊。这样返回的就是wasd是里面呢包含了每个单词。这样把每个单词。转换成。淘宝兔的形式啊。what map对吧？这个RD。下面来执行reduce。BYK操作啊，所以基于K进行求和。嗯。it is by k。嗯。有我。啊。下面呢，将这个结果数据打印到控制台。嗯。认识。对吧，你看这个代码是不是也和咱们前面写那个link代码很像呀。启动任务。嗯。注意它下面这种写法不太一样，和那个SPA离线写法都不一样啊。等待任务形式啊。嗯。啊。好，这样的话你就可以开启一个发达人命实时流处理程序了。那我们在这呢，把这个socket给它打开。来执行。这个没事啊，还是那个when you choose啊，这个不用管了。把这个日志清一下，好，下面注意在这我来输入点数据。右。the me。回来看到没有， hello2161。只要说你输入的那两项数据在它的一个时间段之内，对吧，在五秒之内，它其实就把它切到一块儿了。这就可以啊。所以说呢，你可以这样理解，它相当于是每隔五秒把前五秒的数据给你封装成一个batch。然后后面呢，其实执行的就类似于Spark核心的那个代码Spark I的对吧。其实就是离线的那一套。它前面的话是按照时间去切这个小批，后面的话把你一小批一小批去处理，这样的话可以达到一个进食时的一个效果啊，所以说这个就是方向十命它的一个执行的原理。好，这是实现，接下来呢，我们使用加来实现一下。键package。SPA。world can&#39;t。加。把这个需求拿过来。那下面呢，是一个密方法，好，那接下注意首先还是要获取这套什么使命contact这些东西。先获取。创建。sten。啊。所以这里面你去Java这面你要获取这个。Java。streaming。context。嗯。后面呢，传的还是一个。时间。R。u减。second。嗯嗯。SIC。那上面还是要创建这个SPA配对项啊。嗯。but。嗯。master。LOCAL2。name。好，这样也可以。注意这块报错。对，报错了，一般是你那个包引错了。你可以看一下，把鼠标放到这个上面，你看。说什么这是什么Java FX里面什么？这是有问题的。对吧，我们用的话肯定是用Spark里面。对，其实啊，你这后面是少了一个S啊。嗯。这样也可以。你看这个时候用的是发使命里面的。好。下面是通过。获取实时产生的数据。soirit。这个点零四。等零一。来阿。对接收到的数据使用空格进行切割。转换成。三个单词。嗯。哪一点find map？对，那这里面的话，我们就需要写一个函数了啊。你有一个map function。然后要返回一个swim。我们就不把那个map的代码也写进去啊。这样的话，它是一个。我可以这样来直接来写ari。there as list，它里面呢，直接line there。后面做。直接。这样就可以啊，因为它最终返回一个联系啊。这种写法啊，它返回的速度，这样把这个速度转成list，再把它转成这个就可以。我咋？那接下来是把每个。单词转换。喂。double two的形式。我1MAP。所以呢，这里面也是需要写一个函数的啊。你有一个T。嗯。这个呢是in。嗯。因为你最终要法是一个P2列，这就淘宝里面第一列，淘宝里面第二列。new。double two。这呢，其实就是一个over了，把名字改一下啊，看起来清晰点。war。一。这个呢，叫派。安你。接下来实行。YK。嗯。reduce。function。I1 I2。这个呢，就叫word count。最后将结果数据啊引到。台。我们直接使用那个不好1D啊。这里面呢，我们给它传一个你一个VID方。这里面其实也好办，这个呢，就是一个。higher。不好意思。然后这里面的话，再给它传一个VD方式。对，这个就是具体那个他。嗯。嗯。相量二。这样的话就可以把里面这些数据啊，给它迭代出来。然后呢，就剩下最后这个。行任务。start。还有一个，等待任务停止。好一场，好一起。嗯。嗯。好，这就可以了。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket模拟产生数据，实时计算数据中单词出现的次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCountScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      <span class="comment">//注意：此处的local[2]表示启动2个进程，一个进程负责读取数据源的数据，一个进程负责处理数据</span></span><br><span class="line">      .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      .setAppName(<span class="string">"StreamWordCountScala"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建StreamingContext，指定数据处理间隔为5秒</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//通过socket获取实时产生的数据</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD = ssc.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对接收到的数据使用空格进行切割，转换成单个单词</span></span><br><span class="line">    <span class="keyword">val</span> wordsRDD = linesRDD.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把每个单词转换成tuple2的形式</span></span><br><span class="line">    <span class="keyword">val</span> tupRDD = wordsRDD.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//执行reduceByKey操作</span></span><br><span class="line">    <span class="keyword">val</span> wordcountRDD = tupRDD.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将结果数据打印到控制台</span></span><br><span class="line">    wordcountRDD.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232245799.png" alt="image-20230423224530561"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232245196.png" alt="image-20230423224547229"></p><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket模拟产生数据，实时计算数据中单词出现的次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamWordCountJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//创建SparkConf配置对象</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">                .setAppName(<span class="string">"StreamWordCountJava"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建StreamingContext</span></span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//通过socket获取实时产生的数据</span></span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; linesRDD = ssc.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//对接收到的数据使用空格进行切割，转换成单个单词</span></span><br><span class="line">        JavaDStream&lt;String&gt; wordsRDD = linesRDD.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(line.split(<span class="string">" "</span>)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//把每个单词转换为tuple2的形式</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairRDD = wordsRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行reduceByKey操作</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; wordCountRDD = pairRDD.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> i1 + i2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">//将结果数据打印到控制台</span></span><br><span class="line">        wordCountRDD.foreachRDD(<span class="keyword">new</span> VoidFunction&lt;JavaPairRDD&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(JavaPairRDD&lt;String, Integer&gt; pair)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                pair.foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        System.out.println(tup._1+<span class="string">"---"</span>+tup._2);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//启动任务</span></span><br><span class="line">        ssc.start();</span><br><span class="line">        <span class="comment">//等待任务停止</span></span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SparkStreaming整合Kafka"><a href="#SparkStreaming整合Kafka" class="headerlink" title="SparkStreaming整合Kafka"></a>SparkStreaming整合Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这个命和卡夫卡的整合。我们的需求是这样的，使用SPA人命实时消费卡夫卡中的数据。这种场景也是比较常见的。注意，在这你想使用卡夫卡，我们需要引入对应的一个依赖。它那个依赖是什么呢？到官网来看一下，进到官网点那个。文档SPA人命。你这里面来搜一下啊。就梳里的卡夫卡往下边走。到这块。看到没有，把这个点开右键啊，打开一个新页面。看到没有？你这个SPA命想要和卡夫卡进行交互，想要从卡夫卡里面去消费数据，你需要添加对应的依赖。这个呢是针对0.8.2.1级以上的，这个呢是针对卡夫卡零点十几以上的，那我们用的话肯定要用这个了，要用新一点的啊。那就这个东西。你可以。把它复制过来，到这儿来搜一下。就这个。2.4.3。格拉，02:11。把这个拿回来就可以了。嗯。这样就可以了。那下面呢，我们就来写一下具体的一个代码。swim。卡不卡？bug消费。卡夫卡中的数据。嗯。首先呢，还是希望见。streaming。嗯。context。康复。然后呢，还有一个second。嗯。有没有？我们还使用这个五秒。具体这个时间间隔啊，需要根据你们的业务而定啊。等于你有一个Spark。said master。LOCAL2。set name。嗯。获取消费卡夫卡的数据流。那怎么获取呢？现在我们需要用的这个卡夫卡u。csa。direct。这里面需要传两个泛型参数，three。SH。这里面呢，首先把这个SSC传给他。嗯，所以接下来需要第二个参数。什么那个location。street。这个。它这边会有提示啊。对吧，这几个参数。好使，用它呢，来调一个P开头的这个。就第一个就行。接下来只听下面那个。street。用这个。这个发型呢，还是string？对，这里面你要指定一下topic，对他接触的是一个topic s啊是一个。然后后面呢是一个map map里面传的是卡夫卡的一些参数。这里面这些都是固定写法啊。那下面我们就要呈现这两个。指定。指定卡夫卡的配置信息。好不好？等于一个map。object。我们直接在里面给它初始化就行啊。显得不报错。还有这个topic，在这一个它指定性。嗯。只要topics。注意，在这我们需要传一个。里面呢，可以同时使用多套贝。也就是说，它可以同时从多个topic里面去读取数据，都是可以的。那我们在这一个，我们就写一个就行。那既然把这个参数给它完善一下啊，嗯，首先需要指定卡不卡的。broke地址信息。would strive。B01。9092。零二。9092。039092。嗯。接下来我们需要指定那个K的序列化类型。K点。s Li。展开一个反序变化啊D。Siri。a。对啊。你如果怕拼数的话，可以先把后面这个写。后面的话，我们使用这个class of。spring。size对吧。可以把这个给它复制过去，把这个D改成小写就行了啊。这样也可以。嗯。还有这个value的。序列化类型。嗯。那就把这个改一下就行，改成V。都是死顿类型啊对，这就是咱们前面指定的那个对吧，配合没有一个泛型啊。嗯。那下面来指定那个。消费者ID。就那个YD。胳膊的ID。好，下面呢，再指定一下消费策略。there there。这块呢，只能一个。最后我们来指定一个自动提交。在设置啊。enable。there also？好。这样就可以了。这是一些核心的参数。OK，这样的话就可以从它里面去读取数据啊，这样就可以获取到一个类似于卡夫卡。我在这里面，我们可以把它称为。嗯，这是他们的一个概念啊。这个数据流。那下面我们就可以处理数据了。后面你可以调map啊，map啊这些算子去处理就可以。嗯。这样每次获取到一条数据啊，每次几的一条数据。然后把里面数据迭代出来之后呢，把它封装成一个他报，因为它本身呢是一个record一行记录，那我们在这呢。对的点。先获取的，你们K。然后再获取value。嗯。在这我们就把这个数据打印出来。将数据。印到后来。因为其实你在这只要能获取到数据，你后期你想做map map reduce go是不是都可以呀，对吧，那个就没什么区别了。启动任务。start。等待任务停止。嗯。嗯，好。那下面呢，我们把这个运行起来。但是呢，你发现这块他报错了。看到没有？31行。遇到问题不要怕啊，这个问题我们要排查一下。他说这个类型啊，有点问题。他呢，发现了是一个布尔类型，结果他需要了是一个OB。所以这块的话，你需要在这这样来指定一下。强制执行类型Java点拉点布尔。嗯。这样就可以了啊来执行。好，这样就可以了。那接下来呢，我们来开启一个生产者，往里面写点儿数据。其中一个卡不卡，剩下的我里面写着数据啊。其控制台的生产者。嗯。hello Spark。嗯。看到没有打一回啊。我把它停一下啊。注意。它这个呢，we know，为什么呀。因为现在我们卡卡里面数据啊，其实只有value是没有那个K的啊，所以说你K答出来是no，我们G的那些数据啊，一般都放到value里面啊。就是放到外。这个K的话是为了判定你这个数据到底是放哪个分区里面啊，一般会传一个K。所以说我们那种说法一般是不传的，然后随机分啊。这是没有问题的。那这样的话，我们就可以把那个卡字卡里面数据给他消费出来。是吧，那后面就可以实现你的业务逻辑。OK。那接下来呢，我们使用这个加代码来实现一下。卡夫卡。加了。嗯。把这个注释拿过来。好，首先呢，获取这个streaming context。在指定读取数据的。时间间隔为五秒。嗯。有一个Java。streaming context。好点。这个大家看五秒。嗯。new。said master。logo。嗯。下载APP name。把这个拿过来，嗯。而且这个变量Co。嗯。那接下来我们来获取消费卡夫卡的数据流。还是那个卡不卡。great。direct，那首先SC。后面还是一样的。嗯。嗯。嗯。嗯。嗯。好，接下来是这个。consumer。这个。there。首先是一个topics，还有一个。搞不搞？注意这块啊。你需要指定泛型，你这个泛型写到哪了。我们在SKY面里面是放在这个位置，但是在这里面你写这还是不对的啊，你在写前面。three。W。好。接下来创建这两个啊，把这个topic，还有这个卡夫卡。在线行。指令要读取的。名称。先写这个。three。topics。嗯。挨着。七。那接下来是这个。有一个map。object。好，不搞。下面就往里面添加参数了啊。RI。service。我们俩复制一下吧。这个又是提花。嗯。嗯嗯。第二个呢，是这个K的这个虚化类型。所以这个你别导错包了啊，你要导这个。巴阿巴奇，看到没有，卡布卡点common这个body。name。嗯。嗯嗯。说不爱你。嗯。also。offset。there reet。at。ne。点点commit。好，这样就可以了。of Australia。好，那接下来数数去。嗯。嗯嗯。选一个map，因为一个function。我们最终返回是一个double two。里面是一个string。LW。好，这个就是一个record。我们可以在这直接。你了一个。two，嗯。你告你。six。嗯。我看点160。这样转换成淘宝之后，后期用起来也方便。SP对吧。将数据打印的。启动任务，嗯。start。等待任务停止。嗯。好一场。嗯。嗯。好，这样就可以了，来。把它执行一下。好看没有，这是之前那条数据啊。我们可以再往里面加一条。哈哈哈。可以吧，也是可以的啊。好，这就是Java代码的一个实现。好，那针对Spark命这一块呢，我们暂时就讲到这儿，因为后期大部分的实施计算需求，我们需要使用link去实现了。在这呢，我们是把这个SPA命最常见那个消费卡不卡数据这种案例呢给大家讲一下。</span><br></pre></td></tr></table></figure><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spark 消费Kafka中的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"StreamKafkaScala"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定Kafka的配置信息</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Object</span>](</span><br><span class="line">      <span class="comment">//kafka的broker地址信息</span></span><br><span class="line">      <span class="string">"bootstrap.servers"</span>-&gt;<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>,</span><br><span class="line">      <span class="comment">//key的序列化类型</span></span><br><span class="line">      <span class="string">"key.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//value的序列化类型</span></span><br><span class="line">      <span class="string">"value.deserializer"</span>-&gt;classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="comment">//消费者组id</span></span><br><span class="line">      <span class="string">"group.id"</span>-&gt;<span class="string">"con_2"</span>,</span><br><span class="line">      <span class="comment">//消费策略</span></span><br><span class="line">      <span class="string">"auto.offset.reset"</span>-&gt;<span class="string">"latest"</span>,</span><br><span class="line">      <span class="comment">//自动提交offset</span></span><br><span class="line">      <span class="string">"enable.auto.commit"</span>-&gt;(<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//指定要读取的topic的名称</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"t1"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">    <span class="keyword">val</span> kafkaDStream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    kafkaDStream.map(record=&gt;(record.key(),record.value()))</span><br><span class="line">      <span class="comment">//将数据打印到控制台</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动任务</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待任务停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232301727.png" alt="image-20230423225935601"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232301407.png" alt="image-20230423225949397"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.spark;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spark 消费Kafka中的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="comment">//创建StreamingContext，指定读取数据的时间间隔为5秒</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">                .setAppName(<span class="string">"StreamKafkaJava"</span>);</span><br><span class="line">        JavaStreamingContext ssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafka的配置信息</span></span><br><span class="line">        HashMap&lt;String, Object&gt; kafkaParams = <span class="keyword">new</span> HashMap&lt;String, Object&gt;();</span><br><span class="line">        kafkaParams.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"key.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"value.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        kafkaParams.put(<span class="string">"group.id"</span>,<span class="string">"con_2"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"auto.offset.reset"</span>,<span class="string">"latest"</span>);</span><br><span class="line">        kafkaParams.put(<span class="string">"enable.auto.commit"</span>,<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定要读取的topic名称</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        topics.add(<span class="string">"t1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取消费kafka的数据流</span></span><br><span class="line">        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; kafkaStream = KafkaUtils.createDirectStream(</span><br><span class="line">                ssc,</span><br><span class="line">                LocationStrategies.PreferConsistent(),</span><br><span class="line">                ConsumerStrategies.&lt;String, String&gt;Subscribe(topics, kafkaParams)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">//处理数据</span></span><br><span class="line">        kafkaStream.map(<span class="keyword">new</span> Function&lt;ConsumerRecord&lt;String, String&gt;, Tuple2&lt;String,String&gt;&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(ConsumerRecord&lt;String, String&gt; record)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(record.key(),record.value());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();<span class="comment">//将数据打印到控制台</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//启动任务</span></span><br><span class="line">        ssc.start();</span><br><span class="line">        <span class="comment">//等待任务停止</span></span><br><span class="line">        ssc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-3.html</id>
    <published>2023-04-20T08:46:48.000Z</published>
    <updated>2023-04-20T08:46:48.353Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-2.html</id>
    <published>2023-04-20T08:46:43.000Z</published>
    <updated>2023-04-23T14:21:05.429Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十七周-Flink极速上手篇-Flink高级进阶之路-2"><a href="#第十七周-Flink极速上手篇-Flink高级进阶之路-2" class="headerlink" title="第十七周 Flink极速上手篇-Flink高级进阶之路-2"></a>第十七周 Flink极速上手篇-Flink高级进阶之路-2</h1><h2 id="Kafka-Consumer的使用"><a href="#Kafka-Consumer的使用" class="headerlink" title="Kafka Consumer的使用"></a>Kafka Consumer的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们来看一下flink中针对kafka connect的专题，提供了很多的connect组件，其中应用比较广泛的就是kafka这个connect。我们就针对kafka在flink的应用做详细的分析。针对flink流处里啊，最常用的组件就是kafka。原始日志数据产生后，会被日志采集工具采集到kafka中，让flink去处理。处理之后的数据可能也会继续写入到kafka中。kafka可以作为flink的datasource和datasink来使用。并且kafka中的partition机制和flink的并行度机制可以深度结合，提高数据的读取效率和写入效率。那我们想要在flink中使用kafka，需要添加对应的依赖。(先在flink官网中找到依赖的名字，再到maven中去找符合的版本)</span><br><span class="line"></span><br><span class="line">那在具体执行这个代码之前啊，我们先需要把那个zookeeper集群，还有kafka集群给他起来。我这些相关的服务呢，已经起来了。这是入K班了，这是卡不卡都已经起来啊。好，那下面注意，我们还需要做一件事情。因为我们在这里面呢，用到了一个T1这个topic，所以说在这我们需要去创建这个topic。找一下之前的命令。嗯。其一。这个分区设置为五，后面因子是为二。发现一个T。好，创建成功。那下面呢，我们就可以去启动代码。启动电板之后，那我们需要往那个卡夫卡里面模拟产生数据。这个时候呢，我们可以启动一个基于控制台的一个生产者来模拟产生数据。嗯。使用这个卡不卡console producer。把这个复制一下。好，这个套背上就是T1。那这个时候呢，我们接着就来模拟产生数据。hello。看到没有消费到。再加一个。hello。没问题吧，是可以的，这样的话我们就可以消费卡夫卡中的数据了。好，这个是代码实践，接下来我们使用这个Java代码来实现一下。先创建一个package。嗯。stream。搞不搞？SS。把这个复制过来。嗯。嗯嗯。嗯。好，首先呢，还是获取一个连环应。execution。因为第2GET。因为。下面的env.S。嗯。在这儿，我们需要去利用这个。Li。卡不卡？three。那这里面啊，传一个topic。嗯。第一，嗯。第二个，你有一个simple。视频，game。第三个pop。嗯嗯。有一个薄。嗯。嗯嗯嗯。首先呢，我们在里面set property。我可以把这个呢直接拿过来。嗯。好，下面呢，said。格布利。卡不卡星本。把它拿过来。嗯。感注释，这个就是指定卡夫卡作为S。嗯。这是指令。普林格卡夫卡consumer的相关配置。接下来呢，我们将读取到的数据啊，一到控制台。嗯。嗯。嗯嗯。嗯。嗯。包的异常。嗯。来把这个启动起来。好，那我们在这边呢，再模拟产生的数据。哈哈哈。没问题吧，是可以的。好，这就是Java代码的一个实现。</span><br></pre></td></tr></table></figure><h3 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">      env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>() prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304231050885.png" alt="image-20230423105004127"></p><h3 id="java"><a href="#java" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.kafkaconnector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaSourceJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">        String topic = <span class="string">"t1"</span>;</span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">        prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = <span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), prop);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafka作为source</span></span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.addSource(kafkaConsumer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将读取到的数据打印到控制台</span></span><br><span class="line">        text.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"StreamKafkaSourceJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KafkaConsumer消费策略设置"><a href="#KafkaConsumer消费策略设置" class="headerlink" title="KafkaConsumer消费策略设置"></a>KafkaConsumer消费策略设置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对卡夫卡康消费数据的时候会有一些策略，我们来看一下。首先这个是默认的消费策略。下面还有一个ear，从最早的开始消费，latest，从最新的开始消费。已经呢，现在要出来一个C呢。按照指定的时间戳往后面开始消费。下面呢，我们来演示一下。我直接在这里面来设置一下。嗯。卡夫卡consumer。的消费策略设置。这个其实我们在讲卡不卡的时候也详细分解过啊，其实是类似的。首先我们看下这个默认策略。嗯。直接使用它来设置点带。start from group of，注意这个呢，其实就默认你不设置它默就类。它是什么意思呢？它会读取。group ID。对应。保存的outside。开始消费数据。那读取不到的话呢。则根据卡夫卡中。这个参数auto点。reite。参数的值开始。消费数据。因为如果说你是第一次使用这个消费者，那么他之前肯定是没有保存这个对应的office的信息，那这样的话呢，他就会根据这个参数的值来开始进行消费。那这个值的话，它那要么是early latest对吧，要么是从最新的，要么是从最近的。那下一次的话呢，他就会根据你之前指定的这个global ID对应的保存的那个开始往下面继续消费数据。那既然下面这个呢，是从那个最早的记录开始，消费主义啊，不搞consumer there that。from earliest。从最早的记录。开始消费。独具。忽略。你提交。信息。这样的话，他就不管你有没有提交，都会每次都从那个最早的数据开始消费。那对了，还有一个从。最新的记录开始消费。也是忽略这个已提交的奥赛的信息。嗯。start。home latest。嗯。那还有一个是从指定的时间戳开始消费数据。对于每个分区。其时间戳大于或等于指定时间戳的记录。江北。作为70位。嗯。that。大的from。这里面你给他传一个时间戳就行了啊，我这边随便写一个行吗？这就是这几种测量啊。其实我们在讲卡不卡的时候，也详细分析过这几种词，那在那就把这个默认的给它打开吧。就你这个呢，你在这儿设置不设置，它其实都是一个默认的策略。这个呢，就是针对这里面这个卡夫卡抗性板，它这个消费策略的一个设置。其实咱们在工作中啊，一般最常见的，那其实就是一种默认的技术。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304231057297.png" alt="image-20230423105719815"></p><h3 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KafkaConsumer的容错"><a href="#KafkaConsumer的容错" class="headerlink" title="KafkaConsumer的容错"></a>KafkaConsumer的容错</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下kafka consumer的容错。flink中呢也有这个checkpoint机制，checkpoint呢是flink实现容错机制的核心功能，它能够根据配置周期性的基于流中各个算子任务的state来生成快照。从而将这些state的数据定期持久化存储下来。当flink的程序一旦意外崩溃时，重新运行程序时，可以有选择的从这些快照进行恢复。从而修正因为故障带来的程序数据异常。</span><br><span class="line"></span><br><span class="line">当这个checkpoint机制开启的时候。consumer呢？它会定期把kafka的offset的信息，还有其他算子任务的信息一块保存起来。当job失败重启的时候，弗会从最近一次的checkpoint中进行恢复数据。重新消费kafka中的数据。那为了能够使用支持容错的kafka，我们需要开启checkpoint，那如何开启呢？很简单。直接呢，就env.enableCheckpointing，指定一个周期就行。这个5000呢，表示五秒，就是每隔五秒执行一次checkpoint。他会周期性的执行。</span><br><span class="line">来，我们来看一下。我呢，就直接在这儿。来配置啊。en enable check。这毫秒啊。每隔5000毫秒。执行一次checkpoint。这个呢，其实就是设置。这个point的周期啊。那针对这个呢，它还有一些相关的配置。那我接着呢，把这个配置拿过来。把这个复制一下。搞一下包。这个是。针对checkpoint的相关配置。下面这个参数的意思呢？表示设置一下checkpoint的一个语义，它可以提供这种锦一词的语义。下面这个呢，表示两次切之间它的一个时间间隔。这个呢，表示呢，必须要在指定时间之内完成one。其实就是给这个check呢，设置一个超时时间，超过这个时间了就被丢弃了。下面这个呢，表示呢，同一时间只允许执行一个checkpoint。下面这个三注意。他呢表示呀，当我们对这个link程序执行一个cancel之后，就是把这个link程序停掉之后，我们呢，会保留这个这个波段数据，这样的话，我们可以根据实际需要，后期呢来恢复这些数据。这是它相关的一些配置啊，</span><br><span class="line"></span><br><span class="line">那其实呢，在这块还有一个配置。设置这个state数据存储的位置，默认情况下的数据会保存在task manager内存中。当我们执行checkpoint的时候呢，会将这个实际的数据存储到jobmanager内存中。这个具体的存储位置呢，取决于StateBackend的配置。FLink呢，一共提供了三种存储方式，第一种是MemoryStateBackend，第二种呢是FsStateBackend，第三种是RocksSBStateBackend。</span><br><span class="line"></span><br><span class="line">我们先分析一下第一种基于内存的。这个时候呢，state数据保存在这个Java堆内存中，当我们执行checkpoint的时候，它呢会把state快照数据啊保存到job manager的内存中。基于内存的呢，在生产环境下面不建议使用。对吧，因为你重启之后，它内存里面数据就没了，所以说是没有意义的。</span><br><span class="line"></span><br><span class="line">第二种呢，FsStateBackend。数据呢？保存在task measure内存中，当我们执行check point的时候，会把的快照数据保存到配置的文件系统中。我们可以使用hdfs等分布式文件系统。这个呢是可以用的。</span><br><span class="line"></span><br><span class="line">当然还有一种叫RocksSBStateBackend啊，它跟上面的都略有不同，它会在本地文件系统中维护这个state。state的会直接写入本地的RocksDB中。同时它需要配置一个远端的文件系统，一般呢是Hdfs。那我们在做checkpoint的时候。会把本地的数据直接复制到远端的文件系统中。故障切换的时候，直接从远端的文件系统中恢复数据到本地。RocksDB克服了state的受内存限制的缺点，同时又能够持久化到远端文件系统中。推荐在生产环境中使用。</span><br><span class="line"></span><br><span class="line">所以在这里我们使用第三种RocksSBStateBackend</span><br><span class="line">maven添加依赖flink-statebackend-rocksdb</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.contrib.streaming.state.<span class="type">RocksDBStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.<span class="type">CheckpointConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink从kafka中消费数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSourceScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每隔5000 ms执行一次checkpoint(设置checkpoint的周期)</span></span><br><span class="line">    env.enableCheckpointing(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//针对checkpoint的相关配置</span></span><br><span class="line">    <span class="comment">//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointingMode(<span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    <span class="comment">//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)</span></span><br><span class="line">    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>)</span><br><span class="line">    <span class="comment">//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)</span></span><br><span class="line">    env.getCheckpointConfig.setCheckpointTimeout(<span class="number">60000</span>)</span><br><span class="line">    <span class="comment">//同一时间只允许执行一个Checkpoint</span></span><br><span class="line">    env.getCheckpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint</span></span><br><span class="line">      env.getCheckpointConfig.enableExternalizedCheckpoints(<span class="type">CheckpointConfig</span>.<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line">    <span class="comment">//设置状态数据存储的位置</span></span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://bigdata01:9000/flink/checkpoints"</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaConsumer相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t1"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line">    prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"con1"</span>)</span><br><span class="line">    <span class="keyword">val</span> kafkaConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka consumer的消费策略设置</span></span><br><span class="line">    <span class="comment">//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据</span></span><br><span class="line">    kafkaConsumer.setStartFromGroupOffsets()</span><br><span class="line">    <span class="comment">//从最早的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromEarliest()</span></span><br><span class="line">    <span class="comment">//从最新的记录开始消费数据，忽略已提交的offset信息</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromLatest()</span></span><br><span class="line">    <span class="comment">//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置</span></span><br><span class="line">    <span class="comment">//kafkaConsumer.setStartFromTimestamp(176288819)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为source</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> text = env.addSource(kafkaConsumer)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将读取到的数据打印到控制台上</span></span><br><span class="line">    text.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSourceScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106563.png" alt="image-20230423210612299"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106374.png" alt="image-20230423210628262"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232106556.png" alt="image-20230423210654942"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232107250.png" alt="image-20230423210749485"></p><h3 id="Kafka-Consumers-Offset自动提交"><a href="#Kafka-Consumers-Offset自动提交" class="headerlink" title="Kafka Consumers Offset自动提交"></a>Kafka Consumers Offset自动提交</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink没有开启checkpoint时，offset的提交由之前的enable.auto.commit和auto.commit.interval.ms决定</span><br><span class="line"></span><br><span class="line">当开启了，由checkpoint每次执行时提交</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232113418.png" alt="image-20230423211307520"></p><h2 id="KafkaProducer的使用"><a href="#KafkaProducer的使用" class="headerlink" title="KafkaProducer的使用"></a>KafkaProducer的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下在flink中如何向kafka中写数据，此时需要用到kafka producer。</span><br><span class="line"></span><br><span class="line">所有的数据都写入指定topic的一个分区里面。注意，他会把所有数据写到这个topic的一个分区。那这样的话，其实呢，在我们实习当中，这样是不合适的啊。我们使用操作的肯定是要使用多个分区，你要把数据分别写到不同的分区里面，这样的话后期我们去消费也可以并行消费，提高消费能力，对吧？那你如果都搞一个分区里面，那其实相当于我这个topic卡就一个分区。这样后期我这个处理能力是有限制的，所以说呢，如果不想自定义分具体。也不想使用默认的可以直接。使用一个null即可。</span><br></pre></td></tr></table></figure><h3 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaProducer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.internals.<span class="type">KafkaSerializationSchemaWrapper</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.partitioner.<span class="type">FlinkFixedPartitioner</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink向Kafka中生产数据 </span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamKafkaSinkScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启checkpoint</span></span><br><span class="line">    <span class="comment">//env.enableCheckpointing(5000)</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定FlinkKafkaProducer的相关配置</span></span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">"t2"</span></span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//指定kafka作为sink</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     KafkaSerializationSchemaWrapper的几个参数</span></span><br><span class="line"><span class="comment">     1：topic：指定需要写入的topic名称即可</span></span><br><span class="line"><span class="comment">     2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中</span></span><br><span class="line"><span class="comment">     默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面</span></span><br><span class="line"><span class="comment">     如果不想自定义分区器，也不想使用默认的，可以直接使用null即可</span></span><br><span class="line"><span class="comment">     3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳</span></span><br><span class="line"><span class="comment">     如果写入了，那么在watermark的案例中，使用extractTimestamp()提起时间戳的时候</span></span><br><span class="line"><span class="comment">     就可以直接使用recordTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> kafkaProducer = <span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](topic, <span class="keyword">new</span> <span class="type">KafkaSerializationSchemaWrapper</span>[<span class="type">String</span>](topic, <span class="literal">null</span>, <span class="literal">false</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()), prop, <span class="type">FlinkKafkaProducer</span>.<span class="type">Semantic</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    text.addSink(kafkaProducer)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"StreamKafkaSinkScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232135912.png" alt="image-20230423213547295"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232137221.png" alt="image-20230423213734278"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232138202.png" alt="image-20230423213834818"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmak里查看</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232144712.png" alt="image-20230423214449177"></p><h3 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.kafkaconnector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flink向Kafka中生产数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamKafkaSinkJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定FlinkKafkaProducer相关配置</span></span><br><span class="line">        String topic = <span class="string">"t2"</span>;</span><br><span class="line">        Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">        prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//指定kafak作为sink</span></span><br><span class="line">        FlinkKafkaProducer&lt;String&gt; kafkaProducer = <span class="keyword">new</span> FlinkKafkaProducer&lt;&gt;(topic, <span class="keyword">new</span> KafkaSerializationSchemaWrapper&lt;String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">new</span> SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE);</span><br><span class="line">        text.addSink(kafkaProducer);</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"StreamKafkaSinkJava"</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="KafkaProducer的容错"><a href="#KafkaProducer的容错" class="headerlink" title="KafkaProducer的容错"></a>KafkaProducer的容错</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下kafkaProducer的容错。如果flink开启了checkpoint，那针对flink kafka producer可以提供仅一次予以保证。我们可以通过这个参数来指定三种不同的语义。</span><br><span class="line">点就是不支持任何语音，这个呢是至少一次，这个呢仅一次，那more呢是至少一次。好，那在代码里面怎么体现呢？来看一下。注意看到没有，我们刚才指的就是一个锦一四啊。但是这个时候注意你还要开启这个table。开启这个了。下面那些参数我暂时就先不指定了，行吗？好，那下面呢，我们来执行一下。来开启这个稍的。好注意这时候呢，给大家看一个比较神奇的现象。你看刚才我们把这个搜下打开啊，结果它停了，你再把它打开。他还会请。看没有？什么原因呢？那时候我这块也没有报错呀。注意这个呢，是因为这个原因。我们之前啊，在这加了一个logo的配置文件，对吧。注意这个日级别，我之前给它改成error。我们把这个调一下。调成那个警告级别。因为这个时候有一些日他没有打出来警告信息看不到啊。来再启动把这个打开。对，他这个其实应该是error级别的，但是他写的什么写的不太好，他把这个日志写成那种warning级别，警告级别的，所以说呢，我们之前使用那个error级别的，监控不到这些日志信息啊。啊，停一下吧。</span><br><span class="line"></span><br><span class="line">来分析一下啊。不要往后面看这。还有什么呀，这个事物时间比这个博客里面配置的这个时间还要大。就是说，生产者中设置的事物超时时间大于卡夫卡博客中设置的事物超时时间。因为卡夫卡服务中默认事物的超时时间是15分钟，但是呢，弗林格卡夫卡保留它里面设置的事物超时间默认是一小时，这个仅一次语义啊，它需要依赖这个事物。如果从Li应用程序崩溃到完全重启的时间超过了卡夫卡的事物超时时间，那么将会有数据丢失，所以我们需要合理的配置事物超时时间。因此，在使用这个仅一次语义之前，建议增加卡夫卡博克中这个transaction.max.timeout.ms的值。把这个值啊给它调大。那下面呢，我们就来修改一下卡夫卡里面这个配置，这个配置在哪啊，其实就那个server.properties里面啊。买那个可以试一下。它里面是没有这个参数的，你直接在这把它拿过来。给它做个值。我们也给它改成一小时吧。这个你转换成毫秒是3600000。那么是五个零啊，这样的话就一小时。把这个复制一下。对，这个集群里面所有机器都要改啊。嗯。好，可以了，注意改完之后我们需要重写。那你先把这个卡夫卡集群停掉。好停掉之后再去启动，启动的话，我们使用它这个命令啊。前面加了一个GMX，这样的话我们可以使那个CMA来减轻它里面一些信息啊。嗯。好，这个起来了。嗯。嗯。这个呢也可以啊。好，这个也可以了。那接下来我们重新再执行这个样本，对吧，把这个再看一下。嗯。看到没有，此时他就不报错了啊，我们可以在这来验证一下，先确一下里面的数据对吧。是这样。5211。这个停了，因为刚才我们把那个卡夫卡停掉之后啊，这个c map哎，就停掉了。把它起来。所以这个没不对。嗯。嗯。第三。对吧，这里面是这了来。我们输点作业。好变了吧，对吧。说明这个数据写进来了，并且这块呢也没报错啊。OK，这样就可以了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304232152824.png" alt="image-20230423215255115"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.kafkaconnector</span><br><span class="line"></span><br><span class="line">import java.util.Properties</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Flink向Kafka中生产数据 </span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object StreamKafkaSinkScala &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;开启checkpoint</span><br><span class="line">    env.enableCheckpointing(5000)</span><br><span class="line">      </span><br><span class="line">    val text &#x3D; env.socketTextStream(&quot;bigdata04&quot;, 9001)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;指定FlinkKafkaProducer的相关配置</span><br><span class="line">    val topic &#x3D; &quot;t2&quot;</span><br><span class="line">    val prop &#x3D; new Properties()</span><br><span class="line">    prop.setProperty(&quot;bootstrap.servers&quot;,&quot;bigdata01:9092,bigdata02:9092,bigdata03:9092&quot;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;指定kafka作为sink</span><br><span class="line">    &#x2F;*</span><br><span class="line">     KafkaSerializationSchemaWrapper的几个参数</span><br><span class="line">     1：topic：指定需要写入的topic名称即可</span><br><span class="line">     2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中</span><br><span class="line">     默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面</span><br><span class="line">     如果不想自定义分区器，也不想使用默认的，可以直接使用null即可</span><br><span class="line">     3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳</span><br><span class="line">     如果写入了，那么在watermark的案例中，使用extractTimestamp()提起时间戳的时候</span><br><span class="line">     就可以直接使用recordTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp</span><br><span class="line">     *&#x2F;</span><br><span class="line">    val kafkaProducer &#x3D; new FlinkKafkaProducer[String](topic, new KafkaSerializationSchemaWrapper[String](topic, null, false, new SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE)</span><br><span class="line">    text.addSink(kafkaProducer)</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;StreamKafkaSinkScala&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十七周 Flink极速上手篇-Flink高级进阶之路-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%83%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%B7%AF-1.html</id>
    <published>2023-04-20T08:44:51.000Z</published>
    <updated>2023-04-22T10:36:03.302Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十七周-Flink极速上手篇-Flink高级进阶之路-1"><a href="#第十七周-Flink极速上手篇-Flink高级进阶之路-1" class="headerlink" title="第十七周 Flink极速上手篇-Flink高级进阶之路-1"></a>第十七周 Flink极速上手篇-Flink高级进阶之路-1</h1><h2 id="Window的概念和类型"><a href="#Window的概念和类型" class="headerlink" title="Window的概念和类型"></a>Window的概念和类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">大家好，前面我们学习了flink中的基本概念，集群部署以及核心API的使用，下面我们来学习一下flink中的高级特性的使用。首先，我们需要掌握中的window、time以及whatermark使用。然后我们需要掌握kafka-connector使用，这个是针对kafka一个专题。最后我们会学习一下Spark中的流式计算sparkStreaming，之前在学习spark的时候我们没有涉及这块，在这儿我们和flink一块来学习，可以加深理解，因为它们都是流式计算引擎。</span><br><span class="line"></span><br><span class="line">下面呢，我们首先进入第一块flink中的window和time。flink认为批处理是流处理的一个特例，所以flink底层引擎是一个流式引擎，这上面呢实现了流处理和批处理。而window呢，就是从流处理到批处理的一个桥梁。通常来讲啊，这个window啊，是一种可以把无界数据切割为有界数据块的手段，例如对流动的所有元素进行计数是不可能的，因为通常流是无限的。或者呢，可以称之为是无界了。所以说流上的聚合需要由window来划分范围，比如计算过去五分钟或者最后100个元素的和。</span><br><span class="line"></span><br><span class="line">window可以是以时间驱动的time window，例如每30秒，或者是以数据驱动的count window，例如每100个元素。DataStream API提供了基于time和count的window。同时，由于某些特殊的需要，dataStreamAPI也提供了定制化的window操作，供用户自定义window。</span><br><span class="line"></span><br><span class="line">这个window呀，根据类型可以分为这两种。第一种是滚动窗口，它呢表示窗口内的数据没有重叠，第二种呢是滑动窗口，它呢表示窗口内的数据有重叠。</span><br><span class="line"></span><br><span class="line">那下面我们来看个图分析一下，首先看这个滚动窗口，这个S轴呢是一个时间轴，你看这个是一个窗口的大小，这是WINDOW1 window2 window3，注意每个窗口内的数据是没有重叠的，这个就是滚动窗口。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201704808.png" alt="image-20230420170417289"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面来看这个滑动窗口，这个S轴呢，还是一个时间轴，你看这个是一个温度的大小。这个表示是一个温度的滑动间隔，这是WINDOW1这个红色的，它这个窗口从这到这儿，下面这个呢，WINDOW2，注意这个窗口它是从这儿到这儿，这个蓝色的看到没有，它里面呢，包含了WINDOW1里面的一部分数据。那你看WINDOW3 window3里面它包含了WINDOW2里面的一部分数据，所以说这个滑动窗口，它们每个窗口之间呀，会有数据重叠，这个就这两种窗口它的一个区别</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201705935.png" alt="image-20230420170515739"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我针对这个窗口的类型做了一个汇总。你看这是window window下面有time window有count window还有自定义window，那这些window再往下面你看它呢，可以实现滚动窗口或者滑动窗口，对吧？不管你是基于time的，还是基于count的，还是自定义的，你们都可以实现滚动窗口或者是滑动窗口。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201707926.png" alt="image-20230420170745169"></p><h3 id="TimeWindow的使用"><a href="#TimeWindow的使用" class="headerlink" title="TimeWindow的使用"></a>TimeWindow的使用</h3><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这些window的具体应用，首先来看第一个time window。time window呢是根据时间对数据流切分窗口，time window可以支持滚动窗口和滑动窗口。</span><br><span class="line"></span><br><span class="line">其中它有这么两种用法，来看一下time window。</span><br><span class="line">timeWindow(Time.seconds(10))</span><br><span class="line">注意，首先这个。他呢是表示。滚动窗口的窗口大小为十秒。对每十秒内的数据,进行聚合计算。这个呢，其实就是设置一个滚动窗口。</span><br><span class="line"></span><br><span class="line">timeWindow(Time.seconds(10),Time.seconds(5))</span><br><span class="line">那下面这个呢，对应的它设置的就是一个滑动窗口，因为它除了有一个窗口大小，它还滑动一个间隔。表示滑动窗口的窗口大小为十秒,滑动间隔为五秒,就是每隔五秒计算前十秒内的数据，所以说是两种用法，一种是滚动，一种是滑动。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TimeWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TimeWindowOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//TimeWindow之滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    <span class="comment">/*text.flatMap(_.split(" "))</span></span><br><span class="line"><span class="comment">      .map((_,1))</span></span><br><span class="line"><span class="comment">      .keyBy(0)</span></span><br><span class="line"><span class="comment">      //窗口大小</span></span><br><span class="line"><span class="comment">      .timeWindow(Time.seconds(10))</span></span><br><span class="line"><span class="comment">      .sum(1)</span></span><br><span class="line"><span class="comment">      .print()*/</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//TimeWindow之滑动窗口：每隔5秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">        .timeWindow(<span class="type">Time</span>.seconds(<span class="number">10</span>),<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print()</span><br><span class="line">    env.execute(<span class="string">"TimeWindowOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeWindow滚动窗口</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201729458.png" alt="image-20230420172901278"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201729476.png" alt="image-20230420172914398"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timeWindow滑动窗口，黑色第一次输入，蓝色第二次输入</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201732506.png" alt="image-20230420173241489"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第三次打印，蓝色</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201736543.png" alt="image-20230420173625064"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TimeWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeWindowOpJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//TimeWindow之滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        <span class="comment">/*text.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">            @Override</span></span><br><span class="line"><span class="comment">            public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span><br><span class="line"><span class="comment">                    throws Exception &#123;</span></span><br><span class="line"><span class="comment">                String[] words = line.split(" ");</span></span><br><span class="line"><span class="comment">                for (String word: words) &#123;</span></span><br><span class="line"><span class="comment">                    out.collect(new Tuple2&lt;String, Integer&gt;(word,1));</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;).keyBy(0)</span></span><br><span class="line"><span class="comment">                //窗口大小</span></span><br><span class="line"><span class="comment">                .timeWindow(Time.seconds(10))</span></span><br><span class="line"><span class="comment">                .sum(1)</span></span><br><span class="line"><span class="comment">                .print();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//TimeWindow之滑动窗口：每隔5秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word: words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">10</span>),Time.seconds(<span class="number">5</span>))</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"TimeWindowOpJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CountWindow的使用"><a href="#CountWindow的使用" class="headerlink" title="CountWindow的使用"></a>CountWindow的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下count的Window的使用，count Window是根据元素个数对数据流切分窗口。count window也可以支持滚动窗口和滑动窗口。</span><br><span class="line">countWindow(5) 表示滚动窗口的大小,是五个元素。也就是当窗口中填满五个元素的时候，就会对窗口进行计算</span><br><span class="line">countWindow(5,1)</span><br><span class="line">表示滑动窗口的窗口大小是五个元素，滑动的间隔为一个元素，也就是说每新增一个元素就会对前面五个元素计算一次</span><br><span class="line"></span><br><span class="line">那我们再验证一下。有有有。写四个啊。看到没有，这个又要直行了。对吧，你后面再加哈，注意。此时呢，它就不会再执行了，因为你是一个滚动窗口啊，最终呢，你再满足有五个元素之后，它才会重新执行，这是一个滚动窗口。把这个听一下。把这个注意事项啊，我们给它加进来，这是一个解释啊。由于我们在这里使用了可以。会相对数据分组。如果某个分组对应的数据窗口，数据窗口内达到了五个元素，这个窗口才会被主发执行，如果你不使用KPI的话，他就不会在这儿做区分了，所以他接收到所有的数据，在这儿会统一计算。不过那个时候你就需要使用这个count window or这个咱们后面再分析啊，接着我们先使用这个K方式，后面呢直接使用这个count window，好，这是一个滚动窗口，下面呢，我们来实现一个滑动窗口。它的豌豆之滑动窗口。每隔一个元素计算一次前五个元素。map。空格切一下。小点一。零零，它的window，注意第一个参数是窗口大小，第二个是滑动间隔。嗯。窗口大小。第二个参数。滑动间隔。一。BA。好，那接着要把上面这个的读调，嗯。把这个socket呢，再给它打开。嗯。好，那我们到这儿来数数句，hello you。注意它直行了，为什么呀，因为它的滑动间隔是一，只要间隔一个元素，它就会执行，它呢会往前推找五个元素，但是它前面并没有五个元素，就只有这一个，所以说最终的结果呢，就是这样好。那下面呢，我继续往里面添加元素。hello，你。看那个效果，看到没有，hello就两次了，me是一次对吧，hello已经变成两次了，那下面我们还按照刚才这个逻辑。hello，加三次，你看加三次，它其实最终呢，输出了三条如玉，这次是三，这次是四，这次是五。没问题吧，因为你新增一条数据，它就会往前推五条数据去统计。嗯。看到没有2345。这也是可以的啊，然后再加个什么，hello。还是50。you。为什么一直是五次呢？因为它只会往前面统计五个元素啊。好，这就滑动窗口，下面我们来使用Java代码来实现一下。放着window。op加。嗯。嗯嗯。先获取一个环境。get。嗯。嗯。嗯。看window。直滚动方口。每隔五个元素计算一次。前五个元素。加个小碟red map，你有一个red map。注意我们在这呢，还把这个map和map它这个逻辑整合一块，说输入是词频输出是。in。嗯。来。慢点，split。不了。我。在这个。嗯。WORD1。对吧，这样看一下后面一个a。零。嗯。放了window。五。嗯嗯。some。嗯。这个是窗口大小。好，接下来讲第二个把这个注释呢，从这复制一下吧。所以这是每格啊。嗯。好，这个前面啊，其实都一样啊，只有一个地方不一样，对吧。就是把这个放到温度这块，给它改一下就行。和两个参数，嗯。第一个参数窗口大小，第二个参数。滑动间隔。嗯嗯。因为一点。嗯。顺便抛个异常。好，这就可以了，在这我们可以助调一个。验证一下这个滑动窗口。赶紧回来把这个打开。OK。好。没问题吧，没问题啊。这就是Java代码，实现这个count window。</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CountWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CountWindowOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意：由于我们在这里使用keyBy，会先对数据分组</span></span><br><span class="line"><span class="comment">     * 如果某个分组对应的数据窗口内达到了5个元素，这个窗口才会被触发执行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">//CountWindow之滚动窗口：每隔5个元素计算一次前5个元素</span></span><br><span class="line">    <span class="comment">/*text.flatMap(_.split(" "))</span></span><br><span class="line"><span class="comment">      .map((_,1))</span></span><br><span class="line"><span class="comment">      .keyBy(0)</span></span><br><span class="line"><span class="comment">      //指定窗口大小</span></span><br><span class="line"><span class="comment">      .countWindow(5)</span></span><br><span class="line"><span class="comment">      .sum(1)</span></span><br><span class="line"><span class="comment">      .print()*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//CountWindow之滑动窗口：每隔1个元素计算一次前5个元素</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">        .countWindow(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">        .sum(<span class="number">1</span>)</span><br><span class="line">        .print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"CountWindowOpScala"</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201756278.png" alt="image-20230420175629114"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201756533.png" alt="image-20230420175643559"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以我在这再输三个，看到没有，到这儿才刚开始执行了一次，他把这个hello打印出来五个。那这个you和me为什么没有打印呢？注意了，所以啊，我们在这啊执行了KBY会对这个数据进行分组，如果某个分组对应的数据窗口内达到了五个元素，这个窗口才会被处罚执行，所以说这个时候相当于是hello对应的那个窗口，它里面够五个元素了，它才会执行。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201758363.png" alt="image-20230420175819357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201758141.png" alt="image-20230420175806865"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">看一下count滑动窗口执行结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801677.png" alt="image-20230420180125565"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801117.png" alt="image-20230420180113979"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201801863.png" alt="image-20230420180150478"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201802271.png" alt="image-20230420180220305"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201803247.png" alt="image-20230420180355733"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201805077.png" alt="image-20230420180527334"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201806314.png" alt="image-20230420180609855"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201806384.png" alt="image-20230420180630486"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * CountWindow的使用</span></span><br><span class="line"><span class="comment"> * 1：滚动窗口</span></span><br><span class="line"><span class="comment"> * 2：滑动窗口</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountWindowOpJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//CountWindow之滚动窗口：每隔5个元素计算一次前5个元素</span></span><br><span class="line">        <span class="comment">/*text.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">            @Override</span></span><br><span class="line"><span class="comment">            public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span><br><span class="line"><span class="comment">                    throws Exception &#123;</span></span><br><span class="line"><span class="comment">                String[] words = line.split(" ");</span></span><br><span class="line"><span class="comment">                for (String word : words) &#123;</span></span><br><span class="line"><span class="comment">                    out.collect(new Tuple2&lt;String, Integer&gt;(word,1));</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;).keyBy(0)</span></span><br><span class="line"><span class="comment">                //窗口大小</span></span><br><span class="line"><span class="comment">                .countWindow(5)</span></span><br><span class="line"><span class="comment">                .sum(1)</span></span><br><span class="line"><span class="comment">                .print();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//CountWindow之滑动窗口：每隔1个元素计算一次前5个元素</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//第一个参数：窗口大小，第二个参数：滑动间隔</span></span><br><span class="line">                .countWindow(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"CountWindowOpJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自定义Window的使用"><a href="#自定义Window的使用" class="headerlink" title="自定义Window的使用"></a>自定义Window的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下自定义window。其实呢，window还可以再细分一下。可以把它分为呢，一种是基于Key的window。一种是不基于Key的window。其实就是说咱们在使用window之前是否执行了key操作啊，咱们前面演示的都是这种基于Key的window。你看我们在做window之前，前面呢都做了Keyby对吧，那如果呢，需求中不需要根据Key进行分组，你在使用window的时候啊，我们需要对应的去使用那个timeWindowAll和countWindowAll。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304201829109.png" alt="image-20230420182825536"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">你使用KeyBy之后的话，它就只能调那个timeWindow,countWindow，这个需要注意一下啊，那如果说是我们自定义的window。如何使用呢？对吧，针对这两种情况。来看一下。针对这个基于Key的window呀，我们需要使用这个window函数</span><br><span class="line"></span><br><span class="line">那针对下面这种不基于Key的window呢，我们可以直接使用这个windowAll就可以了。其实呀，我们前面所说的那个timewindow和timewindowall底层用的就是这个window和windowall，你可以这样理解timewindow是官方封装好的window。所以说呢，timewindow和countwindow呢，都是官方封装好了。</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingProcessingTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：自定义MyTimeWindow</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyTimeWindowScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//自定义MyTimeWindow滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">    text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">      .map((_,<span class="number">1</span>))</span><br><span class="line">      .keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//窗口大小</span></span><br><span class="line">  .window(<span class="type">TumblingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)))</span><br><span class="line">      .sum(<span class="number">1</span>)</span><br><span class="line">      .print()</span><br><span class="line">    env.execute(<span class="string">"MyTimeWindowScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个呢，和咱们之前啊使用的什么timewindow那个效果是一样的。这样的话更加灵活一些，我们想怎么定义都可以啊。如果你不使用这个KeyBy的话，那下面你就可以使用windowAll是一样的效果</span><br></pre></td></tr></table></figure><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.window;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingProcessingTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：自定义MyTimeWindow</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTimeWindowJava</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自定义MyTimeWindow滚动窗口：每隔10秒计算一次前10秒时间窗口内的数据</span></span><br><span class="line">        text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">                    <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word,<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">//窗口大小</span></span><br><span class="line">                .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .sum(<span class="number">1</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"MyTimeWindowJava"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Window中的增量聚合和全量聚合"><a href="#Window中的增量聚合和全量聚合" class="headerlink" title="Window中的增量聚合和全量聚合"></a>Window中的增量聚合和全量聚合</h3><h4 id="增量聚合"><a href="#增量聚合" class="headerlink" title="增量聚合"></a>增量聚合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下Window聚合。就是在进行Window聚合操作的时候呢，可以分为两种情况。一种呢是增量聚合，还有一种是全量聚合。</span><br><span class="line"></span><br><span class="line">那下面我们首先来看一下这个增量聚合。增量聚合呢，它表示呀，窗口中每进入一条数据就进行一次计算，常见的一些增量聚合函数如下:</span><br><span class="line">reduce() aggregate() sum() min() max()</span><br><span class="line"></span><br><span class="line">那下面呢，我们来看一个增量聚合的案例啊，就是累加求和</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202150610.png" alt="image-20230420215009349"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">它的具体执行过程是这样的。第一次进来一条数据八，则立刻进行累加，求和结果为八，第二次进来一条数据12，则立刻进行累加，求和结果为20。第三次进来一条数据七，则立刻进行累加求和，结果为27。第四次进来一条数据，则立刻进行累加求和，结果为37。这就是这个增量聚合它的一个执行流程。</span><br><span class="line"></span><br><span class="line">那下面呢，我们来看一下reduce函数的一个使用，从这里面我们可以看出来，reduce是每次获取一条数据和上一次的执行结果求和。也就是来一条数据，立刻计算一次，这个就是增量聚合。</span><br></pre></td></tr></table></figure><h4 id="全量聚合"><a href="#全量聚合" class="headerlink" title="全量聚合"></a>全量聚合</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们来看一下全量集合。全量集合呀，它就是等属于窗口的数据都到齐了，才开始进行聚合计算，可以实现对窗口内的数据进行排序等需求。常见的一些全量聚合函数为：</span><br><span class="line">apply(windowFunction)，还有这个process(processWindowFunction)</span><br><span class="line">apply呢，它里面接触的是windowfunction,process里面接触是process windowfunction</span><br><span class="line">注意这个processwindowfunction比windowfunction提供了更多的上下文信息啊。那下面呢，我们来看一个全量聚合的一个案例，求最大值</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202157070.png" alt="image-20230420215701906"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第四次进来调数据十，此时窗口触发，这时候才会对窗口内的数据进行排序，然后获取最大值。</span><br></pre></td></tr></table></figure><h5 id="全量聚合apply"><a href="#全量聚合apply" class="headerlink" title="全量聚合apply"></a>全量聚合apply</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202158547.png" alt="image-20230420215834620"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来看一下这个apply函数的一个使用。从这你可以看出来，他接触的是一个iterable，可以认为是一个集合。他可以把这个窗口的数据啊，一次性全都传过来，当这个窗口触发的时候，才会真正执行这个代码。</span><br></pre></td></tr></table></figure><h5 id="全量聚合process"><a href="#全量聚合process" class="headerlink" title="全量聚合process"></a>全量聚合process</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202200468.png" alt="image-20230420220026246"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢是一个process。你看他接触的也是一个iterable，所以说呢，你在这里面就可以获取到这个窗口里面的所有数据了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个呢就是Windows中的全量聚合和增量聚合，后面呢我们就会用到这个apply，还有process它的一个使用，因为有时候我们需要对这个窗口内的所有数据去做一些全量的操作，这样的话就不能用这种增量聚合，而要用这种全量聚合。</span><br></pre></td></tr></table></figure><h3 id="Flink中的Time"><a href="#Flink中的Time" class="headerlink" title="Flink中的Time"></a>Flink中的Time</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对流数据的time可以分为以下三种。第一个Event Time表示事件产生的时间，它通常由事件中的时间戳来描述。第二个ingestion time表示事件进入flink的时间。第三个processing time，它表示事件被处理时当前系统的时间，那这几种时间呀，我们通过这个图可以很清晰的看出来它们之间的关系。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202206362.png" alt="image-20230420220641443"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">首先是even time，这个就是数据产生的时间。第二个是ingestion time表示呢，他进入flink时间，其实就是被那个source把它读取过来那个时间。第三个呢，是这个processing time，它其实呢，就是flink里面具体的算子，在处理的时候它的一个时间，那接下来我们来看一个案例。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304202209327.png" alt="image-20230420220902227"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意你看数据呢，是在十点的时候产生的。结果呢，在晚上八点的时候才被flink读取走。那flink真正在处理的时候呢？是8.02秒。</span><br><span class="line">注意，如果说呀，我们想要统计每分钟内接口调用失败的错误日志个数。那这个时候使用哪个时间才有意义呢？因为数据有可能会出现延迟。如果使用那个数据进入flink的时间或者window处理的时间，其实是没有意义的。这个时候我们需要使用原始日中的时间才是有意义的，这个才是数据产生的时间，我们基于这个时间去统计才有意义。</span><br><span class="line">那我们在flink流水中默认使用的是哪个时间呢？某种情况下，flink在流处理中使用的时间是这个processingtime。那如果说我们想要修改的话，怎么改呢？可以使用这个env去改env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)可以设置这个time或者是这个IngestionTime。好，这就是flink中的三种time。</span><br></pre></td></tr></table></figure><h3 id="Watermark的分析"><a href="#Watermark的分析" class="headerlink" title="Watermark的分析"></a>Watermark的分析</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211000567.png" alt="image-20230421100013693"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实时计算中，数据时间比较敏感。有eventTime和processTime区分，一般来说eventTime是从原始的消息中提取过来的，processTime是Flink自己提供的，Flink中一个亮点就是可以基于eventTime计算，这个功能很有用，因为实时数据可能会经过比较长的链路，多少会有延时，并且有很大的不确定性，对于一些需要精确体现事件变化趋势的场景中，单纯使用processTime显然是不合理的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">前面提到了Time的概念，如果我们使用Processing Time，那么在Flink消费数据的时候，它完全不需要关心数据本身的时间，意思也就是说不需要关心数据到底是延迟数据还是乱序数据。因为 Processing Time 只是代表数据在 Flink 被处理时的时间，这个时间是顺序的。</span><br><span class="line">但是如果你使用的是 Event Time 的话，那么你就不得不面临着这么个问题：事件乱序 &amp; 事件延迟。</span><br><span class="line">所以…</span><br><span class="line">为了解决这个问题，Flink中引入了WaterMark机制，即水印的概念。、</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211008547.png" alt="image-20230421100850143"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">然而在有些场景下，尤其是特别依赖于事件时间而不是处理时间，比如：</span><br><span class="line">错误日志的时间戳，代表着发生的错误的具体时间，开发们只有知道了这个时间戳，才能去还原那个时间点系统到底发生了什么问题，或者根据那个时间戳去关联其他的事件，找出导致问题触发的罪魁祸首</span><br><span class="line">设备传感器或者监控系统实时上传对应时间点的设备周围的监控情况，通过监控大屏可以实时查看，不错漏重要或者可疑的事件</span><br><span class="line">比如我做过的充电桩实时报文分析，就必须依赖报文产生的时间，即事件时间</span><br><span class="line">…</span><br><span class="line">针对上面的问题（事件乱序 &amp; 事件延迟），Flink 引入了 Watermark 机制来解决。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">统计 8:00 ~ 9:00 这个时间段打开淘宝 App 的用户数量，Flink 这边可以开个窗口做聚合操作，但是由于网络的抖动或者应用采集数据发送延迟等问题，于是无法保证在窗口时间结束的那一刻窗口中是否已经收集好了在 8:00 ~ 9:00 中用户打开 App 的事件数据，但又不能无限期的等下去？当基于事件时间的数据流进行窗口计算时，最为困难的一点也就是如何确定对应当前窗口的事件已经全部到达。然而实际上并不能百分百的准确判断，因此业界常用的方法就是基于已经收集的消息来估算是否还有消息未到达，这就是 Watermark 的思想。</span><br><span class="line"></span><br><span class="line">Watermark 是一种衡量 Event Time 进展的机制，它是数据本身的一个隐藏属性，数据本身携带着对应的 Watermark。Watermark 本质来说就是一个时间戳，代表着比这时间戳早的事件已经全部到达窗口，即假设不会再有比这时间戳还小的事件到达，这个假设是触发窗口计算的基础，只有 Watermark 大于窗口对应的结束时间，窗口才会关闭和进行计算。按照这个标准去处理数据，那么如果后面还有比这时间戳更小的数据，那么就视为迟到的数据，对于这部分迟到的数据，Flink 也有相应的机制（下文会讲）去处理。</span><br></pre></td></tr></table></figure><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个timestamp.watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现。</span><br><span class="line"></span><br><span class="line">流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（out-of-order或者说late element）。</span><br><span class="line"></span><br><span class="line">但是对于late element，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">比如：</span><br><span class="line">08:00任务开启，设置1分钟的滚动窗口，在08:00:00-08:01:00为第一个窗口，08:01:00-08:02:00为第二个窗口；</span><br><span class="line">现在有一条数据的事件时间是08:00:50，但是这条数据却在08:01:10到达，按照正常的处理，窗口会在结束时间（08:01:00）的时候就触发计算，那么这条数据就会被丢弃；</span><br><span class="line">但是开启WaterMark后，窗口在08:01:00时不会触发；</span><br><span class="line">因为采用的是EventTime，而数据本身时间是08:00:50，所以该条数据肯定会落到第一个窗口；</span><br><span class="line">假设在08:01:10时的WaterMark为08:01:00（WaterMark可以理解为一个时间戳），发现这个WaterMark和第一个窗口的结束时间相等，此时触发第一个窗口的计算操作，此时这条延迟数据正好参与到计算中；</span><br><span class="line">此时只有水印大于或等于窗口结束时间才会触发窗口的关闭和计算；</span><br><span class="line">此时就不会丢数据。</span><br></pre></td></tr></table></figure><h4 id="WaterMark的传递"><a href="#WaterMark的传递" class="headerlink" title="WaterMark的传递"></a>WaterMark的传递</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Watermark在向下游传递时，是广播到下游所有的子任务中，如果多并行度下有多个watermark传递到下游时，取最小的watermark。</span><br></pre></td></tr></table></figure><h4 id="WaterMark设置"><a href="#WaterMark设置" class="headerlink" title="WaterMark设置"></a>WaterMark设置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注：如果你采用的是事件时间，即你设置了 env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">那么你就必须设置获取事件时间的方法，否则会报错（如果是从kafka消费数据，不设置水印的话，默认采用kafka消息自带的时间戳作为事件时间）</span><br><span class="line"></span><br><span class="line">数据处理中需要通过调用DataStream中的 assignTimestampsAndWatermarks方法来分配时间和水印，该方法可以传入两种参数，一个是AssignerWithPeriodicWatermarks，另一个是 AssignerWithPunctuatedWatermarks。</span><br><span class="line">所以设置Watermark是有如下两种方式：</span><br><span class="line"></span><br><span class="line">AssignerWithPunctuatedWatermarks：数据流中每一个递增的 EventTime都会产生一个Watermark。</span><br><span class="line">AssignerWithPeriodicWatermarks：周期性的（一定时间间隔或者达到一定的记录条数）产生一个 Watermark。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实际生产中用第二种的比较多，它会周期性产生Watermark的方式，但是必须结合时间或者积累条数两个维度，否则在极端情况下会有很大的延时。</span><br></pre></td></tr></table></figure><h3 id="开发Watermark代码"><a href="#开发Watermark代码" class="headerlink" title="开发Watermark代码"></a>开发Watermark代码</h3><h4 id="乱序数据处理"><a href="#乱序数据处理" class="headerlink" title="乱序数据处理"></a>乱序数据处理</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了what mark的一些基本原理，可能大家对它还不够了解，下面我们来通过这个案例加深大家对what mark的理解。我们来分析一下这个案例。乱序数据处理</span><br><span class="line"></span><br><span class="line">通过socket模拟数据。数据的格式是这样的。前面的话代表的是具体的业务数据，后边的话是一个时间戳，这是一个毫秒的时间戳。中间用逗号分隔。</span><br><span class="line"></span><br><span class="line">其中，时间戳是数据产生的时间。也就是even time。那产生这个数据之后呢？然后使用map函数，把数据转换为tuple2的形式。接着再调用这个函数assignTimestampsAndWatermarks。使用这个方法来抽取timestamp并生成watermark。</span><br><span class="line">接着，再调用window打印信息，来验证window被触发的时机。最后验证乱序数据的处理方式，这是我们一个大致的一个处理流程。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">      env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳(EventTime)和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>) <span class="comment">// currentMaxTimstamp它的第一个参数值应该是传错了</span></span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样(这里和前面自定义window时，传的参数有点不一样，这里是event)</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>],<span class="type">String</span>,<span class="type">Tuple</span>,<span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup=&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr+<span class="string">","</span>+arr.length+<span class="string">","</span>+sdf.format(arr.head)+<span class="string">","</span>+sdf.format(arr.last)+<span class="string">","</span>+sdf.format(window.getStart)+<span class="string">","</span>+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="根据数据跟踪观察Watermark"><a href="#根据数据跟踪观察Watermark" class="headerlink" title="根据数据跟踪观察Watermark"></a>根据数据跟踪观察Watermark</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211425983.png" alt="image-20230421142543318"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211428308.png" alt="image-20230421142830892"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211435405.png" alt="image-20230421143511791"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211438988.png" alt="image-20230421143817886"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211440736.png" alt="image-20230421144017493"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211441254.png" alt="image-20230421144059007"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211441192.png" alt="image-20230421144132566"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211442566.png" alt="image-20230421144203353"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到这里，window仍然没有被触发，此时watermark的时间已经等于第一条数据的eventtime了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211445272.png" alt="image-20230421144516628"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211445660.png" alt="image-20230421144534532"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">window仍然没有被触发，此时，我们数据已经发送到2026-10-01 10:11:33了，根据eventtime来算，最早的数据已经过去了11s了，window还没开始计算，那到底什么时候会触发window呢？</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211450426.png" alt="image-20230421145025475"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211451804.png" alt="image-20230421145103798"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211452915.png" alt="image-20230421145208979"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到这里，我们做了一个说明。</span><br><span class="line">window的触发机制，是先按照自然时间将window划分，如果window大小是3s，那么1min内会把window划分成如下的形式(左闭右开的区间)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211459275.png" alt="image-20230421145940525"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211500839.png" alt="image-20230421150008692"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">window的设定无关数据本身，而是系统定义好了的。</span><br><span class="line">输入的数据，根据自身的eventtime，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;&#x3D;eventtime时，就符合了window触发的条件了，最终决定window触发，还是由eventtime所属window中的window_end_time决定。</span><br><span class="line"></span><br><span class="line">上面的测试中，最后一条数据到达后，其水位线(watermark)已经上升至10:11:24，正好是最早的一条记录所在window的window_end_time，所以window就被触发了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211511116.png" alt="image-20230421151156542"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211513051.png" alt="image-20230421151309893"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时，watermark时间虽然已经等于第二条数据的时间，但是由于其没有达到第二条数据所在window，但是由于其没有达到第二条数据所在window的结束时间，所以window并没有被触发。那么，第二条数据所在的window时间区间如下。 </span><br><span class="line">[00:00:24,00:00:27)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">也就是说，我们必须输入一个10:11:37的数据，第二条数据所在的window才会被触发，我们继续输入。</span><br><span class="line">0001,1790820697000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211520479.png" alt="image-20230421152050886"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时，我们已经看到，window的触发条件要符合以下几个条件：</span><br><span class="line">1.watermark时间&gt;&#x3D;wind_end_time</span><br><span class="line">2.在[window_start_time,window_end_time)区间中有数据存在(注意是左闭右开的区间)</span><br></pre></td></tr></table></figure><h3 id="Watermark-EventTime处理乱序数据"><a href="#Watermark-EventTime处理乱序数据" class="headerlink" title="Watermark+EventTime处理乱序数据"></a>Watermark+EventTime处理乱序数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我们前面那个测试啊，数据呢，都是按照这个时间顺序递增的，都是有序的，那现在呢，我们来输入一些的数据，来看看这个whatmark，结合这个一的eventtime机制是如何处理这些乱写数据的。那我们在上面那个基础之上啊，再输入两行数据。</span><br><span class="line">注意这个呢，没有触发对吧</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211904910.png" alt="image-20230421190455535"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211905517.png" alt="image-20230421190548292"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们再输入一条43秒的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211916109.png" alt="image-20230421191608587"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211919959.png" alt="image-20230421191923374"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304211920175.png" alt="image-20230421192030363"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">大家注意他没有这个33的。我这个窗口呢，是[30,33)，你看我这个一的代表里面数据是有这个33的，为什么这个33没有输出呢。因为这个窗口啊，它是一个左闭右开的。那这个33的话，它其实啊，属于下一个窗口，就是33到36的那个窗口。</span><br><span class="line">好。所以上面这个结果其实已经表明对迟到的数据了，flink可以通过这个watermark来实现处理一定范围内的乱序数据。因为现在我们允许的最大乱序时间是十秒。就是十秒之内乱序是OK的，那如果超过了这个十秒怎么办？也就是说呢，对于这个迟到(late element)太久的数据，flink是怎么处理的呢？</span><br></pre></td></tr></table></figure><h3 id="延时数据的三种处理方式"><a href="#延时数据的三种处理方式" class="headerlink" title="延时数据的三种处理方式"></a>延时数据的三种处理方式</h3><h4 id="丢弃"><a href="#丢弃" class="headerlink" title="丢弃"></a>丢弃</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们就来看一下针对迟到太久的数据，它的一些处理方案，现在呢一共有三种。</span><br><span class="line">第一种是丢弃默认的啊，那我们再来演示一下。那我们首先呢，来输入一个乱序很多的数据来测试一下</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221025938.png" alt="image-20230422102536854"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意，下面呢，我们再来输入几个一定的eventtime小于whatmark的时间</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221027933.png" alt="image-20230422102748626"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">看到没有，这三条他都没有触发这个窗口的执行啊，因为你现在你输入的数据所在的窗口已经执行过了。flink默认对这些迟到的数据的处理方案就是丢弃。这几条数据，30对应的那个窗口数据是不是已经执行过了呀，那这样过来它直接丢弃，这是默认的一个处理方案。</span><br></pre></td></tr></table></figure><h4 id="allowedLateness"><a href="#allowedLateness" class="headerlink" title="allowedLateness"></a>allowedLateness</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">那接下来看第二种，你可以通过这个allowedLateness来指定一个允许数据延迟的时间本身啊，我们之前通过那个watermark已经设置了一个数据的延迟时间是十秒，对吧。你可以通过这个参数啊，再给他指定一个延迟时间，就类似于我们上班打卡官方延迟对吧，类似于公司统一层面允许大家呢弹性半小时。但是你们这个部门呢，可以再多谈十分钟，有这种效果。</span><br><span class="line"></span><br><span class="line">在某些情况下，我们希望对迟到的数据再提供一个宽容时间。那flink提供了这个方法，可以实现对迟到的数据啊，再给它设置一个延迟时间，在指定延迟时间内到达数据还是可以触发window执行的。所以这时候我们需要去改一下代码了。主要呢，就增加这一行就行。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpForAllowedLatenessScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间 10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//允许数据迟到2秒</span></span><br><span class="line">      .allowedLateness(<span class="type">Time</span>.seconds(<span class="number">2</span>))</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>],<span class="type">String</span>,<span class="type">Tuple</span>,<span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup=&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr+<span class="string">","</span>+arr.length+<span class="string">","</span>+sdf.format(arr.head)+<span class="string">","</span>+sdf.format(arr.last)+<span class="string">","</span>+sdf.format(window.getStart)+<span class="string">","</span>+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221044535.png" alt="image-20230422104446863"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221047820.png" alt="image-20230422104738555"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，我们再输入几条in time小于wordmark的一个数据来验证一下效果。好，来看一下。注意你会发现，你看。这三条数据过来的时候，窗口同样被触发了，因为之前的话，我们是这个30到33这个窗口对吧。我在这输的这三条数据，一个是30秒了，三十一三十二，它们都属于那个窗口。岁数，你看窗口都被吃光。你看这时候打印的窗口数据是两条，这是三条，这是四条对吧。所以说呢，每条数据都触发了温度的执行啊。这三条数据。那下面我们再输一条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221050530.png" alt="image-20230422105039949"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221058268.png" alt="image-20230422105851955"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221100903.png" alt="image-20230422110036594"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这时候啊，我们把这个whatmark呀，给它调到34。往上面调一下。看到没有，这次呢，它是没有触发的啊是34。数据呢是44，这样的话，whatmark变成了34。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221103748.png" alt="image-20230422110328208"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221105374.png" alt="image-20230422110533985"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时呢，把whatmark上升到了34。此时呢，我们再输入几条这种迟到的数据来验证一下效果。因为刚才的话，我们验证了它是可以执行的啊。嗯。结果你会发现，看到没有，这三条又执行了。我们发现数的这三条数据呢，它都触发了window执行。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221107423.png" alt="image-20230422110715967"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221108095.png" alt="image-20230422110824496"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们再输入一行数据，把这个我的妈再调一下，调到35。嗯。对吧，这是刚才调到35。给你输入一下45的数据，我慢了变成35，我把这个清一下。这时候它就上升到了35，</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221112669.png" alt="image-20230422111217143"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意下面我们再输入几条十到的数据，还是那个三十三十一三十二啊。嗯。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221114742.png" alt="image-20230422111430693"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221116286.png" alt="image-20230422111654208"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意此时这个窗口就不会触发，相当于啊，你这个时候你这个迟到的数据啊，我就不管了。来分析一下啊。你看我们又发现这几条数据啊，它都没有触发window啊。那分析一下，当这个呀等于这个33的时候，它正好呢，是属于这个window n time对吧，正好相等，所以说呢，它会触发这个30到33这个窗口执行。当这个窗口执行过后啊，我们再输入30到33这个窗口内的数据的时候呢，会发现这个窗口是可以被触发的。当我们把这个mark提升到34秒的时候，我们再输入这个窗口内的数据，发现温度还是可以被触发的。那当我们把这个mark牺升到35的时候，再输入这个窗口数据，发现window不会被触发。这是为什么呢？这是因为我们在前面设置了这个参数。又给它多加了两秒延迟，因此呢，可以允许延迟在两秒内的数据继续触发温度执行。所以说当你这个我的ma等于34的时候，是可以触发温度的，但是35就不行了，这个需要注意一下。这块有个总结啊，对于这个窗口而言啊，它允许两秒的迟到数据，也就是说呢，你第一次触发是在这个o ma呢，它大于等于这个window and time的时候，对吧，那第二次或者啊以后多次触发条件是这样的。小于window n time加上这个。就是允许的迟到时间加这个二。对吧，并且呢，这个窗口有迟到数据的时候，它就会被触发。那所以说你看，当我们这个omark等于34的时候，我们输入什么三十三十一三十二秒的数据，它是可以出发的，因为这些数据它们的window n time呢，都是这个33。那又是说了，你这个你看是三十四三十四的话，你小于33加二对吧，它是小于的这个是不是出了啊，但是呢，当这个等于35的时候呢，我们再去输入这个三十三十一三十二，这个其实就迟到太久太久太久了。这些数据呢，我n time呢，还是这个33，此时注意我mark是三十五三十五小于这个33加二嘛，你不要不要去较这个真啊。这个是秒，你说一个秒加个二，这啥意思呢？对吧，是尾电门啊，就是33秒加上两秒就是三十五三  十五是不小于35。所以说就是暴走，那所以说最终这些数据呢，迟到时间太久了。本来呢，公司打卡弹性半小时，你们部门啊，又谈了十分钟，结果呢，你还是没有满足这个要求啊。这样就没办法对吧，那这时候呢，就不会再触发温度的执行了，就把你扔掉不管你了。这就是第二种处理方案。</span><br></pre></td></tr></table></figure><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221121053.png" alt="image-20230422112107568"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221121425.png" alt="image-20230422112129357"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221603475.png" alt="image-20230422160343061"></p><h4 id="sideOutputLateData"><a href="#sideOutputLateData" class="headerlink" title="sideOutputLateData"></a>sideOutputLateData</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面呢，还有一种处理方案。这个呢，就是收集迟到数据。通过这个函数呢，可以把迟到的数据啊，给它统一收集，统一存储，方便后期排查，问题就是你为什么迟到这么久对吧，这个呢也需要去调整代码，你呢先创建这个out，咱们前面是不是已经用过呀</span><br><span class="line"></span><br><span class="line">注意咱们刚才讲那个第二种方案，其实可以和第三种结合到一块儿来使用，都是可以的啊，你再给他延迟两秒，如果说他还是没有到达，对吧，那你就把它保存起来，丢了保存起来。当然也可以单独使用，都是可以的啊，这个需要具体根据你们的业务需求来定。</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">OutputTag</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">WindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">TumblingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Sorting</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Watermark+EventTime解决数据乱序问题</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkOpForSideOutputLateDataScala</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">//设置使用数据产生的时间：EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    <span class="comment">//设置全局并行度为1</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置自动周期性的产生watermark，默认值为200毫秒</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="comment">//将数据转换为tuple2的形式</span></span><br><span class="line">    <span class="comment">//第一列表示具体的数据，第二列表示是数据产生的时间戳</span></span><br><span class="line">    <span class="keyword">val</span> tupStream = text.map(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = line.split(<span class="string">","</span>)</span><br><span class="line">      (arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分配(提取)时间戳和watermark</span></span><br><span class="line">    <span class="keyword">val</span> waterMarkStream = tupStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>)) <span class="comment">//最大允许的数据乱序时间 10s</span></span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>]] &#123;</span><br><span class="line">        <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">        <span class="keyword">var</span> currentMaxTimstamp = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从数据流中抽取时间戳作为EventTime</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> timestamp = element._2</span><br><span class="line">          currentMaxTimstamp = <span class="type">Math</span>.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          <span class="comment">//计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark=currentMaxTimstamp-OutOfOrderness</span></span><br><span class="line">          <span class="keyword">val</span> currentWatermark = currentMaxTimstamp - <span class="number">10000</span>L</span><br><span class="line">          <span class="comment">//此print语句仅仅是为了在学习阶段观察数据的变化</span></span><br><span class="line">          println(<span class="string">"key:"</span> + element._1 + <span class="string">","</span> + <span class="string">"eventtime:["</span> + element._2 + <span class="string">"|"</span> + sdf.format(element._2) + <span class="string">"],currentMaxTimstamp:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentMaxTimstamp) + <span class="string">"],watermark:["</span> + currentWatermark + <span class="string">"|"</span> + sdf.format(currentWatermark) + <span class="string">"]"</span>)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//保存被丢弃的数据</span></span><br><span class="line">    <span class="keyword">val</span> outputTag = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">Long</span>]](<span class="string">"late-data"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resStream = waterMarkStream.keyBy(<span class="number">0</span>)</span><br><span class="line">      <span class="comment">//按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span></span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">3</span>)))</span><br><span class="line">      <span class="comment">//保存被丢弃的数据</span></span><br><span class="line">      .sideOutputLateData(outputTag)</span><br><span class="line">      <span class="comment">//使用全量聚合的方式处理window中的数据</span></span><br><span class="line">      .apply(<span class="keyword">new</span> <span class="type">WindowFunction</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">Long</span>], <span class="type">String</span>, <span class="type">Tuple</span>, <span class="type">TimeWindow</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Tuple</span>, window: <span class="type">TimeWindow</span>, input: <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Long</span>)], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">val</span> keyStr = key.toString</span><br><span class="line">          <span class="comment">//将window中的数据保存到arrBuff中</span></span><br><span class="line">          <span class="keyword">val</span> arrBuff = <span class="type">ArrayBuffer</span>[<span class="type">Long</span>]()</span><br><span class="line">          input.foreach(tup =&gt; &#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//将arrBuff转换为arr</span></span><br><span class="line">          <span class="keyword">val</span> arr = arrBuff.toArray</span><br><span class="line">          <span class="comment">//对arr中的数据进行排序</span></span><br><span class="line">          <span class="type">Sorting</span>.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> sdf = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyy-MM-dd HH:mm:ss"</span>)</span><br><span class="line">          <span class="comment">//将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span></span><br><span class="line">          <span class="keyword">val</span> result = keyStr + <span class="string">","</span> + arr.length + <span class="string">","</span> + sdf.format(arr.head) + <span class="string">","</span> + sdf.format(arr.last) + <span class="string">","</span> + sdf.format(window.getStart) + <span class="string">","</span> + sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;) </span><br><span class="line"></span><br><span class="line">    <span class="comment">//把迟到的数据取出来，暂时打印到控制台，实际工作中可以选择存储到其它存储介质中</span></span><br><span class="line">    <span class="comment">//例如：redis，kafka</span></span><br><span class="line">    <span class="keyword">val</span> sideOutput = resStream.getSideOutput(outputTag)</span><br><span class="line">    sideOutput.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将流中的结果数据也打印到控制台</span></span><br><span class="line">    resStream.print()</span><br><span class="line">    env.execute(<span class="string">"WatermarkOpScala"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们来验证一下，先输入这两行数据。第一条。所以你看第一次发了一个30，这是43对吧。此时，这头的mark是33。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221620417.png" alt="image-20230422162002780"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221620154.png" alt="image-20230422162014105"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们再输入几条event time小于watermark的一个时间来测试一下啊。现在你这个窗口已经执行过了，我们再往里添加数据来看一下效果。还这三条数据啊。来看一下，注意这个呢，是正常他都会打印的，你看这时候你在输入这三条的时候，他注意它那个窗口就没有执行了，下面这个数据不是那个窗口打印出来，窗口打印出来数据是这种格式啊。这个是谁打印的呀？所以说呢，针对这个迟到的数据，我们就把它放到这里边儿了，这样后期呢，你就可以把这数据可以存到其他地方，方便你们去排查问题，为什么这个数据来这么迟啊，对吧，可以分析一下问题，看是网络原因啊或者是其他原因。是这个啊。那这时候呢，你看针对这个迟到的数据，我们就可以通过这个set out来保存到这个out中。后期你想在保存的其他存储介质中也是没有任何问题的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221622695.png" alt="image-20230422162202360"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221623442.png" alt="image-20230422162306215"></p><h3 id="在多并行度下的watermark应用"><a href="#在多并行度下的watermark应用" class="headerlink" title="在多并行度下的watermark应用"></a>在多并行度下的watermark应用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前面呢，我们演示了在单并行度下whatmark的使用，下面呢我们来看一下在多并行度下面watermark的一个使用。咱们前面的话在这设置为一。如果说你把这行代码给它做掉的话，你不设置的话。那我们在IDE中去执行的时候，默认呢，它会读取我本地的CPU的数量来设置默认命度。那所以说我在这把这个给它直接做掉就行了。做了之后啊，你可以在这啊加一个系统ID，这样的话我们就知道了是哪条数据被哪个线程所处理。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.scala.window</span><br><span class="line"></span><br><span class="line">import java.text.SimpleDateFormat</span><br><span class="line">import java.time.Duration</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.eventtime.&#123;SerializableTimestampAssigner, WatermarkStrategy&#125;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.api.scala.function.WindowFunction</span><br><span class="line">import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line">import org.apache.flink.streaming.api.windowing.windows.TimeWindow</span><br><span class="line">import org.apache.flink.util.Collector</span><br><span class="line"></span><br><span class="line">import scala.collection.mutable.ArrayBuffer</span><br><span class="line">import scala.util.Sorting</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Watermark+EventTime解决数据乱序问题</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">object WatermarkOpMoreParallelismScala &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    &#x2F;&#x2F;设置使用数据产生的时间：EventTime</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">    &#x2F;&#x2F;设置全局并行度为1</span><br><span class="line">    env.setParallelism(2)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;设置自动周期性的产生watermark，默认值为200毫秒</span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(200)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val text &#x3D; env.socketTextStream(&quot;bigdata04&quot;, 9001)</span><br><span class="line">    import org.apache.flink.api.scala._</span><br><span class="line">    &#x2F;&#x2F;将数据转换为tuple2的形式</span><br><span class="line">    &#x2F;&#x2F;第一列表示具体的数据，第二列表示是数据产生的时间戳</span><br><span class="line">    val tupStream &#x3D; text.map(line &#x3D;&gt; &#123;</span><br><span class="line">      val arr &#x3D; line.split(&quot;,&quot;)</span><br><span class="line">      (arr(0), arr(1).toLong)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;分配(提取)时间戳和watermark</span><br><span class="line">    val waterMarkStream &#x3D; tupStream.assignTimestampsAndWatermarks(WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)) &#x2F;&#x2F;最大允许的数据乱序时间 10s</span><br><span class="line">      .withTimestampAssigner(new SerializableTimestampAssigner[Tuple2[String, Long]] &#123;</span><br><span class="line">        val sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)</span><br><span class="line">        var currentMaxTimstamp &#x3D; 0L</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;从数据流中抽取时间戳作为EventTime</span><br><span class="line">        override def extractTimestamp(element: (String, Long), recordTimestamp: Long): Long &#x3D; &#123;</span><br><span class="line">          val timestamp &#x3D; element._2</span><br><span class="line">          currentMaxTimstamp &#x3D; Math.max(timestamp, currentMaxTimstamp)</span><br><span class="line">          &#x2F;&#x2F;计算当前watermark，为了打印出来方便观察数据，没有别的作用，watermark&#x3D;currentMaxTimstamp-OutOfOrderness</span><br><span class="line">          val currentWatermark &#x3D; currentMaxTimstamp - 10000L</span><br><span class="line">          </span><br><span class="line">          val threadId &#x3D; Thread.currentThread().getId</span><br><span class="line">          &#x2F;&#x2F;此print语句仅仅是为了在学习阶段观察数据的变化</span><br><span class="line">          println(&quot;threadId:&quot;+threadId+&quot;,key:&quot; + element._1 + &quot;,&quot; + &quot;eventtime:[&quot; + element._2 + &quot;|&quot; + sdf.format(element._2) + &quot;],currentMaxTimstamp:[&quot; + currentWatermark + &quot;|&quot; + sdf.format(currentMaxTimstamp) + &quot;],watermark:[&quot; + currentWatermark + &quot;|&quot; + sdf.format(currentWatermark) + &quot;]&quot;)</span><br><span class="line">          element._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    waterMarkStream.keyBy(0)</span><br><span class="line">      &#x2F;&#x2F;按照消息的EventTime分配窗口，和调用TimeWindow效果一样</span><br><span class="line">      .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class="line">      &#x2F;&#x2F;使用全量聚合的方式处理window中的数据</span><br><span class="line">      .apply(new WindowFunction[Tuple2[String,Long],String,Tuple,TimeWindow] &#123;</span><br><span class="line">        override def apply(key: Tuple, window: TimeWindow, input: Iterable[(String, Long)], out: Collector[String]): Unit &#x3D; &#123;</span><br><span class="line">          val keyStr &#x3D; key.toString</span><br><span class="line">          &#x2F;&#x2F;将window中的数据保存到arrBuff中</span><br><span class="line">          val arrBuff &#x3D; ArrayBuffer[Long]()</span><br><span class="line">          input.foreach(tup&#x3D;&gt;&#123;</span><br><span class="line">            arrBuff.append(tup._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          &#x2F;&#x2F;将arrBuff转换为arr</span><br><span class="line">          val arr &#x3D; arrBuff.toArray</span><br><span class="line">          &#x2F;&#x2F;对arr中的数据进行排序</span><br><span class="line">          Sorting.quickSort(arr)</span><br><span class="line"></span><br><span class="line">          val sdf &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)</span><br><span class="line">          &#x2F;&#x2F;将目前window内排序后的数据，以及window的开始时间和window的结束时间打印出来，便于观察</span><br><span class="line">          val result &#x3D; keyStr+&quot;,&quot;+arr.length+&quot;,&quot;+sdf.format(arr.head)+&quot;,&quot;+sdf.format(arr.last)+&quot;,&quot;+sdf.format(window.getStart)+&quot;,&quot;+sdf.format(window.getEnd)</span><br><span class="line">          out.collect(result)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;WatermarkOpScala&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这时候窗口并没有触发。比如我们发现这个window没有被触发，因为这个时候呢，这七条数据啊，都是被不同的线程处理的，每个线程呢，都有一个watermark。我们前面分析了，在这种多并行度的情况下呢，whatmark呢，它呢有一个对齐机制，它呢会取所有材中最小的那个wordmark。所以说我们现在有八个并行度，你这七条数据呢，都被不同的线路所处理啊。到现在呢，还没有获取到最小的那个我。所以说呢，这个window是无法被处罚执行的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221654047.png" alt="image-20230422165413793"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221654240.png" alt="image-20230422165427792"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为这个线路太多了，验证起来了，不太好验证，所以说啊这样。把这个稍微再改一下。我们也不用了一个默认的八个了，我们给它改成两个吧。这个也是多并行度了。好。接下来呢，我们往里面输这么三条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221700312.png" alt="image-20230422170004992"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221702331.png" alt="image-20230422170229823"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一条，没有触发。接下来，第二条，其实理论上如果是单线程的话，这个时候这个窗口已经被触发，但是现在呢，还没有触发。这第三条数据。嗯。好。看到没有，这个时候他就出发。看一下这块的一个总结。此时呢，我们会发现，当第三条数据输入完以后，这个窗口呢，它就被触发了。你前两条数据啊，输入之后呢，它获取到的那个具体的wordmark是20。这个时候呢，它对应的window中呢，是没有数据的，所以说呢，什么都没有执行，当你第三条数据输入之后呢，它获取到那个最小的mark呢，就是33了，这个时候呢，它对应的窗口就是它，它里面有数据，所以说呢，这个window就触发了。</span><br></pre></td></tr></table></figure><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来针对这个watermark案例做一个总结。我们在link中，针对这个wordmark，我们该如何设置它的最大乱序时间？主力。最大乱序时间。首先第一点这个要结合我们自己的业务以及呢数据的实际情况去设置，如果说呢，这个最大落地时间设置的太小，而我们那个自身数据啊，发送时由于网络等原因导致乱序或者迟到太多，那么呢，最终的结果就是会有很多数据被丢弃。这样的话，对我们数据的正确性影响太大。那对于这个严重外序的数据呢？我们需要严格统计数据的最大延迟时间。这样才能最大程度保证计算数据的一个准确度。延时时间呢？实时太小会影响数据准确性。延时时间是太大，不仅影响数据的一个实时性。更加的会加重link作业的一个负担。所以说不是对1TIME要求特别严格的数据，尽量呢不要采用这种一问time的方式来处理数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221721954.png" alt="image-20230422172108445"></p><h2 id="Flink并行度"><a href="#Flink并行度" class="headerlink" title="Flink并行度"></a>Flink并行度</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">大家好，下面呢，我们来分析一下Flink的并行度。一个flink程序由多个组件组成，datasource、transformation、datasink。</span><br><span class="line">一个组件呢，由多个并行的实例来执行，或者说呢，是由多个线程来执行。一个组件的并行实际数目呢？就被称之为该组件的并行度。其实就是说你这个组件有多少个线程去执行，那么它的并行度就是多少。</span><br></pre></td></tr></table></figure><h3 id="task-manager和slot之间的关系"><a href="#task-manager和slot之间的关系" class="headerlink" title="task manager和slot之间的关系"></a>task manager和slot之间的关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面呢，在具体分析这个并行度之前，我们先分析一下这个task manager和slot之间的关系。flink的每个task manager为集群提供的slot的数量通常与每个task manager的可用CPU数量成正比。一般情况下的数量就是每个task manager的可用CPU数量。这个task manager节点就是我们集群的一个从节点。那上面这个slot数量就是这个task manager具有的一个并发执行能力。这里面啊，实行的就是具体的一些实例。source、map、keyBy、sink。还有这个图也是一样的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221751734.png" alt="image-20230422175152422"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221755921.png" alt="image-20230422175512133"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">那接下来我们就来看一下这个并行度该如何来设置。任务的并行度可以通过四个层面来设置。首先第一个是算式层面。第二个是执行环境层面。第三个是客户端层面。第四个呢，是系统层面。</span><br><span class="line">那这四个层面，他们执行的优先级是什么样的？注意。这个算式层面了大于执行环境层面的，执行环境层面了大于客户端层面了，客户端层面了大于系统层面。这是他们之间的优先级。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221757173.png" alt="image-20230422175706876"></p><h3 id="Operator-Level"><a href="#Operator-Level" class="headerlink" title="Operator Level"></a>Operator Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那下面我们来具体分析一下这四种。首先看这个算子层面的。算子层面其实很简单，首先呢，在这去设置就可以了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221758256.png" alt="image-20230422175809024"></p><h3 id="Execution-Environment-Level"><a href="#Execution-Environment-Level" class="headerlink" title="Execution Environment Level"></a>Execution Environment Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个执行环境层面的。主要呢，是在这个ENV后面来设置一个并行度。这设置的是一个全局的并行度。当然，你也可以选择在下面针对某一个算子再去改它的并行度也是可以的。因为你那个算子层面并行度是大于这个执行环境层面这个并行度的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221758507.png" alt="image-20230422175856304"></p><h3 id="Client-Level"><a href="#Client-Level" class="headerlink" title="Client Level"></a>Client Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来是一个客户端层面。这个并行度呢，可以在客户端提交Job的时候来设定。通过那个-P参数来动态指令就可以了。具体呢，是这样的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221802253.png" alt="image-20230422180247995"></p><h3 id="System-Level"><a href="#System-Level" class="headerlink" title="System Level"></a>System Level</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那最后呢，是这个系统层面了。我们在系统层面可以通过在这个配置文件里面来设定。parallelism.default属性来指定所有执行环境的默认并行度啊，当然了，你是可以在具体的任务里面再去动态的去改这个并行度。因为他们呢，可以覆盖这个系统层面的并行度。</span><br></pre></td></tr></table></figure><h3 id="并行度案例分析"><a href="#并行度案例分析" class="headerlink" title="并行度案例分析"></a>并行度案例分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面呢，我们来通过一些案例来具体分析一下Flink中的并行度。首先看这个图。这个图里面呢，它表示啊，我们这个集群是有三个从节点。M1，M2，M3，注意每个节点上面具有三个slot。这个表示这个从节点，它具有的3个并发处理能力。那如何实现三个呢？在这个flink-conf.yaml里面来配置了taskmanager.numberOfTaskSlots，把它设置为3。这样话相当于我这个节点上面有三个空闲CPU。那这样的话，我这个集群啊，目前具有的一个处理能力就是9 slot</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221811958.png" alt="image-20230422181155611"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一个案例，它的并行度为1，那如何让它的并行度为1呢？很简单，你在提交这个任务的时候，什么参数都不设置就行。并且我们在开发这个word代码的时候，里面啊，也不设置并行度相关的代码，这样就可以了，这样它就会默认呢，读取这个flink-conf.yaml里面的parallelism的值。这个参数的默认值为1。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221817269.png" alt="image-20230422181742166"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第二个案例，如何实现让它的并行度为2呢？你可以通过这几种方式，首先呢，去改了一份文件。把里面这个默认参数值改为二，或者说我们在动态提交的时候通过-P来指定。或者我们通过这个env来设置都是可以的。这样的话呢，我抗里面它的一个冰度都为二。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221823152.png" alt="image-20230422182315555"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221827716.png" alt="image-20230422182700747"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第三个案例，它的并行度为9，那如何实现呢？你要么在这个配置文件里面，把这个参数设置为9，要么呢动态指定。要么呢，通过env来设置都是可以的。这样的话，它就是9份了。这样就占满了，那说我能不能把这个并行度设为10呢？不能，因为你现在最终呢，只有九个slot。这个需要注意啊。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304221831842.png" alt="image-20230422183105341"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第四个。我看呀，它这个并行度呢，还是9，但是注意针对这个sink组件的并行度啊，给它设置为1啊。我们在这主要分析一下这个新的组件并行度，全局设置为9，就是根据咱们前面这个案例。这三种你用哪种都可以。但是呢，我们还需要把这个新的组件并行度设置为1，那怎么设置呢？就说你在代码里面啊，通过算式层面来把这个新组件的并行度设置为1，这样的话它就会覆盖那个全局的那个9。当然你其他组件还是按那个九那个并行度去执行，而我这个组件的话，我在这给它覆盖掉，使用一给它覆盖掉。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-5</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-5.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-5.html</id>
    <published>2023-04-17T08:47:07.000Z</published>
    <updated>2023-04-19T08:27:36.561Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第四章-Hbase"><a href="#第四章-Hbase" class="headerlink" title="第四章 Hbase"></a>第四章 Hbase</h1><h2 id="01-Hbase基本原理"><a href="#01-Hbase基本原理" class="headerlink" title="01 Hbase基本原理"></a>01 Hbase基本原理</h2><h3 id="Region定位–region"><a href="#Region定位–region" class="headerlink" title="Region定位–region"></a>Region定位–region</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182307738.png" alt="image-20230418230749193"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在HBase中，表的所有行都是按照RowKey的字典序排列的，表在行的方向上分割为多个分区（Region）。如图1所示。</span><br><span class="line">每张表一开始只有一个Region，但是随着数据的插入，HBase会根据一定的规则将表进行水平拆分，形成两个Region，当表中的行越来越多时，就会产生越来越多的Region，而这些Region无法存储到一台机器上时，需要分布存储到多台机器上。每个Region服务器负责管理一个Region，通常在每个Region服务器上会放置10~1000个Region，HBase中Region的物理存储如图2所示。</span><br><span class="line"></span><br><span class="line">客户端在插入，删除，查询数据时需要知道哪个Region服务器上存有自己所需的数据，这个查找Region的过程称之为Region定位。</span><br><span class="line"></span><br><span class="line">HBase中每个Region由三个主要要素组成，包括Region所属的表、包含的第一行和包含的最后一行。</span><br></pre></td></tr></table></figure><h3 id="Region定位–META表"><a href="#Region定位–META表" class="headerlink" title="Region定位–META表"></a>Region定位–META表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">有了Region标识符，就可以唯一标识每个Region。为了定位每个Region所在的位置，就可以构建一张映射表，映射表的每个条目包含两项内容，一个是Region标识符，另一个是Region服务器标识，这个条目就表示Region和Region服务器之间的对应关系，从而就可以知道某个Region被保存在哪个Region服务器中。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182308191.png" alt="image-20230418230821649"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">映射表包含了关于Region的元数据，因此也被称为“元数据表”，又名“meta表”。可以scan命令查看meta</span><br><span class="line"></span><br><span class="line">表的结构如图3所示。</span><br><span class="line">Meta表中每一行记录了一个Region的信息。</span><br><span class="line">首先RowKey包含表名、起始行键和时间戳信息。</span><br><span class="line">中间用逗号隔开，第一个Region的起始行键为空。</span><br><span class="line">时间戳只有用.隔开的为分区名称的编码字符串，该信息是由前面的表名、起始行键和时间戳进行字符串编码后形成。</span><br><span class="line">Meta表里有一个列族info。info包含了三个列，分别为regionInfo、server和serverstartcode。</span><br><span class="line">Regioninfo中记录了Region的详细信息，包括行键范围StartKey和EndKey、列族列表和属性。</span><br><span class="line">Server记录了管理该Region的Region服务器的地址，如localhost:16201。</span><br><span class="line">Serverstartcode记录了Region服务器开始托管该Region的时间。</span><br></pre></td></tr></table></figure><h3 id="Region定位–Region定位"><a href="#Region定位–Region定位" class="headerlink" title="Region定位–Region定位"></a>Region定位–Region定位</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182309337.png" alt="image-20230418230932193"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在HBase的早期设计中，Region的查找是通过三层架构来进行查询的，即在集群中有一个总入口ROOT表，记录了meta表分区信息及各个入口地址，这个ROOT表存放在某个Region服务器上，但是在Zookeeper中保存有它的地址。这种早期的三层架构通过先找到ROOT表，从中获取分区meta表位置，然后再获取分区meta表信息，得到Region所在的Region服务器。</span><br><span class="line">从0.96版本以后，三层架构被改为二层架构，去掉了ROOT表，同时Zookeeper中的&#x2F;hbase&#x2F;root-region-server也被去掉。meta表所在的RegionServer信息直接存储在Zookeeper中的&#x2F;hbase&#x2F;meta-region-server中。如图所示</span><br><span class="line">当客户端进行数据操作时，根据操作的表名和行键通过一定的顺序寻找对应的分区数据。</span><br><span class="line">客户端通过Zookeeper获取到Meta表分区存储的地址，然后在对应Region服务器上获取meta表的信息，得到所需表和行键所在的Region信息，然后在从Region服务器上找到所需的数据。一般客户端获取到Region信息后会进行缓存，下次再查询不必从Zookeeper开始寻址。</span><br></pre></td></tr></table></figure><h3 id="数据存储与读取"><a href="#数据存储与读取" class="headerlink" title="数据存储与读取"></a>数据存储与读取</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">HBase集群数据的构成如图所示</span><br><span class="line">HBase的核心模块是Region服务器。</span><br><span class="line">Region服务器由多个Region块构成，Region块中存储的一系列连续的数据集。</span><br><span class="line">Region服务器主要构成部分是：HLog和Region块。</span><br><span class="line">HLog记录该Region的操作日志。</span><br><span class="line">Region对象由多个Store组成，每个Store对应当前分区中的一个列族，每个Store管理一块内存，即MemStore。</span><br><span class="line">当MemStore中的数据达到一定条件时会写入到StoreFile文件中，因此每个Store包含若干个StoreFile文件。StoreFile文件对应HDFS中的HFile文件。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182338403.png" alt="image-20230418233830777"></p><h4 id="MemStore"><a href="#MemStore" class="headerlink" title="MemStore"></a>MemStore</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当Region服务器收到写请求的时候，Region服务器会将请求转至相应的Region。数据首先写入到Memstore，然后当到达一定的阀值的时候，Memstore中的数据会被刷到HFile中进行持久性存储。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase将最近接收到的数据缓存在MemStore中，在持久化到HDFS之前完成排序，再顺序写入HDFS，为后续数据的检索做了优化。因为MemStore缓存的是最近增加的数据，所以也提高了对近期数据的操作速度。在持久化写入之前，在内存中对行键或单元格做些优化。</span><br></pre></td></tr></table></figure><h4 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Store是Region服务器的核心，存储的是同一个列族下的数据，每个Store包含有一块MemStore和0个或多个StoreFile。StoreFile是HBase中最小的数据存储单元。</span><br><span class="line"></span><br><span class="line">  Store存储是HBase存储的核心，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。MemStore是Sorted Memory Buffer（内存写缓存），用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile(底层实现是HFile)， 当StoreFile文件数量增长到一定阈值，会触发Compaction合并操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase将最近接收到的数据缓存在MemStore中，在持久化到HDFS之前完成排序，再顺序写入HDFS，为后续数据的检索做了优化。因为MemStore缓存的是最近增加的数据，所以也提高了对近期数据的操作速度。在持久化写入之前，在内存中对行键或单元格做些优化。</span><br></pre></td></tr></table></figure><h4 id="Store的合并分裂"><a href="#Store的合并分裂" class="headerlink" title="Store的合并分裂"></a>Store的合并分裂</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182339676.png" alt="image-20230418233949215"></p><h4 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182340842.png" alt="image-20230418234015451"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MemStore内存中的数据写到StoreFile文件中，StoreFile底层是以HFile的格式保存。</span><br><span class="line">HFile的存储格式如图7所示</span><br><span class="line">HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。</span><br><span class="line">Trailer中有指针指向其他数据块的起始点，FileInfo记录了文件的一些meta信息。</span><br><span class="line">每个Data块的大小可以在创建一个Table的时候通过参数指定（默认块大小64KB）。</span><br><span class="line">每个Data块除了开头的Magic以外就是一个键值对拼接而成，Magic内容就是一些随机数字，目的是防止数据损坏。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HFile里面的每个键值对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182340877.png" alt="image-20230418234045908"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">键值对结构以两个固定长度的数值开始，分别表示Key的长度和Value的长度。</span><br><span class="line">紧接着是Key，Key以RowLength开始，是固定长度的数值，表示RowKey的长度，</span><br><span class="line">紧接着是RowKey，然后是固定长度的数值ColumnFamilyLength，表示Family的长度，</span><br><span class="line">然后是Family列族，接着是Qualifier列标识符，Key最后以两个固定长度的数值Time Stamp和Key Type（Put&#x2F;Delete）结束。</span><br><span class="line">Value部分没有这么复杂的结构，就是纯粹的二进制数据。</span><br></pre></td></tr></table></figure><h3 id="数据存储与读取-1"><a href="#数据存储与读取-1" class="headerlink" title="数据存储与读取"></a>数据存储与读取</h3><h4 id="HBase写文件流程"><a href="#HBase写文件流程" class="headerlink" title="HBase写文件流程"></a>HBase写文件流程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">客户端首先访问zookeeper，从meta表得到写入数据对应的region信息和相应的region服务器。</span><br><span class="line">找到相应的region服务器,把数据分别写到HLog和MemStore上一份</span><br><span class="line">MemStore达到一个阈值后则把数据刷成一个StoreFile文件。（若MemStore中的数据有丢失，则可以总HLog上恢复）</span><br><span class="line">当多个StoreFile文件达到一定的大小后，会触发Compact合并操作，合并为一个StoreFile，（这里同时进行版本的合并和数据删除。）</span><br><span class="line">当Storefile大小超过一定阈值后，会把当前的Region分割为两个（Split分裂），并由Hmaster分配到相应的HRegionServer，实现负载均衡。</span><br></pre></td></tr></table></figure><h4 id="HBase读文件流程"><a href="#HBase读文件流程" class="headerlink" title="HBase读文件流程"></a>HBase读文件流程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">客户端先访问zookeeper，从meta表读取Region的信息对应的服务器。</span><br><span class="line">客户端向对应Region服务器发送读取数据的请求，Region接收请求后，先从MemStore找数据，如果没有，再到StoreFile上读取，然后将数据返回给客户端。</span><br></pre></td></tr></table></figure><h3 id="WAL机制"><a href="#WAL机制" class="headerlink" title="WAL机制"></a>WAL机制</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191129537.png" alt="image-20230419112916756"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">分布式环境下，必须要考虑到系统出错的情形，比如Region服务器发生故障时，MemStore缓存中还没有被写入文件的数据会全部丢失。</span><br><span class="line">因此，HBase采用HLog来保证系统发生故障时能够恢复到正常的状态。</span><br><span class="line">如图所示，每个Region服务器都有一个HLog文件，同一个Region服务器的Region对象共用一个HLog，HLog是一种预写日志（Write Ahead Log）文件，就是说，用户更新数据必须首先被记入日志后才能写入MemStore缓存，当缓存内容对应的日志已经被写入磁盘后，即日志写成功后，缓存的内容才会被写入磁盘。</span><br><span class="line">HBase系统中，每个Region服务器只需要一个HLog文件，所有Region对象共用一个HLog，而不是每个Region使用一个HLog。在这种Region对象共用一个HLog的方式中，多个Region对象的进行更新操作需要修改日志时，只需要不断把日志记录追加到单个日志文件中，而不需要同时打开、写入到多个日志文件中，因此可以减少磁盘寻址次数，提高对表的写操作性能。</span><br></pre></td></tr></table></figure><h2 id="02-Hbase-Region管理"><a href="#02-Hbase-Region管理" class="headerlink" title="02 Hbase Region管理"></a>02 Hbase Region管理</h2><h3 id="HFile合并"><a href="#HFile合并" class="headerlink" title="HFile合并"></a>HFile合并</h3><h4 id="Minor合并"><a href="#Minor合并" class="headerlink" title="Minor合并"></a>Minor合并</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191135924.png" alt="image-20230419113533020"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前面讲过</span><br><span class="line">用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile(底层实现是Hfile)， 当StoreFile文件数量增长到一定阈值，会触发Compaction合并操作。</span><br><span class="line">HFile的合并分为两种类型，分别是Minor合并和Major合并，这两种合并都发生在Store内部，不是Region的合并。</span><br><span class="line">Minor合并是把多个小HFile合并生成一个大的Hfile</span><br><span class="line">执行合并时，HBase读出已有的多个HFile的内容，把记录写入到一个新文件中。然后把新文件设置为激活状态，并标记旧文件为删除。在Minor合并中，这些标记为删除的旧文件是没有被移除的，任然会出现在HFile中，只有在进行Major合并时才会移除这些旧文件。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191136210.png" alt="image-20230419113609308"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">对需要进行Minor合并的文件的选择是触发式的，当达到触发条件才会进行Minor合并，而触发条件有很多，比如在将MemStore的数据刷到HFile时会申请对Store下符合条件的HFile进行合并，或者定期对Store内的HFile进行合并。另外对选择合并的HFile也是有条件的，如表1所示。</span><br><span class="line">在执行Minor合并时，会根据上述配置参数选择合适的HFile进行合并。Minor合并对HBase的性能是有轻微影响的，所以合并的HFile数量是有限的，默认最多为10个。</span><br></pre></td></tr></table></figure><h4 id="Major合并"><a href="#Major合并" class="headerlink" title="Major合并"></a>Major合并</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191136792.png" alt="image-20230419113630473"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Major合并针对的是给定Region的一个列族的所有Hfile。</span><br><span class="line">它将Store中的所有HFile合并成一个大文件，有时也会对整个表的同一列族的HFile进行合并，这是一个耗时和耗费资源的操作，会影响集群性能。</span><br><span class="line">一般情况下都是做Minor合并，不少集群是禁止Major合并的，只有在集群负载较小时进行手动Major合并，或者配置Major合并周期，默认为7天。</span><br><span class="line">另外Major合并时会清理Minor合并中被标记删除的HFile。</span><br><span class="line">如上右图所示</span><br></pre></td></tr></table></figure><h3 id="Region拆分"><a href="#Region拆分" class="headerlink" title="Region拆分"></a>Region拆分</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191146636.png" alt="image-20230419114652571"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Region拆分是HBase能够拥有良好扩展性的最重要因素。一旦一个Region的负载过大或者超过阈值时，会被分裂成新的两个Region，如图所示。</span><br><span class="line">这个过程是由RegionServer完成，其拆分流程如下：</span><br><span class="line">（1）将需要拆分的Region下线，阻止所有对该Region的客户端请求，master会检测到Region的状态为SPLITING；</span><br><span class="line">（2）将一个Region拆分成两个子Region，先在父Region下建立两个引用文件，分别指向Region的首行和末行，这时两个引用文件并不会从父Region中拷贝数据；</span><br><span class="line">（3）之后在HDFS上建立两个子Region的目录，分别拷贝上一步建立的引用文件，每个子Region分别占父Region的一半数据。拷贝完成后删除两个引用文件。</span><br><span class="line">（4）完成子Region创建后，向.META.表发送新产生的region的元数据信息；</span><br><span class="line">（5）Region的拆分信息更新到Hmaster，并且每个Region进入可用状态。</span><br></pre></td></tr></table></figure><h4 id="拆分策略"><a href="#拆分策略" class="headerlink" title="拆分策略"></a>拆分策略</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191147462.png" alt="image-20230419114724603"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上表列举的切分策略中，切分点的定义是一致的，即当Region中最大Store的大小大于设置阈值之后才会触发拆分。而不同策略中，阈值的定义是不同的，且对集群中Region的分布有很大的影响。</span><br></pre></td></tr></table></figure><h3 id="Region合并"><a href="#Region合并" class="headerlink" title="Region合并"></a>Region合并</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">当RegionServer中的Region数到达最大阈值时，RegionServer就会发起Region合并。其合并过程如下：</span><br><span class="line">（1）客户端发起Region合并处理并发送Region合并请求给Master；</span><br><span class="line">（2）Master在RegionServer上把Region移到一起并发起一个Region合并操作的请求；</span><br><span class="line">（3）RegionServer将准备合并的Region下线，然后进行合并；</span><br><span class="line">（4）从.META.表删除被合并的Region元数据，新的合并了的Region的元数据被更新写入.META.表中；</span><br><span class="line">（5）合并的Region被设置为上线状态并接受访问，同时更新Region信息到Master。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从Region的拆分可以看到随着表的增大，Region的数量是越来越大的，如果很多Region，它们中Memstore也过多，内存大小会触发RegionServer级别的限制，会频繁出现数据从内存刷到HFile的操作，就会对用户请求产生较大的影响，可能阻塞该RegionServer上的更新操作。过多Region会增加ZooKeeper的负担。因此当RegionServer中的Region数到达最大阈值时，RegionServer就会发起Region合并。</span><br></pre></td></tr></table></figure><h3 id="Region负载均衡"><a href="#Region负载均衡" class="headerlink" title="Region负载均衡"></a>Region负载均衡</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304191148622.png" alt="image-20230419114829628"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在分布式系统中，负载均衡是一个非常重要的功能，在HBase中通过Region的数量来实现负载均衡。</span><br><span class="line">每次负载均衡操作分为两步进行，首先生成负载均衡计划表，然后按照计划表执行Region的分配。</span><br><span class="line">Master内部使用一套集群负载评分的算法，来评估HBase某一个表的Region是否需要进行重新分配。</span><br><span class="line">这套算法分别从RegionServer中Region的数目、表的Region数，MenStore大小、StoreFile大小，数据本地性等几个维度来对集群进行评分，评分越低代表集群的负载越合理。</span><br><span class="line">确定需要负载均衡后，在根据不同策略选择Region进行分配，负载均衡策略有三种，如表所示。</span><br><span class="line">根据上述策略选择分配Region后再继续对整个表的所有Region进行评分，如果依然未达到标准，循环执行上述操作直至整个集群达到负载均衡的状态。</span><br></pre></td></tr></table></figure><h2 id="03-Hbase集群管理"><a href="#03-Hbase集群管理" class="headerlink" title="03 Hbase集群管理"></a>03 Hbase集群管理</h2><h3 id="运维管理"><a href="#运维管理" class="headerlink" title="运维管理"></a>运维管理</h3><h4 id="移除RegionServer节点"><a href="#移除RegionServer节点" class="headerlink" title="移除RegionServer节点"></a>移除RegionServer节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当集群由于升级或更换硬件等原因需要在单台机器上停止守护进程时，需要确保集群的其他部分正常工作，并且确保从客户端应用来看停用时间最短。满足此条件必须把这台RegionServer服务的Region主动转移到其他RegionServer上，而不是让HBase被动地对此RegionServer的下线进行反应。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">集群运行时，有些操作任务是必需的，包括增加和移除节点。</span><br><span class="line">用户可以在指定节点的HBase目录下使用hbase-damon.sh stop命令来停止集群中的一个RegionServer。执行此命令后，RegionServer先将所有Region关闭，然后再把自己的进程停止，RegionServer在ZooKeeper中对应的临时节点将会过期。Master检测到RegionServer停止服务后将此RegionServer上的Region重新分配到其他机器上。</span><br><span class="line">HBase也提供了脚本来主动转移Region到其他RegionServer，然后下掉下线的RegionServer这样会让整个过程更加安全。在HBase的bin目录下提供了graceful_stop.sh脚本可以完成这种主动移除节点的功能。此脚本停止一个RegionServer的过程如下：</span><br><span class="line">（1）关闭Region均衡器；</span><br><span class="line">（2）从需要停止的RegionServer上移出Region，并随机把他们分配给集群中其他服务器；</span><br><span class="line">（3）停止RegionServer进程</span><br></pre></td></tr></table></figure><h4 id="增加RegionServer节点"><a href="#增加RegionServer节点" class="headerlink" title="增加RegionServer节点"></a>增加RegionServer节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">随着应用系统需求的增长，整个HBase集群需要进行扩展，这时就需要往HBase集群中增加一个节点。添加一个新的RegionServer是运行集群的常用操作，首先需要修改conf目录下的regionserver文件，然后将此文件复制到集群中所有机器上，这样可以使用启动脚本就能够添加新的服务器。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HBase底层是以HDFS来存储数据的，一般部署HBase集群时，HDFS的DataNode和HBase的RegionServer位于同一台物理机上。</span><br><span class="line">所以往HBase集群增加一个RegionServer之前需要往HDFS里增加DataNode，</span><br><span class="line">等待DataNode进程启动并加入HDFS集群后，再启动HBase的RegionServer进程。</span><br><span class="line">启动新增节点上的RegionServer可以使用命令hbase-damon.sh start，启动成功后可以在Master用户界面看到此节点。</span><br><span class="line">如果需要重新均衡分配每个节点上的Region，则使用HBase的负载均衡功能。</span><br></pre></td></tr></table></figure><h4 id="增加Master备份节点"><a href="#增加Master备份节点" class="headerlink" title="增加Master备份节点"></a>增加Master备份节点</h4><h3 id="数据管理"><a href="#数据管理" class="headerlink" title="数据管理"></a>数据管理</h3><h4 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h4><h4 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h4><h4 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h4><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-4</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-4.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-4.html</id>
    <published>2023-04-17T08:47:02.000Z</published>
    <updated>2023-04-19T08:35:25.572Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第三章-Hbase"><a href="#第三章-Hbase" class="headerlink" title="第三章 Hbase"></a>第三章 Hbase</h1><h2 id="05-Hbase过滤器"><a href="#05-Hbase过滤器" class="headerlink" title="05 Hbase过滤器"></a>05 Hbase过滤器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">可以根据主键、列簇、列、版本等更多的条件来对数据进行过滤。</span><br><span class="line">类似SQL中的WHERE</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; show_filters</span><br><span class="line">DependentColumnFilter                                       KeyOnlyFilter                                               ColumnCountGetFilter                                       SingleColumnValueFilter                                     PrefixFilter                                               SingleColumnValueExcludeFilter                             FirstKeyOnlyFilter                                         ColumnRangeFilter                                           TimestampsFilter                                           FamilyFilter                                               QualifierFilter                                             ColumnPrefixFilter                                         RowFilter                                                   MultipleColumnPrefixFilter                                 InclusiveStopFilter                                         PageFilter                                                 ValueFilter                                                 ColumnPaginationFilter</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">过滤器语法格式：</span><br><span class="line"></span><br><span class="line">scan&#x2F;get  ‘表名’，&#123;Filter &#x3D;&gt; “过滤器 ( 比较运算符，’比较器’)”&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182137386.png" alt="image-20230418213720968"></p><h3 id="RowFilter"><a href="#RowFilter" class="headerlink" title="RowFilter"></a>RowFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RowFilter：针对rowkey进行字符串的比较过滤器。</span><br><span class="line"></span><br><span class="line">举例：</span><br><span class="line">例1：显示行键包含0的键值对；</span><br><span class="line">scan &#39;student&#39;,&#123;FILTER&#x3D;&gt;&quot;RowFilter(&#x3D;,&#39;substring:0’)”&#125;</span><br><span class="line">例2：显示行键字节顺序大于002的键值对；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;RowFilter(&gt;,&#39;binary:002&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="PrefixFilter"><a href="#PrefixFilter" class="headerlink" title="PrefixFilter"></a>PrefixFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PrefixFilter：rowkey前缀比较器，一种更简单的比较行键前缀的命令，等值比较。</span><br><span class="line">举例：</span><br><span class="line"></span><br><span class="line">例1：显示行键前缀为0开头的键值对；</span><br><span class="line">scan &#39;student&#39;,&#123;FILTER&#x3D;&gt;&quot;RowFilter(&#x3D;,&#39;substring:0’)”&#125;</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;PrefixFilter(&#39;003&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="KeyOnlyFilter"><a href="#KeyOnlyFilter" class="headerlink" title="KeyOnlyFilter"></a>KeyOnlyFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KeyOnlyFilter：</span><br><span class="line">只对cell的键进行过滤和显示，但不显示值。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182142565.png" alt="image-20230418214240115"></p><h3 id="FirstKeyOnlyFilter"><a href="#FirstKeyOnlyFilter" class="headerlink" title="FirstKeyOnlyFilter"></a>FirstKeyOnlyFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FirstKeyOnlyFilter：只扫描相同键的第一个cell，其键值对都会显示出来。</span><br><span class="line"></span><br><span class="line">例4：统计表的逻辑行数；</span><br><span class="line">count &#39;student’</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;FirstKeyOnlyFilter()&quot;</span><br><span class="line"></span><br><span class="line">hbase(main):008:0&gt; scan &#39;student&#39;, FILTER&#x3D;&gt;&quot;FirstKeyOnlyFilter()&quot;</span><br><span class="line">ROW                    COLUMN+CELL                           001                   column&#x3D;grades:englisg, timestamp&#x3D;1541485306878, value&#x3D;80        </span><br><span class="line"> 002                   column&#x3D;grades:bigdata, timestamp&#x3D;1541485403649, value&#x3D;88        </span><br><span class="line"> 003                   column&#x3D;grades:bigdata, timestamp&#x3D;1541485412686, value&#x3D;80        </span><br><span class="line">3 row(s) in 0.0400 seconds</span><br></pre></td></tr></table></figure><h3 id="InclusiveStopFilter"><a href="#InclusiveStopFilter" class="headerlink" title="InclusiveStopFilter"></a>InclusiveStopFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">InclusiveStopFilter：替代ENDROW返回终止条件行；</span><br><span class="line">例5：显示起始行键为001，结束行为003的记录；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182149501.png" alt="image-20230418214950938"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182153377.png" alt="image-20230418215313360"></p><h3 id="FamilyFilter"><a href="#FamilyFilter" class="headerlink" title="FamilyFilter"></a>FamilyFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FamilyFilter：针对列族进行比较和过滤。</span><br><span class="line"></span><br><span class="line">例1：显示列族前缀为stu开头的键值对；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,&#39;substring:stu’)”</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,‘binary:stu’)”</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182155971.png" alt="image-20230418215532681"></p><h3 id="QualifierFilter"><a href="#QualifierFilter" class="headerlink" title="QualifierFilter"></a>QualifierFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">QualifierFilter：列标识过滤器。</span><br><span class="line"></span><br><span class="line">例2：显示列名为name的记录；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;substring:name&#39;)&quot;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182200092.png" alt="image-20230418220036827"></p><h3 id="ColumnPrefixFilter"><a href="#ColumnPrefixFilter" class="headerlink" title="ColumnPrefixFilter"></a>ColumnPrefixFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ColumnPrefixFilter：对列名前缀进行过滤。</span><br><span class="line">例2：显示列名为name的记录；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPrefixFilter(&#39;name’)”</span><br><span class="line"></span><br><span class="line">等价于scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;substring:name&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MultipleColumnPrefixFilter：可以指定多个前缀</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">例3：显示列名为name和age的记录；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;MultipleColumnPrefixFilter(&#39;name&#39;,&#39;age&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="ColumnRangeFilter"><a href="#ColumnRangeFilter" class="headerlink" title="ColumnRangeFilter"></a>ColumnRangeFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ColumnRangeFilter ：设置范围按字典序对列名进行过滤；</span><br><span class="line">例4：查询列名在bi和na之间的记录</span><br><span class="line">Student表中有以下列族和列名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182205876.png" alt="image-20230418220523574"></p><h3 id="TimestampsFilter"><a href="#TimestampsFilter" class="headerlink" title="TimestampsFilter"></a>TimestampsFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TimestampsFilter ：时间戳过滤器。支持等值方式比较，但可以设置多个时间戳</span><br><span class="line"></span><br><span class="line">例5：只查询时间戳为1和2的键值对；</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;TimestampsFilter(2,4)&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):030:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;TimestampsFilter(2,4)&quot;</span><br><span class="line">ROW                    COLUMN+CELL                                   004                   column&#x3D;stuinfo:age, timestamp&#x3D;2, value&#x3D;19      004                   column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry 004                   column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male   1 row(s) in 0.0150 seconds</span><br></pre></td></tr></table></figure><h3 id="ValueFilter"><a href="#ValueFilter" class="headerlink" title="ValueFilter"></a>ValueFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ValueFilter ：值过滤器。</span><br><span class="line"></span><br><span class="line">例6：查询值等于19的所有键值对</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;binary:19’)”</span><br><span class="line">scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ValueFilter(&#x3D;,&#39;substring:19&#39;)&quot;</span><br></pre></td></tr></table></figure><h3 id="SingleColumnValueFilter"><a href="#SingleColumnValueFilter" class="headerlink" title="SingleColumnValueFilter"></a>SingleColumnValueFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SingleColumnValueFilter：在指定的列族和列中进行值过滤器。</span><br><span class="line">例7：查询stuinfo列族age列中值等于19的所有键值对</span><br><span class="line"></span><br><span class="line">scan &#39;student&#39;,&#123;COLUMN&#x3D;&gt;&#39;stuinfo:age’,</span><br><span class="line">FILTER&#x3D;&gt;&quot;SingleColumnValueFilter(&#39;stuinfo&#39;,&#39;age&#39;,&#x3D;,&#39;binary:19&#39;)&quot;&#125;</span><br><span class="line"></span><br><span class="line">SingleColumnValueExcludeFilter：在指定的列族和列中进行值过滤器，与SingleColumnValueFilter功能相反。</span><br></pre></td></tr></table></figure><h3 id="ColumnCountGetFilter"><a href="#ColumnCountGetFilter" class="headerlink" title="ColumnCountGetFilter"></a>ColumnCountGetFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ColumnCountGetFilter：限制每个逻辑行返回的键值对数</span><br><span class="line">例7：返回行键为001的前3个键值对</span><br><span class="line"></span><br><span class="line">hbase(main):004:0&gt; get &#39;student&#39;, &#39;001&#39;,FILTER&#x3D;&gt;&quot;ColumnCountGetFilter(3)&quot;</span><br><span class="line">COLUMN                    CELL                                       grades:englisg           timestamp&#x3D;1541485306878, value&#x3D;80           grades:math              timestamp&#x3D;1541485384199, value&#x3D;90           stuinfo:age              timestamp&#x3D;1541485224974, value&#x3D;18           3 row(s) in 0.0950 seconds</span><br></pre></td></tr></table></figure><h3 id="PageFilter"><a href="#PageFilter" class="headerlink" title="PageFilter"></a>PageFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">PageFilter ：基于行的分页过滤器，设置返回行数。</span><br><span class="line"></span><br><span class="line">hbase(main):005:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;PageFilter(1)&quot;</span><br><span class="line">ROW                       COLUMN+CELL                                   001                      column&#x3D;grades:englisg, timestamp&#x3D;1541485306878, value&#x3D;80                 </span><br><span class="line"> 001                      column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90                    </span><br><span class="line"> 001                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485224974, value&#x3D;18                    </span><br><span class="line"> 001                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485170696, value&#x3D;alice                </span><br><span class="line"> 001                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485235855, value&#x3D;female                </span><br><span class="line">1 row(s) in 0.0680 seconds</span><br></pre></td></tr></table></figure><h3 id="ColumnPaginationFilter"><a href="#ColumnPaginationFilter" class="headerlink" title="ColumnPaginationFilter"></a>ColumnPaginationFilter</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ColumnPaginationFilter：基于列的进行分页过滤器，需要设置偏移量与返回数量 。</span><br><span class="line"></span><br><span class="line">例9：显示每行第1列之后的2个键值对。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):006:0&gt; scan &#39;student&#39;</span><br><span class="line">ROW                       COLUMN+CELL                                                     001                      column&#x3D;grades:englisg, timestamp&#x3D;1541485306878, value&#x3D;80         001                      column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90           001                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485224974, value&#x3D;18           001                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485170696, value&#x3D;alice       001                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485235855, value&#x3D;female       002                      column&#x3D;grades:bigdata, timestamp&#x3D;1541485403649, value&#x3D;88         002                      column&#x3D;grades:englisg, timestamp&#x3D;1541485319132, value&#x3D;85         002                      column&#x3D;grades:math, timestamp&#x3D;1541485376414, value&#x3D;78           002                      column&#x3D;stuinfo:class, timestamp&#x3D;1541485278646, value&#x3D;1802       002                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485187403, value&#x3D;nancy       002                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485245291, value&#x3D;male</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):007:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(2,1)&quot;</span><br><span class="line">ROW                       COLUMN+CELL                                                     001                      column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90           001                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485224974, value&#x3D;18           002                      column&#x3D;grades:englisg, timestamp&#x3D;1541485319132, value&#x3D;85         002                      column&#x3D;grades:math, timestamp&#x3D;1541485376414, value&#x3D;78           003                      column&#x3D;grades:englisg, timestamp&#x3D;1541485328260, value&#x3D;90         003                      column&#x3D;grades:math, timestamp&#x3D;1541485368087, value&#x3D;80           004                      column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry                   004                      column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                     </span><br><span class="line">4 row(s) in 0.0840 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">003                      column&#x3D;grades:bigdata, timestamp&#x3D;1541485412686, value&#x3D;80       003                      column&#x3D;grades:englisg, timestamp&#x3D;1541485328260, value&#x3D;90       003                      column&#x3D;grades:math, timestamp&#x3D;1541485368087, value&#x3D;80           003                      column&#x3D;stuinfo:age, timestamp&#x3D;1541485209410, value&#x3D;19           003                      column&#x3D;stuinfo:class, timestamp&#x3D;1541485271479, value&#x3D;1803       003                      column&#x3D;stuinfo:name, timestamp&#x3D;1541485198223, value&#x3D;harry       003                      column&#x3D;stuinfo:sex, timestamp&#x3D;1541485253075, value&#x3D;male         004                      column&#x3D;stuinfo:age, timestamp&#x3D;2, value&#x3D;19                       004                      column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry                   004                      column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                              </span><br><span class="line">4 row(s) in 0.0980 seconds</span><br></pre></td></tr></table></figure><h3 id="组合使用过滤器"><a href="#组合使用过滤器" class="headerlink" title="组合使用过滤器"></a>组合使用过滤器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">组合使用过滤器：使用AND或OR等连接符，组合多个过滤器进行组合扫描。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">例10：组合过滤器的使用</span><br><span class="line">hbase(main):008:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(2,1) AND ValueFilter(&#x3D;,&#39;substring:ma&#39;)&quot;</span><br><span class="line">ROW                       COLUMN+CELL                                                              </span><br><span class="line"> 004                      column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                              </span><br><span class="line">1 row(s) in 0.1040 seconds</span><br><span class="line">hbase(main):010:0&gt; scan &#39;student&#39;,FILTER&#x3D;&gt;&quot;ColumnPaginationFilter(1,1) OR ValueFilter(&#x3D;,&#39;substring:ma&#39;)&quot;</span><br><span class="line">ROW                           COLUMN+CELL                                                 001                          column&#x3D;grades:math, timestamp&#x3D;1541485384199, value&#x3D;90       001                          column&#x3D;stuinfo:sex, timestamp&#x3D;1541485235855, value&#x3D;female   002                          column&#x3D;grades:englisg, timestamp&#x3D;1541485319132, value&#x3D;85   002                          column&#x3D;stuinfo:sex, timestamp&#x3D;1541485245291, value&#x3D;male     003                          column&#x3D;grades:englisg, timestamp&#x3D;1541485328260, value&#x3D;90   003                          column&#x3D;stuinfo:sex, timestamp&#x3D;1541485253075, value&#x3D;male     004                          column&#x3D;stuinfo:name, timestamp&#x3D;2, value&#x3D;curry               004                          column&#x3D;stuinfo:sex, timestamp&#x3D;4, value&#x3D;male                                        </span><br><span class="line">4 row(s) in 0.0440 seconds</span><br></pre></td></tr></table></figure><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">FamilyFilter：针对列族进行比较和过滤。</span><br><span class="line">QualifierFilter：列标识过滤器。</span><br><span class="line">ColumnPrefixFilter：对列名前缀进行过滤。</span><br><span class="line">MultipleColumnPrefixFilter：可以指定多个前缀</span><br><span class="line">ColumnRangeFilter ：设置范围按字典序对列名进行过滤；</span><br><span class="line">TimestampsFilter ：时间戳过滤器。支持等值方式比较，但可以设置多个时间戳</span><br><span class="line">ValueFilter ：值过滤器。</span><br><span class="line">SingleColumnValueFilter ：在指定的列族和列中进行值过滤器。</span><br><span class="line">SingleColumnValueExcludeFilter：在指定的列族和列中进行值过滤器，与SingleColumnValueFilter功能相反。</span><br><span class="line">ColumnCountGetFilter ：限制每个逻辑行返回的键值对数</span><br><span class="line">PageFilter ：基于行的分页过滤器，设置返回行数。</span><br><span class="line">ColumnPaginationFilter ：基于列的进行分页过滤器，需要设置偏移量与返回数量 。</span><br></pre></td></tr></table></figure><h2 id="06-Hbase-Java编程方法"><a href="#06-Hbase-Java编程方法" class="headerlink" title="06 Hbase Java编程方法"></a>06 Hbase Java编程方法</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182232258.png" alt="image-20230418223225014"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">首先介绍基于JAVA的编程方法，HBase是基于java语言开发，用户可以利用包含java语言在内的多种语言进行调用开发。由于java是原生语言，因此利用java进行应用开发，以及过滤器等内容的开发最为方便。</span><br><span class="line"></span><br><span class="line">HBase对java开发环境并无特殊要求，只要将用到的HBase的库包加入引用路径即可。</span><br><span class="line">本节讲述以eclipse为java的集成开发环境，具体步骤如下：</span><br><span class="line">首先在eclipse中建立标准的java工程，给工程命名为hbase，其他使用默认配置，按步骤完成项目的创建。</span><br><span class="line">然后打开hbase项目的属性（对应图中序号1），选择java构建路径标签页（2），选择库（3），点击添加外部jar按钮（4）。然后弹出框会让你选择hbase安装目录下的lib。将需要的包导入工程，导入成功后会在工程里引用的库中出现你所选择的jar包。（解说完后播放操作视频hbase_java环境配置.mp4）</span><br></pre></td></tr></table></figure><h3 id="包导入"><a href="#包导入" class="headerlink" title="包导入"></a>包导入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">使用Hadoop和HBase的环境配置</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line"></span><br><span class="line">HBase的客户端接口</span><br><span class="line">import org.apache.hadoop.hbase.*;</span><br><span class="line">import org.apache.hadoop.hbase.client.*;</span><br><span class="line"></span><br><span class="line">HBase工具包</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line">HBase过滤器</span><br><span class="line">import org.apache.hadoop.hbase.filter.*;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工程目录src下新建类文件，在java文件中import需要的HBase包，比如HBase的环境配置包需要导入Configuration包（对应显示红色）</span><br><span class="line">HBase客户端接口需要导入hbase.client(绿色两行)</span><br><span class="line">工具包需要导入hbase.util.Bytes（蓝色），而如果使用过滤器需要导入hbase.filter包。如果还需要其他包可以从hbase lib里去寻找，并导入工程即可。</span><br></pre></td></tr></table></figure><h3 id="建立连接"><a href="#建立连接" class="headerlink" title="建立连接"></a>建立连接</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public static Configuration conf;</span><br><span class="line">public static Connection connection;</span><br><span class="line">           public static Admin admin;</span><br><span class="line"></span><br><span class="line">public void getconnect() throws IOException</span><br><span class="line">&#123;</span><br><span class="line">conf&#x3D;HBaseConfiguration.create();</span><br><span class="line">conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;cm-cdh01&quot;);</span><br><span class="line">conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</span><br><span class="line">try&#123;</span><br><span class="line">connection&#x3D;ConnectionFactory.createConnection(conf);</span><br><span class="line">admin&#x3D;connection.getAdmin();</span><br><span class="line">&#125;</span><br><span class="line">catch(IOException e)&#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">环境配置好后，接下来开始连接数据库，在分布式环境下，客户端访问HBase需要通过ZooKeeper的地址和端口来获取当前活跃的Master和所需的RegionServer地址，</span><br><span class="line">首先建立三个全局变量，conf、connection和admin（高亮显示红色字体），conf用来描述zookeeper集群的访问地址，connect用来建立连接，admin是创建的数据库管理员，执行具体的数据表操作。</span><br><span class="line">Conf使用set方法来设置集群地址和端口号，然后使用ConnectionFactory来建立连接，并让admin获取管理员的操作权限，至此已经java客户端已经连接上hbase数据库。</span><br></pre></td></tr></table></figure><h3 id="Admin接口"><a href="#Admin接口" class="headerlink" title="Admin接口"></a>Admin接口</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182235010.png" alt="image-20230418223516389"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来具体看下HBase API中常用的接口 ，首先是Admin接口，Admin用于管理HBase数据库的表信息，如创建表，用到了createtable，删除表用到disabletable和deletetable，与Hbase中的create，delete和disable命令对应。另外admin接口还可以使用listtables方法列出hbase中所有的表，使用getTableDescriptor来获取表的结构信息，分别于hbase shell中的，list 和describe命令。</span><br><span class="line"></span><br><span class="line">注意listtables和getTableDescriptor方法返回的是HTableDescriptor类结构的数据。</span><br></pre></td></tr></table></figure><h3 id="创建和删除表"><a href="#创建和删除表" class="headerlink" title="创建和删除表"></a>创建和删除表</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182236492.png" alt="image-20230418223634346"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接着通过具体代码来看下Admin接口提供的方法的使用。首先是创建表，在创建表之前用tablesexists来判断表是否存在，注意设定tableName对象的语法，使用valueOf方法设置表名。如果表已经存在则使用disableTable和deleteTable方法来删除表，注意这里与hbaseshell中操作一样，先禁用表再删除表（红框）。</span><br><span class="line"></span><br><span class="line">（这一段对应蓝色文字框）HTableDescriptor类用来描述表结构，包括HBase中表格的详细信息，例如表中的列族、该表的类型、是否只读、MemStore的最大空间等，并且提供了一些操作表的方法，比如增加列族addFamliy()、删除列族removeFamily()和设置属性值setValue()等方法。</span><br><span class="line"></span><br><span class="line">（这一段对应绿色文字框）HColumnDescriptor类则用来描述列族，比如列族的版本数，压缩设置等。此类通常在添加列族或者创建表的时候使用，一旦列族建立就不能被修改，只有通过删除列族，再创建新的列族来间接修改。HCloumnDescriptor类提供getName()、getValue()和setValue等方法对列族的数据进行操作。</span><br><span class="line"></span><br><span class="line">（对应绿色虚线框的内容）这里的代码使用了两种方式创建列族（绿色框），建立stuinfo时（黄色三行），先通过对象HColumnDescriptor自定义列族属性，比如列族名和块大小，然后根据列族属性使用addfamily建立列族。而建立grades列族时（绿色行）直接使用addfamily方法根据默认属性建立的。</span><br><span class="line"></span><br><span class="line">描述和建立完列族信息后，通过createTable建立表格。</span><br></pre></td></tr></table></figure><h3 id="Table接口"><a href="#Table接口" class="headerlink" title="Table接口"></a>Table接口</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304182236561.png" alt="image-20230418223657412"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再来看下另一个接口Table，如果不需要创建表，直接插入数据，可以不用建立Admin对象，使用Table接口即可。Table接口主要用来进行数据的操作，比如删除指定行使用delete，获取指定行的数据使用get，以及向表中添加数据使用put方法。</span><br></pre></td></tr></table></figure><h3 id="Put数据更新"><a href="#Put数据更新" class="headerlink" title="Put数据更新"></a>Put数据更新</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public static void addData() throws IOException &#123;</span><br><span class="line">Table table &#x3D; connection.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">Put put &#x3D; new Put(Bytes.toBytes(“001&quot;));</span><br><span class="line">put.addColumn(Bytes.toBytes(&quot;StuInfo&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(“harry&quot;));</span><br><span class="line">                               put.addColumn(Bytes.toBytes(&quot;StuInfo&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(“harry&quot;));</span><br><span class="line">table.put(put);</span><br><span class="line">table.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Put put1 &#x3D; new Put(Bytes.toBytes(&quot;002&quot;));</span><br><span class="line">put1.addColumn(Bytes.toBytes(&quot;StuInfo&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;jess&quot;));</span><br><span class="line">Put put2 &#x3D; new Put(Bytes.toBytes(&quot;003&quot;));</span><br><span class="line">put2.addColumn(Bytes.toBytes(&quot;Grades&quot;), Bytes.toBytes(&quot;english&quot;), Bytes.toBytes(&quot;98&quot;));</span><br><span class="line">List&lt;Put&gt; putList &#x3D; new ArrayList&lt;Put&gt;();</span><br><span class="line">putList.add(put1);</span><br><span class="line">putList.add(put2);</span><br><span class="line">table.put(putList);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先来看下put方法，put主要用来插入，也可以对已有的记录进行更新，在使用put方法前，需要根据表名建立和它的连接（红色行）</span><br><span class="line"></span><br><span class="line">与HBase shell相同，使用put方法可以逐条的插入数据，put对象中首先指明行键为001，并通过addColumn方法加入键值对，addColumn方法的参数分别为列族stuinfo，列name和值harry。这里加入了2个键值对，说明这一行有2列，table.Put方法将put对象写入内存和日志，此时数据已经可以被查出。</span><br><span class="line"></span><br><span class="line">另外可以采用链表的方式一次性插入多个键值对，如下图，put1插入1键值对，put2插入1个键值对，然后建立链表putList，使用add方法将两个put对象插入到链表中，最后一次性插入到表中。</span><br></pre></td></tr></table></figure><h3 id="查询数据-Get"><a href="#查询数据-Get" class="headerlink" title="查询数据-Get"></a>查询数据-Get</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public static void getRow(String tableName, String rowKey) throws IOException &#123;  </span><br><span class="line">Table table &#x3D; connection.getTable(TableName.valueOf(&quot;student&quot;));</span><br><span class="line">Get get &#x3D; new Get(Bytes.toBytes(rowKey));  </span><br><span class="line">Result result &#x3D; table.get(get);  </span><br><span class="line">for (Cell cell : result.rawCells()) &#123;  </span><br><span class="line">System.out.println(  </span><br><span class="line">&quot;行键:&quot; + new String(CellUtil. getCellKeyAsString(cell)) + &quot;\t&quot; +  </span><br><span class="line">&quot;列族:&quot; + new String(CellUtil.cloneFamily(cell)) + &quot;\t&quot; +   </span><br><span class="line">&quot;列名:&quot; + new String(CellUtil.cloneQualifier(cell)) + &quot;\t&quot; +   </span><br><span class="line">&quot;值:&quot; + new String(CellUtil.cloneValue(cell)) + &quot;\t&quot; +  </span><br><span class="line">&quot;时间戳:&quot; + cell.getTimestamp());  </span><br><span class="line">&#125;  </span><br><span class="line">table.close();  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">插入数据后，可以根据行键进行查询，比如get方法，此代码中，先建立连接，通过get对象描述查询条件，再通过table.get方法进行实际查询（紫色两行）。查询结构写入result中，由于get方法一次获取一个逻辑行，即可能包含多个键值对，因此查询结构通过循环的方法将逐个键值对输出显示。</span><br><span class="line"></span><br><span class="line">显示时，getCellKeyAsString用来获取行键，cloneFamily用来获取列族，cloneQualifier用来获取列名，cloneValue获取具体值，getTimestamp获取时间戳</span><br></pre></td></tr></table></figure><h3 id="查询数据-Scan"><a href="#查询数据-Scan" class="headerlink" title="查询数据-Scan"></a>查询数据-Scan</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public static void scanTable(String tableName) throws IOException &#123;  </span><br><span class="line">       Table table &#x3D; connection.getTable(TableName.valueOf(&quot;student&quot;)); </span><br><span class="line">       Scan scan &#x3D; new Scan();  </span><br><span class="line">       ResultScanner results &#x3D; table.getScanner(scan);  </span><br><span class="line">       for (Result result : results) &#123;  </span><br><span class="line">           for (Cell cell : result.rawCells()) &#123;  </span><br><span class="line">               System.out.println(  </span><br><span class="line">                       &quot;行键:&quot; + new String(CellUtil.getCellKeyAsString(cell)) + &quot;\t&quot; +  </span><br><span class="line">                       &quot;列族:&quot; + new String(CellUtil.cloneFamily(cell)) + &quot;\t&quot; +   </span><br><span class="line">                       &quot;列名:&quot; + new String(CellUtil.cloneQualifier(cell)) + &quot;\t&quot; +   </span><br><span class="line">                       &quot;值:&quot; + new String(CellUtil.cloneValue(cell)) + &quot;\t&quot; +  </span><br><span class="line">                       &quot;时间戳:&quot; + cell.getTimestamp());  </span><br><span class="line">           &#125;  </span><br><span class="line">       &#125;   </span><br><span class="line">       table.close();</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Scan的操作语法与get类似，都需要建立连接，描述查询条件，再进行实际查询，核心语句是table.getScanner，后续的显示页和之前代码基本相同，但是，由于scan的结果得到的多个逻辑行，且每个逻辑行包含多个键值对，因此采用二层循环的方式来显示每一个键值对的内容，（红框所示）。</span><br></pre></td></tr></table></figure><h3 id="删除行和列"><a href="#删除行和列" class="headerlink" title="删除行和列"></a>删除行和列</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Table table &#x3D; connection.getTable(TableName.valueOf(tableName));</span><br><span class="line"></span><br><span class="line">1，删除指定行       </span><br><span class="line">Delete delete1 &#x3D; new Delete(Bytes.toBytes(‘001’));</span><br><span class="line"></span><br><span class="line">2，删除指定列族</span><br><span class="line">Delete delete2 &#x3D; new Delete(Bytes.toBytes(‘002’));</span><br><span class="line">delete2.addFamily(Bytes.toBytes(‘stuinfo’));</span><br><span class="line"></span><br><span class="line">3，删除指定列族中的列</span><br><span class="line">Delete delete3 &#x3D; new Delete(Bytes.toBytes(‘003’));</span><br><span class="line">delete3.addColumn(Bytes.toBytes(‘grades’),Bytes.toBytes(‘math’));</span><br><span class="line"></span><br><span class="line">调用 table.delete执行删除  </span><br><span class="line">table.delete(delete);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">完成了数据的新增和查询，Hbase api也提供删除数据，可以指定行键，列族和列进行删除，</span><br><span class="line">如第一个delete对象只有行键属性，因此会删除一个逻辑行，即所有行键为001的键值对都将被删除；</span><br><span class="line">第二个delete对象通过addfamily方法加入了列族参数stuinfo，因此只会删除再stuinfo列族中，行键为002的键值对；</span><br><span class="line">第三个delete对象通过addColumn方法加入列名参数math，同时也指定了列族grades，因此只会删除行键为003，grades列族中math列的键值对。</span><br><span class="line"></span><br><span class="line">最后调用table.delete方法来完成删除。</span><br></pre></td></tr></table></figure><h2 id="07-Hbase-Python编程方法"><a href="#07-Hbase-Python编程方法" class="headerlink" title="07 Hbase Python编程方法"></a>07 Hbase Python编程方法</h2><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-3</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-3.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-3.html</id>
    <published>2023-04-17T08:46:58.000Z</published>
    <updated>2023-04-17T15:26:37.989Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第三章-Hbase"><a href="#第三章-Hbase" class="headerlink" title="第三章 Hbase"></a>第三章 Hbase</h1><h2 id="01-Hbase数据模型"><a href="#01-Hbase数据模型" class="headerlink" title="01 Hbase数据模型"></a>01 Hbase数据模型</h2><h3 id="逻辑模型"><a href="#逻辑模型" class="headerlink" title="逻辑模型"></a>逻辑模型</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172154639.png" alt="image-20230417215406265"></p><h3 id="HBase相关概念"><a href="#HBase相关概念" class="headerlink" title="HBase相关概念"></a>HBase相关概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（1）表（table）：HBase采用表来组织数据；</span><br><span class="line">（2）行（row）：每个表都由行组成，每个行由行键（row key）来标识，行键可以是任意字符串；</span><br><span class="line">（3）列族（column family）：一个table有许多个列族，列族是列的集合，属于表结构，也是表的基本访问控制单元；</span><br><span class="line">（4）列标识（column qualifier）：属于某一个Column Family：Column Qualifier形式标识，每条记录可动态添加</span><br><span class="line">（5）时间戳（timestamp）：时间戳用来区分数据的不同版本；</span><br><span class="line">（6）单元格（cell）：在table中，cell中存储的数据没有数据类型，是字节数组byte[] ，通过&lt;RowKey，Column Family: Column Qualifier，Timestamp&gt;元组来访问单元格</span><br></pre></td></tr></table></figure><h3 id="物理模型"><a href="#物理模型" class="headerlink" title="物理模型"></a>物理模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库特点：</span><br><span class="line">表结构预先定义；</span><br><span class="line">每列的数据类型不同；</span><br><span class="line">空值占用存储空间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HBase特点：</span><br><span class="line">只需定义表名和列族，可以动态添加列族和列；</span><br><span class="line">数据都是字符串类型；</span><br><span class="line">空值不占用存储空间；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172203728.png" alt="image-20230417220304381"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172214589.png" alt="image-20230417221413176"></p><h3 id="实际存储方式"><a href="#实际存储方式" class="headerlink" title="实际存储方式"></a>实际存储方式</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172218013.png" alt="image-20230417221843341"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172220818.png" alt="image-20230417222012709"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172220613.png" alt="image-20230417222052208"></p><h2 id="02-Hbase数据定义"><a href="#02-Hbase数据定义" class="headerlink" title="02 Hbase数据定义"></a>02 Hbase数据定义</h2><h3 id="HBase-Shell"><a href="#HBase-Shell" class="headerlink" title="HBase Shell"></a>HBase Shell</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase Shell：HBase的命令行工具，最简单的接口，适合HBase管理使用；</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost bin]# hbase shell</span><br><span class="line">HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.</span><br><span class="line">Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell</span><br><span class="line">Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt; </span><br><span class="line"></span><br><span class="line">命令：help,status,version,exit,quit</span><br></pre></td></tr></table></figure><h3 id="数据定义"><a href="#数据定义" class="headerlink" title="数据定义"></a>数据定义</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172230877.png" alt="image-20230417223050379"></p><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法：creat‘表名’，‘列族名’</span><br><span class="line">描述：</span><br><span class="line">●  必须指定表名和列族；</span><br><span class="line">●  可以创建多个列族；</span><br><span class="line">●  可以对标和列族指明一些参数；</span><br><span class="line">●  参数大小写敏感；</span><br><span class="line">●  字符串参数需要包含在单引号中；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172233954.png" alt="image-20230417223313646"></p><h4 id="表相关操作"><a href="#表相关操作" class="headerlink" title="表相关操作"></a>表相关操作</h4><h5 id="exsit"><a href="#exsit" class="headerlink" title="exsit"></a>exsit</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exsit：查看某个表是否存在</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172239630.png" alt="image-20230417223907126"></p><h5 id="List"><a href="#List" class="headerlink" title="List"></a>List</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List：查看当前所有的表名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172239913.png" alt="image-20230417223934111"></p><h5 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe：查看选定表的列族及其参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172236116.png" alt="image-20230417223645853"></p><h5 id="Alter"><a href="#Alter" class="headerlink" title="Alter"></a>Alter</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Alter：修改表结构</span><br><span class="line">功能：</span><br><span class="line">修改表中列族的参数信息；</span><br><span class="line">增加列族；</span><br><span class="line">移除或删除已有的列族；</span><br><span class="line"></span><br><span class="line">注意：删除列族时，表中至少有两个列族组成；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172241746.png" alt="image-20230417224143643"></p><h5 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop：删除表</span><br><span class="line">注意：删除表之前需要先禁用表。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172242902.png" alt="image-20230417224227529"></p><h5 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate：删除表中所有数据，想到于对表完成禁用、删除，按原结构重新建立表结构的过程</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172242617.png" alt="image-20230417224251643"></p><h2 id="03-Hbase数据操作"><a href="#03-Hbase数据操作" class="headerlink" title="03 Hbase数据操作"></a>03 Hbase数据操作</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172254865.png" alt="image-20230417225437539"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在HBase中对数据的增删改查命令如表所示，由put命令向表中添加和修改数据，get和scan命令用来查询数据，delete删除列族或列的数据。接下来详细介绍这几个命令的具体用法。</span><br></pre></td></tr></table></figure><h3 id="put"><a href="#put" class="headerlink" title="put"></a>put</h3><h4 id="为单元格插入数据"><a href="#为单元格插入数据" class="headerlink" title="为单元格插入数据"></a>为单元格插入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">语法：put  ‘表名’，‘行键’，‘列族：列限定符’，‘单元格值’，时间戳</span><br><span class="line">描述：必须指定表名、行键、列族、列限定符。</span><br><span class="line">参数区分大小，字符串使用单引号。</span><br><span class="line">只能插入单条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172255143.png" alt="image-20230417225550657"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在HBase中，更新数据时，不管是添加新的数据还是修改数据都使用put命令，它的语法结构如ppt所示，put命令所带的第一个参数为表名，指定某一张表，第二参数为行键的名称，用来指定某一行，第三个参数是为列族和列的名称，中间用冒号隔开，列族名必须是已经创建的，否则HBase会报错；列名是临时定义的，所以列族里的列是可以随意扩展的。第四个参数为单元格的值，在HBase里，所有数据都是字符串的形式。最后一个参数为时间戳，如果不设置时间戳，系统会自动插入当前时间为时间戳。</span><br><span class="line">HBase中所有命令参数是区分大小写的，字符串是需要包含在单引号中的，这一点在介绍后面操作命令不再提示。</span><br><span class="line">从命令形式来看，put只能插入单元格的数据，如果需要将逻辑表中的一行数据插入到HBase中需要执行几条put命令。</span><br><span class="line">比如，需要将此逻辑表的第一行数据（左边图和红色虚线框）插入HBase中，需要执行5条命令（右图）</span><br></pre></td></tr></table></figure><h4 id="更新单元格数据"><a href="#更新单元格数据" class="headerlink" title="更新单元格数据"></a>更新单元格数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">描述：</span><br><span class="line">如果指定的单元格已经存在，则put为更新数据；</span><br><span class="line">单元格会保存指定version&#x3D;&gt;n的多个版本数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172302591.png" alt="image-20230417230227658"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">另外如果由‘表名’，‘行键’，‘列族：列限定符’指定的单元格已经存在表中，则执行put命令为数据更新操作，</span><br><span class="line"></span><br><span class="line">比如，在执行了左边的5条命令后（左图），再执行这条命令（鼠标指向“put ‘Student’, ‘0001’, ‘StuInfo:Name’,‘Tom Green‘,1），学号为1的学生姓名将改成了tom green。</span><br><span class="line"></span><br><span class="line">默认情况下数据更新后，旧版本的数据将不可见，但如果建表时对列族指定了Version属性值，则旧版数据依然存在，用户查询时可以获得最新的多个版本；</span><br></pre></td></tr></table></figure><h3 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">语法：delete  ‘表名’，‘行键’，‘列族&lt;：列限定符&gt;’，&lt;时间戳&gt;</span><br><span class="line">描述：必须指定表名、行键和列族，列限定符和时间戳是可选参数；</span><br><span class="line">Delete最小删除粒度为单元格，且不能跨列族删除。</span><br><span class="line"></span><br><span class="line">(1)delete ‘Student’, ‘0001’, ‘Grades’</span><br><span class="line">(2)delete ‘Student’, ‘0001’, ‘Grades:Math’ </span><br><span class="line">(3)delete ‘Student’, ‘0001’, ‘Grades:Math’,2</span><br><span class="line">(4)Deleteall ‘Student’, ‘0001’</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase中删除数据采用delete命令，其语法与put命令类似，必须指定表名，行键，和列族。而列限定符和时间戳是可选的</span><br><span class="line">下面通过举例说明delete的使用，如第一条命令，只指定了表名行键和列族，表示删除student表中，学号为0001的学生所有的成绩信息。即将表中第一行grades列族的信息全部删除。</span><br><span class="line">第二条命令，指定了列族和列限定符，表示只删除这个学生的数学成绩。</span><br><span class="line">第三条命令，指定了列族和列限定符的同时，还指定了时间戳，表示所有时间戳小于等于2的数据都会被删掉。注意这里不是只删除时间戳等于2的数据。</span><br><span class="line"></span><br><span class="line">从上面语法和命令来看，delete最小的删除粒度为单元格，而且不能跨列族删除，如果想删除表中所有列族在某个行键上的数据，也就是说想删除一个逻辑行，可以使用deleteall命令，例如第四条命令，则删除0001学号学生的所有信息，包括stuinfo列族中的基本信息和grades列族中的所有成绩信息。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete操作并不会马上删除数据，只是将对应的数据打上删除标记（tombstone），只有在数据产生合并时，数据才会被删除。</span><br></pre></td></tr></table></figure><h3 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h3><h4 id="get"><a href="#get" class="headerlink" title="get"></a>get</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">get：根据行键获取一条数据</span><br><span class="line">scan：扫描一个表，可以指定行键范围，或使用过滤器</span><br><span class="line">语法：get  ‘表名’，‘行键’，&lt;‘列族：列限定符’，时间戳&gt;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172313248.png" alt="image-20230417231312108"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第三条命令指定了列族和时间戳范围，</span><br><span class="line">第四条命令则指定列族和显示的版本数，其结果如图所示（蓝色图），在执行此命令之前先向表的stuinfo列族的name列插入了三个版本的数据，注意这里前提是stuinfo列族在创建时已指定VERSION参数可以保存最近的3个版本的数据。在向同一单元格put三条数据后，再执行第四条命令，显示的结果可以看到，只将最近两个更新的数据显示出来了（红色虚线框）</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172315547.png" alt="image-20230417231510110"></p><h4 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">语法：scan  ‘表名’，&#123;&lt; ‘列族：列限定符’，时间戳&gt;&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172321582.png" alt="image-20230417232102333"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">另外一种数据查询方式使用scan命令进行全表扫描，scan命令必须带的参数是表名，其他参数都可选，还可以指定输出行键范围，以及使用过滤器来对全表数据进行过滤显示。</span><br><span class="line">依然通过举例说明scan命令的方法。</span><br><span class="line">第一条命令指定表名查询全表数据；如图所示将表中所有行和所有列族信息都显示出来了。</span><br><span class="line">第二条命令指定列族名称，显示student表中stuinfo列族的所有数据，注意与get不同的是，get只获得某一行的，而scan获取所有行的stuinfo列族数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172324207.png" alt="image-20230417232442038"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第五条命令指定输出行的范围；显示结果输出起始行和结束行但不包括结束行的数据，如图的命令只显示了001行的数据，并没有显示003行。</span><br><span class="line"></span><br><span class="line">另外这些限定条件可以组合使用，中间使用逗号隔开，如第六条命令所示：查询起始行为001，结束行为002的所有行的stuinfo列族的数据信息。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-2</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.html</id>
    <published>2023-04-17T08:46:52.000Z</published>
    <updated>2023-04-19T13:24:55.759Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第二章-Hbase"><a href="#第二章-Hbase" class="headerlink" title="第二章 Hbase"></a>第二章 Hbase</h1><h2 id="01-Hbase简介"><a href="#01-Hbase简介" class="headerlink" title="01 Hbase简介"></a>01 Hbase简介</h2><h3 id="什么是HBase"><a href="#什么是HBase" class="headerlink" title="什么是HBase"></a>什么是HBase</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase是一个开源的NoSQL数据库，参考google的BigTable建模，使用Java语言实现，运行于HDFS文件系统上，为Hadoop提供类似BigTable的服务，可以存储海量稀疏的数据，并具备一定的容错性、高可靠性及伸缩性。</span><br><span class="line"></span><br><span class="line">具备NoSQL数据库的特点：</span><br><span class="line">不支持SQL的跨行事务</span><br><span class="line">不满足完整性约束条件</span><br><span class="line">灵活的数据模型</span><br></pre></td></tr></table></figure><h3 id="HBase的发展历程"><a href="#HBase的发展历程" class="headerlink" title="HBase的发展历程"></a>HBase的发展历程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Apache HBase最初是Powerset公司为了处理自然语言搜索产生的海量数据而开展的项目</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171750807.png" alt="image-20230417175013383"></p><h3 id="HBase特性"><a href="#HBase特性" class="headerlink" title="HBase特性"></a>HBase特性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">容量巨大</span><br><span class="line">列存储</span><br><span class="line">稀疏性</span><br><span class="line">扩展性</span><br><span class="line">高可靠性</span><br></pre></td></tr></table></figure><h4 id="容量巨大"><a href="#容量巨大" class="headerlink" title="容量巨大"></a>容量巨大</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171755056.png" alt="image-20230417175551796"></p><h4 id="列存储"><a href="#列存储" class="headerlink" title="列存储"></a>列存储</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171758370.png" alt="image-20230417175839159"></p><h4 id="稀疏性"><a href="#稀疏性" class="headerlink" title="稀疏性"></a>稀疏性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。</span><br></pre></td></tr></table></figure><h4 id="扩展性"><a href="#扩展性" class="headerlink" title="扩展性"></a>扩展性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">纵向扩展：不断优化主服务器的性能，提高存储空间和性能</span><br><span class="line"></span><br><span class="line">横向扩展：不断向集群添加服务器来提供存储空间和性能</span><br><span class="line"></span><br><span class="line">HBase是横向扩展的，理论上无限横向扩展</span><br></pre></td></tr></table></figure><h4 id="高可靠性"><a href="#高可靠性" class="headerlink" title="高可靠性"></a>高可靠性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于HDFS的多副本机制</span><br><span class="line"></span><br><span class="line">WAL（Write-Ahead-Log）预写机制</span><br><span class="line"></span><br><span class="line">Replication机制</span><br></pre></td></tr></table></figure><h3 id="Hbase安装"><a href="#Hbase安装" class="headerlink" title="Hbase安装"></a>Hbase安装</h3><h4 id="单机模式"><a href="#单机模式" class="headerlink" title="单机模式"></a>单机模式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以本地文件系统作为基础，所有进程运行在一个JVM上，一般用于测试</span><br></pre></td></tr></table></figure><h4 id="伪分布式"><a href="#伪分布式" class="headerlink" title="伪分布式"></a>伪分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">主从模式，以hdfs文件系统为基础，所有进程运行在一个JVM中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">修改：hbase-env.sh，hbase-site.xml文件</span><br><span class="line">hbase-env.sh:配置java路径，配置zookeeper是否随hbase一起启动，还是先启动zookeeper，再hbase</span><br><span class="line"></span><br><span class="line">hbase-site.xml：主要是zookeeper,hadoop，是否集群部署的一些设置</span><br></pre></td></tr></table></figure><h4 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">主从模式，以hdfs文件系统为基础，守护进程运行在多个jvm上</span><br><span class="line"></span><br><span class="line">除了上述两个文件，还需设置集群有哪些节点文件的配置</span><br></pre></td></tr></table></figure><h2 id="02-HDFS原理"><a href="#02-HDFS原理" class="headerlink" title="02 HDFS原理"></a>02 HDFS原理</h2><h3 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS- 分布式文件系统"></a>HDFS- 分布式文件系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HDFS即Hadoop分布式文件系统（Hadoop Distributed File System）</span><br><span class="line">提供高可靠性和高吞吐量的文件存储服务</span><br><span class="line"></span><br><span class="line">通过软件设计来保证系统的可靠性</span><br><span class="line"></span><br><span class="line">具有容错性，高可靠性，高可扩展性，高吞吐率。</span><br></pre></td></tr></table></figure><h3 id="HDFS基本架构"><a href="#HDFS基本架构" class="headerlink" title="HDFS基本架构"></a>HDFS基本架构</h3><h3 id="HDFS-块"><a href="#HDFS-块" class="headerlink" title="HDFS- 块"></a>HDFS- 块</h3><h3 id="HDFS-NameNode"><a href="#HDFS-NameNode" class="headerlink" title="HDFS-NameNode"></a>HDFS-NameNode</h3><h3 id="HDFS-SecondaryNameNode"><a href="#HDFS-SecondaryNameNode" class="headerlink" title="HDFS-SecondaryNameNode"></a>HDFS-SecondaryNameNode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">定期的合并edits和fsimage文件</span><br><span class="line">Checkpiont：合并的时间点，默认3600秒，或editlog文件达到64M。</span><br></pre></td></tr></table></figure><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2%5C202304171810344.png" alt="image-20230417181053843"></p><h3 id="HDFS-DataNode"><a href="#HDFS-DataNode" class="headerlink" title="HDFS-DataNode"></a>HDFS-DataNode</h3><h3 id="HDFS读文件流程"><a href="#HDFS读文件流程" class="headerlink" title="HDFS读文件流程"></a>HDFS读文件流程</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171814207.png" alt="image-20230417181429835"></p><h4 id="HDFS读写机制-读文件机制"><a href="#HDFS读写机制-读文件机制" class="headerlink" title="HDFS读写机制-读文件机制"></a>HDFS读写机制-读文件机制</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171815977.png" alt="image-20230417181523597"></p><h4 id="HDFS读写机制-写文件机制"><a href="#HDFS读写机制-写文件机制" class="headerlink" title="HDFS读写机制-写文件机制"></a>HDFS读写机制-写文件机制</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171816393.png" alt="image-20230417181609459"></p><h3 id="HDFS副本机制"><a href="#HDFS副本机制" class="headerlink" title="HDFS副本机制"></a>HDFS副本机制</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171819478.png" alt="image-20230417181934890"></p><h3 id="HDFS容错"><a href="#HDFS容错" class="headerlink" title="HDFS容错"></a>HDFS容错</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171821864.png" alt="image-20230417182129386"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、NameNode出错：用Secondary NameNode备份的fsimage恢复</span><br><span class="line">2、DataNode出错：DataNode与NameNode通过“心跳”报告状态，当DataNode失效后，副本数减少，而NameNode会定期检查各节点的副本数量， 检查出问题后会启动数据冗余机制。</span><br><span class="line">3、数据出错：数据写入同时保存总和校验码，读取时校验。</span><br></pre></td></tr></table></figure><h2 id="03-Hbase组件和功能"><a href="#03-Hbase组件和功能" class="headerlink" title="03 Hbase组件和功能"></a>03 Hbase组件和功能</h2><h3 id="HBase-架构"><a href="#HBase-架构" class="headerlink" title="HBase 架构"></a>HBase 架构</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172115566.png" alt="image-20230417211518368"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172116455.png" alt="image-20230417211612969"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Client</span><br><span class="line">包含访问HBase的接口(用户通过client来访问hbase)并维护cache(region的位置)来加快对HBase的访问</span><br><span class="line"></span><br><span class="line">Zookeeper</span><br><span class="line">保证任何时候，集群中只有一个活跃master</span><br><span class="line">存贮所有Region的寻址入口。</span><br><span class="line">实时监控Region server的上线和下线信息。并实时通知Master</span><br><span class="line">存储HBase的schema和table元数据</span><br><span class="line"></span><br><span class="line">Master</span><br><span class="line">为Region server分配region</span><br><span class="line">负责Region server的负载均衡</span><br><span class="line">发现失效的Region server并重新分配其上的region</span><br><span class="line">管理用户对table的增删改操作</span><br><span class="line"></span><br><span class="line">RegionServer</span><br><span class="line">Region server维护region，处理对这些region的IO请求</span><br><span class="line">Region server负责切分在运行过程中变得过大的region</span><br><span class="line"></span><br><span class="line">Region</span><br><span class="line">HBase自动把表水平划分成多个区域(region)，每个region会保存一个表里面某段连续的数据</span><br><span class="line"></span><br><span class="line">每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变）</span><br><span class="line"></span><br><span class="line">当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上。</span><br><span class="line"></span><br><span class="line">Memstore与storefile</span><br><span class="line">一个region由多个store组成，一个store对应一个CF（列族）</span><br><span class="line"></span><br><span class="line">store包括位于内存中的memstore和位于磁盘的storefile写操作先写入memstore，当memstore中的数据达到某个阈值，hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile</span><br><span class="line"></span><br><span class="line">当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile</span><br><span class="line"></span><br><span class="line">当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡</span><br><span class="line"></span><br><span class="line">客户端检索数据，先在memstore找，找不到再找storefile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。</span><br><span class="line">HRegion由一个或者多个Store组成，每个store保存一个columns family。</span><br><span class="line">每个Strore又由一个memStore和0至多个StoreFile组成。如图：StoreFile以HFile格式保存在HDFS上。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172142998.png" alt="image-20230417214228696"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172144442.png" alt="image-20230417214420908"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-1</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-1.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-1.html</id>
    <published>2023-04-17T08:46:47.000Z</published>
    <updated>2023-04-19T10:07:34.371Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><h2 id="数据库基本知识"><a href="#数据库基本知识" class="headerlink" title="数据库基本知识"></a>数据库基本知识</h2><h3 id="什么是数据库？"><a href="#什么是数据库？" class="headerlink" title="什么是数据库？"></a>什么是数据库？</h3><h3 id="什么是数据模型？"><a href="#什么是数据模型？" class="headerlink" title="什么是数据模型？"></a>什么是数据模型？</h3><h4 id="有哪些数据模型？"><a href="#有哪些数据模型？" class="headerlink" title="有哪些数据模型？"></a>有哪些数据模型？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库技术发展至今，传统数据库根据不同的数据模型，主要有以下几种：层次型、网状型和关系型。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171657536.png" alt="image-20230417165719863"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171700245.png" alt="image-20230417170008924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">关系模型要点回顾   </span><br><span class="line">1. 数据结构：</span><br><span class="line">现实世界的实体以及实体之间的各种联系均用关系来表示</span><br><span class="line">数据逻辑结构：二维表</span><br><span class="line">   </span><br><span class="line">   2. 完整性约束条件</span><br><span class="line">域完整性，实体完整性，参照完整性</span><br><span class="line"></span><br><span class="line">    3. 关系操作</span><br><span class="line">选择，投影，连接 等等关系运算；操作对象和结果都是集合</span><br></pre></td></tr></table></figure><h3 id="关系型数据库的优点"><a href="#关系型数据库的优点" class="headerlink" title="关系型数据库的优点"></a>关系型数据库的优点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库的特点</span><br><span class="line">（1）容易理解：用二维表表示</span><br><span class="line">（2）使用方便：通用的SQL语言。</span><br><span class="line">（3）易于维护：丰富的完整性约束大大减低了数据冗余和数据不一致的可能性。</span><br></pre></td></tr></table></figure><h3 id="关系型数据库的不足"><a href="#关系型数据库的不足" class="headerlink" title="关系型数据库的不足"></a>关系型数据库的不足</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对海量数据的读写效率低</span><br><span class="line">    表中有大量数据时，数据的读写速率非常的缓慢</span><br><span class="line">无法适应多变的数据结构</span><br><span class="line">    现代网络中存在大量的半结构化、非结构化数据，针对结构化数据而设计的关系型数据库系统来说，对这些不断变化的数据结构，很难进行高效的处理。</span><br><span class="line">高并发读写的瓶颈</span><br><span class="line">     当数据量达到一定规模时由于关系型数据库的系统逻辑非常复杂，使得在并发处理时非常容易发生死锁，导致其读写速度下滑严重。</span><br><span class="line">可扩展性的限制</span><br><span class="line">由于关系型数据库存在类似的join操作，使得数据库在扩展方面很困难。</span><br></pre></td></tr></table></figure><h2 id="NOSQL数据库理论基础"><a href="#NOSQL数据库理论基础" class="headerlink" title="NOSQL数据库理论基础"></a>NOSQL数据库理论基础</h2><h3 id="什么是NoSQL"><a href="#什么是NoSQL" class="headerlink" title="什么是NoSQL"></a>什么是NoSQL</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171707315.png" alt="image-20230417170659240"></p><h3 id="分布式数据库的特征"><a href="#分布式数据库的特征" class="headerlink" title="分布式数据库的特征"></a>分布式数据库的特征</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">分布式数据库必须具有如下特征，才能应对不断增长的海量数据。</span><br><span class="line">● 高可扩展性：分布式数据库必须具有高可扩展性，能够动态地增添存储节点以实现存储容量的线性扩展</span><br><span class="line">● 高并发性：分布式数据库必须及时响应大规模用户的读&#x2F;写请求，能对海量数据进行随机读写</span><br><span class="line">● 高可用性：分布式数据库必须提供容错机制，能够实现对数据的冗余备份，保证数据和服务的高度可靠性</span><br></pre></td></tr></table></figure><h3 id="NoSQL的特点"><a href="#NoSQL的特点" class="headerlink" title="NoSQL的特点"></a>NoSQL的特点</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171713669.png" alt="image-20230417171305105"></p><h3 id="分布式数据库的数据管理"><a href="#分布式数据库的数据管理" class="headerlink" title="分布式数据库的数据管理"></a>分布式数据库的数据管理</h3><h4 id="什么是数据库系统？"><a href="#什么是数据库系统？" class="headerlink" title="什么是数据库系统？"></a>什么是数据库系统？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库系统  &#x3D;  数据库管理系统     +     数据库</span><br></pre></td></tr></table></figure><h4 id="什么是数据库管理系统？"><a href="#什么是数据库管理系统？" class="headerlink" title="什么是数据库管理系统？"></a>什么是数据库管理系统？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库管理系统(Database Management System)是一种操纵和管理数据库的大型软件，用于建立、使用和维护数据库，简称DBMS。主要任务就是对外提供数据，对内要管理数据。</span><br></pre></td></tr></table></figure><h4 id="数据处理方式：集中式VS分布式"><a href="#数据处理方式：集中式VS分布式" class="headerlink" title="数据处理方式：集中式VS分布式"></a>数据处理方式：集中式VS分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">集中式数据库是指数据库中的数据集中存储在一台计算机上，数据的处理也集中在一台机器上完成。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分布式数据库是指利用高速计算机网络将物理上分散的多个数据存储单元连接起来组成一个逻辑上统一的数据库。</span><br></pre></td></tr></table></figure><h4 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">C:一致性（consistency）（强一致性）</span><br><span class="line">它是指任何一个读操作总是能够读到之前完成的写操作的结果。所有节点在同一时间具有相同的数据。</span><br><span class="line"></span><br><span class="line">A:可用性（Availability）（高可用性）</span><br><span class="line">每个请求都能在确定时间内返回一个响应，无论请求是成功或失败。</span><br><span class="line"></span><br><span class="line">P:分区容忍性（Partition Tolerance）</span><br><span class="line">它是指在一个集群，即系统中的一部分节点无法和其他节点进行通信，系统也能正常运行。也就是说，系统中部分信息的丢失或失败不会影响系统的继续运作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">当处理CAP的问题时，可以有几个明显的选择：</span><br><span class="line">CA：也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最简单的做法是把所有与事务相关的内容都放到同一台机器上。</span><br><span class="line">CP：也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务</span><br><span class="line">AP：也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允许系统返回不一致的数据</span><br></pre></td></tr></table></figure><h5 id="设计原则：在C、A、P之中取舍"><a href="#设计原则：在C、A、P之中取舍" class="headerlink" title="设计原则：在C、A、P之中取舍"></a>设计原则：在C、A、P之中取舍</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171721678.png" alt="image-20230417172155421"></p><h2 id="ACID、BASE与一致性"><a href="#ACID、BASE与一致性" class="headerlink" title="ACID、BASE与一致性"></a>ACID、BASE与一致性</h2><h3 id="ACID与BASE"><a href="#ACID与BASE" class="headerlink" title="ACID与BASE"></a>ACID与BASE</h3><h4 id="为什么会出现ACID、BASE-？"><a href="#为什么会出现ACID、BASE-？" class="headerlink" title="为什么会出现ACID、BASE ？"></a>为什么会出现ACID、BASE ？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CAP理论定义了分布式存储的根本问题，但并没有指出一致性和可用性之间到底应该如何权衡。于是出现了ACID、BASE ，给出了权衡A与C的一种可行方案。</span><br><span class="line">ACID和BASE代表了在一致性-可用性两点之间进行选择的设计哲学</span><br><span class="line">ACID强调一致性被关系数据库使用，BASE强调可用性被大多数Nosql使用</span><br></pre></td></tr></table></figure><h4 id="ACID是什么？"><a href="#ACID是什么？" class="headerlink" title="ACID是什么？"></a>ACID是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">指数据库事务正确执行的四个基本要素的缩写。包含：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">原子性：一个事务的所有系列操作步骤被看成是一个动作，所有的步骤要么全部完成要么都不会完成。</span><br><span class="line"></span><br><span class="line">一致性：事务执行前后，数据库的状态都满足所有的完整性约束。不能发生表与表之间存在外键约束，但是有数据却违背这种约束性。</span><br><span class="line"></span><br><span class="line">隔离性：并发执行的事务是隔离的，保证多个事务互不影响，隔离能够确保并发执行的事务能够顺序一个接一个执行，通过隔离，一个未完成事务不会影响另外一个未完成事务。</span><br><span class="line"></span><br><span class="line">持久性：一个事务一旦提交，它对数据库中数据的改变就应该是永久性的，不会因为和其他操作冲突而取消这个事务。</span><br></pre></td></tr></table></figure><h4 id="BASE原则又是什么？"><a href="#BASE原则又是什么？" class="headerlink" title="BASE原则又是什么？"></a>BASE原则又是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BASE原则 &#x3D; 基本可用性（Basically Available）+软状态（Soft state）+最终一致性（Eventuallyconsistent）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基本可用性：分布式系统在出现故障的时候，允许损失部分可用性，即保证核心功能或者当前最重要功能可用，但是其他功能会被削弱。</span><br><span class="line"></span><br><span class="line">软状态：允许系统数据存在中间状态，但不会影响到系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步时存在延时。</span><br><span class="line"></span><br><span class="line">最终一致性：要求系统数据副本最终能够一致，而不需要实时保证数据副本一致。最终一致性是弱一致性的一种特殊情况。</span><br></pre></td></tr></table></figure><h2 id="NoSQL数据库分类"><a href="#NoSQL数据库分类" class="headerlink" title="NoSQL数据库分类"></a>NoSQL数据库分类</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171731393.png" alt="image-20230417173132275"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-5</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-5.html</id>
    <published>2023-04-15T17:08:43.000Z</published>
    <updated>2023-04-17T07:37:40.431Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-5"><a href="#第十四周-消息队列之Kafka从入门到小牛-5" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-5"></a>第十四周 消息队列之Kafka从入门到小牛-5</h1><h2 id="实战：Flume集成Kafka"><a href="#实战：Flume集成Kafka" class="headerlink" title="实战：Flume集成Kafka"></a>实战：Flume集成Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在实际工作中flume和kafka会深度结合使用</span><br><span class="line">1：flume采集数据，将数据实时写入kafka</span><br><span class="line">2：flume从kafka中消费数据，保存到hdfs，做数据备份</span><br><span class="line"></span><br><span class="line">下面我们就来看一个综合案例</span><br><span class="line">使用flume采集日志文件中产生的实时数据，写入到kafka中，然后再使用flume从kafka中将数据消费出来，保存到hdfs上面</span><br><span class="line">那为什么不直接使用flume将采集到的日志数据保存到hdfs上面呢？</span><br><span class="line">因为中间使用kafka进行缓冲之后，后面既可以实现实时计算，又可以实现离线数据备份，最终实现离线计算，所以这一份数据就可以实现两种需求，使用起来很方便，所以在工作中一般都会这样做。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171406839.png" alt="image-20230417140517700"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">下面我们来实现一下这个功能</span><br><span class="line">其实在Flume中，针对Kafka提供的有KafkaSource和KafkaSink</span><br><span class="line">KafkaSource是从kafka中读取数据</span><br><span class="line">KafkaSink是向kafka中写入数据</span><br><span class="line"></span><br><span class="line">所以针对我们目前这个架构，主要就是配置Flume的Agent。</span><br><span class="line">需要配置两个Agent：</span><br><span class="line">第一个Agent负责实时采集日志文件，将采集到的数据写入Kafka中</span><br><span class="line">第二个Agent负责从Kafka中读取数据，将数据写入HDFS中进行备份(落盘)</span><br><span class="line">针对第一个Agent：</span><br><span class="line">source：ExecSource，使用tail -F监控日志文件即可</span><br><span class="line">channel：MemoryChannel</span><br><span class="line">sink：KafkaSink</span><br><span class="line"></span><br><span class="line">针对第二个Agent</span><br><span class="line">Source：KafkaSource</span><br><span class="line">channel：MemoryChannel</span><br><span class="line">sink：HdfsSink</span><br><span class="line"></span><br><span class="line">这里面这些组件其实只有KafkaSource和KafkaSink我们没有使用过，其它的组件都已经用过了。</span><br></pre></td></tr></table></figure><h3 id="配置Agent"><a href="#配置Agent" class="headerlink" title="配置Agent"></a>配置Agent</h3><h4 id="file-to-kafka-conf"><a href="#file-to-kafka-conf" class="headerlink" title="file-to-kafka.conf"></a>file-to-kafka.conf</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来配置第一个Agent：</span><br><span class="line">文件名为： file-to-kafka.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;test.log</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"></span><br><span class="line"># 指定topic名称</span><br><span class="line">a1.sinks.k1.kafka.topic &#x3D; test_r2p5</span><br><span class="line"># 指定kafka地址，多个节点地址使用逗号分割</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03</span><br><span class="line"># 一次向kafka中写多少条数据，默认值为100，在这里为了演示方便，改为1</span><br><span class="line"># 在实际工作中这个值具体设置多少需要在传输效率和数据延迟上进行取舍</span><br><span class="line"># 如果kafka后面的实时计算程序对数据的要求是低延迟，那么这个值小一点比较好</span><br><span class="line"># 如果kafka后面的实时计算程序对数据延迟没什么要求，那么就考虑传输性能，一次多传输一些</span><br><span class="line"># 建议这个值的大小和ExecSource每秒钟采集的数据量大致相等，这样不会频繁向kafka中写数</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize &#x3D; 1</span><br><span class="line">a1.sinks.k1.kafka.producer.acks &#x3D; 1</span><br><span class="line"># 一个Batch被创建之后，最多过多久，不管这个Batch有没有写满，都必须发送出去</span><br><span class="line"># linger.ms和flumeBatchSize(不积到设置的条数，则一直不写入到topic)，哪个先满足先按哪个规则执行，这个值默认是0，在这设置为1</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms &#x3D; 1</span><br><span class="line"># 指定数据传输时的压缩格式，对数据进行压缩，提高传输效率</span><br><span class="line">a1.sinks.k1.kafka.producer.compression.type &#x3D; snappy</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka的producer的相关参数，可以直接在这里设置：a1.sinks.k1.kafka.producer.+。。。</span><br></pre></td></tr></table></figure><h4 id="kafka-to-hdfs-conf"><a href="#kafka-to-hdfs-conf" class="headerlink" title="kafka-to-hdfs.conf"></a>kafka-to-hdfs.conf</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来配置第二个Agent：</span><br><span class="line">文件名为： kafka-to-hdfs.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 一次性向channel中写入的最大数据量，在这为了演示方便，设置为1</span><br><span class="line"># 这个参数的值不要大于MemoryChannel中transactionCapacity的值</span><br><span class="line">a1.sources.r1.batchSize &#x3D; 1</span><br><span class="line"># 最大多长时间向channel写一次数据</span><br><span class="line">a1.sources.r1.batchDurationMillis &#x3D; 2000</span><br><span class="line"># kafka地址</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata</span><br><span class="line"># topic名称，可以指定一个或者多个，多个topic之间使用逗号隔开</span><br><span class="line"># 也可以使用正则表达式指定一个topic名称规则</span><br><span class="line">a1.sources.r1.kafka.topics &#x3D; test_r2p5</span><br><span class="line"># 指定消费者组id</span><br><span class="line">a1.sources.r1.kafka.consumer.group.id &#x3D; flume-con1</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;kafkaout</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data-</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04机器的flume目录下复制两个目录</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cp -r conf conf-file-to-kafka</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cp -r conf conf-kafka-to-hdfs</span><br><span class="line"></span><br><span class="line">修改 conf_file_to_kafka和conf_kafka_to_hdfs中log4j的配置</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-file-to-kafka</span><br><span class="line">[root@bigdata04 conf_file_to_kafka]# vi log4j.properties </span><br><span class="line">flume.root.logger&#x3D;ERROR,LOGFILE</span><br><span class="line">flume.log.file&#x3D;flume-file-to-kafka.log</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-kafka-to-hdfs</span><br><span class="line">[root@bigdata04 conf_kafka_to_hdfs]# vi log4j.properties </span><br><span class="line">flume.root.logger&#x3D;ERROR,LOGFILE</span><br><span class="line">flume.log.file&#x3D;flume-kafka-to-hdfs.log</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">把刚才配置的两个Agent的配置文件复制到这两个目录下</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-file-to-kafka</span><br><span class="line">[root@bigdata04 conf-file-to-kafka]# vi file-to-kafka.conf</span><br><span class="line">.....把file-to-kafka.conf文件中的内容复制进来即可</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-kafka-to-hdfs&#x2F;</span><br><span class="line">[root@bigdata04 conf-kafka-to-hdfs]# vi kafka-to-hdfs.conf</span><br><span class="line">.....把kafka-to-hdfs.conf文件中的内容复制进来即可</span><br></pre></td></tr></table></figure><h3 id="启动Flume-Agent"><a href="#启动Flume-Agent" class="headerlink" title="启动Flume Agent"></a>启动Flume Agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">启动这两个Flume Agent</span><br><span class="line">确保zookeeper集群、kafka集群和Hadoop集群是正常运行的</span><br><span class="line">以及Kafka中的topic需要提前创建好</span><br><span class="line"></span><br><span class="line">创建topic</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 -partions 5 --replication-factor 2 --topic test_r2p5</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">先启动第二个Agent，再启动第一个Agent</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf-kafka-to-hdfs --conf-file conf-kafka-to-hdfs&#x2F;kafka-to-hdfs.conf</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf-file-to-kafka --conf-file conf-file-to-kafka&#x2F;file-to-kafka.conf</span><br><span class="line"></span><br><span class="line">模拟产生日志数据</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;log&#x2F;</span><br><span class="line">[root@bigdata04 log]# echo hello world &gt;&gt; &#x2F;data&#x2F;log&#x2F;test.log</span><br><span class="line"></span><br><span class="line">到HDFS上查看数据，验证结果：</span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -ls &#x2F;kafkaout</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r-- 2 root supergroup 12 2020-06-09 22:59 &#x2F;kafkaout&#x2F;data-.15</span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -cat &#x2F;kafkaout&#x2F;data-.1591714755267.tmp</span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line">此时Flume可以通过tail -F命令实时监控文件中的新增数据，发现有新数据就写入kafka，然后kafka后面的flume落盘程序，以及kafka后面的实时计算程序就可以使用这份数据了。</span><br></pre></td></tr></table></figure><h2 id="实战：Kafka集群平滑升级"><a href="#实战：Kafka集群平滑升级" class="headerlink" title="实战：Kafka集群平滑升级"></a>实战：Kafka集群平滑升级</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">之前我们在使用 Kafka 0.9.0.0版本的时候，遇到一个比较诡异的问题</span><br><span class="line">（背景：这个版本他们遇到一个问题，官方通过升级kafka版本解决了，但他们之前的版本工作中运用于直播平台，所以不可能将集群停了重新部署一套）</span><br><span class="line">针对消费者组增加消费者的时候可能会导致rebalance，进而导致部分consumer不能再消费分区数据</span><br><span class="line">意思就是之前针对这个topic的5个分区只有2个消费者消费数据，后期我动态的把消费者调整为了5个，这样可能会导致部分消费者无法消费分区中的数据。</span><br><span class="line"></span><br><span class="line">针对这个bug这里有一份详细描述：</span><br><span class="line">https:&#x2F;&#x2F;issues.apache.org&#x2F;jira&#x2F;browse&#x2F;KAFKA-2978</span><br><span class="line">此bug官方在0.9.0.1版本中进行了修复</span><br><span class="line">当时我们的线上集群使用的就是0.9.0.0的版本。</span><br><span class="line"></span><br><span class="line">所以我们需要对线上集群在不影响线上业务的情况下进行升级，称为平滑升级(滚动升级)，也就是升级的时候不影响线上的正常业务运行(但还是要选择在业务低峰期时进行升级)。</span><br><span class="line"></span><br><span class="line">接下来我们就查看了官网文档(0.9.0.0)，上面有针对集群平滑升级的一些信息</span><br><span class="line">http:&#x2F;&#x2F;kafka.apache.org&#x2F;090&#x2F;documentation.html#upgrade</span><br><span class="line">在验证这个升级流程的时候我们是在测试环境下，先模拟线上的集群环境，进行充分测试，可千万不能简单测试一下就直接搞到测试环境去做，这样是很危险的。</span><br><span class="line">由于当时这个kafka集群我们还没有移交给运维负责，并且运维当时对这个框架也不是很熟悉，所以才由我们开发人员来进行平滑升级，否则这种框架升级的事情肯定是交给运维去做的。</span><br><span class="line"></span><br><span class="line">那接下来看一下具体的平滑升级步骤</span><br><span class="line">小版本之间集群升级不需要额外修改集群的配置文件。只需要按照下面步骤去执行即可。</span><br><span class="line">假设kafka0.9.0.0集群在三台服务器上，需要把这三台服务器上的kafka集群升级到0.9.0.1版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：提前在集群的三台机器上把0.9.0.1的安装包，解压、配置好。</span><br><span class="line">主要是log.dirs这个参数，0.9.0.1中的这个参数和0.9.0.0的这个参数一定要保持一致，这样新版本的kafka才可以识别之前的kakfa中的数据。</span><br><span class="line">在集群升级的过程当中建议通过CMAK(kafkamanager)查看集群的状态信息，比较方便</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171529822.png" alt="image-20230417152957587"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1：先stop掉0.9.0.0集群中的第一个节点，然后去CMAK上查看集群的broker信息，确认节点确实已停掉。并且再查看一下，节点的副本下线状态。确认集群是否识别到副本下线状态。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171531285.png" alt="image-20230417153133889"></p><p> <img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171533475.png" alt="image-20230417153328093"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后在当前节点把kafka0.9.0.1启动起来。再回到CMAK中查看broker信息，确认刚启动的节点是否已正确显示，并且还要确认这个节点是否可以正常接收和发送数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171534542.png" alt="image-20230417153455185"></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2：按照第一步的流程去依次操作剩余节点即可，就是先把0.9.0.0版本的kafka停掉，再把0.9.0.1版本的kafka启动即可。</span><br><span class="line"></span><br><span class="line">注意：每操作一个节点，需要稍等一下，确认这个节点可以正常接收和发送数据之后，再处理下一个节点。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.html</id>
    <published>2023-04-15T16:08:29.000Z</published>
    <updated>2023-04-19T16:39:50.981Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="Kafka技巧篇"><a href="#Kafka技巧篇" class="headerlink" title="Kafka技巧篇"></a>Kafka技巧篇</h1><h2 id="Kafka集群参数调忧"><a href="#Kafka集群参数调忧" class="headerlink" title="Kafka集群参数调忧"></a>Kafka集群参数调忧</h2><h3 id="JVM参数调忧"><a href="#JVM参数调忧" class="headerlink" title="JVM参数调忧"></a>JVM参数调忧</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">默认启动的Broker进程只会使用1G内存，在实际使用中会导致进程频繁GC，影响Kafka集群的性能和稳</span><br><span class="line">定性</span><br><span class="line">通过 jstat -gcutil &lt;pid&gt; 1000 查看到kafka进程GC情况</span><br><span class="line">主要看 YGC,YGCT,FGC,FGCT 这几个参数，如果这几个值不是很大，就没什么问题</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">YGC：young gc发生的次数</span><br><span class="line">YGCT：young gc消耗的时间</span><br><span class="line">FGC：full gc发生的次数</span><br><span class="line">FGCT：full gc消耗的时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">13248 Kafka</span><br><span class="line">18087 Jps</span><br><span class="line">1679 QuorumPeerMain</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jstat -gcutil 13248 1000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162304926.png" alt="image-20230416230418172"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果你发现YGC很频繁，或者FGC很频繁，就说明内存分配的少了</span><br><span class="line">此时需要修改kafka-server-start.sh中的KAFKA_HEAP_OPTS</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export KAFKA_HEAP_OPTS&#x3D;&quot;-Xmx10g -Xms10g -XX:MetaspaceSize&#x3D;96m -XX:+UseG1GC -XX</span><br><span class="line"></span><br><span class="line">xms:初始化内存</span><br><span class="line">xmx:最大内存</span><br><span class="line">建议设置成一样大，否则可能进行内存交换</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个配置表示给kafka分配了10G内存</span><br></pre></td></tr></table></figure><h3 id="Replication参数调忧"><a href="#Replication参数调忧" class="headerlink" title="Replication参数调忧"></a>Replication参数调忧</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">replica.socket.timeout.ms&#x3D;60000</span><br><span class="line">这个参数的默认值是30秒，它是控制partiton副本之间socket通信的超时时间，如果设置的太小，有可能会由于网络原因导致造成误判，认为某一个partition副本连不上了。</span><br><span class="line"></span><br><span class="line">replica.lag.time.max.ms&#x3D;50000</span><br><span class="line">如果一个副本在指定的时间内没有向leader节点发送任何请求，或者在指定的时间内没有同步完leader中的数据，则leader会将这个节点从Isr列表中移除。</span><br><span class="line"></span><br><span class="line">这个参数的值默认为10秒</span><br><span class="line">如果网络不好，或者kafka压力较大，建议调大该值，否则可能会频繁出现副本丢失，进而导致集群需要频繁复制副本，导致集群压力更大，会陷入一个恶性循环</span><br></pre></td></tr></table></figure><h3 id="Log参数调优"><a href="#Log参数调优" class="headerlink" title="Log参数调优"></a>Log参数调优</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这块是针对Kafka中数据文件的删除时机进行设置，不是对kafka本身的日志参数配置</span><br><span class="line">log.retention.hours&#x3D;24</span><br><span class="line">这个参数默认值为168，单位是小时，就是7天，默认对数据保存7天，可以在这调整数据保存的时间，我们在实际工作中改为了只保存1天，因为kafka中的数据我们会在hdfs中进行备份，保存一份，所以就没有必要在kafka中保留太长时间了。</span><br><span class="line"></span><br><span class="line">在kafka中保留只是为了能够让你在指定的时间内恢复数据，或者重新消费数据，如果没有这种需求，那就没有必要设置太长时间。</span><br><span class="line"></span><br><span class="line">这里分析的Replication的参数和Log参数都是在server.properties文件中进行配置</span><br><span class="line"></span><br><span class="line">JVM参数是在kafka-server-start.sh脚本中配置</span><br><span class="line"></span><br><span class="line">broker参数调优更多在开发文档里有</span><br></pre></td></tr></table></figure><h2 id="Kafka-Topic命名小技巧"><a href="#Kafka-Topic命名小技巧" class="headerlink" title="Kafka Topic命名小技巧"></a>Kafka Topic命名小技巧</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">针对Kafka中Topic命名的小技巧</span><br><span class="line">建议在给topic命名的时候在后面跟上r2p10之类的内容</span><br><span class="line">r2：表示Partition的副本因子是2</span><br><span class="line">p10：表示这个Topic的分区数是10</span><br><span class="line"></span><br><span class="line">这样的好处是后期我们如果要写消费者消费指定topic的数据，通过topic的名称我们就知道应该设置多少个消费者消费数据效率最高。</span><br><span class="line">因为一个partition同时只能被一个消费者消费，所以效率最高的情况就是消费者的数量和topic的分区数量保持一致。在这里通过topic的名称就可以直接看到，一目了然。</span><br><span class="line"></span><br><span class="line">但是也有一个缺点，就是后期如果我们动态调整了topic的partiton，那么这个topic名称上的partition数量就不准了，针对这个topic，建议大家一开始的时候就提前预估一下，可以多设置一些partition，我们</span><br><span class="line">在工作中的时候针对一些数据量比较大的topic一般会设置40-50个partition，数据量少的topic一般设置5-10个partition，这样后期调整topic partiton数量的场景就比较少了。</span><br></pre></td></tr></table></figure><h2 id="Kafka集群监控管理工具"><a href="#Kafka集群监控管理工具" class="headerlink" title="Kafka集群监控管理工具"></a>Kafka集群监控管理工具</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">现在我们操作Kafka都是在命令行界面中通过脚本操作的，后面需要传很多参数，用起来还是比较麻烦的，那kafka没有提供web界面的支持吗？</span><br><span class="line">很遗憾的告诉你，Apache官方并没有提供，不过好消息是有一个由雅虎开源的一个工具，目前用起来还是不错的。</span><br><span class="line"></span><br><span class="line">它之前的名字叫KafkaManager，后来改名字了，叫CMAK</span><br><span class="line">CMAK是目前最受欢迎的Kafka集群管理工具，最早由雅虎开源，用户可以在Web界面上操作Kafka集群</span><br><span class="line">可以轻松检查集群状态(Topic、Consumer、Offset、Brokers、Replica、Partition)</span><br><span class="line"></span><br><span class="line">那下面我们先去下载这个CMAK</span><br><span class="line">需要到github上面去下载</span><br><span class="line">在github里面搜索CMAK即可</span><br></pre></td></tr></table></figure><h3 id="下载CMAK"><a href="#下载CMAK" class="headerlink" title="下载CMAK"></a>下载CMAK</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162330282.png" alt="image-20230416233039880"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162330225.png" alt="image-20230416233050053"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162331098.png" alt="image-20230416233123771"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162331745.png" alt="image-20230416233141274"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：由于cmak-3.0.0.4.zip版本是在java11这个版本下编译的，所以在运行的时候也需要使用java11这个版本，我们目前服务器上使用的是java8这个版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">我们为什么不使用java11版本呢？因为自2019年1月1日1起，java8之后的更新版本在商业用途的时候就需要收费授权了。</span><br><span class="line">在这针对cmak-3.0.0.4这个版本，如果我们想要使用的话有两种解决办法</span><br><span class="line">1：下载cmak的源码，使用jdk8编译</span><br><span class="line">2：额外安装一个jdk11(自己用不属于商业用途，现实公司很少有用java8以后的)</span><br><span class="line">如果想要编译的话需要安装sbt这个工具对源码进行编译，sbt是Scala 的构建工具, 类似于Maven。</span><br><span class="line"></span><br><span class="line">由于我们在这使用不属于商业用途，所以使用jdk11是没有问题的，那就不用重新编译了。</span><br><span class="line">下载jdk11，jdk-11.0.7_linux-x64_bin.tar.gz</span><br><span class="line">将jdk11的安装包上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">只需要解压即可，不需要配置环境变量，因为只有cmak这个工具才需要使用jdk11</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# tar -zxvf jdk-11.0.7_linux-x64_bin.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来把 cmak-3.0.0.4.zip 上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">1：解压</span><br><span class="line">[root@bigdata01 soft]# unzip cmak-3.0.0.4.zip </span><br><span class="line">-bash: unzip: command not found</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：如果提示-bash: unzip: command not found，则说明目前不支持unzip命令，可以使用yum在线安装</span><br><span class="line">建议先清空一下yum缓存，否则使用yum可能无法安装unzip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# yum clean all </span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Cleaning repos: base extras updates</span><br><span class="line">Cleaning up list of fastest mirrors</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# yum install -y unzip</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">.....</span><br><span class="line">Running transaction</span><br><span class="line"> Installing : unzip-6.0-21.el7.x86_64 1&#x2F;1 </span><br><span class="line"> Verifying : unzip-6.0-21.el7.x86_64 1&#x2F;1 </span><br><span class="line">Installed:</span><br><span class="line"> unzip.x86_64 0:6.0-21.el7 </span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">再重新解压</span><br><span class="line">[root@bigdata01 soft]# unzip cmak-3.0.0.4.zip</span><br></pre></td></tr></table></figure><h3 id="配置CMAK"><a href="#配置CMAK" class="headerlink" title="配置CMAK"></a>配置CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2：修改CMAK配置</span><br><span class="line">首先修改bin目录下的cmak脚本</span><br><span class="line">在里面配置JAVA_HOME指向jdk11的安装目录，否则默认会使用jdk8</span><br><span class="line">[root@bigdata01 soft]# cd cmak-3.0.0.4</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# cd bin&#x2F;</span><br><span class="line">[root@bigdata01 bin]# vi cmak</span><br><span class="line">....</span><br><span class="line">JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk-11.0.7</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">然后修改conf目录下的application.conf文件</span><br><span class="line">只需要在里面增加一行cmak.zkhosts参数的配置即可，指定zookeeper的地址</span><br><span class="line"></span><br><span class="line">注意：在这里指定zookeeper地址主要是为了让CMAK在里面保存数据，这个zookeeper地址不一定是kafka集群使用的那个zookeeper集群，随便哪个zookeeper集群都可以。(cmak需要报错它自己的东西)</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# cd conf&#x2F;</span><br><span class="line">[root@bigdata01 conf]# vi application.conf </span><br><span class="line">....</span><br><span class="line">cmak.zkhosts&#x3D;&quot;bigdata01:2181,bigdata02:2181,bigdata03:2181&quot;</span><br><span class="line">....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3：修改kafka启动配置</span><br><span class="line">想要在CMAK中查看kafka的一些指标信息，在启动kafka的时候需要指定JMX_PORT</span><br><span class="line"></span><br><span class="line">停止kafka集群</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh </span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh </span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">重新启动kafka集群，指定JXM_PORT</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br></pre></td></tr></table></figure><h3 id="启动CMAK"><a href="#启动CMAK" class="headerlink" title="启动CMAK"></a>启动CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">4：启动cmak</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# bin&#x2F;cmak -Dconfig.file&#x3D;conf&#x2F;application.conf -</span><br><span class="line"></span><br><span class="line">如果想把cmak放在后台执行的话需要添加上nohup和&amp;</span><br><span class="line">1 [root@bigdata01 cmak-3.0.0.4]# nohup bin&#x2F;cmak -Dconfig.file&#x3D;conf&#x2F;application.conf -Dhttp.port&#x3D;9001 &amp;</span><br><span class="line"></span><br><span class="line">cmak默认监听端口9000，但这样和hdfs的端口重复了</span><br></pre></td></tr></table></figure><h3 id="访问CMAK"><a href="#访问CMAK" class="headerlink" title="访问CMAK"></a>访问CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5：访问cmak</span><br><span class="line">http:&#x2F;&#x2F;bigdata01:9001&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162356804.png" alt="image-20230416235630637"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6：操作CMAK</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4%5C202304162355045.png" alt="image-20230416235520700"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162355502.png" alt="image-20230416235548395"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这几个参数配置好了以后还需要配置以下几个线程池相关的参数，这几个参数默认值是1，在保存的时候会提示需要大于1，所以可以都改为10</span><br><span class="line">最后点击Save按钮保存即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brokerViewThreadPoolSize：10</span><br><span class="line">offsetCacheThreadPoolSize：10</span><br><span class="line">kafkaAdminClientThreadPoolSize：10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后进来是这样的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170003610.png" alt="image-20230417000304032"></p><h4 id="查看kafak集群的所有broker信息"><a href="#查看kafak集群的所有broker信息" class="headerlink" title="查看kafak集群的所有broker信息"></a>查看kafak集群的所有broker信息</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170003829.png" alt="image-20230417000358325"></p><h4 id="查看kafak集群的所有topic信息"><a href="#查看kafak集群的所有topic信息" class="headerlink" title="查看kafak集群的所有topic信息"></a>查看kafak集群的所有topic信息</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170008382.png" alt="image-20230417000847024"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170007471.png" alt="image-20230417000718104"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170007023.png" alt="image-20230417000734624"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击topic的消费者信息是可以进来查看的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170016794.png" alt="image-20230417001617419"></p><h4 id="创建一个topic"><a href="#创建一个topic" class="headerlink" title="创建一个topic"></a>创建一个topic</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170016835.png" alt="image-20230417001657421"></p><h4 id="给topic增加分区"><a href="#给topic增加分区" class="headerlink" title="给topic增加分区"></a>给topic增加分区</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170017969.png" alt="image-20230417001716328"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是CMAK中常见的功能，当然了这里面还要一些我们没有说到的功能就留给大家以后来发掘了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.html</id>
    <published>2023-04-15T14:14:17.000Z</published>
    <updated>2023-04-19T15:35:18.244Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-3"><a href="#第十四周-消息队列之Kafka从入门到小牛-3" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-3"></a>第十四周 消息队列之Kafka从入门到小牛-3</h1><h2 id="Kafka核心之存储和容错机制"><a href="#Kafka核心之存储和容错机制" class="headerlink" title="Kafka核心之存储和容错机制"></a>Kafka核心之存储和容错机制</h2><h3 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在kafka中每个topic包含1到多个partition，每个partition存储一部分Message。每条Message包含三个属性，其中有一个是offset。</span><br><span class="line"></span><br><span class="line">问题来了：offset相当于partition中这个message的唯一id，那么如何通过id高效的找到message？</span><br><span class="line">两大法宝：分段+索引(分段表示一个partition会存储多个文件)</span><br><span class="line"></span><br><span class="line">kafak中数据的存储方式是这样的：</span><br><span class="line">1、每个partition由多个segment【片段】组成，每个segment文件中存储多条消息，</span><br><span class="line">2、每个partition在内存中对应一个index，记录每个segment文件中的第一条消息偏移量。</span><br><span class="line"></span><br><span class="line">Kafka中数据的存储流程是这样的：</span><br><span class="line">生产者生产的消息会被发送到topic的多个partition上，topic收到消息后往对应partition的最后一个segment上添加该消息，segment达到一定的大小后会创建新的segment。</span><br><span class="line">来看这个图，可以认为是针对topic中某个partition的描述</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160035193.png" alt="image-20230416003228787"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">图中左侧就是索引，右边是segment文件，左边的索引里面会存储每一个segment文件中第一条消息的偏移量，由于消息的偏移量都是递增的，这样后期查找起来就方便了，先到索引中判断数据在哪个</span><br><span class="line">segment文件中，然后就可以直接定位到具体的segment文件了，这样再找具体的那一条数据就很快了，因为都是有序的。</span><br></pre></td></tr></table></figure><h3 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h3><h4 id="Broker节点宕机"><a href="#Broker节点宕机" class="headerlink" title="Broker节点宕机"></a>Broker节点宕机</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当Kafka集群中的一个Broker节点宕机，会出现什么现象？</span><br><span class="line"></span><br><span class="line">下面来演示一下</span><br><span class="line">使用kill -9 杀掉bigdata01中的broker进程测试</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">7522 Jps</span><br><span class="line">2054 Kafka</span><br><span class="line">1679 QuorumPeerMain</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# kill 2054</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">我们可以先通过zookeeper来查看一下，因为当kafka集群中的broker节点启动之后，会自动向zookeeper中进行注册，保存当前节点信息</span><br><span class="line">....]# bin&#x2F;zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers</span><br><span class="line">[ids, seqid, topics]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[1, 2]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160046086.png" alt="image-20230416004647924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时发现zookeeper的&#x2F;brokers&#x2F;ids下面只有2个节点信息</span><br><span class="line">可以通过get命令查看节点信息，这里面会显示对应的主机名和端口号</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] get &#x2F;brokers&#x2F;ids&#x2F;1</span><br><span class="line">&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLA</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160050443.png" alt="image-20230416005045245"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">然后再使用describe查询topic的详细信息，会发现此时的分区的leader全部变成了目前存活的另外两个节点</span><br><span class="line"></span><br><span class="line">此时可以发现Isr中的内容和Replicas中的不一样了，因为Isr中显示的是目前正常运行的节点</span><br><span class="line"></span><br><span class="line">所以当Kafka集群中的一个Broker节点宕机之后，对整个集群而言没有什么特别的大影响，此时集群会给partition重新选出来一些新的Leader节点</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160053702.png" alt="image-20230416005338215"></p><h4 id="新增一个Broker节点"><a href="#新增一个Broker节点" class="headerlink" title="新增一个Broker节点"></a>新增一个Broker节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当Kafka集群中新增一个Broker节点，会出现什么现象？</span><br><span class="line">新加入一个broker节点，zookeeper会自动识别并在适当的机会选择此节点提供服务</span><br><span class="line"></span><br><span class="line">再次启动bigdata01节点中的broker进程测试</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">此时到zookeeper中查看一下</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line">.....</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers</span><br><span class="line">[ids, seqid, topics]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[0, 1, 2]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160058241.png" alt="image-20230416005822131"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">发现broker.id为0的这个节点信息也有了</span><br><span class="line"></span><br><span class="line">在通过describe查看topic的描述信息，Isr中的信息和Replicas中的内容是一样的了</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper localhost:2181 --topic hello</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160059016.png" alt="image-20230416005947958"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">但是启动后有个问题：发现新启动的这个节点不会是任何分区的leader？怎么重新均匀分配呢？</span><br><span class="line">1、Broker中的自动均衡策略（默认已经有）</span><br><span class="line">auto.leader.rebalance.enable&#x3D;true</span><br><span class="line">leader.imbalance.check.interval.seconds 默认值：300</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2、手动执行：</span><br><span class="line">bin&#x2F;kafka-leader-election.sh --bootstrap-server localhost:9092 --election-type pareferred --all-topic-partitions</span><br><span class="line"></span><br><span class="line">Successfully completed leader election (PREFERRED) for partitions hello-4, he</span><br><span class="line"></span><br><span class="line">执行后的效果如下，这样就实现了均匀分配</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160105050.png" alt="image-20230416010500020"></p><h2 id="Kafka生产消费者实战"><a href="#Kafka生产消费者实战" class="headerlink" title="Kafka生产消费者实战"></a>Kafka生产消费者实战</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们使用基于console的生产者和消费者对topic实现了数据的生产和消费，，这个基于控制台的生产者和消费者主要是让我们做测试用的</span><br><span class="line">在实际工作中，我们有时候需要将生产者和消费者功能集成到我们已有的系统中，此时就需要写代码实现生产者和消费者的逻辑了。</span><br><span class="line">在这我们使用java代码来实现生产者和消费者的功能</span><br></pre></td></tr></table></figure><h3 id="Kafka-Java代码编程"><a href="#Kafka-Java代码编程" class="headerlink" title="Kafka Java代码编程"></a>Kafka Java代码编程</h3><h4 id="Java代码实现生产者代码"><a href="#Java代码实现生产者代码" class="headerlink" title="Java代码实现生产者代码"></a>Java代码实现生产者代码</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160114267.png" alt="image-20230416011434393"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先创建maven项目， db_kafka</span><br><span class="line"></span><br><span class="line">添加kafka的maven依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.kafka&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;kafka-clients&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">开发生产者代码</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.kafka;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Java代码实现生产者代码</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerDemo</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">         <span class="comment">//指定kafka的broker地址</span></span><br><span class="line">         prop.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">         <span class="comment">//指定key-value数据的序列化格式(key就是之前讲的，如果指定了数据有key，则可以根据它来将数据放入哪一个partition，一般用不到；但这里要知道不然要报错)</span></span><br><span class="line">         prop.put(<span class="string">"key.serializer"</span>, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         prop.put(<span class="string">"value.serializer"</span>, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定topic</span></span><br><span class="line">         String topic = <span class="string">"hello"</span>; </span><br><span class="line">         <span class="comment">//创建kafka生产者</span></span><br><span class="line">         KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String,String&gt;(prop);</span><br><span class="line">         <span class="comment">//向topic中生产数据(这里也没有传入key，只传入了value)</span></span><br><span class="line">         producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="string">"hello kafka"</span>))</span><br><span class="line">         <span class="comment">//关闭链接</span></span><br><span class="line">         producer.close();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Java代码实现消费者代码"><a href="#Java代码实现消费者代码" class="headerlink" title="Java代码实现消费者代码"></a>Java代码实现消费者代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.kafka;</span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Collection;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Java代码实现消费者代码</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerDemo</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">         <span class="comment">//指定kafka的broker地址(之前控制台那里server没s)</span></span><br><span class="line">         prop.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">         <span class="comment">//指定key-value的反序列化类型</span></span><br><span class="line">         prop.put(<span class="string">"key.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         prop.put(<span class="string">"value.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定消费者组(之前控制台那里，会自动生成)</span></span><br><span class="line">         prop.put(<span class="string">"group.id"</span>, <span class="string">"con-1"</span>);</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//创建消费者</span></span><br><span class="line">         KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String,String&gt;(prop);</span><br><span class="line">        Collection&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">         topics.add(<span class="string">"hello"</span>);</span><br><span class="line">         <span class="comment">//订阅指定的topic</span></span><br><span class="line">         consumer.subscribe(topics);</span><br><span class="line">         <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">             <span class="comment">//消费数据【注意：需要修改jdk编译级别为1.8，否则Duration.ofSeconds(1)会语法报错</span></span><br><span class="line">             ConsumerRecords&lt;String, String&gt; poll = consumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">             <span class="keyword">for</span> (ConsumerRecord&lt;String,String&gt; consumerRecord : poll) &#123;</span><br><span class="line">             System.out.println(consumerRecord);</span><br><span class="line">             &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1. 关闭kafka服务器的防火墙</span><br><span class="line">2. 配置windows的hosts文件 添加kafka节点的hostname和ip的映射关系。[如果我们的hosts文件中没有对kafka节点的hostnam和ip的映射关系做配置，在这经过多次尝试连接不上就会报错]</span><br><span class="line"></span><br><span class="line">先开启消费者。</span><br><span class="line">发现没有消费到数据，这个topic中是有数据的，为什么之前的数据没有消费出来呢？(就是前面讲的，默认会从consumer生成后生成的数据读取)不要着急，先带着这个问题往下面看</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3%5Cimage-20230416014022148.png" alt="image-20230416014022148"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再开启生产者，生产者会生产一条数据，然后就结束</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160143273.png" alt="image-20230416014143241"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时回到kafka的消费者端就可以看到消费出来的数据了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160142342.png" alt="image-20230416014214263"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以这个时候我们发现，新产生的数据我们是可以消费到的，但是之前的数据我们就无法消费了，那下面我们来分析一下这个问题</span><br></pre></td></tr></table></figure><h4 id="消费者代码扩展"><a href="#消费者代码扩展" class="headerlink" title="消费者代码扩展"></a>消费者代码扩展</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x2F;&#x2F;开启消费者自动提交offset功能，默认就是开启的</span><br><span class="line">prop.put(&quot;enable.auto.commit&quot;,&quot;true&quot;);</span><br><span class="line">&#x2F;&#x2F;自动提交offset的时间间隔，单位是毫秒(在开启自动提交时，它默认开启，且默认值是5000)</span><br><span class="line">prop.put(&quot;auto.commit.interval.ms&quot;,&quot;5000&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">注意：正常情况下，kafka消费数据的流程是这样的</span><br><span class="line">先根据group.id指定的消费者组到kafka中查找之前保存的offset信息</span><br><span class="line"></span><br><span class="line">如果查找到了，说明之前使用这个消费者组消费过数据，则根据之前保存的offset继续进行消费</span><br><span class="line"></span><br><span class="line">如果没查找到(说明第一次消费)，或者查找到了，但是查找到的那个offset对应的数据已经不存</span><br><span class="line"></span><br><span class="line">这个时候消费者该如何消费数据？</span><br><span class="line">(因为kafka默认只会保存7天的数据，超过时间数据会被删除)</span><br><span class="line"></span><br><span class="line">此时会根据auto.offset.reset的值执行不同的消费逻辑</span><br><span class="line"></span><br><span class="line">这个参数的值有三种:[earliest,latest,none]</span><br><span class="line">earliest：表示从最早的数据开始消费(从头消费)</span><br><span class="line">latest【默认】：表示从最新的数据开始消费</span><br><span class="line">none：如果根据指定的group.id没有找到之前消费的offset信息，就会抛异常</span><br><span class="line"></span><br><span class="line">(工作中earliest和latest常用)</span><br><span class="line"></span><br><span class="line">解释：【查找到了，但是查找到的那个offset对应的数据已经不存在了】 </span><br><span class="line">假设你第一天使用一个消费者去消费了一条数据，然后就把消费者停掉了，等了7天之后，你又使用这个消费者去消费数据</span><br><span class="line">这个时候，这个消费者启动的时候会到kafka里面查询它之前保存的offset信息</span><br><span class="line">但是那个offset对应的数据已经被删了，所以此时再根据这个offset去消费是消费不到数据的</span><br><span class="line"></span><br><span class="line">总结，一般在实时计算的场景下，这个参数的值建议设置为latest，消费最新的数据</span><br><span class="line"></span><br><span class="line">这个参数只有在消费者第一次消费数据，或者之前保存的offset信息已过期的情况下才会生效</span><br><span class="line"> *&#x2F;</span><br><span class="line"></span><br><span class="line">prop.put(&quot;auto.offset.reset&quot;,&quot;latest&quot;);</span><br><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时我们来验证一下，</span><br><span class="line">先启动一次生产者，再启动一次消费者，看看消费者能不能消费到这条数据，如果能消费到，就说明此时是根据上次保存的offset信息进行消费了。结果发现是可以消费到的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：消费者消费到数据之后，不要立刻关闭程序，要至少等5秒，因为自动提交offset的时机是5秒提交一次</span><br><span class="line"></span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 4, leaderEpoch &#x3D; 5, offset &#x3D; 0, Cre</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">将auto.offset.reset置为earliest，修改一下group.id的值，相当于使用一个新的消费者，验证一下，看是否能把这个topic中的所有数据都取出来，因为新的消费者第一次肯定是获取不到offset信息的，</span><br><span class="line">所以就会根据auto.offset.reset的值来消费数据</span><br><span class="line"></span><br><span class="line">prop.put(&quot;group.id&quot;, &quot;con-2&quot;);</span><br><span class="line">prop.put(&quot;auto.offset.reset&quot;,&quot;earliest&quot;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 2, leaderEpoch &#x3D; 0, offset &#x3D; 0, Cre</span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 3, leaderEpoch &#x3D; 3, offset &#x3D; 0, Cre</span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 4, leaderEpoch &#x3D; 5, offset &#x3D; 0, Cre</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162130163.png" alt="image-20230416213026828"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时，关闭消费者(需要等待5秒，这样才会提交offset)，再重新启动，发现没有消费到数据，说明此时就</span><br><span class="line">根据上次保存的offset来消费数据了，因为没有新数据产生，所以就消费不到了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最后来处理一下程序输出的日志警告信息，这里其实示因为缺少依赖日志依赖</span><br><span class="line">在pom文件中添加log4j的依赖，然后将 log4j.properties 添加到 resources目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;info,stdout</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout &#x3D; org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Target &#x3D; System.out</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br></pre></td></tr></table></figure><h3 id="Consumer消费offset查询"><a href="#Consumer消费offset查询" class="headerlink" title="Consumer消费offset查询"></a>Consumer消费offset查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka0.9版本以前，消费者的offset信息保存在zookeeper中</span><br><span class="line">从kafka0.9开始，使用了新的消费API，消费者的信息会保存在kafka里面的__consumer_offsets这个topic中</span><br><span class="line"></span><br><span class="line">因为频繁操作zookeeper性能不高，所以kafka在自己的topic中负责维护消费者的offset信息。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162135924.png" alt="image-20230416213511507"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如何查询保存在kafka中的Consumer的offset信息呢？</span><br><span class="line">使用kafka-consumer-groups.sh这个脚本可以查看目前所有的consumer group</span><br><span class="line"></span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-consumer-groups.sh --list --bootstrap-server localhost:9092</span><br><span class="line"></span><br><span class="line">con-1</span><br><span class="line">con-2 (前面视频里修改过)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">具体查看某一个consumer group的信息</span><br><span class="line">GROUP：当前消费者组，通过group.id指定的值</span><br><span class="line">TOPIC：当前消费的topic</span><br><span class="line">PARTITION：消费的分区</span><br><span class="line">CURRENT-OFFSET：消费者消费到这个分区的offset</span><br><span class="line">LOG-END-OFFSET：当前分区中数据的最大offset</span><br><span class="line">LAG：当前分区未消费数据量</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group con-1</span><br><span class="line">GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG</span><br><span class="line">con-1 hello 4 1 1 0 </span><br><span class="line">con-1 hello 2 1 1 0 </span><br><span class="line">con-1 hello 3 1 1 0</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162141088.png" alt="image-20230416214127550"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">partition:是指消费者消费了哪些分区</span><br><span class="line">current-offset:当前消费了的数据的offset</span><br><span class="line">log-end-offset:最新数据的offset</span><br><span class="line">lag:还有多少条数据没消费</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时再执行一次生产者代码，生产一条数据，重新查看一下这个消费者的offset情况</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162148346.png" alt="image-20230416214839816"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如何分析生产的数据能不能及时消费掉：查看lag</span><br><span class="line">如果lag值比较大：就需要增加消费者个数，同一个代码执行多次(但group.id不能变)</span><br></pre></td></tr></table></figure><h3 id="Consumer消费顺序"><a href="#Consumer消费顺序" class="headerlink" title="Consumer消费顺序"></a>Consumer消费顺序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当一个消费者消费一个partition时候，消费的数据顺序和此partition数据的生产顺序是一致的</span><br><span class="line"></span><br><span class="line">当一个消费者消费多个partition时候，消费者按照partition的顺序，首先消费一个partition，当消费完一个partition最新的数据后再消费其它partition中的数据</span><br><span class="line"></span><br><span class="line">总之：如果一个消费者消费多个partiton，只能保证消费的数据顺序在一个partition内是有序的</span><br><span class="line"></span><br><span class="line">也就是说消费kafka中的数据只能保证消费partition内的数据是有序的，多个partition之间是无序的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162201198.png" alt="image-20230416220156464"></p><h3 id="Kafka的三种语义"><a href="#Kafka的三种语义" class="headerlink" title="Kafka的三种语义"></a>Kafka的三种语义</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka可以实现以下三种语义，这三种语义是针对消费者而言的：</span><br></pre></td></tr></table></figure><h4 id="至少一次：at-least-once"><a href="#至少一次：at-least-once" class="headerlink" title="至少一次：at-least-once"></a>至少一次：at-least-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这种语义有可能会对数据重复处理</span><br><span class="line">实现至少一次消费语义的消费者也很简单。</span><br><span class="line">1: 设置enable.auto.commit为false，禁用自动提交offset</span><br><span class="line">2: 消息处理完之后手动调用consumer.commitSync()提交offset</span><br><span class="line">这种方式是在消费数据之后，手动调用函数consumer.commitSync()异步提交offset，有可能处理多次的场景是消费者的消息处理完并输出到结果库，但是offset还没提交，这个时候消费者挂掉了，再重启的时候会重新消费并处理消息，所以至少会处理一次</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162213700.png" alt="image-20230416221328144"></p><h4 id="至多一次：at-most-once"><a href="#至多一次：at-most-once" class="headerlink" title="至多一次：at-most-once"></a>至多一次：at-most-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">这种语义有可能会丢失数据</span><br><span class="line">至多一次消费语义是kafka消费者的默认实现。配置这种消费者最简单的方式是</span><br><span class="line">1: enable.auto.commit设置为true。</span><br><span class="line">2: auto.commit.interval.ms设置为一个较低的时间范围。</span><br><span class="line">由于上面的配置，此时kafka会有一个独立的线程负责按照指定间隔提交offset。</span><br><span class="line"></span><br><span class="line">消费者的offset已经提交，但是消息还在处理中(还没有处理完)，这个时候程序挂了，导致数据没有被成功处理，再重启的时候会从上次提交的offset处消费，导致上次没有被成功处理的消息就丢失了。</span><br></pre></td></tr></table></figure><h4 id="仅一次：exactly-once"><a href="#仅一次：exactly-once" class="headerlink" title="仅一次：exactly-once"></a>仅一次：exactly-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这种语义可以保证数据只被消费处理一次。</span><br><span class="line">实现仅一次语义的思路如下：</span><br><span class="line">1: 将enable.auto.commit设置为false，禁用自动提交offset</span><br><span class="line">2: 使用consumer.seek(topicPartition，offset)来指定offset</span><br><span class="line">3: 在处理消息的时候，要同时保存住每个消息的offset。以原子事务的方式保存offset和处理的消息结果，这个时候相当于自己保存offset信息了，把offset和具体的数据绑定到一块，数据真正处理成功的时候才会保存offset信息</span><br><span class="line">这样就可以保证数据仅被处理一次了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.html</id>
    <published>2023-04-15T14:13:26.000Z</published>
    <updated>2023-04-16T03:01:48.465Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="大数据开发工程师-第十四周-消息队列之Kafka从入门到小牛-2"><a href="#大数据开发工程师-第十四周-消息队列之Kafka从入门到小牛-2" class="headerlink" title="大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2"></a>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2</h1><h2 id="Kafka使用初体验"><a href="#Kafka使用初体验" class="headerlink" title="Kafka使用初体验"></a>Kafka使用初体验</h2><h3 id="Kafka中Topic的操作"><a href="#Kafka中Topic的操作" class="headerlink" title="Kafka中Topic的操作"></a>Kafka中Topic的操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka集群安装好了以后我们就想向kafka中添加一些数据</span><br><span class="line">想要添加数据首先需要创建topic</span><br><span class="line">那接下来看一下针对topic的一些操作</span><br></pre></td></tr></table></figure><h4 id="新增Topic"><a href="#新增Topic" class="headerlink" title="新增Topic"></a>新增Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">指定2个分区，2个副本，注意：副本数不能大于集群中Broker的数量</span><br><span class="line"></span><br><span class="line">因为每个partition的副本必须保存在不同的broker，否则没有意义，如果partition的副本都保存在同一个broker，那么这个broker挂了，则partition数据依然会丢失</span><br><span class="line"></span><br><span class="line">在这里我使用的是3个节点的kafka集群，所以副本数我就暂时设置为2，最大可以设置为3</span><br><span class="line"></span><br><span class="line">如果你们用的是单机kafka的话，这里的副本数就只能设置为1了，这个需要注意一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 2 --replication-factor 2</span><br><span class="line">--topic hello</span><br><span class="line">Created topic hello.</span><br></pre></td></tr></table></figure><h4 id="查询Topic"><a href="#查询Topic" class="headerlink" title="查询Topic"></a>查询Topic</h4><h5 id="查询Kafka中的所有Topic列表"><a href="#查询Kafka中的所有Topic列表" class="headerlink" title="查询Kafka中的所有Topic列表"></a>查询Kafka中的所有Topic列表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">查询Kafka中的所有Topic列表以及查看指定Topic的详细信息</span><br><span class="line">查询kafka中所有的topic列表</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line">hello</span><br><span class="line"></span><br><span class="line">topic数据是存在zookeeper中，所以直接指定zookeeper地址就可以了(有些地方需要指定kafka地址)</span><br></pre></td></tr></table></figure><h5 id="查看指定Topic的详细信息"><a href="#查看指定Topic的详细信息" class="headerlink" title="查看指定Topic的详细信息"></a>查看指定Topic的详细信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看指定topic的详细信息</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper localhost:2181 --topic hello</span><br><span class="line">Topic: hello PartitionCount: 2 ReplicationFactor: 2 Configs: </span><br><span class="line">Topic: hello Partition: 0 Leader: 2 Replicas: 2,0 Is</span><br><span class="line">Topic: hello Partition: 1 Leader: 0 Replicas: 0,1 Is</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152231741.png" alt="image-20230415223043384"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">第一个行显示指定topic所有partitions的一个总结</span><br><span class="line">PartitionCount：表示这个Topic一共有多少个partition</span><br><span class="line">ReplicationFactor：表示这个topic中partition的副本因子是几个</span><br><span class="line">Config：这个表示创建Topic时动态指定的配置信息，在这我们没有额外指定配置信息</span><br><span class="line"></span><br><span class="line">下面每一行给出的是一个partition的信息，如果只有一个partition，则只显示一行。</span><br><span class="line">Topic：显示当前的topic名称</span><br><span class="line">Partition：显示当前topic的partition编号</span><br><span class="line">Leader：Leader partition所在的节点编号，这个编号其实就是broker.id的值，</span><br><span class="line">来看这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152234654.png" alt="image-20230415223420514"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这个图里面的hello这个topic有两个partition，其中partition1的leader所在的节点是broker1，partition2的leader所在的节点是broker2</span><br><span class="line"></span><br><span class="line">Replicas：当前partition所有副本所在的节点编号【包含Leader所在的节点】，如果设置多个副本的话，这里会显示多个，不管该节点是否是Leader以及是否存活。</span><br><span class="line"></span><br><span class="line">Isr：当前partition处于同步状态的所有节点，这里显示的所有节点都是存活状态的，并且跟Leader同步的(包含Leader所在的节点)</span><br><span class="line"></span><br><span class="line">所以说Replicas和Isr的区别就是</span><br><span class="line">如果某个partition的副本所在的节点宕机了，在Replicas中还是会显示那个节点，但是在Isr中就不会显示了，Isr中显示的都是处于正常状态的节点。</span><br></pre></td></tr></table></figure><h4 id="修改Topic"><a href="#修改Topic" class="headerlink" title="修改Topic"></a>修改Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">修改一般是修改Topic的partition数量，只能增加</span><br><span class="line"></span><br><span class="line">为什么partition只能增加？</span><br><span class="line">因为数据是存储在partition中的，如果可以减少partition的话，那么partition中的数据就丢了</span><br><span class="line"></span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --alter --zookeeper localhost:2181 --partitions 5 --topic hello </span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partitio</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line">修改之后再来查看一下topic的详细信息</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper </span><br><span class="line">Topic: hello PartitionCount: 5 ReplicationFactor: 2 Configs: </span><br><span class="line"> Topic: hello Partition: 0 Leader: 2 Replicas: 2,0 I</span><br><span class="line"> Topic: hello Partition: 1 Leader: 0 Replicas: 0,1 I</span><br><span class="line"> Topic: hello Partition: 2 Leader: 1 Replicas: 1,2 I</span><br><span class="line"> Topic: hello Partition: 3 Leader: 2 Replicas: 2,1 I</span><br><span class="line"> Topic: hello Partition: 4 Leader: 0 Replicas: 0,2</span><br></pre></td></tr></table></figure><h4 id="删除Topic"><a href="#删除Topic" class="headerlink" title="删除Topic"></a>删除Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">删除Kafka中的指定Topic</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --delete --zookeeper localhost 2181 --topic hello</span><br><span class="line">Topic hello is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br><span class="line"></span><br><span class="line">删除操作是不可逆的，删除Topic会删除它里面的所有数据</span><br><span class="line"></span><br><span class="line">注意：Kafka从1.0.0开始默认开启了删除操作，之前的版本只会把Topic标记为删除状态，需要设置delete.topic.enable为true才可以真正删除</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果不想开启删除功能，可以设置delete.topic.enable为false，这样删除topic的时候只会把它标记为删除状态，此时这个topic依然可以正常使用。</span><br><span class="line">delete.topic.enable可以配置在server.properties文件中</span><br></pre></td></tr></table></figure><h3 id="Kafka中的生产者和消费者"><a href="#Kafka中的生产者和消费者" class="headerlink" title="Kafka中的生产者和消费者"></a>Kafka中的生产者和消费者</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了Kafka中的topic的创建方式，下面我们可以向topic中生产数据以及消费数据了</span><br><span class="line">生产数据需要用到生产者</span><br><span class="line">消费数据需要用到消费者</span><br><span class="line"></span><br><span class="line">kafka默认提供了基于控制台的生产者和消费者，方便测试使用</span><br><span class="line">生产者： bin&#x2F;kafka-console-producer.sh</span><br><span class="line">消费者： bin&#x2F;kafka-console-consumer.sh</span><br></pre></td></tr></table></figure><h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">先来看一下如何向里面生产数据</span><br><span class="line">直接使用kafka提供的基于控制台的生产者</span><br><span class="line">先创建一个topic【5个分区，2个副本】：</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost 2181 --partitions 5 --replication-factor 2 --topic hello</span><br><span class="line">Created topic hello.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">向这个topic中生产数据</span><br><span class="line">broker-list：kafka的服务地址[多个用逗号隔开](这里需要用到kafka地址，上面创建topic时指定的是zookeeper，这里用本地地址和使用bigdata01:9092,bigdata02:9092,bigdata03:9092是一样的)</span><br><span class="line">topic：topic名称</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-producer.sh --broker-list localhost:9092 --topic hello</span><br><span class="line">&gt;hehe</span><br></pre></td></tr></table></figure><h4 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">下面来创建一个消费者消费topic中的数据</span><br><span class="line">bootstrap-server：kafka的服务地址</span><br><span class="line">topic:具体的topic下面来创建一个消费者消费topic中的数据</span><br><span class="line">1 [root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hello</span><br><span class="line"></span><br><span class="line">发现消费不到刚才生产的数据，为什么呢？</span><br><span class="line">因为kafka的消费者默认是消费最新生产的数据，如果想消费之前生产的数据需要添加一个参数--from-beginning，表示从头消费的意思</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hello --from-beginning</span><br><span class="line"></span><br><span class="line">hehe</span><br><span class="line"></span><br><span class="line">这里创建消费者的机器01、02、03都可以</span><br></pre></td></tr></table></figure><h4 id="案例：QQ群聊天"><a href="#案例：QQ群聊天" class="headerlink" title="案例：QQ群聊天"></a>案例：QQ群聊天</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">通过kafka可以模拟QQ群聊天的功能，我们来看一下</span><br><span class="line">首先在kafka中创建一个新的topic，可以认为是我们在QQ里面创建了一个群，群号是88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost 2181 --partitions 5 --replication-factor 2 --topic 88888888</span><br><span class="line">Created topic 88888888.</span><br><span class="line"></span><br><span class="line">然后我把你们都拉到这个群里面，这样我在群里面发消息你们就都能收到了</span><br><span class="line">在bigdata02和bigdata03上开启消费者，可以认为是把这两个人拉到群里面了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic 88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic 88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">然后我在bigdata01上开启生产者发消息，这样bigdata02和bigdata03都是可以收到的。</span><br><span class="line">这样就可以认为在群里的人都能收到我发的消息，类似于发广播。</span><br><span class="line">这个其实主要利用了kafka中的多消费者的特性，每个消费者都可以消费到相同的数据</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-producer.sh --broker-list localhost:9092 --topic 88888888 </span><br><span class="line">&gt;hello everyone</span><br></pre></td></tr></table></figure><h2 id="Kafka核心扩展内容"><a href="#Kafka核心扩展内容" class="headerlink" title="Kafka核心扩展内容"></a>Kafka核心扩展内容</h2><h3 id="Broker扩展"><a href="#Broker扩展" class="headerlink" title="Broker扩展"></a>Broker扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Broker的参数可以配置在server.properties这个配置文件中，Broker中支持的完整参数在官方文档中有体现</span><br><span class="line">具体链接为：</span><br><span class="line">http:&#x2F;&#x2F;kafka.apache.org&#x2F;24&#x2F;documentation.html#brokerconfigs</span><br><span class="line">针对Broker的参数，我们主要分析两块</span><br></pre></td></tr></table></figure><h4 id="Log-Flush-Policy"><a href="#Log-Flush-Policy" class="headerlink" title="Log Flush Policy"></a>Log Flush Policy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1：Log Flush Policy：设置数据flush到磁盘的时机</span><br><span class="line">为了减少磁盘写入的次数,broker会将消息暂时缓存起来,当消息的个数达到一定阀值或者过了一定的时间间隔后,再flush到磁盘,这样可以减少磁盘IO调用的次数。</span><br><span class="line"></span><br><span class="line">这块主要通过两个参数控制</span><br><span class="line">log.flush.interval.messages 一个分区的消息数阀值，达到该阈值则将该分区的数据flush到磁盘，注意这里是针对分区，因为topic是一个逻辑概念，分区是真实存在的，每个分区会在磁盘上产生一个目录</span><br><span class="line">[root@bigdata01 kafka-logs]# ll</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:12 88888888-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:12 88888888-3</span><br><span class="line">-rw-r--r--. 1 root root 4 Jun 8 15:23 cleaner-offset-checkpoint</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-12</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-15</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-18</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-21</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-24</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-27</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-3</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-30</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-33</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-36</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-39</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-42</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-45</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-48</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-6</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-9</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-1</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-4 </span><br><span class="line">hello topic有5个分区，但这里只有2个目录的原因是：没写几条数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160022785.png" alt="image-20230416002248398"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个参数的默认值为9223372036854775807，long的最大值</span><br><span class="line">默认值太大了，所以建议修改，可以使用server.properties中针对这个参数指定的值10000，需要去掉注释之后这个参数才生效。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">log.flush.interval.ms间隔指定时间</span><br><span class="line">默认间隔指定的时间将内存中缓存的数据flush到磁盘中，由文档可知，这个参数的默认值为null，此时会使用log.flush.scheduler.interval.ms参数的值，log.flush.scheduler.interval.ms参数的值默认是 9223372036854775807，long的最大值</span><br><span class="line"></span><br><span class="line">所以这个值也建议修改，可以使用server.properties中针对这个参数指定的值1000，单位是毫秒，表示每1秒写一次磁盘，这个参数也需要去掉注释之后才生效</span><br></pre></td></tr></table></figure><h4 id="Log-Retention-Policy"><a href="#Log-Retention-Policy" class="headerlink" title="Log Retention Policy"></a>Log Retention Policy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">设置数据保存周期，默认7天</span><br><span class="line">kafka中的数据默认会保存7天，如果kafka每天接收的数据量过大，这样是很占磁盘空间的，建议修改数据保存周期，我们之前在实际工作中是将数据保存周期改为了1天。</span><br><span class="line"></span><br><span class="line">数据保存周期主要通过这几个参数控制</span><br><span class="line">log.retention.hours，这个参数默认值为168，单位是小时，就是7天，可以在这调整数据保存的时间，超过这个时间数据会被自动删除</span><br><span class="line">log.retention.bytes，这个参数表示当分区的文件达到一定大小的时候会删除它，如果设置了按照指定周期删除数据文件，这个参数不设置也可以，这个参数默认是没有开启的</span><br><span class="line">log.retention.check.interval.ms，这个参数表示检测的间隔时间，单位是毫秒，默认值是300000，就是5分钟，表示每5分钟检测一次文件看是否满足删除的时机</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">默认根据时间就行了，后期在时间范围内还可以从kafka中恢复数据</span><br></pre></td></tr></table></figure><h3 id="Producer扩展"><a href="#Producer扩展" class="headerlink" title="Producer扩展"></a>Producer扩展</h3><h4 id="producer发送数据到partition的方式"><a href="#producer发送数据到partition的方式" class="headerlink" title="producer发送数据到partition的方式"></a>producer发送数据到partition的方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Producer默认是随机将数据发送到topic的不同分区中，也可以根据用户设置的算法来根据消息的key来计算输入到哪个partition里面</span><br><span class="line"></span><br><span class="line">此时需要通过partitioner来控制，这个知道就行了，因为在实际工作中一般在向kafka中生产数据的都是不带key的，只有数据内容，所以一般都是使用随机的方式发送数据</span><br></pre></td></tr></table></figure><h4 id="producer的数据通讯方式"><a href="#producer的数据通讯方式" class="headerlink" title="producer的数据通讯方式"></a>producer的数据通讯方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">在这里有一个需要注意的内容就是</span><br><span class="line">针对producer的数据通讯方式：同步发送和异步发送</span><br><span class="line"></span><br><span class="line">同步是指：生产者发出数据后，等接收方发回响应以后再发送下个数据的通讯方式。</span><br><span class="line">异步是指：生产者发出数据后，不等接收方发回响应，接着发送下个数据的通讯方式。</span><br><span class="line"></span><br><span class="line">具体的数据通讯策略是由acks参数控制的</span><br><span class="line">acks默认为1，表示需要Leader节点回复收到消息，这样生产者才会发送下一条数据</span><br><span class="line">acks：all，表示需要所有Leader+副本节点回复收到消息（acks&#x3D;-1），这样生产者才会发送下一条数据</span><br><span class="line">acks：0，表示不需要任何节点回复，生产者会继续发送下一条数据</span><br><span class="line">再来看一下这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152350669.png" alt="image-20230415235025189"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">我们在向hello这个topic生产数据的时候，可以在生产者中设置acks参数，</span><br><span class="line">acks设置为1，表示我们在向hello这个topic的partition1这个分区写数据的时候，只需要让leader所在的broker1这个节点回复确认收到的消息就可以了，这样生产者就可以发送下一条数据了</span><br><span class="line"></span><br><span class="line">如果acks设置为all，则需要partition1的这两个副本所在的节点(包含Leader)都回复收到消息，生产者才会发送下一条数据</span><br><span class="line"></span><br><span class="line">如果acks设置为0，表示生产者不会等待任何partition所在节点的回复，它只管发送数据，不管你有没有收到，所以这种情况丢失数据的概率比较高。</span><br></pre></td></tr></table></figure><h5 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对这块在面试的时候会有一个面试题：Kafka如何保证数据不丢？</span><br><span class="line">其实就是通过acks机制保证的，如果设置acks为all，则可以保证数据不丢，因为此时把数据发送给kafka之后，会等待对应partition所在的所有leader和副本节点都确认收到消息之后才会认为数据发送成功了，所以在这种策略下，只要把数据发送给kafka之后就不会丢了。</span><br><span class="line"></span><br><span class="line">如果acks设置为1，则当我们把数据发送给partition之后，partition的leader节点也确认收到了，但是leader回复完确认消息之后，leader对应的节点就宕机了，副本partition还没来得及将数据同步过去，所以会存在丢失的可能性。</span><br><span class="line">不过如果宕机的是副本partition所在的节点，则数据是不会丢的</span><br><span class="line"></span><br><span class="line">如果acks设置为0的话就表示是顺其自然了，只管发送，不管kafka有没有收到，这种情况表示对数据丢不丢都无所谓了。</span><br></pre></td></tr></table></figure><h3 id="Consumer扩展"><a href="#Consumer扩展" class="headerlink" title="Consumer扩展"></a>Consumer扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">在消费者中还有一个消费者组的概念</span><br><span class="line">每个consumer属于一个消费者组，通过group.id指定消费者组</span><br><span class="line"></span><br><span class="line">那组内消费和组间消费有什么区别吗？</span><br><span class="line">组内：消费者组内的所有消费者消费同一份数据；</span><br><span class="line"></span><br><span class="line">注意：在同一个消费者组中，一个partition同时只能有一个消费者消费数据</span><br><span class="line">如果消费者的个数小于分区的个数，一个消费者会消费多个分区的数据。</span><br><span class="line">如果消费者的个数大于分区的个数，则多余的消费者不消费数据</span><br><span class="line">所以，对于一个topic,同一个消费者组中推荐不能有多于分区个数的消费者,否则将意味着某些消费者将无法获得消息。</span><br><span class="line"></span><br><span class="line">组间：多个消费者组消费相同的数据，互不影响。</span><br><span class="line">来看下面这个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160000840.png" alt="image-20230416000001364"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Kafka集群有两个节点，Broker1和Broker2</span><br><span class="line">集群内有一个topic，这个topic有4个分区，P0,P1,P2,P3</span><br><span class="line"></span><br><span class="line">下面有两个消费者组</span><br><span class="line">Consumer Group A和Consumer Group B</span><br><span class="line">其中Consumer Group A中有两个消费者C1和C2，由于这个topic有4个分区，所以，C1负责消费两个分区的数据，C2负责消费两个分区的数据，这个属于组内消费</span><br><span class="line">Consumer Group B有5个消费者，C3~C7，其中C3,C4,C5,C6分别消费一个分区的数据，而C7就是多余出来的了，因为现在这个消费者组内的消费者的数量比对应的topic的分区数量还多，但是一个分区同时只能被一个消费者消费，所以就会有一个消费者处于空闲状态。这个也属于组内消费</span><br><span class="line">Consumer Group A和Consumer Group B这两个消费者组属于组间消费，互不影响。</span><br></pre></td></tr></table></figure><h3 id="Topic、Partition扩展"><a href="#Topic、Partition扩展" class="headerlink" title="Topic、Partition扩展"></a>Topic、Partition扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">每个partition在存储层面是append log文件。</span><br><span class="line">新消息都会被直接追加到log文件的尾部，每条消息在log文件中的位置称为offset(偏移量)。</span><br><span class="line">越多partitions可以容纳更多的consumer,有效提升并发消费的能力。</span><br><span class="line"></span><br><span class="line">具体什么时候增加topic的数量？什么时候增加partition的数量呢？</span><br><span class="line"></span><br><span class="line">业务类型增加需要增加topic、数据量大需要增加partition</span><br></pre></td></tr></table></figure><h3 id="Message扩展"><a href="#Message扩展" class="headerlink" title="Message扩展"></a>Message扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每条Message包含了以下三个属性：</span><br><span class="line">1. offset对应类型：long，表示此消息在一个partition中的起始的位置。可以认为offset是partition中Message的id，自增的</span><br><span class="line">2. MessageSize 对应类型：int32 此消息的字节大小。</span><br><span class="line">3. data，类型为bytes,是message的具体内容。</span><br><span class="line">看这个图，加深对Topic、Partition、Message的理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160016235.png" alt="image-20230416001618719"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里也体现了kafka高吞吐量的原因：磁盘顺序读写由于内存随机访问</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2%5C202304160022785.png" alt></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BTableAPI%E5%92%8CSQL-5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BTableAPI%E5%92%8CSQL-5.html</id>
    <published>2023-04-08T15:22:12.000Z</published>
    <updated>2023-04-20T08:46:55.682Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-Flink核心API之TableAPI和SQL-5"><a href="#第十六周-Flink极速上手篇-Flink核心API之TableAPI和SQL-5" class="headerlink" title="第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5"></a>第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5</h1><h2 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意:Table API和SQL现在还处于活跃开发阶段，还没有完全实现Flink中所有的特性。不是所有的[Table API，SQL]和[流，批]的组合都是支持的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL的由来：</span><br><span class="line">Flink针对标准的流处理和批处理提供了两种关系型API，Table API和SQL。Table API允许用户以一种很直观的方式进行select、filter和join操作。Flink SQL基于Apache Calcite实现标准SQL。针对批处理和流处理可以提供相同的处理语义和结果。</span><br><span class="line">Flink Table API、SQL和Flink的DataStream API、DataSet API是紧密联系在一起的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL是一种关系型API，用户可以像操作Mysql数据库表一样的操作数据，而不需要写代码，更不需要手工的对代码进行调优。另外，SQL作为一个非程序员可操作的语言，学习成本很低，如果一个系统提供 SQL支持，将很容易被用户接受。</span><br><span class="line"></span><br><span class="line">如果你想要使用Table API和SQL的话，需要添加下面的依赖</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-api-scala-bridge_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如果你想在 本地 IDE中运行程序，还需要添加下面的依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-planner-blink_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如果你用到了老的执行引擎，还需要添加下面这个依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-planner_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注意：由于部分table相关的代码是用Scala实现的，所以，这个依赖也是必须的。【这个依赖我们在前面开发DataStream程序的时候已经添加过了】</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL通过join API集成在一起，这个join API的核心概念是Table，Table可以作为查询的输入和输出。</span><br><span class="line">针对Table API和SQL我们主要讲解以下内容</span><br><span class="line">1：Table API和SQL的使用</span><br><span class="line">2：DataStream、DataSet和Table之间的互相转换</span><br></pre></td></tr></table></figure><h2 id="Table-API-和SQL的使用"><a href="#Table-API-和SQL的使用" class="headerlink" title="Table API 和SQL的使用"></a>Table API 和SQL的使用</h2><h3 id="创建TableEnvironment对象"><a href="#创建TableEnvironment对象" class="headerlink" title="创建TableEnvironment对象"></a>创建TableEnvironment对象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">想要使用Table API和SQL，首先要创建一个TableEnvironment对象。</span><br><span class="line">下面我们来创建一个TableEnvironment对象</span><br></pre></td></tr></table></figure><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.&#123;<span class="type">BatchTableEnvironment</span>, <span class="type">Stream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">EnvironmentSettings</span>, <span class="type">TableEnvironment</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建TableEnvironment对象</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateTableEnvironmentScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL不需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 则针对stream和batch都可以使用TableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定底层使用Blink引擎，以及数据处理模式-stream</span></span><br><span class="line">         <span class="comment">//从1.11版本开始，Blink引擎成为Table API和SQL的默认执行引擎，在生产环境下面，推</span></span><br><span class="line">         <span class="keyword">val</span> sSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build()</span><br><span class="line">         <span class="comment">//创建TableEnvironment对象</span></span><br><span class="line">         <span class="keyword">val</span> sTableEnv = <span class="type">TableEnvironment</span>.create(sSettings)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定底层使用Blink引擎，以及数据处理模式-batch</span></span><br><span class="line">         <span class="keyword">val</span> bSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inBatchMode().build()</span><br><span class="line">         <span class="comment">//创建TableEnvironment对象</span></span><br><span class="line">         <span class="keyword">val</span> bTableEnv = <span class="type">TableEnvironment</span>.create(bSettings)</span><br><span class="line">         </span><br><span class="line">         </span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 针对stream需要使用StreamTableEnvironment</span></span><br><span class="line"><span class="comment">         * 针对batch需要使用BatchTableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="comment">//创建StreamTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> ssEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> ssSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build()</span><br><span class="line">         <span class="keyword">val</span> ssTableEnv = <span class="type">StreamTableEnvironment</span>.create(ssEnv, ssSettings)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//创建BatchTableEnvironment</span></span><br><span class="line">         <span class="comment">//注意：此时只能使用旧的执行引擎，新的Blink执行引擎不支持和DataSet转换</span></span><br><span class="line">         <span class="keyword">val</span> bbEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> bbTableEnv = <span class="type">BatchTableEnvironment</span>.create(bbEnv)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建TableEnvironment对象</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CreateTableEnvironmentJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL不需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 则针对stream和batch都可以使用TableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="comment">//创建TableEnvironment对象-stream</span></span><br><span class="line">         EnvironmentSettings sSettings = EnvironmentSettings.newInstance().use</span><br><span class="line">         TableEnvironment sTableEnv = TableEnvironment.create(sSettings);</span><br><span class="line">         <span class="comment">//创建TableEnvironment对象-batch</span></span><br><span class="line">         EnvironmentSettings bSettings = EnvironmentSettings.newInstance().use</span><br><span class="line">         TableEnvironment bTableEnv = TableEnvironment.create(bSettings);</span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 针对stream需要使用StreamTableEnvironment</span></span><br><span class="line"><span class="comment">         * 针对batch需要使用BatchTableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="comment">//创建StreamTableEnvironment</span></span><br><span class="line">         StreamExecutionEnvironment ssEnv = StreamExecutionEnvironment.getExec</span><br><span class="line">         EnvironmentSettings ssSettings = EnvironmentSettings.newInstance().us</span><br><span class="line">         StreamTableEnvironment ssTableEnv = StreamTableEnvironment.create(ssE</span><br><span class="line">         <span class="comment">//创建BatchTableEnvironment</span></span><br><span class="line">         <span class="comment">//注意：此时只能使用旧的执行引擎，新的Blink执行引擎不支持和DataSet转换</span></span><br><span class="line">         ExecutionEnvironment bbEnv = ExecutionEnvironment.getExecutionEnviron</span><br><span class="line">         BatchTableEnvironment bbTableEnv = BatchTableEnvironment.create(bbEnv</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Table-API和-SQL的使用"><a href="#Table-API和-SQL的使用" class="headerlink" title="Table API和 SQL的使用"></a>Table API和 SQL的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来演示一下Table API和SQL的使用</span><br><span class="line">目前创建Table的很多方法都过时了，都不推荐使用了，例如：registerTableSource、connect等方法</span><br><span class="line">目前官方推荐使用executeSql的方式，executeSql里面支持</span><br><span class="line">1DDL&#x2F;DML&#x2F;DQL&#x2F;SHOW&#x2F;DESCRIBE&#x2F;EXPLAIN&#x2F;USE等语法</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121052838.png" alt="image-20230412105247826"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">EnvironmentSettings</span>, <span class="type">TableEnvironment</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TableAPI 和 SQL的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableAPIAndSQLOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取TableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> sSettings = <span class="type">EnvironmentSettings</span>.newInstance.useBlinkPlanner.inStreamingMode().build()</span><br><span class="line">         <span class="keyword">val</span> sTableEnv = <span class="type">TableEnvironment</span>.create(sSettings)</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * connector.type：指定connector的类型</span></span><br><span class="line"><span class="comment">         * connector.path：指定文件或者目录地址</span></span><br><span class="line"><span class="comment">         * format.type：文件数据格式化类型</span></span><br><span class="line"><span class="comment">         * 注意：SQL语句如果出现了换行，行的末尾可以添加空格或者\n都可以，最后一行不用添</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         sTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//使用Table API实现数据查询和过滤等操作</span></span><br><span class="line">         <span class="comment">/*import org.apache.flink.table.api._</span></span><br><span class="line"><span class="comment">         val result = sTableEnv.from("myTable")</span></span><br><span class="line"><span class="comment">         .select($"id",$"name")</span></span><br><span class="line"><span class="comment">         .filter($"id" &gt; 1)*/</span></span><br><span class="line">        </span><br><span class="line">         <span class="comment">//使用SQL实现数据查询和过滤等操作</span></span><br><span class="line">         <span class="keyword">val</span> result = sTableEnv.sqlQuery(<span class="string">"select id,name from myTable where id &gt; 1"</span> )</span><br><span class="line">         <span class="comment">//输出结果到控制台</span></span><br><span class="line">         result.execute.print()</span><br><span class="line">          <span class="comment">//创建输出表</span></span><br><span class="line">         sTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table newTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\res',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">         <span class="comment">//输出结果到表newTable中</span></span><br><span class="line">         result.executeInsert(<span class="string">"newTable"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121053478.png" alt="image-20230412105345232"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">注意：针对SQL建表语句的写法还有一种比较清晰的写法</span><br><span class="line"></span><br><span class="line">sTableEnv.executeSql(</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line"> |create table myTable(</span><br><span class="line"> |id int,</span><br><span class="line"> |name string</span><br><span class="line"> |) with (</span><br><span class="line"> |&#39;connector.type&#39; &#x3D; &#39;filesystem&#39;,</span><br><span class="line"> |&#39;connector.path&#39; &#x3D; &#39;D:\data\source&#39;,</span><br><span class="line"> |&#39;format.type&#39; &#x3D; &#39;csv&#39;</span><br><span class="line"> |)</span><br><span class="line"> |&quot;&quot;&quot;.stripMargin)</span><br></pre></td></tr></table></figure><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TableAPI 和 SQL的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableAPIAndSQLOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//获取TableEnvironment</span></span><br><span class="line">         EnvironmentSettings sSettings = EnvironmentSettings.newInstance().use</span><br><span class="line">         TableEnvironment sTableEnv = TableEnvironment.create(sSettings);</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         sTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>);</span><br><span class="line">         <span class="comment">//使用Table API实现数据查询和过滤等操作</span></span><br><span class="line">         <span class="comment">/*Table result = sTableEnv.from("myTable")</span></span><br><span class="line"><span class="comment">         .select($("id"), $("name"))</span></span><br><span class="line"><span class="comment">         .filter($("id").isGreater(1));*/</span></span><br><span class="line">             <span class="comment">//使用SQL实现数据查询和过滤等操作</span></span><br><span class="line">         Table result = sTableEnv.sqlQuery(<span class="string">"select id,name from myTable where </span></span><br><span class="line"><span class="string">         //输出结果到控制台</span></span><br><span class="line"><span class="string">         result.execute().print();</span></span><br><span class="line"><span class="string">         //创建输出表</span></span><br><span class="line"><span class="string">         sTableEnv.executeSql("</span><span class="string">" +</span></span><br><span class="line"><span class="string">         "</span><span class="function">create table <span class="title">newTable</span><span class="params">(\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>id <span class="keyword">int</span>,\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>name string\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>)</span> <span class="title">with</span> <span class="params">(\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span><span class="string">'connector.type'</span> = <span class="string">'filesystem'</span>,\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span><span class="string">'connector.path'</span> = <span class="string">'D:\\data\\res'</span>,\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span><span class="string">'format.type'</span> = <span class="string">'csv'</span>\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>)</span>")</span>;</span><br><span class="line">         <span class="comment">//输出结果到表newTable中</span></span><br><span class="line">         result.executeInsert(<span class="string">"newTable"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DataStream、DataSet和Table之间的互相转换"><a href="#DataStream、DataSet和Table之间的互相转换" class="headerlink" title="DataStream、DataSet和Table之间的互相转换"></a>DataStream、DataSet和Table之间的互相转换</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL可以很容易的和DataStream和DataSet程序集成到一块。通过TableEnvironment，可以把DataStream或者DataSet注册为Table，这样就可以使用Table API和SQL查询了。通过</span><br><span class="line">TableEnvironment也可以把Table对象转换为DataStream或者DataSet，这样就可以使用DataStream或者DataSet中的相关API了。</span><br></pre></td></tr></table></figure><h3 id="使用DataStream创建表"><a href="#使用DataStream创建表" class="headerlink" title="使用DataStream创建表"></a>使用DataStream创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1：使用DataStream创建表，主要包含下面这两种情况</span><br><span class="line">使用DataStream创建view视图</span><br><span class="line">使用DataStream创建table对象</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataStream转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataStreamToTableScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> ssEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> ssSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build()</span><br><span class="line">         <span class="keyword">val</span> ssTableEnv = <span class="type">StreamTableEnvironment</span>.create(ssEnv, ssSettings)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//获取DataStream</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> stream = ssEnv.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mack"</span>)))</span><br><span class="line">         <span class="comment">//第一种：将DataStream转换为view视图</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line">   ssTableEnv.createTemporaryView(<span class="string">"myTable"</span>,stream,<span class="symbol">'id</span>,<span class="symbol">'name</span>)</span><br><span class="line">         ssTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().print()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//第二种：将DataStream转换为table对象</span></span><br><span class="line">         <span class="keyword">val</span> table = ssTableEnv.fromDataStream(stream, $<span class="string">"id"</span>, $<span class="string">"name"</span>)</span><br><span class="line">         table.select($<span class="string">"id"</span>,$<span class="string">"name"</span>)</span><br><span class="line">         .filter($<span class="string">"id"</span> &gt; <span class="number">1</span>)</span><br><span class="line">         .execute()</span><br><span class="line">         .print()</span><br><span class="line">         <span class="comment">//注意：'id,'name 和 $"id", $"name" 这两种写法是一样的效果</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataStream转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataStreamToTableJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         StreamExecutionEnvironment ssEnv = StreamExecutionEnvironment.getExec</span><br><span class="line">         EnvironmentSettings ssSettings = EnvironmentSettings.newInstance().us</span><br><span class="line">         StreamTableEnvironment ssTableEnv = StreamTableEnvironment.create(ssE</span><br><span class="line">         <span class="comment">//获取DataStream</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataStreamSource&lt;Tuple2&lt;Integer, String&gt;&gt; stream = ssEnv.fromCollecti</span><br><span class="line">         <span class="comment">//第一种：将DataStream转换为view视图</span></span><br><span class="line">         ssTableEnv.createTemporaryView(<span class="string">"myTable"</span>,stream,$(<span class="string">"id"</span>),$(<span class="string">"name"</span>));</span><br><span class="line">         ssTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().p</span><br><span class="line">         <span class="comment">//第二种：将DataStream转换为table对象</span></span><br><span class="line">         Table table = ssTableEnv.fromDataStream(stream, $(<span class="string">"id"</span>), $(<span class="string">"name"</span>));</span><br><span class="line">         table.select($(<span class="string">"id"</span>), $(<span class="string">"name"</span>))</span><br><span class="line">         .filter($(<span class="string">"id"</span>).isGreater(<span class="number">1</span>))</span><br><span class="line">         .execute()</span><br><span class="line">         .print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用DataSet创建表"><a href="#使用DataSet创建表" class="headerlink" title="使用DataSet创建表"></a>使用DataSet创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：此时只能使用旧的执行引擎，新的Blink执行引擎不支持和DataSet转换</span><br></pre></td></tr></table></figure><h4 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.&#123;<span class="type">BatchTableEnvironment</span>, <span class="type">Stream</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataSet转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSetToTableScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> bbEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> bbTableEnv = <span class="type">BatchTableEnvironment</span>.create(bbEnv)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//获取DataSet</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> set = bbEnv.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mack"</span>)))</span><br><span class="line">         <span class="comment">//第一种：将DataSet转换为view视图</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line">         bbTableEnv.createTemporaryView(<span class="string">"myTable"</span>,set,<span class="symbol">'id</span>,<span class="symbol">'name</span>)</span><br><span class="line">         bbTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().print</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//第二种：将DataSet转换为table对象</span></span><br><span class="line">         <span class="keyword">val</span> table = bbTableEnv.fromDataSet(set, $<span class="string">"id"</span>, $<span class="string">"name"</span>)</span><br><span class="line">         table.select($<span class="string">"id"</span>,$<span class="string">"name"</span>)</span><br><span class="line">         .filter($<span class="string">"id"</span> &gt; <span class="number">1</span>)</span><br><span class="line">         .execute()</span><br><span class="line">         .print()</span><br><span class="line">         <span class="comment">//注意：'id,'name 和 $"id", $"name" 这两种写法是一样的效果</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataSet转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataSetToTableJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         ExecutionEnvironment bbEnv = ExecutionEnvironment.getExecutionEnviron</span><br><span class="line">         BatchTableEnvironment bbTableEnv = BatchTableEnvironment.create(bbEnv</span><br><span class="line">         <span class="comment">//获取DataSet</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; set = bbEnv.fromCollection(data);</span><br><span class="line">         <span class="comment">//第一种：将DataSet转换为view视图</span></span><br><span class="line">         bbTableEnv.createTemporaryView(<span class="string">"myTable"</span>,set,$(<span class="string">"id"</span>),$(<span class="string">"name"</span>));</span><br><span class="line">         bbTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().p</span><br><span class="line">         <span class="comment">//第二种：将DataSet转换为table对象</span></span><br><span class="line">         Table table = bbTableEnv.fromDataSet(set, $(<span class="string">"id"</span>), $(<span class="string">"name"</span>));</span><br><span class="line">         table.select($(<span class="string">"id"</span>), $(<span class="string">"name"</span>))</span><br><span class="line">         .filter($(<span class="string">"id"</span>).isGreater(<span class="number">1</span>))</span><br><span class="line">         .execute()</span><br><span class="line">         .print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">将Table转换为DataStream或者DataSet时，你需要指定生成的 DataStream或者DataSet的数据类型，即，Table 的每行数据要转换成的数据类型。通常最方便的选择是转换成Row。以下列表概述了不同选项的功能：</span><br><span class="line">Row: 通过角标映射字段，支持任意数量的字段，支持null值，无类型安全（type-safe）检查。</span><br><span class="line">POJO: Java中的实体类，这个实体类中的字段名称需要和Table中的字段名称保持一致，支持任意数量的字段，支持null值，有类型安全检查。</span><br><span class="line">Case Class: 通过角标映射字段，不支持null值，有类型安全检查。</span><br><span class="line">Tuple: 通过角标映射字段，Scala中限制22个字段，Java中限制25个字段，不支持null值，有类型安全检查。</span><br><span class="line">Atomic Type: Table必须有一个字段，不支持null值，有类型安全检查。</span><br></pre></td></tr></table></figure><h3 id="将Table转换为DataStream"><a href="#将Table转换为DataStream" class="headerlink" title="将Table转换为DataStream"></a>将Table转换为DataStream</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3：将表转换成DataStream流式查询的结果Table会被动态地更新，即每个新的记录到达输入流时结果就会发生变化。因此，转换此动态查询的DataStream需要对表的更新进行编码。</span><br><span class="line"></span><br><span class="line">有几种模式可以将Table转换为DataStream。</span><br><span class="line">Append Mode:这种模式只适用于当动态表仅由INSERT更改修改时(仅附加)，之前添加的数据不会被更新。</span><br><span class="line">Retract Mode:可以始终使用此模式，它使用一个Boolean标识来编码INSERT和DELETE更改。</span><br></pre></td></tr></table></figure><h4 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将table转换成 DataStream</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableToDataStreamScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> ssEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> ssSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inSt</span><br><span class="line">         <span class="keyword">val</span> ssTableEnv = <span class="type">StreamTableEnvironment</span>.create(ssEnv, ssSettings)</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         ssTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         <span class="keyword">val</span> table = ssTableEnv.from(<span class="string">"myTable"</span>)</span><br><span class="line">         <span class="comment">//将table转换为DataStream</span></span><br><span class="line">         <span class="comment">//如果只有新增(追加)操作，可以使用toAppendStream</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> appStream = ssTableEnv.toAppendStream[<span class="type">Row</span>](table)</span><br><span class="line">         appStream.map(row=&gt;(row.getField(<span class="number">0</span>).toString.toInt,row.getField(<span class="number">1</span>).toString))</span><br><span class="line">         .print()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//如果有增加操作，还有删除操作，则使用toRetractStream</span></span><br><span class="line">         <span class="keyword">val</span> retStream = ssTableEnv.toRetractStream[<span class="type">Row</span>](table)</span><br><span class="line">         retStream.map(tup=&gt;&#123;</span><br><span class="line">         <span class="keyword">val</span> flag = tup._1</span><br><span class="line">         <span class="keyword">val</span> row = tup._2</span><br><span class="line">         <span class="keyword">val</span> id = row.getField(<span class="number">0</span>).toString.toInt</span><br><span class="line">         <span class="keyword">val</span> name = row.getField(<span class="number">1</span>).toString</span><br><span class="line">         (flag,id,name)</span><br><span class="line">         &#125;).print()</span><br><span class="line">         <span class="comment">//注意：将table对象转换为DataStream之后，就需要调用StreamExecutionEnvironment</span></span><br><span class="line">         ssEnv.execute(<span class="string">"TableToDataStreamScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121150371.png" alt="image-20230412115003343"></p><h4 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将table转换成 DataStream</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableToDataStreamJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         StreamExecutionEnvironment ssEnv = StreamExecutionEnvironment.getExec</span><br><span class="line">         EnvironmentSettings ssSettings = EnvironmentSettings.newInstance().us</span><br><span class="line">         StreamTableEnvironment ssTableEnv = StreamTableEnvironment.create(ssE</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         ssTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>);</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         Table table = ssTableEnv.from(<span class="string">"myTable"</span>);</span><br><span class="line">         <span class="comment">//将table转换为DataStream</span></span><br><span class="line">         <span class="comment">//如果只有新增(追加)操作，可以使用toAppendStream</span></span><br><span class="line">                                                                 DataStream&lt;Row&gt; appStream = ssTableEnv.toAppendStream(table, Row.clas</span><br><span class="line">         appStream.map(<span class="keyword">new</span> MapFunction&lt;Row, Tuple2&lt;Integer,String&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, String&gt; <span class="title">map</span><span class="params">(Row row)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">int</span> id = Integer.parseInt(row.getField(<span class="number">0</span>).toString());</span><br><span class="line">         String name = row.getField(<span class="number">1</span>).toString();</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, String&gt;(id, name);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         <span class="comment">//如果有增加操作，还有删除操作，则使用toRetractStream</span></span><br><span class="line">         DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retStream = ssTableEnv.toRetractStre</span><br><span class="line">         retStream.map(<span class="keyword">new</span> MapFunction&lt;Tuple2&lt;Boolean, Row&gt;, Tuple3&lt;Boolean,Int</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Boolean, Integer, String&gt; <span class="title">map</span><span class="params">(Tuple2&lt;Boolean, Row&gt; t</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         Boolean flag = tup.f0;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">int</span> id = Integer.parseInt(tup.f1.getField(<span class="number">0</span>)</span>.<span class="title">toString</span><span class="params">()</span>)</span>;</span><br><span class="line">         String name = tup.f1.getField(<span class="number">1</span>).toString();</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Boolean, Integer, String&gt;(flag, id, name);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         ssEnv.execute(<span class="string">"TableToDataStreamJava"</span>);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="将表转换成DataSet"><a href="#将表转换成DataSet" class="headerlink" title="将表转换成DataSet"></a>将表转换成DataSet</h3><h4 id="scala-5"><a href="#scala-5" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">BatchTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将table转换成 DataSet</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableToDataSetScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> bbEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> bbTableEnv = <span class="type">BatchTableEnvironment</span>.create(bbEnv)</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         bbTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         <span class="keyword">val</span> table = bbTableEnv.from(<span class="string">"myTable"</span>)</span><br><span class="line">         <span class="comment">//将table转换为DataSet</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> set = bbTableEnv.toDataSet[<span class="type">Row</span>](table)</span><br><span class="line">         set.map(row=&gt;(row.getField(<span class="number">0</span>).toString.toInt,row.getField(<span class="number">1</span>).toString))</span><br><span class="line">         .print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121153273.png" alt="image-20230412115320971"></p><h4 id="java-5"><a href="#java-5" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 将table转换成 DataSet</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableToDataSetJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         ExecutionEnvironment bbEnv = ExecutionEnvironment.getExecutionEnviron</span><br><span class="line">         BatchTableEnvironment bbTableEnv = BatchTableEnvironment.create(bbEnv</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         bbTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>);</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         Table table = bbTableEnv.from(<span class="string">"myTable"</span>);</span><br><span class="line">         <span class="comment">//将table转换为DataSet</span></span><br><span class="line">         DataSet&lt;Row&gt; set = bbTableEnv.toDataSet(table, Row<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">         set.map(<span class="keyword">new</span> MapFunction&lt;Row, Tuple2&lt;Integer,String&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, String&gt; <span class="title">map</span><span class="params">(Row row)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">int</span> id = Integer.parseInt(row.getField(<span class="number">0</span>).toString());</span><br><span class="line">         String name = row.getField(<span class="number">1</span>).toString();</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, String&gt;(id, name);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataSetAPI-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataSetAPI-4.html</id>
    <published>2023-04-08T15:21:40.000Z</published>
    <updated>2023-04-20T06:52:42.408Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-Flink核心API之DataSetAPI-4"><a href="#第十六周-Flink极速上手篇-Flink核心API之DataSetAPI-4" class="headerlink" title="第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4"></a>第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4</h1><h2 id="DataSet-API"><a href="#DataSet-API" class="headerlink" title="DataSet API"></a>DataSet API</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataSet API主要可以分为3块来分析：DataSource、Transformation、Sink。</span><br><span class="line"></span><br><span class="line">DataSource是程序的数据源输入。</span><br><span class="line">Transformation是具体的操作，它对一个或多个输入数据源进行计算处理，例如map、flatMap、filter等操作。</span><br><span class="line"></span><br><span class="line">DataSink是程序的输出，它可以把Transformation处理之后的数据输出到指定的存储介质中。</span><br></pre></td></tr></table></figure><h3 id="DataSet-API之DataSource"><a href="#DataSet-API之DataSource" class="headerlink" title="DataSet API之DataSource"></a>DataSet API之DataSource</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对DataSet批处理而言，其实最多的就是读取HDFS中的文件数据，所以在这里我们主要介绍两个DataSource组件。</span><br><span class="line"></span><br><span class="line">基于集合</span><br><span class="line">fromCollection(Collection)，主要是为了方便测试使用。它的用法和DataStreamAPI中的用法一样，我们已经用过很多次了。</span><br><span class="line"></span><br><span class="line">基于文件</span><br><span class="line">readTextFile(path)，读取hdfs中的数据文件。这个前面我们也使用过了。</span><br></pre></td></tr></table></figure><h3 id="DataSet-API之Transformation"><a href="#DataSet-API之Transformation" class="headerlink" title="DataSet API之Transformation"></a>DataSet API之Transformation</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">算子 解释</span><br><span class="line">map 输入一个元素进行处理，返回一个元素</span><br><span class="line">mapPartition 类似map，一次处理一个分区的数据</span><br><span class="line">flatMap 输入一个元素进行处理，可以返回多个元素</span><br><span class="line">filter 对数据进行过滤，符合条件的数据会被留下</span><br><span class="line">reduce 对当前元素和上一次的结果进行聚合操作</span><br><span class="line">aggregate sum(),min(),max()等</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里面的算子我们都是比较熟悉的，在前面DatatreamAPI中都用过，用法都是一样的，所以在这就不再演示了</span><br><span class="line">mapPartition这个算子我们在Flink中还没用过，不过在Spark中是用过的，用法也是一样的</span><br><span class="line"></span><br><span class="line">其实mapPartition就是一次处理一批数据，如果在处理数据的时候想要获取第三方资源连接，建议使用mapPartition，这样可以一批数据获取一次连接，提高性能。</span><br><span class="line">下面来演示一下Flink中mapPartition的使用</span><br></pre></td></tr></table></figure><h4 id="mapPartition"><a href="#mapPartition" class="headerlink" title="mapPartition"></a>mapPartition</h4><h5 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MapPartition的使用：一次处理一个分区的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchMapPartitionScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//生成数据源数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="string">"hello you"</span>, <span class="string">"hello me"</span>))</span><br><span class="line">         <span class="comment">//每次处理一个分区的数据</span></span><br><span class="line">         text.mapPartition(it=&gt;&#123;</span><br><span class="line">         <span class="comment">//可以在此处创建数据库连接，建议把这块代码放到try-catch代码块中</span></span><br><span class="line">         <span class="comment">//注意：此时是每个分区获取一个数据库连接，不需要每处理一条数据就获取一次连接，</span></span><br><span class="line">         <span class="keyword">val</span> res = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">         it.foreach(line=&gt;&#123;</span><br><span class="line">         <span class="keyword">val</span> words = line.split(<span class="string">" "</span>)</span><br><span class="line">         <span class="keyword">for</span>(word &lt;- words)&#123;</span><br><span class="line">         res.append(word)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         res</span><br><span class="line">         <span class="comment">//关闭数据库连接</span></span><br><span class="line">         &#125;).print()</span><br><span class="line">         <span class="comment">//No new data sinks have been defined since the last execution.</span></span><br><span class="line">         <span class="comment">//The last execution refers to the latest call to 'execute()', 'count()', </span></span><br><span class="line">         <span class="comment">//注意：针对DataSetAPI，如果在后面调用的是count、collect、print，则最后不需要指定execute即可</span></span><br><span class="line">         <span class="comment">//env.execute("BatchMapPartitionScala")</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapPartitionFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MapPartition的使用：一次处理一个分区的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchMapPartitionJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//生成数据源数据</span></span><br><span class="line">         DataSource&lt;String&gt; text = env.fromCollection(Arrays.asList(<span class="string">"hello you"</span>,<span class="string">"hello me"</span>));</span><br><span class="line">         <span class="comment">//每次处理一个分区的数据</span></span><br><span class="line">         text.mapPartition(<span class="keyword">new</span> MapPartitionFunction&lt;String, String&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mapPartition</span><span class="params">(Iterable&lt;String&gt; iterable, Collector&lt;String&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">//可以在此处创建数据库连接，建议把这块代码放到try-catch代码块中</span></span><br><span class="line">             Iterator&lt;String&gt; it = iterable.iterator();</span><br><span class="line">         <span class="keyword">while</span>(it.hasNext())&#123;</span><br><span class="line">             String line = it.next();</span><br><span class="line">             String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">             <span class="keyword">for</span>(String word: words)&#123;</span><br><span class="line">             out.collect(word);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//关闭数据库连接</span></span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">下面还有一些transformation算子</span><br><span class="line">算子 解释</span><br><span class="line">distinct 返回数据集中去重之后的元素</span><br><span class="line">join 内连接</span><br><span class="line">outerJoin 外连接</span><br><span class="line">cross 获取两个数据集的笛卡尔积</span><br><span class="line">union 返回多个数据集的总和，数据类型需要一致</span><br><span class="line">first-n 获取集合中的前N个元素</span><br><span class="line">distinct算子比较简单，就是对数据进行全局去重。</span><br><span class="line">join：内连接，可以连接两份数据集</span><br></pre></td></tr></table></figure><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><h5 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * join：内连接</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchJoinScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mick"</span>)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"bj"</span>), (<span class="number">2</span>, <span class="string">"sh"</span>), (<span class="number">4</span>, <span class="string">"gz"</span>)))</span><br><span class="line">         <span class="comment">//对两份数据集执行join操作</span></span><br><span class="line">         text1.join(text2)</span><br><span class="line">         <span class="comment">//注意：这里的where和equalTo实现了类似于on fieldA=fieldB的效果</span></span><br><span class="line">         <span class="comment">//where：指定左边数据集中参与比较的元素角标</span></span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         <span class="comment">//equalTo指定右边数据集中参与比较的元素角标</span></span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;(first,second)=&gt;&#123;</span><br><span class="line">             (first._1,first._2,second._2)</span><br><span class="line">         &#125;&#125;.print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101353385.png" alt="image-20230410135332014"></p><h5 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.JoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * join：内连接</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchJoinJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data1 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text1 = env.fromCollection(data1)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data2 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"bj"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"sh"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">4</span>,<span class="string">"gz"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text2 = env.fromCollection(data2)</span><br><span class="line">         <span class="comment">//对两份数据集执行join操作</span></span><br><span class="line">         text1.join(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         <span class="comment">//三个输入参数：</span></span><br><span class="line">         <span class="comment">//第一个tuple2是左边数据集的类型，</span></span><br><span class="line">         <span class="comment">//第二个tuple2是右边数据集的类型，</span></span><br><span class="line">         <span class="comment">//第三个tuple3是此函数返回的数据集类型</span></span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Integer,String&gt;,Tuple3&lt;Integer,String,String&gt;&gt;()</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Integer,String&gt; secondfirst,Tuple2&lt;Integer,String</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         return new Tuple3&lt;Integer, String, String&gt;(first.f0,f</span></span></span><br><span class="line"><span class="function"><span class="params">         &#125;</span></span></span><br><span class="line"><span class="function"><span class="params">         &#125;)</span>.<span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outerJoin：外连接</span><br></pre></td></tr></table></figure><h4 id="outerJoin"><a href="#outerJoin" class="headerlink" title="outerJoin"></a>outerJoin</h4><h5 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * outerJoin：外连接</span></span><br><span class="line"><span class="comment"> * 一共有三种情况</span></span><br><span class="line"><span class="comment"> * 1：leftOuterJoin</span></span><br><span class="line"><span class="comment"> * 2：rightOuterJoin</span></span><br><span class="line"><span class="comment"> * 3：fullOuterJoin</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchOuterJoinScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mick"</span>)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"bj"</span>), (<span class="number">2</span>, <span class="string">"sh"</span>), (<span class="number">4</span>, <span class="string">"gz"</span>)))</span><br><span class="line">         <span class="comment">//对两份数据集执行leftOuterJoin操作</span></span><br><span class="line">         text1.leftOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;</span><br><span class="line">         (first,second)=&gt;&#123;</span><br><span class="line">         <span class="comment">//注意：second中的元素可能为null</span></span><br><span class="line">         <span class="keyword">if</span>(second==<span class="literal">null</span>)&#123;</span><br><span class="line">         (first._1,first._2,<span class="string">"null"</span>)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         (first._1,first._2,second._2)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;.print()</span><br><span class="line">         println(<span class="string">"========================================"</span>)</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.rightOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;</span><br><span class="line">         (first,second)=&gt;&#123;</span><br><span class="line">         <span class="comment">//注意：first中的元素可能为null</span></span><br><span class="line">         <span class="keyword">if</span>(first==<span class="literal">null</span>)&#123;</span><br><span class="line">             (second._1,<span class="string">"null"</span>,second._2)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         (first._1,first._2,second._2)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;.print()</span><br><span class="line">         println(<span class="string">"========================================"</span>)</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.fullOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;</span><br><span class="line">         (first,second)=&gt;&#123;</span><br><span class="line">         <span class="comment">//注意：first和second中的元素都有可能为null</span></span><br><span class="line">             <span class="keyword">if</span>(first==<span class="literal">null</span>)&#123;</span><br><span class="line">         (second._1,<span class="string">"null"</span>,second._2)</span><br><span class="line">         &#125;<span class="keyword">else</span> <span class="keyword">if</span>(second==<span class="literal">null</span>)&#123;</span><br><span class="line">         (first._1,first._2,<span class="string">"null"</span>)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         (first._1,first._2,second._2)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;.print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101516270.png" alt="image-20230410151623671"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101516094.png" alt="image-20230410151648029"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101517219.png" alt="image-20230410151719016"></p><h5 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.JoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * outerJoin：外连接</span></span><br><span class="line"><span class="comment"> * 一共有三种情况</span></span><br><span class="line"><span class="comment"> * 1：leftOuterJoin</span></span><br><span class="line"><span class="comment"> * 2：rightOuterJoin</span></span><br><span class="line"><span class="comment"> * 3：fullOuterJoin</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchOuterJoinJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironme</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data1 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text1 = env.fromCollection(data1)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data2 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"bj"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"sh"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">4</span>,<span class="string">"gz"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text2 = env.fromCollection(data2)</span><br><span class="line">         <span class="comment">//对两份数据集执行leftOuterJoin操作</span></span><br><span class="line">         text1.leftOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Intege</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Intege</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">if</span>(second==<span class="keyword">null</span>)</span></span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">                                                    &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         System.out.println(<span class="string">"=============================================="</span>);</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.rightOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Intege</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Intege</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">if</span>(first==<span class="keyword">null</span>)</span></span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(second</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         System.out.println(<span class="string">"=============================================="</span>);</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.fullOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Intege</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Intege</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">if</span>(first==<span class="keyword">null</span>)</span></span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(second</span><br><span class="line">         &#125;<span class="keyword">else</span> <span class="keyword">if</span>(second==<span class="keyword">null</span>)&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="cross"><a href="#cross" class="headerlink" title="cross"></a>cross</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross：获取两个数据集的笛卡尔积</span><br></pre></td></tr></table></figure><h5 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * cross：获取两个数据集的笛卡尔积</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchCrossScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化第一份数据</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">         <span class="comment">//初始化第二份数据</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>(<span class="string">"a"</span>, <span class="string">"b"</span>))</span><br><span class="line">         <span class="comment">//执行cross操作</span></span><br><span class="line">             text1.cross(text2).print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101509307.png" alt="image-20230410150858841"></p><h5 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * cross：获取两个数据集的笛卡尔积</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchCrossJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironme</span><br><span class="line">         <span class="comment">//初始化第一份数据</span></span><br><span class="line">         DataSource&lt;Integer&gt; text1 = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>));</span><br><span class="line">         <span class="comment">//初始化第二份数据</span></span><br><span class="line">         DataSource&lt;String&gt; text2 = env.fromCollection(Arrays.asList(<span class="string">"a"</span>, <span class="string">"b"</span>)</span><br><span class="line">         <span class="comment">//执行cross操作</span></span><br><span class="line">         text1.cross(text2).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">union：返回两个数据集的总和，数据类型需要一致</span><br><span class="line">和DataStreamAPI中的union操作功能一样</span><br><span class="line">first-n：获取集合中的前N个元素</span><br></pre></td></tr></table></figure><h4 id="first-n"><a href="#first-n" class="headerlink" title="first-n"></a>first-n</h4><h5 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.<span class="type">Order</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * first-n：获取集合中的前N个元素</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchFirstNScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> data = <span class="type">ListBuffer</span>[<span class="type">Tuple2</span>[<span class="type">Int</span>,<span class="type">String</span>]]()</span><br><span class="line">         data.append((<span class="number">2</span>,<span class="string">"zs"</span>))</span><br><span class="line">         data.append((<span class="number">4</span>,<span class="string">"ls"</span>))</span><br><span class="line">         data.append((<span class="number">3</span>,<span class="string">"ww"</span>))</span><br><span class="line">         data.append((<span class="number">1</span>,<span class="string">"aw"</span>))</span><br><span class="line">         data.append((<span class="number">1</span>,<span class="string">"xw"</span>))</span><br><span class="line">         data.append((<span class="number">1</span>,<span class="string">"mw"</span>))</span><br><span class="line">        <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(data)</span><br><span class="line">         <span class="comment">//获取前3条数据，按照数据插入的顺序</span></span><br><span class="line">         text.first(<span class="number">3</span>).print()</span><br><span class="line">         println(<span class="string">"=================================="</span>)</span><br><span class="line">         <span class="comment">//根据数据中的第一列进行分组，获取每组的前2个元素</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).first(<span class="number">2</span>).print()</span><br><span class="line">         println(<span class="string">"=================================="</span>)</span><br><span class="line">         <span class="comment">//根据数据中的第一列分组，再根据第二列进行组内排序[倒序],获取每组的前2个元素</span></span><br><span class="line">         <span class="comment">//分组排序取TopN</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).sortGroup(<span class="number">1</span>,<span class="type">Order</span>.<span class="type">DESCENDING</span>).first(<span class="number">2</span>).print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.Order;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * first-n：获取集合中的前N个元素</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchFirstNJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironme</span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"zs"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">4</span>,<span class="string">"ls"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"ww"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"aw"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"xw"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"mw"</span>));</span><br><span class="line">         <span class="comment">//初始化数据</span></span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text = env.fromCollection(data);</span><br><span class="line">         <span class="comment">//获取前3条数据，按照数据插入的顺序</span></span><br><span class="line">         text.first(<span class="number">3</span>).print();</span><br><span class="line">         System.out.println(<span class="string">"===================================="</span>);</span><br><span class="line">         <span class="comment">//根据数据中的第一列进行分组，获取每组的前2个元素</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).first(<span class="number">2</span>).print();</span><br><span class="line">         System.out.println(<span class="string">"===================================="</span>);</span><br><span class="line">         <span class="comment">//根据数据中的第一列分组，再根据第二列进行组内排序[倒序],获取每组的前2个元素</span></span><br><span class="line">         <span class="comment">//分组排序取TopN</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).sortGroup(<span class="number">1</span>, Order.DESCENDING).first(<span class="number">2</span>).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="DataSet-API之DataSink"><a href="#DataSet-API之DataSink" class="headerlink" title="DataSet API之DataSink"></a>DataSet API之DataSink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Flink针对DataSet提供了一些已经实现好的数据目的地</span><br><span class="line">其中最常见的是向HDFS中写入数据</span><br><span class="line"></span><br><span class="line">writeAsText()：将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取</span><br><span class="line">writeAsCsv()：将元组以逗号分隔写入文件中，行及字段之间的分隔是可配置的，每个字段的值来自对象的toString()方法</span><br><span class="line"></span><br><span class="line">还有一个是print：打印每个元素的toString()方法的值</span><br><span class="line">这个print是测试的时候使用的。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataStreamAPI-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataStreamAPI-3.html</id>
    <published>2023-04-08T15:20:20.000Z</published>
    <updated>2023-04-20T07:11:33.646Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-Flink核心API之DataStreamAPI-3"><a href="#第十六周-Flink极速上手篇-Flink核心API之DataStreamAPI-3" class="headerlink" title="第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3"></a>第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3</h1><h2 id="Flink核心API"><a href="#Flink核心API" class="headerlink" title="Flink核心API"></a>Flink核心API</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091728382.png" alt="image-20230409172217160"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Flink中提供了4种不同层次的API，每种API在简洁和易表达之间有自己的权衡，适用于不同的场景。目前上面3个会用得比较多。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">低级API(Stateful Stream Processing)：提供了对时间和状态的细粒度控制，简洁性和易用性较差，主要应用在一些复杂事件处理逻辑上。</span><br><span class="line"></span><br><span class="line">核心API(DataStream&#x2F;DataSet API)：主要提供了针对流数据和批数据的处理，是对低级API进行了一些封装，提供了filter、sum、max、min等高级函数，简单易用，所以这些API在工作中应用还是比较广泛的。</span><br><span class="line"></span><br><span class="line">Table API：一般与DataSet或者DataStream紧密关联，可以通过一个DataSet或DataStream创建出一个Table，然后再使用类似于filter, join,或者select这种操作。最后还可以将一个Table对象转成</span><br><span class="line">DataSet或DataStream。</span><br><span class="line"></span><br><span class="line">SQL：Flink的SQL底层是基于Apache Calcite，Apache Calcite实现了标准的SQL，使用起来比其他API更加灵活，因为可以直接使用SQL语句。Table API和SQL可以很容易地结合在一块使用，因为它们都返回Table对象。</span><br><span class="line"></span><br><span class="line">针对这些API我们主要学习下面这些</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091729767.png" alt="image-20230409172651151"></p><h3 id="DataStream-API"><a href="#DataStream-API" class="headerlink" title="DataStream API"></a>DataStream API</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream API主要分为3块：DataSource、Transformation、DataSink。</span><br><span class="line"></span><br><span class="line">DataSource是程序的输入数据源。</span><br><span class="line">Transformation是具体的操作，它对一个或多个输入数据源进行计算处理，例如map、flatMap和filter等操作。</span><br><span class="line"></span><br><span class="line">DataSink是程序的输出，它可以把Transformation处理之后的数据输出到指定的存储介质中。</span><br></pre></td></tr></table></figure><h4 id="DataStream-API之DataSoure"><a href="#DataStream-API之DataSoure" class="headerlink" title="DataStream API之DataSoure"></a>DataStream API之DataSoure</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataSource是程序的输入数据源，Flink提供了大量内置的DataSource，也支持自定义DataSource，不过目前Flink提供的这些已经足够我们正常使用了。</span><br><span class="line">Flink提供的内置输入数据源：包括基于socket、基于Collection</span><br><span class="line">还有就是Flink还提供了一批Connectors，可以实现读取第三方数据源，</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091734753.png" alt="image-20230409173429387"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Flink内置：表示Flink中默认自带的。</span><br><span class="line"></span><br><span class="line">Apache Bahir：表示需要添加这个依赖包之后才能使用的。</span><br><span class="line"></span><br><span class="line">针对source的这些Connector，我们在实际工作中最常用的就是Kafka</span><br><span class="line">当程序出现错误的时候，Flink的容错机制能恢复并继续运行程序，这种错误包括机器故障、网络故障、程序故障等</span><br><span class="line"></span><br><span class="line">针对Flink提供的常用数据源接口，如果程序开启了checkpoint快照机制，Flink可以提供这些容错性保证</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091735114.png" alt="image-20230409173548905"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对这些常用的DataSouce，基于socket的我们之前已经用过了，下面我们来看一下基于Collection集合的。</span><br><span class="line"></span><br><span class="line">针对Kafka的这个我们在后面会详细分析，在这里先不讲。</span><br><span class="line"></span><br><span class="line">由于我们后面还会学到批处理的功能，所以在项目里面创建几个包，把流处理和批处理的代码分开，后期看起来比较清晰。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091750998.png" alt="image-20230409174935218"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来在 com.imooc.scala.stream 里面创建一个包：source，将代码放到source包里面</span><br></pre></td></tr></table></figure><h5 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.source</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于collection的source的使用</span></span><br><span class="line"><span class="comment"> * 注意：这个source的主要应用场景是模拟测试代码流程的时候使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamCollectionSourceScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//使用collection集合生成DataStream</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">         text.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamCollectionSource"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.source;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于collection的source的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamCollectionSourceJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">             StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//使用collection集合生成DataStream</span></span><br><span class="line">         DataStreamSource&lt;Integer&gt; text = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>));</span><br><span class="line">         text.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">     env.execute(<span class="string">"StreamCollectionSourceJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="DataStream-API之Transformation"><a href="#DataStream-API之Transformation" class="headerlink" title="DataStream API之Transformation"></a>DataStream API之Transformation</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transformation是Flink程序的计算算子，负责对数据进行处理，Flink提供了大量的算子，其实Flink中的大部分算子的使用和spark中算子的使用是一样的，下面我们来看一下：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">算子 解释</span><br><span class="line">map 输入一个元素进行处理，返回一个元素</span><br><span class="line">flatMap 输入一个元素进行处理，可以返回多个元素</span><br><span class="line">filter 对数据进行过滤，符合条件的数据会被留下</span><br><span class="line">keyBy 根据key分组，相同key的数据会进入同一个分区</span><br><span class="line">reduce 对当前元素和上一次的结果进行聚合操作</span><br><span class="line">aggregations sum(),min(),max()等</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这里面的算子的用法其实和spark中对应算子的用法是一致的，这里面的map、flatmap、keyBy、reduce、sum这些算子我们都用过了。</span><br><span class="line">所以这里面的算子就不再单独演示了。</span><br><span class="line">往下面看。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">算子 解释</span><br><span class="line">union 合并多个流，多个流的数据类型必须一致</span><br><span class="line">connect 只能连接两个流，两个流的数据类型可以不同</span><br><span class="line">split 根据规则把一个数据流切分为多个流</span><br><span class="line">shuffle 随机分区</span><br><span class="line">rebalance 对数据集进行再平衡，重分区，消除数据倾斜</span><br><span class="line">rescale 重分区</span><br><span class="line">partitionCustom 自定义分区</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这里面的算子我们需要分析一下。</span><br><span class="line">union：表示合并多个流，但是多个流的数据类型必须一致</span><br><span class="line">多个流join之后，就变成了一个流</span><br><span class="line">应用场景：多种数据源的数据类型一致，数据处理规则也一致</span><br></pre></td></tr></table></figure><h5 id="union"><a href="#union" class="headerlink" title="union"></a>union</h5><h6 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 合并多个流，多个流的数据类型必须一致</span></span><br><span class="line"><span class="comment"> * 应用场景：多种数据源的数据类型一致，数据处理规则也一致</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamUnionScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>(<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br><span class="line">         <span class="comment">//合并流</span></span><br><span class="line">         <span class="keyword">val</span> unionStream = text1.union(text2)</span><br><span class="line">         <span class="comment">//打印流中的数据</span></span><br><span class="line">         unionStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamUnionScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 合并多个流，多个流的数据类型必须一致</span></span><br><span class="line"><span class="comment"> * 应用场景：多种数据源的数据类型一致，数据处理规则也一致</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamUnionJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         DataStreamSource&lt;Integer&gt; text1 = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         DataStreamSource&lt;Integer&gt; text2 = env.fromCollection(Arrays.asList(<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>));</span><br><span class="line">         <span class="comment">//合并流</span></span><br><span class="line">         DataStream&lt;Integer&gt; unionStream = text1.union(text2);</span><br><span class="line">         <span class="comment">//打印流中的数据</span></span><br><span class="line">         unionStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamUnionJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">connect：只能连接两个流，两个流的数据类型可以不同</span><br><span class="line"></span><br><span class="line">两个流被connect之后，只是被放到了同一个流中，它们内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。</span><br><span class="line"></span><br><span class="line">connect方法会返回connectedStream，在connectedStream中需要使用CoMap、CoFlatMap这种函数，类似于map和flatmap</span><br></pre></td></tr></table></figure><h6 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.<span class="type">CoMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 只能连接两个流，两个流的数据类型可以不同</span></span><br><span class="line"><span class="comment"> * 应用：可以将两种不同格式的数据统一成一种格式</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamConnectScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromElements(<span class="string">"user:tom,age:18"</span>)</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromElements(<span class="string">"user:jack_age:20"</span>)</span><br><span class="line">         <span class="comment">//连接两个流</span></span><br><span class="line">         <span class="keyword">val</span> connectStream = text1.connect(text2)</span><br><span class="line">         connectStream.map(<span class="keyword">new</span> <span class="type">CoMapFunction</span>[<span class="type">String</span>,<span class="type">String</span>,<span class="type">String</span>] &#123;</span><br><span class="line">         <span class="comment">//处理第1份数据流中的数据</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map1</span></span>(value: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">         value.replace(<span class="string">","</span>,<span class="string">"-"</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//处理第2份数据流中的数据</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map2</span></span>(value: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">         value.replace(<span class="string">"_"</span>,<span class="string">"-"</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamConnectScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091821183.png" alt="image-20230409182145286"></p><h6 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.ConnectedStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.CoMapFunction;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 只能连接两个流，两个流的数据类型可以不同</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamConnectJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text1 = env.fromElements(<span class="string">"user:tom,age:18"</span>);</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text2 = env.fromElements(<span class="string">"user:jack_age:20"</span>);</span><br><span class="line">         <span class="comment">//连接两个流</span></span><br><span class="line">         ConnectedStreams&lt;String, String&gt; connectStream = text1.connect(text2);</span><br><span class="line">         connectStream.map(<span class="keyword">new</span> CoMapFunction&lt;String, String, String&gt;() &#123;</span><br><span class="line">         <span class="comment">//处理第1份数据流中的数据</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">map1</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> value.replace(<span class="string">","</span>,<span class="string">"-"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//处理第2份数据流中的数据</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">map2</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> value.replace(<span class="string">"_"</span>,<span class="string">"-"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamConnectJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="split"><a href="#split" class="headerlink" title="split"></a>split</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">split：根据规则把一个数据流切分为多个流</span><br><span class="line"></span><br><span class="line">注意：split只能分一次流，切分出来的流不能继续分流</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">split需要和select配合使用，选择切分后的流</span><br><span class="line">应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑</span><br></pre></td></tr></table></figure><h6 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation</span><br><span class="line"><span class="keyword">import</span> java.&#123;lang, util&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.collector.selector.<span class="type">OutputSelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据规则把一个数据流切分为多个流</span></span><br><span class="line"><span class="comment"> * 注意：split只能分一次流，切分出来的流不能继续分流</span></span><br><span class="line"><span class="comment"> * split需要和select配合使用，选择切分后的流</span></span><br><span class="line"><span class="comment"> * 应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamSplitScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         <span class="keyword">val</span> splitStream = text.split(<span class="keyword">new</span> <span class="type">OutputSelector</span>[<span class="type">Int</span>] &#123;</span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(value: <span class="type">Int</span>): lang.<span class="type">Iterable</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">         <span class="keyword">val</span> list = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">String</span>]()</span><br><span class="line">         <span class="keyword">if</span>(value % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">         list.add(<span class="string">"even"</span>)<span class="comment">//偶数</span></span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         list.add(<span class="string">"odd"</span>)<span class="comment">//奇数</span></span><br><span class="line">         &#125;</span><br><span class="line">         list</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//选择流</span></span><br><span class="line">         <span class="keyword">val</span> evenStream = splitStream.select(<span class="string">"even"</span>)</span><br><span class="line">         evenStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//二次切分流会报错</span></span><br><span class="line">         <span class="comment">//Consecutive multiple splits are not supported. Splits are deprecated. Pl</span></span><br><span class="line">         <span class="comment">/*val lowHighStream = evenStream.split(new OutputSelector[Int] &#123;</span></span><br><span class="line"><span class="comment">         override def select(value: Int): lang.Iterable[String] = &#123;</span></span><br><span class="line"><span class="comment">         val list = new util.ArrayList[String]()</span></span><br><span class="line"><span class="comment">         if(value &lt;= 5)&#123;</span></span><br><span class="line"><span class="comment">         list.add("low");</span></span><br><span class="line"><span class="comment">         &#125;else&#123;</span></span><br><span class="line"><span class="comment">         list.add("high")</span></span><br><span class="line"><span class="comment">         &#125;</span></span><br><span class="line"><span class="comment">         list</span></span><br><span class="line"><span class="comment">         &#125;</span></span><br><span class="line"><span class="comment">         &#125;)</span></span><br><span class="line"><span class="comment">         val lowStream = lowHighStream.select("low")</span></span><br><span class="line"><span class="comment">         lowStream.print().setParallelism(1)*/</span></span><br><span class="line">         env.execute(<span class="string">"StreamSplitScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.collector.selector.OutputSelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SplitStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据规则把一个数据流切分为多个流</span></span><br><span class="line"><span class="comment"> * 注意：split只能分一次流，切分出来的流不能继续分流</span></span><br><span class="line"><span class="comment"> * split需要和select配合使用，选择切分后的流</span></span><br><span class="line"><span class="comment"> * 应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamSplitJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">             DataStreamSource&lt;Integer&gt; text = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>));</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         SplitStream&lt;Integer&gt; splitStream = text.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">         ArrayList&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         <span class="keyword">if</span> (value % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">         list.add(<span class="string">"even"</span>);<span class="comment">//偶数</span></span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         list.add(<span class="string">"odd"</span>);<span class="comment">//奇数</span></span><br><span class="line">         &#125;</span><br><span class="line">         <span class="keyword">return</span> list;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//选择流</span></span><br><span class="line">         DataStream&lt;Integer&gt; evenStream = splitStream.select(<span class="string">"even"</span>);</span><br><span class="line">         evenStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamSplitJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">目前split切分的流无法进行二次切分，并且split方法已经标记为过时了，官方不推荐使用，现在官方推荐使用side output的方式实现。</span><br></pre></td></tr></table></figure><h5 id="side-output"><a href="#side-output" class="headerlink" title="side output"></a>side output</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我来看一下使用side output如何实现流的多次切分</span><br></pre></td></tr></table></figure><h6 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation</span><br><span class="line"><span class="keyword">import</span> java.&#123;lang, util&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.collector.selector.<span class="type">OutputSelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">ProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">OutputTag</span>, <span class="type">StreamExecutionEnviron</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用sideoutput切分流</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamSideOutputScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         <span class="comment">//首先定义两个sideoutput来准备保存切分出来的数据</span></span><br><span class="line">         <span class="keyword">val</span> outputTag1 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"even"</span>)&#123;&#125;<span class="comment">//保存偶数</span></span><br><span class="line">         <span class="keyword">val</span> outputTag2 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"odd"</span>)&#123;&#125;<span class="comment">//保存奇数</span></span><br><span class="line">         <span class="comment">//注意：process属于Flink中的低级api</span></span><br><span class="line">         <span class="keyword">val</span> outputStream = text.process(<span class="keyword">new</span> <span class="type">ProcessFunction</span>[<span class="type">Int</span>,<span class="type">Int</span>] &#123;</span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">Int</span>, ctx: <span class="type">ProcessFunction</span>[<span class="type">Int</span>, <span class="type">Int</span>]#<span class="type">Context</span>,out:<span class="type">Collector</span>[<span class="type">Int</span>]):</span><br><span class="line">         <span class="keyword">if</span>(value % <span class="number">2</span> == <span class="number">0</span> )&#123;</span><br><span class="line">         ctx.output(outputTag1,value)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         ctx.output(outputTag2,value)</span><br><span class="line">         &#125; </span><br><span class="line">                                     &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//获取偶数数据流</span></span><br><span class="line">         <span class="keyword">val</span> evenStream = outputStream.getSideOutput(outputTag1)</span><br><span class="line">         <span class="comment">//获取奇数数据流</span></span><br><span class="line">         <span class="keyword">val</span> oddStream = outputStream.getSideOutput(outputTag2)</span><br><span class="line">         <span class="comment">//evenStream.print().setParallelism(1)</span></span><br><span class="line">         <span class="comment">//对evenStream流进行二次切分</span></span><br><span class="line">         <span class="keyword">val</span> outputTag11 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"low"</span>)&#123;&#125;<span class="comment">//保存小于等五5的数字</span></span><br><span class="line">         <span class="keyword">val</span> outputTag12 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"high"</span>)&#123;&#125;<span class="comment">//保存大于5的数字</span></span><br><span class="line">         <span class="keyword">val</span> subOutputStream = evenStream.process(<span class="keyword">new</span> <span class="type">ProcessFunction</span>[<span class="type">Int</span>,<span class="type">Int</span>] &#123;</span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">Int</span>, ctx: <span class="type">ProcessFunction</span>[<span class="type">Int</span>, <span class="type">Int</span>]#<span class="type">Context</span>,<span class="type">Collector</span>[<span class="type">Int</span>]):</span><br><span class="line">         <span class="keyword">if</span>(value&lt;=<span class="number">5</span>)&#123;</span><br><span class="line">          ctx.output(outputTag11,value)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         ctx.output(outputTag12,value)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//获取小于等于5的数据流</span></span><br><span class="line">         <span class="keyword">val</span> lowStream = subOutputStream.getSideOutput(outputTag11)</span><br><span class="line">         <span class="comment">//获取大于5的数据流</span></span><br><span class="line">         <span class="keyword">val</span> highStream = subOutputStream.getSideOutput(outputTag12)</span><br><span class="line">         lowStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamSideOutputScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092310468.png" alt="image-20230409231015815"></p><h6 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.OutputTag;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用sideoutput切分流</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamSideoutputJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecuti</span><br><span class="line">         DataStreamSource&lt;Integer&gt; text = env.fromCollection(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>));</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         <span class="comment">//首先定义两个sideoutput来准备保存切分出来的数据</span></span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag1 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"even"</span>) &#123;&#125;;</span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag2 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"odd"</span>) &#123;&#125;;</span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; outputStream = text.process(<span class="keyword">new</span> ProcessFunction&lt;Integer,Integer&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer value, Context ctx, Collector&lt;Integer&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (value % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">         ctx.output(outputTag1, value);</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         ctx.output(outputTag2, value);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//获取偶数数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; evenStream = outputStream.getSideOutput(outputTag1);</span><br><span class="line">         <span class="comment">//获取奇数数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; oddStream = outputStream.getSideOutput(outputTag2);</span><br><span class="line">         <span class="comment">//对evenStream流进行二次切分</span></span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag11 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"low"</span>) &#123;&#125;;</span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag12 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"high"</span>) &#123;&#125;;</span><br><span class="line">         SingleOutputStreamOperator&lt;Integer&gt; subOutputStream = evenStream.process(<span class="keyword">new</span> ProcessFunction&lt;Integer,Integer&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer value, Context ctx, Collector&lt;Integer&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (value &lt;= <span class="number">5</span>) &#123;</span><br><span class="line">         ctx.output(outputTag11, value);</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         ctx.output(outputTag12, value);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//获取小于等于5的数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; lowStream = subOutputStream.getSideOutput(outputTag11);</span><br><span class="line">         <span class="comment">//获取大于5的数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; highStream = subOutputStream.getSideOutput(outputTag12);</span><br><span class="line">         lowStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamSideoutputJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">其实想要实现多级流切分，使用filter算子也是可以实现的，给大家留一个作业，大家可以下去实验一下。</span><br><span class="line">最后针对这几个算子总结一下：</span><br><span class="line">首先是union和connect的区别，如图所示：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092319871.png" alt="image-20230409231907397"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">union可以连接多个流，最后汇总成一个流，流里面的数据使用相同的计算规则</span><br><span class="line">connect值可以连接2个流，最后汇总成一个流，但是流里面的两份数据相互还是独立的，每一份数据使用一个计算规则</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后是流切分</span><br><span class="line">如果是只需要切分一次的话使用split或者side output都可以</span><br><span class="line">如果想要切分多次，就不能使用split了，需要使用side output</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092319304.png" alt="image-20230409231939157"></p><h5 id="分区相关算子"><a href="#分区相关算子" class="headerlink" title="分区相关算子"></a>分区相关算子</h5><h6 id="random"><a href="#random" class="headerlink" title="random"></a>random</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下这几个和分区相关的算子</span><br><span class="line">算子 解释</span><br><span class="line">random 随机分区</span><br><span class="line">rebalance对数据集进行再平衡，重分区，消除数据倾斜|</span><br><span class="line">rescale重分区</span><br><span class="line">custom partition 自定义分区</span><br><span class="line"></span><br><span class="line">random：随机分区，它表示将上游数据随机分发到下游算子实例的每个分区中，在代码层面体现是调用shuffle()函数</span><br><span class="line"></span><br><span class="line">查看源码，shuffle底层对应的是ShufflePartitioner这个类</span><br><span class="line">这个类里面有一个selectChannel函数，这个函数会计算数据将会被发送给哪个分区，里面使用的是random.nextInt，所以说是随机的。</span><br><span class="line"></span><br><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line"> return random.nextInt(numberOfChannels);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="rebalance"><a href="#rebalance" class="headerlink" title="rebalance"></a>rebalance</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rebalance：重新平衡分区(循环分区)，我觉得叫循环分区更好理解，它表示对数据集进行再平衡，消除数据倾斜，为每个分区创建相同的负载，其实就是通过循环的方式给下游算子实例的每个分区分配数据，在代码层面体现是调用rebalance()函数</span><br><span class="line"></span><br><span class="line">查看源码，rebalance底层对应的是RebalancePartitioner这个类</span><br><span class="line">这个类里面有一个setup和selectChannel函数，setup函数会根据分 区数初始化一个随机值nextChannelToSendTo，然后selectChannel函数会使用nextChannelToSendTo加1和分区数取模，把计算的值再赋给nextChannelToSendTo，后面以此类推，其实就可以实现向下游算子实例的多个分区循环发送数据了，这样每个分区获取到的数据基本一致。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public void setup(int numberOfChannels) &#123;</span><br><span class="line"> super.setup(numberOfChannels);</span><br><span class="line"> nextChannelToSendTo &#x3D; ThreadLocalRandom.current().nextInt(numberOfChannels)</span><br><span class="line">&#125;</span><br><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line">     nextChannelToSendTo &#x3D; (nextChannelToSendTo + 1) % numberOfChannels;</span><br><span class="line">     return nextChannelToSendTo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="rescale"><a href="#rescale" class="headerlink" title="rescale"></a>rescale</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rescale：重分区</span><br><span class="line">查看源码，rescale底层对应的是RescalePartitioner这个类</span><br><span class="line">这个类里面有一个selectChannel函数，这里面的numberOfChannels是分区数量，其实也可以认为是我们所说的算子的并行度，因为一个分区是由一个线程负责处理的，它们两个是一一对应的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line"> if (++nextChannelToSendTo &gt;&#x3D; numberOfChannels) &#123;</span><br><span class="line">  nextChannelToSendTo &#x3D; 0;</span><br><span class="line"> &#125;</span><br><span class="line"> return nextChannelToSendTo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">详细的解释在这个类的注释中也是有的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">* The subset of downstream operations to which the upstream operation sends</span><br><span class="line">* elements depends on the degree of parallelism of both the upstream and downs</span><br><span class="line">* For example, if the upstream operation has parallelism 2 and the downstream </span><br><span class="line">* has parallelism 4, then one upstream operation would distribute elements to </span><br><span class="line">* downstream operations while the other upstream operation would distribute to</span><br><span class="line">* two downstream operations. If, on the other hand, the downstream operation h</span><br><span class="line">* 2 while the upstream operation has parallelism 4 then two upstream operation</span><br><span class="line">* distribute to one downstream operation while the other two upstream operatio</span><br><span class="line">* distribute to the other downstream operations.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果上游操作有2个并发，而下游操作有4个并发，那么上游的1个并发结果循环分配给下游的2个并发操作，上游的另外1个并发结果循环分配给下游的另外2个并发操作。</span><br><span class="line">另一种情况，如果上游有4个并发操作，而下游有2个并发操作，那么上游的其中2个并发操作的结果会分配给下游的一个并发操作，而上游的另外2个并发操作的结果则分配给下游的另外1个并发操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：rescale与rebalance的区别是rebalance会产生全量重分区，而rescale不会。</span><br></pre></td></tr></table></figure><h6 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">broadcast：广播分区，将上游算子实例中的数据输出到下游算子实例的每个分区中，适合用于大数据集Join小数据集的场景，可以提高性能。</span><br><span class="line"></span><br><span class="line">查看源码，broadcast底层对应的是BroadcastPartitioner这个类</span><br><span class="line">看这个类中的selectChannel函数代码的注释，提示广播分区不支持选择Channel,因为会输出数据到下游的每个Channel中，就是发送到下游算子实例的每个分区中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Note: Broadcast mode could be handled directly for all the output channels</span><br><span class="line"> * in record writer, so it is no need to select channels via this method.</span><br><span class="line"> *&#x2F;</span><br><span class="line">@Override</span><br><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line"> throw new UnsupportedOperationException(&quot;Broadcast partitioner does not sup</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="custom-partition"><a href="#custom-partition" class="headerlink" title="custom partition"></a>custom partition</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">custom partition：自定义分区，可以按照自定义规则实现</span><br><span class="line">自定义分区需要实现Partitioner接口</span><br><span class="line"></span><br><span class="line">这是针对这几种分区的解释，下面来通过这个图总结一下，加深理解</span><br></pre></td></tr></table></figure><h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092345507.png" alt="image-20230409234549859"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092348975.png" alt="image-20230409234806351"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092350119.png" alt="image-20230409235046659"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后使用代码演示一下它们具体的用法</span><br></pre></td></tr></table></figure><h6 id="scala-5"><a href="#scala-5" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnviro</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 分区规则的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamPartitionOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="comment">//注意：默认情况下Fink任务中算子的并行度会读取当前机器的CPU个数</span></span><br><span class="line">         <span class="comment">//但fromCollection的并行度为1，由源码可知</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//注意：在这里建议将这个隐式转换代码放到类上面</span></span><br><span class="line">         <span class="comment">//因为默认它只在main函数生效，针对下面提取的shuffleOp是无效的，否则也需要在shuffleOp添加这行代码</span></span><br><span class="line">         <span class="comment">//import org.apache.flink.api.scala._</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用shuffle分区规则</span></span><br><span class="line">         <span class="comment">//shuffleOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用rebalance分区规则</span></span><br><span class="line">         <span class="comment">//rebalanceOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用rescale分区规则</span></span><br><span class="line">         <span class="comment">//rescaleOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用broadcast分区规则，此代码一共会打印40条数据，因为print的并行度为4</span></span><br><span class="line">         <span class="comment">//broadcastOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//自定义分区规则：根据数据的奇偶性进行分区</span></span><br><span class="line">         <span class="comment">//注意：此时虽然print算子的并行度是4，但是自定义的分区规则只会把数据分发给2个并行度，所以有两个不干活</span></span><br><span class="line">         <span class="comment">//custormPartitionOp(text)</span></span><br><span class="line">         env.execute(<span class="string">"StreamPartitionOpScala"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">custormPartitionOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         <span class="comment">//.partitionCustom(new MyPartitionerScala,0)//这种写法已经过期</span></span><br><span class="line">         .partitionCustom(<span class="keyword">new</span> <span class="type">MyPartitionerScala</span>, num =&gt; num)<span class="comment">//官方建议使用keyselector</span></span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">broadcastOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .broadcast</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">rescaleOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .rescale</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">rebalanceOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .rebalance</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">shuffleOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         <span class="comment">//由于fromCollection已经设置了并行度为1，所以需要再接一个算子之后才能修改并行度</span></span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .shuffle  </span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">Partitioner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义分区规则：按照数字的奇偶性进行分区</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitionerScala</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>[<span class="type">Int</span>]</span>&#123;</span><br><span class="line">     <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">partition</span></span>(key: <span class="type">Int</span>, numPartitions: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">         println(<span class="string">"分区总数："</span>+numPartitions)</span><br><span class="line">         <span class="keyword">if</span>(key % <span class="number">2</span> == <span class="number">0</span>)&#123;<span class="comment">//偶数分到0号分区</span></span><br><span class="line">         <span class="number">0</span></span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;<span class="comment">//奇数分到1号分区</span></span><br><span class="line">         <span class="number">1</span></span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">custom partition</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100026329.png" alt="image-20230410002655126"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">broadcast 此时每个数据都有四次输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100014718.png" alt="image-20230410001436246"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rescale  四个分区都有输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100013065.png" alt="image-20230410001313667"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rebalance 四个分区都有数据输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100010193.png" alt="image-20230410000959762"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shuffle  不一定每个分区都有数据输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100010853.png" alt="image-20230410001045259"></p><h6 id="java-5"><a href="#java-5" class="headerlink" title="java"></a>java</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.java.stream.transformation;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 分区规则的使用</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class StreamPartitionOpJava &#123;</span><br><span class="line">     public static void main(String[] args) throws Exception&#123;</span><br><span class="line">         StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecuti</span><br><span class="line">         DataStreamSource&lt;Integer&gt; text &#x3D; env.fromCollection(Arrays.asList(1, 2</span><br><span class="line">         &#x2F;&#x2F;使用shuffle分区规则</span><br><span class="line">         &#x2F;&#x2F;shuffleOp(text);</span><br><span class="line">         &#x2F;&#x2F;使用rebalance分区规则</span><br><span class="line">         &#x2F;&#x2F;rebalanceOp(text);</span><br><span class="line">         &#x2F;&#x2F;使用rescale分区规则</span><br><span class="line">         &#x2F;&#x2F;rescaleOp(text);</span><br><span class="line">         &#x2F;&#x2F;使用broadcast分区规则</span><br><span class="line">         &#x2F;&#x2F;broadcastOp(text);</span><br><span class="line">         &#x2F;&#x2F;自定义分区规则</span><br><span class="line">         &#x2F;&#x2F;custormPartitionOp(text);</span><br><span class="line">         env.execute(&quot;StreamPartitionOpJava&quot;);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void custormPartitionOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .partitionCustom(new MyPartitionerJava(), new KeySelector&lt;Inte</span><br><span class="line">         @Override</span><br><span class="line">         public Integer getKey(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void broadcastOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .broadcast()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void rescaleOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .rescale()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void rebalanceOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .rebalance()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void shuffleOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .shuffle()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="DataStream-API之DataSink"><a href="#DataStream-API之DataSink" class="headerlink" title="DataStream API之DataSink"></a>DataStream API之DataSink</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataSink是 输出组件，负责把计算好的数据输出到其它存储介质中</span><br><span class="line">Flink支持把流数据输出到文件中，不过在实际工作中这种场景不多，因为流数据处理之后一般会存储到一些消息队列里面，或者数据库里面，很少会保存到文件中的。</span><br><span class="line"></span><br><span class="line">还有就是print，直接打印，这个其实我们已经用了很多次了，这种用法主要是在测试的时候使用的，方便查看输出的结果信息</span><br><span class="line"></span><br><span class="line">Flink提供了一批Connectors，可以实现输出到第三方目的地</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Flink内置 Apache Bahir</span><br><span class="line">Kafka ActiveMQ</span><br><span class="line">Cassandra Flume</span><br><span class="line">Kinesis Streams Redis</span><br><span class="line">Elasticsearch Akka</span><br><span class="line">Hadoop FileSysterm</span><br><span class="line">RabbitMQ</span><br><span class="line">NiFi</span><br><span class="line">JDBC</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对sink的这些connector，我们在实际工作中最常用的是kafka、redis</span><br><span class="line">针对Flink提供的常用sink组件，可以提供这些容错性保证</span><br><span class="line"></span><br><span class="line">DataSink 容错保证 备注</span><br><span class="line">Redis at least once</span><br><span class="line">Kafka    at least once&#x2F;exactlyonceKafka0.9和0.10提供at least once，Kafka0.11及以上提供</span><br><span class="line">exactly once</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101205370.png" alt="image-20230410120538984"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">针对kafka这个sink组件的使用，我们在后面会统一分析，现在我们来使用一下redis这个sink组件</span><br><span class="line">需求：接收Socket传输过来的数据，把数据保存到Redis的list队列中。</span><br></pre></td></tr></table></figure><h5 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">先到Flink官网，文档，connector,datasteam,redis,添加对应的依赖(一般不是正确的依赖，把名字复制到maven官网，查找)</span><br></pre></td></tr></table></figure><h6 id="scala-6"><a href="#scala-6" class="headerlink" title="scala"></a>scala</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：redis sink是在Bahir这个依赖包中，所以在pom.xml中需要添加对应的依赖</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.bahir&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;flink-connector-redis_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;1.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.sink</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.<span class="type">RedisSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.config.<span class="type">FlinkJedisPoo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.&#123;<span class="type">RedisCommand</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：接收Socket传输过来的数据，把数据保存到Redis的list队列中。</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamRedisSinkScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//组装数据，这里组装的是tuple2类型</span></span><br><span class="line">         <span class="comment">//第一个元素是指list队列的key名称</span></span><br><span class="line">         <span class="comment">//第二个元素是指需要向list队列中添加的元素</span></span><br><span class="line">         <span class="keyword">val</span> listData = text.map(word =&gt; (<span class="string">"l_words_scala"</span>, word))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定redisSink</span></span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">FlinkJedisPoolConfig</span>.<span class="type">Builder</span>().setHost(<span class="string">"bigdata04"</span>).setPort(<span class="number">6379</span>).build()</span><br><span class="line">         <span class="keyword">val</span> redisSink = <span class="keyword">new</span> <span class="type">RedisSink</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">String</span>]](conf, <span class="keyword">new</span> <span class="type">MyRedisMapper</span>)</span><br><span class="line">                                                                listData.addSink(redisSink)</span><br><span class="line">         env.execute(<span class="string">"StreamRedisSinkScala"</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">    </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">extends</span> <span class="title">RedisMapper</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">String</span>]]</span>&#123;</span><br><span class="line">         <span class="comment">//指定具体的操作命令</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCommandDescription</span></span>: <span class="type">RedisCommandDescription</span> = &#123;</span><br><span class="line">         <span class="keyword">new</span> <span class="type">RedisCommandDescription</span>(<span class="type">RedisCommand</span>.<span class="type">LPUSH</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取key</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKeyFromData</span></span>(data: (<span class="type">String</span>, <span class="type">String</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">         data._1</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取value</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValueFromData</span></span>(data: (<span class="type">String</span>, <span class="type">String</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">         data._2</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注意：针对List数据类型，我们在定义getCommandDescription方法的时候，使用new</span><br><span class="line">RedisCommandDescription(RedisCommand.LPUSH);。</span><br><span class="line"></span><br><span class="line">如果是Hash数据类型，在定义getCommandDescription方法的时候，需要使用new</span><br><span class="line">RedisCommandDescription(RedisCommand.HSET,“hashKey”);，在构造函数中需要直接指定Hash数据类型的key的名称。</span><br><span class="line"></span><br><span class="line">注意：执行代码之前，需要先开启socket和redis服务</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">通过socket传递单词</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line"></span><br><span class="line">最终到redis中查看结果</span><br><span class="line">[root@bigdata04 redis-5.0.9]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; lrange l_words_scala 0 -1</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101233738.png" alt="image-20230410123339761"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101234121.png" alt="image-20230410123420552"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最终到redis中查看结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101234377.png" alt="image-20230410123448085"></p><h6 id="java-6"><a href="#java-6" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.sink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.RedisSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoo</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommand;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommandD</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.RedisMapper;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：接收Socket传输过来的数据，把数据保存到Redis的list队列中。</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamRedisSinkJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line">         <span class="comment">//组装数据</span></span><br><span class="line">         SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; listData = text.map(<span class="keyword">new</span> MapFunction&lt;String,String&gt;&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">map</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="string">"l_words_java"</span>, word);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//指定redisSink</span></span><br><span class="line">         FlinkJedisPoolConfig conf = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder().setHost(<span class="string">"bigdata04"</span>).setPort(<span class="number">6379</span>).build();</span><br><span class="line">        </span><br><span class="line">         RedisSink&lt;Tuple2&lt;String, String&gt;&gt; redisSink = <span class="keyword">new</span> RedisSink&lt;&gt;(conf, <span class="keyword">new</span> MyRedisMapper()) </span><br><span class="line">         listData.addSink(redisSink);</span><br><span class="line">         </span><br><span class="line">                                                                             env.execute(<span class="string">"StreamRedisSinkJava"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">        </span><br><span class="line">                                                      <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">implements</span> <span class="title">RedisMapper</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>,<span class="title">String</span>&gt;&gt;</span>&#123;</span><br><span class="line">         <span class="comment">//指定具体的操作命令</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> RedisCommandDescription(RedisCommand.LPUSH);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取key</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getKeyFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> data.f0;</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取value</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">getValueFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> data.f1;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-实战：流处理和批处理程序开发-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%AE%9E%E6%88%98%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%92%8C%E6%89%B9%E5%A4%84%E7%90%86%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%AE%9E%E6%88%98%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%92%8C%E6%89%B9%E5%A4%84%E7%90%86%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91-2.html</id>
    <published>2023-04-07T17:41:10.000Z</published>
    <updated>2023-04-20T06:44:51.056Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-实战：流处理和批处理程序开发-2"><a href="#第十六周-Flink极速上手篇-实战：流处理和批处理程序开发-2" class="headerlink" title="第十六周 Flink极速上手篇-实战：流处理和批处理程序开发-2"></a>第十六周 Flink极速上手篇-实战：流处理和批处理程序开发-2</h1><h2 id="Flink快速上手使用"><a href="#Flink快速上手使用" class="headerlink" title="Flink快速上手使用"></a>Flink快速上手使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">创建maven项目，因为要使用scala编写代码，在src里main里除了java目录，还要创建scala目录，再创建包</span><br><span class="line"></span><br><span class="line">setting里的module里的scala sdk要导入</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304081548060.png" alt="image-20230408154825118"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来在pom.xml 中引入flink相关依赖，前面两个是针对java代码的，后面两个是针对scala代码的，最后一个依赖是这对flink1.11这个版本需要添加的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-streaming-java_2.12&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="Flink-Job开发步骤"><a href="#Flink-Job开发步骤" class="headerlink" title="Flink Job开发步骤"></a>Flink Job开发步骤</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在开发Flink程序之前，我们先来梳理一下开发一个Flink程序的步骤</span><br><span class="line">1：获得一个执行环境</span><br><span class="line">2：加载&#x2F;创建 初始化数据</span><br><span class="line">3：指定操作数据的transaction算子</span><br><span class="line">4：指定数据目的地</span><br><span class="line">5：调用execute()触发执行程序</span><br><span class="line"></span><br><span class="line">注意：Flink程序是延迟计算的，只有最后调用execute()方法的时候才会真正触发执行程序和Spark类似，Spark中是必须要有action算子才会真正执行。</span><br></pre></td></tr></table></figure><h3 id="Streaming-WordCount"><a href="#Streaming-WordCount" class="headerlink" title="Streaming WordCount"></a>Streaming WordCount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">需求</span><br><span class="line">通过socket实时产生一些单词，使用flink实时接收数据，对指定时间窗口内(例如：2秒)的数据进行聚合统计，并且把时间窗口内计算的结果打印出来</span><br><span class="line">代码开发</span><br><span class="line">下面我们就来开发第一个Flink程序。</span><br><span class="line">先使用scala代码开发</span><br></pre></td></tr></table></figure><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.<span class="type">KeySelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过Socket实时产生一些单词，</span></span><br><span class="line"><span class="comment"> * 使用Flink实时接收数据</span></span><br><span class="line"><span class="comment"> * 对指定时间窗口内(例如：2秒)的数据进行聚合统计</span></span><br><span class="line"><span class="comment"> * 并且把时间窗口内计算的结果打印出来</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SocketWindowWordCountScala</span> </span>&#123;</span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意：在执行代码之前，需要先在bigdata04机器上开启socket，端口为9001</span></span><br><span class="line"><span class="comment">     * @param args</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取运行环境</span></span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">         <span class="comment">//处理数据</span></span><br><span class="line">         <span class="comment">//注意：必须要添加这一行隐式转换的代码，否则下面的flatMap方法会报错</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> wordCount = text.flatMap(_.split(<span class="string">" "</span>))<span class="comment">//将每一行数据根据空格切分单词</span></span><br><span class="line">         .map((_,<span class="number">1</span>))<span class="comment">//每一个单词转换为tuple2的形式(单词,1)</span></span><br><span class="line">         <span class="comment">//.keyBy(0)//根据tuple2中的第一列进行分组</span></span><br><span class="line">         .keyBy(tup=&gt;tup._1)<span class="comment">//官方推荐使用keyselector选择器选择数据</span></span><br><span class="line">         .timeWindow(<span class="type">Time</span>.seconds(<span class="number">2</span>))<span class="comment">//时间窗口为2秒，表示每隔2秒钟计算一次接收到的数据</span></span><br><span class="line">         .sum(<span class="number">1</span>)<span class="comment">// 使用sum或者reduce都可以</span></span><br><span class="line">         <span class="comment">//.reduce((t1,t2)=&gt;(t1._1,t1._2+t2._2))</span></span><br><span class="line">             <span class="comment">//使用一个线程执行打印操作</span></span><br><span class="line">         wordCount.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         <span class="comment">//执行程序</span></span><br><span class="line">         env.execute(<span class="string">"SocketWindowWordCountScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在idea等开发工具里面运行代码的时候需要把pom.xml中的scope配置注释掉</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082223638.png" alt="image-20230408222314468"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04上面开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">hello you</span><br><span class="line">hello me</span><br><span class="line">hello you hello me</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082225596.png" alt="image-20230408222535380"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">注意：此时代码执行的时候下面会显示一些红色的log4j的警告信息，提示缺少相关依赖和配置</span><br><span class="line"></span><br><span class="line">将log4j.properties配置文件和log4j的相关maven配置添加到pom.xml文件中</span><br><span class="line"></span><br><span class="line">&lt;!-- log4j的依赖 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082228288.png" alt="image-20230408222823964"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时再执行就没有红色的警告信息了，但是使用info日志级别打印的信息太多了，所以将log4j中的日志级别配置改为error级别</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082229674.png" alt="image-20230408222903540"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket实时产生一些单词</span></span><br><span class="line"><span class="comment"> * 使用Flink实时接收数据</span></span><br><span class="line"><span class="comment"> * 对指定时间窗口内(例如：2秒)的数据进行聚合统计</span></span><br><span class="line"><span class="comment"> * 并且把时间窗口内计算的结果打印出来</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketWindowWordCountJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取运行环境</span></span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line">         <span class="comment">//处理数据</span></span><br><span class="line">         SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordCount = text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String,String&gt;()&#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">         <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">         out.collect(word);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).map(<span class="keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new Tuple2&lt;String, Integer&gt;<span class="params">(word, <span class="number">1</span>)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> tup.f0;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)<span class="comment">//.keyBy(0)</span></span><br><span class="line">         .timeWindow(Time.seconds(<span class="number">2</span>))</span><br><span class="line">         .sum(<span class="number">1</span>);</span><br><span class="line">         <span class="comment">//使用一个线程执行打印操作</span></span><br><span class="line">         wordCount.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         <span class="comment">//执行程序</span></span><br><span class="line">         env.execute(<span class="string">"SocketWindowWordCountJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Batch-WordCount"><a href="#Batch-WordCount" class="headerlink" title="Batch WordCount"></a>Batch WordCount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需求：统计指定文件中单词出现的总次数</span><br><span class="line">下面来开发Flink的批处理代码</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：统计指定文件中单词出现的总次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchWordCountScala</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"> <span class="comment">//获取执行环境</span></span><br><span class="line"> <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"> <span class="keyword">val</span> inputPath = <span class="string">"hdfs://bigdata01:9000/hello.txt"</span></span><br><span class="line"> <span class="keyword">val</span> outPath = <span class="string">"hdfs://bigdata01:9000/out"</span></span><br><span class="line"> <span class="comment">//读取文件中的数据</span></span><br><span class="line"> <span class="keyword">val</span> text = env.readTextFile(inputPath)</span><br><span class="line"> <span class="comment">//处理数据</span></span><br><span class="line"> <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"> <span class="keyword">val</span> wordCount = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"> .map((_, <span class="number">1</span>))</span><br><span class="line"> .groupBy(<span class="number">0</span>)</span><br><span class="line"> .sum(<span class="number">1</span>)</span><br><span class="line"> .setParallelism(<span class="number">1</span>) <span class="comment">// 这里设置并行度是为了将所有数据写到一个文件里，查看比较方便</span></span><br><span class="line"> <span class="comment">//将结果数据保存到文件中</span></span><br><span class="line"> wordCount.writeAsCsv(outPath,<span class="string">"\n"</span>,<span class="string">" "</span>)</span><br><span class="line"> <span class="comment">//执行程序</span></span><br><span class="line"> env.execute(<span class="string">"BatchWordCountScala"</span>)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：这里面执行setParallelism(1)设置并行度为1是为了将所有数据写到一个文件里面，我们查看结果的时候比较方便(此时输出路径会变成一个文件)</span><br><span class="line">还有就是flink在windows中执行代码，使用到hadoop的时候，需要将hadoop-client的依赖添加到项目中，否则会提示不支持hdfs这种文件系统。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在pom.xml文件中增加</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;3.2.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">此时执行代码就可以正常执行了。</span><br><span class="line">执行成功之后到hdfs上查看结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082256026.png" alt="image-20230408225649895"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.AggregateOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：统计指定文件中单词出现的总次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchWordCountJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取执行环境</span></span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">         String inputPath = <span class="string">"hdfs://bigdata01:9000/hello.txt"</span>;</span><br><span class="line">         String outPath = <span class="string">"hdfs://bigdata01:9000/out2"</span>;</span><br><span class="line">         <span class="comment">//读取文件中的数据</span></span><br><span class="line">         DataSource&lt;String&gt; text = env.readTextFile(inputPath);</span><br><span class="line">         <span class="comment">//处理数据(这里flatmap之后，没写map，直接一步到位)</span></span><br><span class="line">         DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; wordCount = text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String,Tuple2&lt;String,Integer&gt;&gt;()&#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">         <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">         out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, <span class="number">1</span>));</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).groupBy(<span class="number">0</span>)</span><br><span class="line">         .sum(<span class="number">1</span>)</span><br><span class="line">         .setParallelism(<span class="number">1</span>);</span><br><span class="line">         <span class="comment">//将结果数据保存到文件中</span></span><br><span class="line">         wordCount.writeAsCsv(outPath,<span class="string">"\n"</span>,<span class="string">" "</span>);</span><br><span class="line">         <span class="comment">//执行程序</span></span><br><span class="line">         env.execute(<span class="string">"BatchWordCountJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-Streaming和Batch的区别"><a href="#Flink-Streaming和Batch的区别" class="headerlink" title="Flink Streaming和Batch的区别"></a>Flink Streaming和Batch的区别</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">流处理Streaming</span><br><span class="line">执行环境：StreamExecutionEnvironment</span><br><span class="line">数据类型：DataStream</span><br><span class="line"></span><br><span class="line">批处理Batch</span><br><span class="line">执行环境：ExecutionEnvironment</span><br><span class="line">数据类型：DataSet</span><br></pre></td></tr></table></figure><h2 id="Flink集群安装部署"><a href="#Flink集群安装部署" class="headerlink" title="Flink集群安装部署"></a>Flink集群安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Flink支持多种安装部署方式</span><br><span class="line">Standalone</span><br><span class="line">ON YARN</span><br><span class="line">Mesos、Kubernetes、AWS…</span><br><span class="line">这些安装方式我们主要讲一下standalone和on yarn。</span><br><span class="line">如果是一个独立环境的话，可能会用到standalone集群模式。</span><br><span class="line">在生产环境下一般还是用on yarn 这种模式比较多，因为这样可以综合利用集群资源。和我们之前讲的spark on yarn是一样的效果</span><br><span class="line">这个时候我们的Hadoop集群上面既可以运行MapReduce任务，Spark任务，还可以运行Flink任务，一举三得。</span><br></pre></td></tr></table></figure><h3 id="standalone"><a href="#standalone" class="headerlink" title="standalone"></a>standalone</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下standalone模式</span><br><span class="line">它的架构是这样的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082323175.png" alt="image-20230408232348677"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">依赖环境</span><br><span class="line">jdk1.8及以上【配置JAVA_HOME环境变量】</span><br><span class="line">ssh免密码登录</span><br><span class="line">在这我们使用bigdata01、02、03这三台机器，这几台机器的基础环境都是ok的，可以直接使用。</span><br><span class="line"></span><br><span class="line">集群规划如下：</span><br><span class="line">master：bigdata01</span><br><span class="line">slave：bigdata02、bigdata03</span><br><span class="line">接下来我们需要先去下载Flink的安装包。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082326209.png" alt="image-20230408232658079"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于目前Flink各个版本之间差异比较大，属于快速迭代阶段，所以在这我们就使用最新版本了，使用Flink1.11.1版本。</span><br><span class="line">安装包下载好以后上传到bigdata01的&#x2F;data&#x2F;soft目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">下面开始安装Flink集群</span><br><span class="line">1：解压</span><br><span class="line"></span><br><span class="line">2：修改配置</span><br><span class="line">[root@bigdata01 soft]# cd flink-1.11.1</span><br><span class="line">[root@bigdata01 flink-1.11.1]# cd conf&#x2F;</span><br><span class="line">[root@bigdata01 conf]# vi flink-conf.yaml </span><br><span class="line">......</span><br><span class="line">jobmanager.rpc.address: bigdata01</span><br><span class="line">......</span><br><span class="line">[root@bigdata01 conf]# vi masters </span><br><span class="line">bigdata01:8081</span><br><span class="line">[root@bigdata01 conf]# vi workers</span><br><span class="line">bigdata02</span><br><span class="line">bigdata03</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">3：将修改完配置的flink目录拷贝到其它两个从节点</span><br><span class="line">[root@bigdata01 soft]# scp -rq flink-1.11.1 bigdata02:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">[root@bigdata01 soft]# scp -rq flink-1.11.1 bigdata03:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line"></span><br><span class="line">4：启动Flink集群</span><br><span class="line">[root@bigdata01 soft]# cd flink-1.11.1</span><br><span class="line">[root@bigdata01 flink-1.11.1]# bin&#x2F;start-cluster.sh </span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host bigdata01.</span><br><span class="line">Starting taskexecutor daemon on host bigdata02.</span><br><span class="line">Starting taskexecutor daemon on host bigdata03.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5：验证一下进程</span><br><span class="line">在bigdata01上执行jps</span><br><span class="line">[root@bigdata01 flink-1.11.1]# jps</span><br><span class="line">3986 StandaloneSessionClusterEntrypoint</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在bigdata02上执行jps</span><br><span class="line">在bigdata03上执行jps</span><br><span class="line">[root@bigdata02 ~]# jps</span><br><span class="line">2149 TaskManagerRunner</span><br><span class="line">[root@bigdata03 ~]# jps</span><br><span class="line">2150 TaskManagerRunner</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">6：访问Flink的web界面</span><br><span class="line">http:&#x2F;&#x2F;bigdata01:8081</span><br><span class="line">7：停止集群，在主节点上执行停止集群脚本</span><br><span class="line">[root@bigdata01 flink-1.11.1]# bin&#x2F;stop-cluster.sh </span><br><span class="line">Stopping taskexecutor daemon (pid: 2149) on host bigdata02.</span><br><span class="line">Stopping taskexecutor daemon (pid: 2150) on host bigdata03.</span><br><span class="line">Stopping standalonesession daemon (pid: 3986) on host bigdata01.</span><br></pre></td></tr></table></figure><h4 id="Standalone集群核心参数"><a href="#Standalone集群核心参数" class="headerlink" title="Standalone集群核心参数"></a>Standalone集群核心参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">参数 解释</span><br><span class="line">jobmanager.memory .process.size 主节点可用内存大小</span><br><span class="line">taskmanager.memory.process.size 从节点可用内存大小</span><br><span class="line">taskmanager.numberOfTaskSlots 从节点可以启动的进程数量，建议设置为从节可用的cpu数量</span><br><span class="line">parallelism.default Flink任务的默认并行度</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1：slot是静态的概念，是指taskmanager具有的并发执行能力</span><br><span class="line">2：parallelism是动态的概念，是指程序运行时实际使用的并发能力</span><br><span class="line">3：设置合适的parallelism能提高程序计算效率，太多了和太少了都不好</span><br></pre></td></tr></table></figure><h3 id="Flink-ON-YARN"><a href="#Flink-ON-YARN" class="headerlink" title="Flink ON YARN"></a>Flink ON YARN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink ON YARN模式就是使用客户端的方式，直接向Hadoop集群提交任务即可。不需要单独启动Flink进程。</span><br><span class="line">注意：</span><br><span class="line">1：Flink ON YARN 模式依赖Hadoop 2.4.1及以上版本</span><br><span class="line">2：Flink ON YARN支持两种使用方式</span><br></pre></td></tr></table></figure><h4 id="Flink-ON-YARN两种使用方式"><a href="#Flink-ON-YARN两种使用方式" class="headerlink" title="Flink ON YARN两种使用方式"></a>Flink ON YARN两种使用方式</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082341963.png" alt="image-20230408234110607"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在工作中建议使用第二种方式。</span><br></pre></td></tr></table></figure><h5 id="Flink-ON-YARN第一种方式"><a href="#Flink-ON-YARN第一种方式" class="headerlink" title="Flink ON YARN第一种方式"></a>Flink ON YARN第一种方式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下第一种方式</span><br><span class="line">第一步：在集群中初始化一个长时间运行的Flink集群</span><br><span class="line">使用yarn-session.sh脚本</span><br><span class="line">第二步：使用flink run命令向Flink集群中提交任务</span><br><span class="line"></span><br><span class="line">注意：使用flink on yarn需要确保hadoop集群已经启动成功</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">下面来具体演示一下</span><br><span class="line">首先在bigdata04机器上安装一个Flink客户端，其实就是把Flink的安装包上传上去解压即可，不需要启动</span><br><span class="line"></span><br><span class="line">接下来在执行yarn-session.sh脚本之前我们需要先设置HADOOP_CLASSPATH这个环境变量，否则，执行</span><br><span class="line">yarn-session.sh是会报错的，提示找不到hadoop的一些依赖。</span><br><span class="line"></span><br><span class="line">在&#x2F;etc&#x2F;profile中配置HADOOP_CLASSPATH</span><br><span class="line">[root@bigdata04 flink-1.11.1]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin</span><br><span class="line">export SPARK_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;spark-2.4.3-bin-hadoop2.7</span><br><span class="line">export SQOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;sqoop-1.4.7.bin__hadoop-2.6.0</span><br><span class="line">export HADOOP_CLASSPATH&#x3D;&#96;$&#123;HADOOP_HOME&#125;&#x2F;bin&#x2F;hadoop classpath&#96;</span><br><span class="line">export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$HIVE_HOME&#x2F;bin:$SPARK_HO</span><br><span class="line">ME&#x2F;bin:$SQOOP_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082349229.png" alt="image-20230408234907861"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来，使用yarn-session.sh在YARN中创建一个长时间运行的Flink集群</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;yarn-session.sh -jm 1024m -tm 1024m -d</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个表示创建一个Flink集群，-jm是指定主节点的内存，-tm是指定从节点的内存，-d是表示把这个进程放到后台去执行。</span><br><span class="line">启动之后，会看到类似这样的日志信息，这里面会显示flink web界面的地址，以及这个flink集群在yarn中对应的applicationid。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082347701.png" alt="image-20230408234705575"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时到YARN的web界面中确实可以看到这个flink集群。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082353556.png" alt="image-20230408235222041"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082354623.png" alt="image-20230408235427696"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以使用屏幕中显示的flink的web地址或者yarn中这个链接都是可以进入这个flink的web界面的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082353738.png" alt="image-20230408235310889"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来向这个Flink集群中提交任务，此时使用Flink中的内置案例</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink run .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这个时候我们使用flink run的时候，它会默认找这个文件，然后根据这个文件找到刚才我们创建的那个永久的Flink集群，这个文件里面保存的就是刚才启动的那个Flink集群在YARN中对应的applicationid。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082357866.png" alt="image-20230408235700425"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082357811.png" alt="image-20230408235714610"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">任务提交上去执行完成之后，再来看flink的web界面，发现这里面有一个已经执行结束的任务了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082359725.png" alt="image-20230408235924462"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这个任务在执行的时候，会动态申请一些资源执行任务，任务执行完毕之后，对应的资源会自动释放掉。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">最后把这个Flink集群停掉，使用yarn的kill命令</span><br><span class="line">[root@bigdata04 flink-1.11.1]# yarn application -kill application_17689063095</span><br><span class="line">2026-01-20 23:25:22,548 INFO client.RMProxy: Connecting to ResourceManager at </span><br><span class="line">Killing application application_1768906309581_0005</span><br><span class="line">2026-01-20 23:25:23,239 INFO impl.YarnClientImpl: Killed application_17689063</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对yarn-session命令，它后面还支持一些其它参数，可以在后面传一个-help参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090002237.png" alt="image-20230409000251119"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这里的-j是指定Flink任务的jar包，此参数可以省略不写也可以</span><br></pre></td></tr></table></figure><h5 id="Flink-ON-YARN第二种方式"><a href="#Flink-ON-YARN第二种方式" class="headerlink" title="Flink ON YARN第二种方式"></a>Flink ON YARN第二种方式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">flink run -m yarn-cluster(创建Flink集群+提交任务)</span><br><span class="line">使用flink run直接创建一个临时的Flink集群，并且提交任务</span><br><span class="line">此时这里面的参数前面加上了一个y参数</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink run -m yarn-cluster -yjm 1024 -ytm 1024 .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar</span><br><span class="line"></span><br><span class="line">提交上去之后，会先创建一个Flink集群，然后在这个Flink集群中执行任务。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090009823.png" alt="image-20230409000904789"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对Flink命令的一些用法汇总</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090010917.png" alt="image-20230409001026836"></p><h4 id="Flink-ON-YARN的好处"><a href="#Flink-ON-YARN的好处" class="headerlink" title="Flink ON YARN的好处"></a>Flink ON YARN的好处</h4> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1：提高大数据集群机器的利用率</span><br><span class="line">2：一套集群，可以执行MR任务，Spark任务，Flink任务等</span><br></pre></td></tr></table></figure><h3 id="向集群中提交Flink任务"><a href="#向集群中提交Flink任务" class="headerlink" title="向集群中提交Flink任务"></a>向集群中提交Flink任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来我们希望把前面我们自己开发的Flink任务提交到集群上面，在这我就使用flink on yarn的第二种方式来向集群提交一个Flink任务。</span><br><span class="line"></span><br><span class="line">第一步：在pom.xml中添加打包配置</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;!-- 编译插件 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;!-- scala编译插件 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;descriptorRefs&gt;</span><br><span class="line">                    &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                &lt;archive&gt;</span><br><span class="line">                    &lt;manifest&gt;</span><br><span class="line">                        &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                        &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                    &lt;&#x2F;manifest&gt;</span><br><span class="line">                &lt;&#x2F;archive&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">    &lt;&#x2F;plugins&gt;</span><br><span class="line">&lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：需要将Flink和Hadoop的相关依赖的score属性设置为provided，这些依赖不需要打进jar包里面。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">第二步：生成jar包： mvn clean package -DskipTests</span><br><span class="line">第三步：将 db_flink-1.0-SNAPSHOT-jar-with-dependencies.jar 上传到bigdata04机器上的 &#x2F;data&#x2F;sof</span><br><span class="line">t&#x2F;flink-1.11.1 目录中(上传到哪个目录都可以)</span><br><span class="line"></span><br><span class="line">第四步：提交Flink任务</span><br><span class="line">注意：提交任务之前，先开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001 </span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink run -m yarn-cluster -c com.imooc.scala.SocketWindowWordCountScala -yjm 1024 -ytm 1024 db_flink-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时到yarn上面可以看到确实新增了一个任务，点击进去可以看到flink的web界面</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090028085.png" alt="image-20230409002811696"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通过socket输入一串内容</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090028348.png" alt="image-20230409002838306"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090029943.png" alt="image-20230409002909775"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090029419.png" alt="image-20230409002934711"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090030195.png" alt="image-20230409003014134"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090030583.png" alt="image-20230409003040435"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090031669.png" alt="image-20230409003059283"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们希望把这个任务停掉，因为这个任务是一个流处理的任务，提交成功之后，它会一直运行。</span><br><span class="line">注意：此时如果我们使用ctrl+c关掉之前提交任务的那个进程，这里的flink任务是不会有任何影响的，可以一直运行，因为flink任务已经提交到hadoop集群里面了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090032922.png" alt="image-20230409003221865"></p><h4 id="停止任务"><a href="#停止任务" class="headerlink" title="停止任务"></a>停止任务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时如果想要停止Flink任务，有两种方式</span><br><span class="line"></span><br><span class="line">1：停止yarn中任务(这种是直接停止yarn中flink集群，任务也就停止了)</span><br><span class="line">[root@bigdata04 flink-1.11.1]# yarn application -kill application_17689629561</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2：停止flink任务</span><br><span class="line">可以在界面上点击这个按钮，或者在命令行中执行flink cancel停止都可以</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090035939.png" alt="image-20230409003528258"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">或者</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink cancel -yid application_176896295613 d7bo35cf4co10xxxxxxxxxxx(具体任务ID)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090039108.png" alt="image-20230409003917743"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">这个flink任务停止之后，对应的那个yarn-session（Flink集群）也就停止了。</span><br><span class="line"></span><br><span class="line">注意：此时flink任务停止之后就无法再查看flink的web界面了，如果想看查看历史任务的执行信息就看不了了，怎么办呢？</span><br><span class="line">咱们之前在学习spark的时候其实也遇到过这种问题，当时是通过启动spark的historyserver进程解决</span><br><span class="line">的。</span><br><span class="line">flink也有historyserver进程，也是可以解决这个问题的。</span><br><span class="line">historyserver进程可以在任意一台机器上启动，在这我们选择在bigdata04机器上启动</span><br><span class="line">在启动historyserver进程之前，需要先修改bigdata04中的flink-conf.yaml配置文件</span><br><span class="line"></span><br><span class="line">[root@bigdata04 flink-1.11.1]# vi conf&#x2F;flink-conf.yaml </span><br><span class="line">......</span><br><span class="line">jobmanager.archive.fs.dir: hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;completed-jobs&#x2F;</span><br><span class="line">historyserver.web.address: 192.168.182.103</span><br><span class="line">historyserver.web.port: 8082</span><br><span class="line">historyserver.archive.fs.dir: hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;completed-jobs&#x2F;</span><br><span class="line">historyserver.archive.fs.refresh-interval: 10000</span><br><span class="line">......</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">然后启动flink的historyserver进程</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;historyserver.sh start</span><br><span class="line"></span><br><span class="line">验证进程</span><br><span class="line">[root@bigdata04 flink-1.11.1]# jps</span><br><span class="line">5894 HistoryServer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：hadoop集群中的historyserver进程也需要启动</span><br><span class="line">在bigdata01、bigdata02、bigdata03节点上启动hadoop的historyserver进程</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br><span class="line">[root@bigdata02 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br><span class="line">[root@bigdata03 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时Flink任务停止之后也是可以访问flink的web界面的。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-初识Flink</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%88%9D%E8%AF%86Flink-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%88%9D%E8%AF%86Flink-1.html</id>
    <published>2023-04-07T16:52:17.000Z</published>
    <updated>2023-04-20T02:30:17.877Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-初识Flink"><a href="#第十六周-Flink极速上手篇-初识Flink" class="headerlink" title="第十六周 Flink极速上手篇-初识Flink"></a>第十六周 Flink极速上手篇-初识Flink</h1><h2 id="初识Flink"><a href="#初识Flink" class="headerlink" title="初识Flink"></a>初识Flink</h2><h3 id="什么是Flink"><a href="#什么是Flink" class="headerlink" title="什么是Flink"></a>什么是Flink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Apache Flink 是一个开源的分布式，高性能，高可用，准确的流处理框架。</span><br><span class="line">分布式：表示flink程序可以运行在很多台机器上，</span><br><span class="line">高性能：表示Flink处理性能比较高</span><br><span class="line">高可用：表示flink支持程序的自动重启机制。</span><br><span class="line">准确的：表示flink可以保证处理数据的准确性。</span><br><span class="line">Flink支持流处理和批处理，虽然我们刚才说了flink是一个流处理框架，但是它也支持批处理。</span><br><span class="line">其实对于flink而言，它是一个流处理框架，批处理只是流处理的一个极限特例而已。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080056558.png" alt="image-20230408005641755"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">左边是数据源，从这里面可以看出来，这些数据是实时产生的一些日志，或者是数据库、文件系统、kv存储系统中的数据。</span><br><span class="line">中间是Flink，负责对数据进行处理。</span><br><span class="line">右边是目的地，Flink可以将计算好的数据输出到其它应用中，或者存储系统中。</span><br></pre></td></tr></table></figure><h3 id="Flink架构图"><a href="#Flink架构图" class="headerlink" title="Flink架构图"></a>Flink架构图</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080059947.png" alt="image-20230408005900310"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">首先图片最下面表示是flink的一些部署模式，支持local，和集群(standalone，yarn)，也支持在云上部署。</span><br><span class="line"></span><br><span class="line">往上一层是flink的核心，分布式的流处理引擎。</span><br><span class="line"></span><br><span class="line">再往上面是flink的API和类库</span><br><span class="line">主要有两大块API，DataStream API和DataSet API，分别做流处理和批处理。</span><br><span class="line">针对DataStream API这块，支持复杂事件处理，和table操作，其实也是支持SQL操作的。</span><br><span class="line">针对Dataset API这块，支持flinkML机器学习，Gelly图计算，table操作，这块也是支持SQL操作的。</span><br><span class="line"></span><br><span class="line">其实从这可以看出来，Flink也是有自己的生态圈的，里面包含了实时计算、离线计算、机器学习、图计算、Table和SQL计算等等</span><br><span class="line">所以说它和Spark还是有点像的，不过它们两个的底层计算引擎是有本质区别的，一会我们会详细分析。</span><br></pre></td></tr></table></figure><h3 id="Flink三大核心组件"><a href="#Flink三大核心组件" class="headerlink" title="Flink三大核心组件"></a>Flink三大核心组件</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080113253.png" alt="image-20230408011332076"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink包含三大核心组件</span><br><span class="line">Data Source，数据源(负责接收数据)，</span><br><span class="line">Transformations 算子(负责对数据进行处理)</span><br><span class="line">Data Sink 输出组件(负责把计算好的数据输出到其它存储介质中)</span><br></pre></td></tr></table></figure><h3 id="Flink的流处理与批处理"><a href="#Flink的流处理与批处理" class="headerlink" title="Flink的流处理与批处理"></a>Flink的流处理与批处理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来分析一下Flink这个计算引擎的核心内容</span><br><span class="line"></span><br><span class="line">在大数据处理领域，批处理和流处理一般被认为是两种不同的任务，一个大数据框架一般会被设计为只能处理其中一种任务</span><br><span class="line"></span><br><span class="line">例如Storm只支持流处理任务，而MapReduce、Spark只支持批处理任务。Spark Streaming是Spark之上支持流处理任务的子系统，看似是一个特例，其实并不是——Spark Streaming采用了一种micro-batch的架构，就是把输入的数据流切分成细粒度的batch，并为每一个batch提交一个批处理的Spark任务，所以Spark Streaming本质上执行的还是批处理任务，和Storm这种流式的数据处理方式是完全不同的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Flink通过灵活的执行引擎，能够同时支持批处理和流处理</span><br><span class="line"></span><br><span class="line">在执行引擎这一层，流处理系统与批处理系统最大的不同在于节点之间的数据传输方式。</span><br><span class="line">对于一个流处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理</span><br><span class="line">这就是典型的一条一条处理</span><br><span class="line"></span><br><span class="line">而对于一个批处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满的时候，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点</span><br><span class="line"></span><br><span class="line">这两种数据传输模式是两个极端，对应的是流处理系统对低延迟的要求和批处理系统对高吞吐量的要求</span><br><span class="line">Flink的执行引擎采用了一种十分灵活的方式，同时支持了这两种数据传输模型</span><br><span class="line">Flink以固定的缓存块为单位进行网络数据传输，用户可以通过缓存块超时值指定缓存块的传输时机。如果缓存块的超时值为0，则Flink的数据传输方式类似前面所说的流处理系统的标准模型，此时系统可以获得最低的处理延迟</span><br><span class="line"></span><br><span class="line">如果缓存块的超时值为无限大，则Flink的数据传输方式类似前面所说的批处理系统的标准模型，此时系统可以获得最高的吞吐量</span><br><span class="line"></span><br><span class="line">这样就比较灵活了，其实底层还是流式计算模型，批处理只是一个极限特例而已。</span><br><span class="line">看一下这个图中显示的三种数据传输模型</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080127620.png" alt="image-20230408012754579"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个：一条一条处理</span><br><span class="line">第二个：一批一批处理</span><br><span class="line">第三个：按照缓存块进行处理，缓存块可以无限小，也可以无限大，这样就可以同时支持流处理和批处理了。</span><br></pre></td></tr></table></figure><h3 id="Storm-vs-SparkStreaming-vs-Flink"><a href="#Storm-vs-SparkStreaming-vs-Flink" class="headerlink" title="Storm vs SparkStreaming vs Flink"></a>Storm vs SparkStreaming vs Flink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来对比一下目前大数据领域中的三种实时计算引擎</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080133033.png" alt="image-20230408013308451"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">Native：表示来一条数据处理一条数据</span><br><span class="line">Mirco-Batch：表示划分小批，一小批一小批的处理数据</span><br><span class="line">组合式：表示是基础API，例如实现一个求和操作都需要写代码实现，比较麻烦，代码量会比较多。</span><br><span class="line">声明式：表示提供的是封装后的高阶函数，例如filter、count等函数，可以直接使用，比较方便，代码量比较少。</span><br></pre></td></tr></table></figure><h3 id="实时计算框架如何选择"><a href="#实时计算框架如何选择" class="headerlink" title="实时计算框架如何选择"></a>实时计算框架如何选择</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：需要关注流数据是否需要进行状态管理</span><br><span class="line">2：消息语义是否有特殊要求At-least-once或者Exectly-once</span><br><span class="line">3：小型独立的项目，需要低延迟的场景，建议使用Storm</span><br><span class="line">4：如果项目中已经使用了Spark，并且秒级别的实时处理可以满足需求，建议使用SparkStreaming</span><br><span class="line">5：要求消息语义为Exectly-once，数据量较大，要求高吞吐低延迟，需要进行状态管理，建议选择Flink</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-1.html</id>
    <published>2023-04-07T16:50:41.000Z</published>
    <updated>2023-04-19T08:49:45.098Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-1"><a href="#第十四周-消息队列之Kafka从入门到小牛-1" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-1"></a>第十四周 消息队列之Kafka从入门到小牛-1</h1><h2 id="初识Kafka"><a href="#初识Kafka" class="headerlink" title="初识Kafka"></a>初识Kafka</h2><h3 id="什么是消息队列"><a href="#什么是消息队列" class="headerlink" title="什么是消息队列"></a>什么是消息队列</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在学习Kafka之前我们先来看一下什么是消息队列</span><br><span class="line">消息队列(Message Queue)：可以简称为MQ</span><br><span class="line">例如：Java中的Queue队列，也可以认为是一个消息队列</span><br><span class="line"></span><br><span class="line">消息队列：顾名思义，消息+队列，其实就是保存消息的队列，属于消息传输过程中的容器。</span><br><span class="line"></span><br><span class="line">消息队列主要提供生产、消费接口供外部调用，做数据的存储和读取</span><br></pre></td></tr></table></figure><h3 id="消息队列分类"><a href="#消息队列分类" class="headerlink" title="消息队列分类"></a>消息队列分类</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">消息队列大致可以分为两种：点对点(P2P)、发布订阅(Pub&#x2F;Sub)</span><br><span class="line">共同点：</span><br><span class="line">针对数据的处理流程是一样的</span><br><span class="line">消息生产者生产消息发送到queue中，然后消息消费者从queue中读取并且消费消息。</span><br><span class="line"></span><br><span class="line">不同点：</span><br><span class="line">点对点(p2p)模型包含：消息队列(Queue)、发送者(Sender)、接收者(Receiver)</span><br><span class="line">一个生产者生产的消息只有一个消费者(Consumer)（消息一旦被消费，就不在消息队列中）消费。</span><br><span class="line">例如QQ中的私聊，我发给你的消息只有你能看到，别人是看不到的</span><br><span class="line"></span><br><span class="line">发布订阅(Pub&#x2F;Sub)模型包含：消息队列(Queue)、主题（Topic）、发布者（Publisher）、订阅者（Subscriber）</span><br><span class="line">每个消息可以有多个消费者，彼此互不影响。比如我发布一个微博：关注我的人都能够看到，或者QQ中的群聊，我在群里面发一条消息，群里面所有人都能看到</span><br><span class="line"></span><br><span class="line">这就是这两种消息队列的区别</span><br><span class="line">我们接下来要学习的Kafka这个消息队列是属于发布订阅模型的</span><br></pre></td></tr></table></figure><h2 id="什么是Kafka"><a href="#什么是Kafka" class="headerlink" title="什么是Kafka"></a>什么是Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Kafka 是一个高吞吐量的、持久性的、分布式发布订阅消息系统</span><br><span class="line">高吞吐量：可以满足每秒百万级别消息的生产和消费。</span><br><span class="line"></span><br><span class="line">为什么这么快？</span><br><span class="line">难道Kafka的数据是放在内存里面的吗？</span><br><span class="line">不是的，Kafka的数据还是放在磁盘里面的</span><br><span class="line">主要是Kafka利用了磁盘顺序读写速度超过内存随机读写速度这个特性。所以说它的吞吐量才这么高</span><br><span class="line"></span><br><span class="line">持久性：有一套完善的消息存储机制，确保数据高效安全的持久化。</span><br><span class="line">分布式：它是基于分布式的扩展、和容错机制；Kafka的数据都会复制到几台服务器上。当某一台机器故障失效时，生产者和消费者切换使用其它的机器。</span><br></pre></td></tr></table></figure><h3 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Kafka的数据时存储是磁盘中的，为什么可以满足每秒百万级别消息的生产和消费？</span><br><span class="line">这是一个面试题，其实就是我们刚才针对高吞吐量的解释：kafka利用了磁盘顺序读写速度超过内存随机读写速度这个特性</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Kafka主要应用在实时计算领域，可以和Flume、Spark、Flink等框架结合在一块使用</span><br><span class="line">例如：我们使用Flume采集网站产生的日志数据，将数据写入到Kafka中，然后通过Spark或者Flink从Kafka中消费数据进行计算，这其实是一个典型的实时计算案例的架构</span><br></pre></td></tr></table></figure><h2 id="Kafka组件介绍"><a href="#Kafka组件介绍" class="headerlink" title="Kafka组件介绍"></a>Kafka组件介绍</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来分析一下Kafka中的组件，加深对kafka的理解</span><br><span class="line">看这个图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304151537112.png" alt="image-20230415153737183"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">先看中间的Kafka Cluster</span><br><span class="line">这个Kafka集群内有两个节点，这些节点在这里我们称之为Broker</span><br><span class="line">Broker：消息的代理，Kafka集群中的一个节点称为一个broker</span><br><span class="line"></span><br><span class="line">在Kafka中有Topic的概念</span><br><span class="line">Topic：称为主题，Kafka处理的消息的不同分类(是一个逻辑概念)。</span><br><span class="line">如果把Kafka认为是一个数据库的话，那么Kafka中的Topic就可以认为是一张表</span><br><span class="line">不同的topic中存储不同业务类型的数据，方便使用</span><br><span class="line"></span><br><span class="line">在Topic内部有partition的概念</span><br><span class="line">Partition：是Topic物理上的分组，一个Topic会被分为1个或者多个partition(分区)，分区个数是在创建topic的时候指定。每个topic都是有分区的，至少1个。</span><br><span class="line"></span><br><span class="line">注意：这里面针对partition其实还有副本的概念，主要是为了提供数据的容错性，我们可以在创建Topic的时候指定partition的副本因子是几个。</span><br><span class="line">在这里面副本因子其实就是2了，其中一个是Leader，另一个是真正的副本</span><br><span class="line">Leader中的这个partition负责接收用户的读写请求，副本partition负责从Leader里面的partiton中同步数据，这样的话，如果后期leader对应的节点宕机了，副本可以切换为leader顶上来。</span><br><span class="line"></span><br><span class="line">在partition内部还有一个message的概念</span><br><span class="line">Message：我们称之为消息，代表的就是一条数据，它是通信的基本单位，每个消息都属于一个partition。</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">在这里总结一下</span><br><span class="line">Broker&gt;Topic&gt;Partition&gt;Message</span><br><span class="line"></span><br><span class="line">接下来还有两个组件，看图中的最左边和最右边</span><br><span class="line">Producer：消息和数据的生产者，向Kafka的topic生产数据。</span><br><span class="line">Consumer：消息和数据的消费者，从kafka的topic中消费数据。</span><br><span class="line"></span><br><span class="line">这里的消费者可以有多个，每个消费者可以消费到相同的数据</span><br><span class="line">最后还有一个Zookeeper服务，Kafka的运行是需要依赖于Zookeeper的，Zookeeper负责协调Kafka集群的正常运行。</span><br></pre></td></tr></table></figure><h2 id="Kafka集群安装部署"><a href="#Kafka集群安装部署" class="headerlink" title="Kafka集群安装部署"></a>Kafka集群安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面我们对Kafka有了一个基本的认识，下面我们就想使用一下Kafka</span><br><span class="line"></span><br><span class="line">在使用之前，需要先把Kafka安装部署起来</span><br><span class="line">Kafka是支持单机和集群模式的，建议大家在学习阶段使用单机模式即可，单机和集群在操作上没有任何区别</span><br><span class="line"></span><br><span class="line">注意：由于Kafka需要依赖于Zookeeper，所以在这我们需要先把Zookeeper安装部署起来</span><br></pre></td></tr></table></figure><h3 id="zookeeper安装部署"><a href="#zookeeper安装部署" class="headerlink" title="zookeeper安装部署"></a>zookeeper安装部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对Zookeeper前期不需要掌握太多，只需要掌握Zookeeper的安装部署以及它的基本操作即可。</span><br><span class="line">Zookeeper也支持单机和集群安装，建议大家在学习阶段使用单机即可，单机和集群在操作上没有任何区别。</span><br><span class="line">在这里我们会针对单机和集群这两种方式分别演示一下</span><br></pre></td></tr></table></figure><h4 id="zookeeper单机安装"><a href="#zookeeper单机安装" class="headerlink" title="zookeeper单机安装"></a>zookeeper单机安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">zookeeper需要依赖于jdk，只要保证jdk已经正常安装即可。</span><br><span class="line">具体安装步骤如下：</span><br><span class="line">1：下载zookeeper的安装包</span><br><span class="line">进入Zookeeper的官网 </span><br><span class="line"></span><br><span class="line">2：把安装包上传到bigdata01机器的&#x2F;data&#x2F;soft目录下</span><br><span class="line">[root@bigdata01 soft]# ll</span><br><span class="line">-rw-r--r--. 1 root root 9394700 Jun 2 21:33 apache-zookeeper-3.5.8-bin.t</span><br><span class="line">3：解压安装包</span><br><span class="line">[root@bigdata01 soft]# tar -zxvf apache-zookeeper-3.5.8-bin.tar.gz</span><br><span class="line">4：修改配置文件</span><br><span class="line">首先将zoo_sample.cfg重命名为zoo.cfg</span><br><span class="line">然后修改 zoo.cfg中的dataDir参数的值，dataDir指向的目录存储的是zookeeper的核心数据，所以这个目录不能使用tmp目录</span><br><span class="line">[root@bigdata01 soft]# cd apache-zookeeper-3.5.8-bin&#x2F;conf</span><br><span class="line">[root@bigdata01 conf]# mv zoo_sample.cfg zoo.cfg</span><br><span class="line">[root@bigdata01 conf]# vi zoo.cfg </span><br><span class="line">dataDir&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;data</span><br><span class="line"></span><br><span class="line">5：启动zookeeper服务</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">6：验证</span><br><span class="line">如果能看到QuorumPeerMain进程就说明zookeeper启动成功</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# jps</span><br><span class="line">1701 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">注意：如果执行jps命令发现没有QuorumPeerMain进程，则需要到logs目录下去查看zookeeper-*.out这个日志文件</span><br><span class="line">也可以通过zkServer.sh脚本查看当前机器中zookeeper服务的状态</span><br><span class="line">注意：使用zkServer.sh默认会连接本机2181端口的zookeeper服务，默认情况下zookeeper会监听2181端口，这个需要注意一下，因为后面我们在使用zookeeper的时候需要知道它监听的端口是哪个。</span><br><span class="line">最下面显示的Mode信息，表示当前是一个单机独立集群</span><br><span class="line"></span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: standalone</span><br><span class="line"></span><br><span class="line">如果没有启动成功的话则会提示连不上服务not running</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">7：操作zookeeper</span><br><span class="line">首先使用zookeeper的客户端工具连接到zookeeper里面，使用bin目录下面的zkCli.sh脚本，默认会连接本机的zookeeper服务</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkCli.sh</span><br><span class="line">Connecting to localhost:2181</span><br><span class="line">.....</span><br><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:None path:null</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这样就进入zookeeper的命令行了。</span><br><span class="line">在这里面可以操作Zookeeper中的目录结构</span><br><span class="line">zookeeper中的目录结构和Linux文件系统的目录结构类似</span><br><span class="line">zookeeper里面的每一个目录我们称之为节点(ZNode)</span><br><span class="line">正常情况下我们可以把ZNode认为和文件系统中的目录类似，但是有一点需要注意：ZNode节点本身是可以存储数据的。</span><br><span class="line"></span><br><span class="line">zookeeper中提供了一些命令可以对它进行一些操作</span><br><span class="line">在命令行下随便输入一个字符，按回车就会提示出zookeeper支持的所有命令</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 8] aa</span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">     addauth scheme auth</span><br><span class="line">     close </span><br><span class="line">     config [-c] [-w] [-s]</span><br><span class="line">     connect host:port</span><br><span class="line">     create [-s] [-e] [-c] [-t ttl] path [data] [acl]</span><br><span class="line">     delete [-v version] path</span><br><span class="line">     deleteall path</span><br><span class="line">     delquota [-n|-b] path</span><br><span class="line">     get [-s] [-w] path</span><br><span class="line">     getAcl [-s] path</span><br><span class="line">     history </span><br><span class="line">    listquota path</span><br><span class="line">     ls [-s] [-w] [-R] path</span><br><span class="line">     ls2 path [watch]</span><br><span class="line">     printwatches on|off</span><br><span class="line">     quit </span><br><span class="line">     reconfig [-s] [-v version] [[-file path] | [-members serverID&#x3D;host:po</span><br><span class="line">     redo cmdno</span><br><span class="line">     removewatches path [-c|-d|-a] [-l]</span><br><span class="line">     rmr path</span><br><span class="line">     set [-s] [-v version] path data</span><br><span class="line">     setAcl [-s] [-v version] [-R] path acl</span><br><span class="line">     setquota -n|-b val path</span><br><span class="line">     stat [-w] path</span><br><span class="line">     sync path</span><br><span class="line">Command not found: Command not found aa</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们来具体看一些比较常用的功能：</span><br></pre></td></tr></table></figure><h5 id="查看根节点下面有什么内容-常用"><a href="#查看根节点下面有什么内容-常用" class="headerlink" title="查看根节点下面有什么内容(常用)"></a>查看根节点下面有什么内容(常用)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls &#x2F;</span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure><h5 id="创建节点"><a href="#创建节点" class="headerlink" title="创建节点"></a>创建节点</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在根节点下面创建一个test节点，在test节点上存储数据hello</span><br><span class="line">[zk: localhost:2181(CONNECTED) 9] create &#x2F;test hello</span><br><span class="line">Created &#x2F;test</span><br></pre></td></tr></table></figure><h5 id="查看节点中的信息-常用"><a href="#查看节点中的信息-常用" class="headerlink" title="查看节点中的信息(常用)"></a>查看节点中的信息(常用)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查看&#x2F;test节点中的内容</span><br><span class="line">[zk: localhost:2181(CONNECTED) 10] get &#x2F;test</span><br><span class="line">hello</span><br></pre></td></tr></table></figure><h5 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个删除命令可以递归删除，这里面还有一个delete命令，也可以删除节点，但是只能删除空节点，如果节点下面还有子节点，想一次性全部删除建议使用deleteall(递归删除，适用于这个节点下还有节点；这里用delete也可以)</span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] deleteall &#x2F;test</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">直接按 ctrl+c 就可以退出这个操作界面，想优雅一些的话可以输入quit退出</span><br><span class="line"></span><br><span class="line">或者quit</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">8：停止zookeeper服务</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh stop</span><br></pre></td></tr></table></figure><h4 id="zookeeper集群安装"><a href="#zookeeper集群安装" class="headerlink" title="zookeeper集群安装"></a>zookeeper集群安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1：集群节点规划，使用三个节点搭建一个zookeeper集群</span><br><span class="line">bigdata01</span><br><span class="line">bigdata02</span><br><span class="line">bigdata03</span><br><span class="line"></span><br><span class="line">2：首先在bigdata01节点上配置zookeeper</span><br><span class="line">解压</span><br><span class="line">修改配置(2888:集群节点进行通信时的端口；3888:集群节点进行选举时用的端口)</span><br><span class="line">[root@bigdata01 soft]# cd apache-zookeeper-3.5.8-bin&#x2F;conf&#x2F;</span><br><span class="line">[root@bigdata01 conf]# mv zoo_sample.cfg zoo.cfg </span><br><span class="line">dataDir&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;data</span><br><span class="line">server.0&#x3D;bigdata01:2888:3888</span><br><span class="line">server.1&#x3D;bigdata02:2888:3888</span><br><span class="line">server.2&#x3D;bigdata03:2888:3888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">创建目录保存myid文件，并且向myid文件中写入内容</span><br><span class="line">myid中的值其实是和zoo.cfg中server后面指定的编号是一一对应的</span><br><span class="line">编号0对应的是bigdata01这台机器，所以在这里指定0</span><br><span class="line">在这里使用echo和重定向实现数据写入</span><br><span class="line">[root@bigdata01 conf]#cd &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# mkdir data</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# cd data</span><br><span class="line">[root@bigdata01 data]# echo 0 &gt; myid</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3：把修改好配置的zookeeper拷贝到其它两个节点</span><br><span class="line">[root@bigdata01 soft]# scp -rq apache-zookeeper-3.5.8-bin bigdata02:&#x2F;data&#x2F;soft</span><br><span class="line">[root@bigdata01 soft]# scp -rq apache-zookeeper-3.5.8-bin bigdata03:&#x2F;data&#x2F;soft</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">4：修改bigdata02和bigdata03上zookeeper中myid文件的内容</span><br><span class="line">首先修改bigdata02节点上的myid文件</span><br><span class="line">[root@bigdata02 ~]# cd &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;data&#x2F;</span><br><span class="line">[root@bigdata02 data]# echo 1 &gt; myid</span><br><span class="line"></span><br><span class="line">然后修改bigdata03节点上的myid文件</span><br><span class="line">[root@bigdata03 ~]# cd &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;data&#x2F;</span><br><span class="line">[root@bigdata03 data]# echo 2 &gt; myid</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">5：启动zookeeper集群</span><br><span class="line">分别在 bigdata01、bigdata02、bigdata03 上启动zookeeper进程</span><br><span class="line">在bigdata01上启动</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">在bigdata02上启动</span><br><span class="line">[root@bigdata02 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">在bigdata03上启动</span><br><span class="line">[root@bigdata03 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">6：验证</span><br><span class="line">分别在bigdata01、bigdata02、bigdata03上执行jps命令验证是否有 QuorumPeerMain 进程</span><br><span class="line">如果都有就说明zookeeper集群启动正常了</span><br><span class="line"></span><br><span class="line">如果没有就到对应的节点的logs目录下查看zookeeper*-*.out日志文件</span><br><span class="line">执行bin&#x2F;zkServer.sh status命令会发现有一个节点显示为leader，其他两个节点为follower</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">7：操作zookeeper</span><br><span class="line">和上面单机的操作方式一样</span><br><span class="line">8：停止zookeeper集群</span><br><span class="line">在bigdata01、bigdata02、bigdata03三台机器上分别执行bin&#x2F;zkServer.sh stop命令</span><br></pre></td></tr></table></figure><h3 id="kafka安装部署"><a href="#kafka安装部署" class="headerlink" title="kafka安装部署"></a>kafka安装部署</h3><h4 id="kafka单机安装"><a href="#kafka单机安装" class="headerlink" title="kafka单机安装"></a>kafka单机安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zookeeper集群安装好了以后就可以开始安装kafka了。</span><br><span class="line"></span><br><span class="line">注意：在安装kafka之前需要先确保zookeeper集群是启动状态。</span><br><span class="line"></span><br><span class="line">kafka还需要依赖于基础环境jdk，需要确保jdk已经安装到位。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1：下载kafka安装包</span><br><span class="line"></span><br><span class="line">注意：kafka在启动的时候不需要安装scala环境，只有在编译源码的时候才需要，因为运行的时候是在jvm虚拟机上运行的，只需要有jdk环境就可以了</span><br><span class="line"></span><br><span class="line">2：把kafka安装包上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">3：解压</span><br><span class="line">4：修改配置文件(config&#x2F;server.properties)</span><br><span class="line">主要参数：</span><br><span class="line">broker.id：集群节点id编号，单机模式不用修改</span><br><span class="line">listeners：默认监听9092端口</span><br><span class="line">log.dirs：注意：这个目录不是存储日志的，是存储Kafka中核心数据的目录，这个目录默认是指</span><br><span class="line">zookeeper.connect：kafka依赖的zookeeper</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对单机模式，如果kafka和zookeeper在同一台机器上，并且zookeeper监听的端口就是那个默认的2181端口，则zookeeper.connect这个参数就不需要修改了。</span><br><span class="line">只需要修改一下log.dirs即可 </span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# cd kafka_2.12-2.4.1&#x2F;config&#x2F;</span><br><span class="line">[root@bigdata01 config]# vi server.properties</span><br><span class="line">log.dirs&#x3D;&#x2F;data&#x2F;soft&#x2F;kafka_2.12-2.4.1&#x2F;kafka-logs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5：启动kafka</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line"></span><br><span class="line">-daemon的作用是让进程后台运行</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">6：验证</span><br><span class="line">启动成功之后会产生一个kafka进程</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">2230 QuorumPeerMain</span><br><span class="line">3117 Kafka</span><br><span class="line">3182 Jps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7：停止kafka</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh</span><br></pre></td></tr></table></figure><h4 id="kafka集群安装"><a href="#kafka集群安装" class="headerlink" title="kafka集群安装"></a>kafka集群安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：集群节点规划，使用三个节点搭建一个kafka集群</span><br><span class="line">bigdata01</span><br><span class="line">bigdata02</span><br><span class="line">bigdata03</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">注意：针对Kafka集群而言，没有主从之分，所有节点都是一样的。</span><br><span class="line">2：首先在bigdata01节点上配置kafka</span><br><span class="line">解压：</span><br><span class="line">[root@bigdata01 soft]# tar -zxvf kafka_2.12-2.4.1.tgz</span><br><span class="line"></span><br><span class="line">修改配置文件</span><br><span class="line"></span><br><span class="line">注意：此时针对集群模式需要修改broker.id、log.dirs、以及 zookeeper.connect</span><br><span class="line"></span><br><span class="line">broker.id的值默认是从0开始的，集群中所有节点的broker.id从0开始递增即可</span><br><span class="line">所以bigdata01节点的broker.id值为0</span><br><span class="line"></span><br><span class="line">log.dirs的值建议指定到一块存储空间比较大的磁盘上面，因为在实际工作中kafka中会存储很多数据，我这个虚拟机里面就一块磁盘，所以就指定到&#x2F;data目录下面了</span><br><span class="line"></span><br><span class="line">zookeeper.connect的值是zookeeper集群的地址，可以指定集群中的一个节点或者多个节点地址，多个节点地址之间使用逗号隔开即可(避免zookeeper当前这个节点挂了，kafka识别不出来，建议写多个)</span><br><span class="line">[root@bigdata01 soft]# cd kafka_2.12-2.4.1&#x2F;config&#x2F;</span><br><span class="line">broker.id&#x3D;0</span><br><span class="line">log.dirs&#x3D;&#x2F;data&#x2F;kafka-logs</span><br><span class="line">zookeeper.connect&#x3D;bigdata01:2181,bigdata02:2181,bigdata03:2181</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">3：将修改好配置的kafka安装包拷贝到其它两个节点</span><br><span class="line">[root@bigdata01 soft]# scp -rq kafka_2.12-2.4.1 bigdata02:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">[root@bigdata01 soft]# scp -rq kafka_2.12-2.4.1 bigdata03:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">4：修改bigdata02和bigdata03上kafka中broker.id的值</span><br><span class="line">首先修改bigdata02节点上的broker.id的值为1</span><br><span class="line">[root@bigdata02 ~]# cd &#x2F;data&#x2F;soft&#x2F;kafka_2.12-2.4.1&#x2F;config&#x2F;</span><br><span class="line">[root@bigdata02 config]# vi server.properties </span><br><span class="line">broker.id&#x3D;1</span><br><span class="line">然后修改bigdata03节点上的broker.id的值为2</span><br><span class="line">[root@bigdata03 ~]# cd &#x2F;data&#x2F;soft&#x2F;kafka_2.12-2.4.1&#x2F;config&#x2F;</span><br><span class="line">[root@bigdata03 config]# vi server.properties </span><br><span class="line">broker.id&#x3D;2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">5：启动集群</span><br><span class="line">分别在bigdata01、bigdata02、bigdata03上启动kafka进程</span><br><span class="line">在bigdata01上启动</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line"></span><br><span class="line">-daemon的作用是让进程后台运行</span><br><span class="line">在bigdata02上启动</span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line">在bigdata03上启动</span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">6：验证</span><br><span class="line">分别在bigdata01、bigdata02、bigdata03上执行jps命令验证是否有kafka进程</span><br><span class="line">如果都有就说明kafka集群启动正常了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304151651403.png" alt="image-20230415165152051"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十三周 综合项目:电商数据仓库之商品订单数仓2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.html</id>
    <published>2023-04-05T14:30:33.000Z</published>
    <updated>2023-04-06T16:35:47.609Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十三周-综合项目-电商数据仓库之商品订单数仓2"><a href="#第十三周-综合项目-电商数据仓库之商品订单数仓2" class="headerlink" title="第十三周 综合项目:电商数据仓库之商品订单数仓2"></a>第十三周 综合项目:电商数据仓库之商品订单数仓2</h1><h2 id="拉链表"><a href="#拉链表" class="headerlink" title="拉链表"></a>拉链表</h2><h3 id="什么是拉链表"><a href="#什么是拉链表" class="headerlink" title="什么是拉链表"></a>什么是拉链表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对订单表、订单商品表，流水表，这些表中的数据是比较多的，如果使用全量的方式，会造成大量的数据冗余，浪费磁盘空间。</span><br><span class="line">所以这种表，一般使用增量的方式，每日采集新增的数据。</span><br><span class="line"></span><br><span class="line">在这注意一点：针对订单表，如果单纯的按照订单产生时间增量采集数据，是有问题的，因为用户可能今天下单，明天才支付，但是Hive是不支持数据更新的，这样虽然MySQL中订单的状态改变了，但是Hive中订单的状态还是之前的状态。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">想要解决这个问题，一般有这么几种方案：</span><br><span class="line">第一种：每天全量导入订单表的数据，这种方案在项目启动初期是没有多大问题的，因为前期数据量不大，但是随着项目的运营，订单量暴增，假设每天新增1亿订单，之前已经累积了100亿订单，如果每天都是全量导入的话，那也就意味着每天都需要把数据库中的100多亿订单数据导入到HDFS中保存一份，这样会极大的造成数据冗余，太浪费磁盘空间了。</span><br><span class="line">第二种：只保存当天的全量订单表数据，每次在导入之前，删除前一天保存的全量订单数据，这种方式虽然不会造成数据冗余，但是无法查询订单的历史状态，只有当前的最新状态，也不太好。</span><br><span class="line">第三种：拉链表，这种方式在普通增量导入方式的基础之上进行完善，把变化的数据也导入进来，这样既不会造成大量的数据冗余，还可以查询订单的历史状态。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">拉链表是针对数据仓库设计中表存储数据的方式而定义的，顾名思义，所谓拉链，就是记录历史。记录一个事物从开始，一直到当前状态的所有历史变化的信息。</span><br><span class="line"></span><br><span class="line">下面就是一张拉链表，存储的是用户的最基本信息以及每条记录的生命周期。我们可以使用这张表拿到当天的最新数据以及之前的历史数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061515178.png" alt="image-20230406151532551"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">说明：</span><br><span class="line">start_time表示该条记录的生命周期开始时间，end_time 表示该条记录的生命周期结束时间；</span><br><span class="line">end_time &#x3D; &#39;9999-12-31&#39;表示该条记录目前处于有效状态；</span><br><span class="line"></span><br><span class="line">如果查询当前所有有效的记录，则使用 SQL</span><br><span class="line">select * from user where end_time &#x3D;&#39;9999-12-31&#39;</span><br><span class="line"></span><br><span class="line">如果查询2026-01-02的历史快照【获取指定时间内的有效数据】，则使用SQL</span><br><span class="line">select * from user where start_time &lt;&#x3D; &#39;2026-01-02&#39; and end_time &gt;&#x3D; &#39;2026-01-02&#39;</span><br><span class="line"></span><br><span class="line">这就是拉链表。</span><br></pre></td></tr></table></figure><h3 id="如何制作拉链表"><a href="#如何制作拉链表" class="headerlink" title="如何制作拉链表"></a>如何制作拉链表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">那针对我们前面分析的订单表，希望使用拉链表的方式实现数据采集，因为每天都保存全量订单数据比较浪费磁盘空间，但是只采集增量的话无法反应订单的状态变化。</span><br><span class="line">所以需要既采集增量，还要采集订单状态变化了的数据。</span><br><span class="line">针对订单表中的订单状态字段有这么几个阶段</span><br><span class="line">未支付</span><br><span class="line">已支付</span><br><span class="line">未发货</span><br><span class="line">已发货</span><br><span class="line">在这我们先分析两种状态：未支付和已支付。</span><br><span class="line"></span><br><span class="line">我们先举个例子：</span><br><span class="line">假设我们的系统是2026年3月1日开始运营的</span><br><span class="line">那么到3月1日结束订单表所有数据如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061527784.png" alt="image-20230406152722507"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061528829.png" alt="image-20230406152831907"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">基于订单表中的这些数据如何制作拉链表？</span><br><span class="line"></span><br><span class="line">实现思路</span><br><span class="line">1：首先针对3月1号中的订单数据构建初始的拉链表，拉链表中需要有一个start_time(数据生效开始时间)和end_time(数据生效结束时间)，默认情况下start_time等于表中的创建时间，end_time初始化为一个无限大的日期9999-12-31</span><br><span class="line">将3月1号的订单数据导入到拉链表中。</span><br><span class="line">此时拉链表中数据如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061531144.png" alt="image-20230406153103346"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2：在3月2号的时候，需要将订单表中发生了变化的数据和新增的订单数据 整合到之前的拉链表中</span><br><span class="line">此时需要先创建一个每日更新表，将每日新增和变化了的数据保存到里面</span><br><span class="line"></span><br><span class="line">然后基于拉链表和这个每日更新表进行left join，根据订单id进行关联，如果可以关联上，就说明这个订单的状态发生了变化，然后将订单状态发生了变化的数据的end_time改为2026-03-01(当天日期-1天)</span><br><span class="line">然后再和每日更新表中的数据执行union all操作，将结果重新insert到拉链表中</span><br><span class="line"></span><br><span class="line">最终拉链表中的数据如下：</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932%5Cimage-20230406153453757.png" alt="image-20230406153453757"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">因为在3月2号的时候，订单id为001的数据的订单状态发生了变化，所以拉链表中订单id为001的原始数据的end_time需要修改为2026-03-01，</span><br><span class="line">然后需要新增一条订单id为001的数据，订单状态为已支付，start_time为2026-03-02，end_time为9999-12-31。</span><br><span class="line">还需要将3月2号新增的订单id为003的数据也添加进来。</span><br></pre></td></tr></table></figure><h3 id="【实战】基于订单表的拉链表实现"><a href="#【实战】基于订单表的拉链表实现" class="headerlink" title="【实战】基于订单表的拉链表实现"></a>【实战】基于订单表的拉链表实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面我们开始实现：</span><br><span class="line">1：首先初始化2026-03-01、2026-03-02和2026-03-03的订单表新增和变化的数据，ods_user_order(直接将数据初始化到HDFS中)，这个表其实就是前面我们所说的每日更新表</span><br><span class="line"></span><br><span class="line">注意：这里模拟使用sqoop从mysql中抽取新增和变化的数据，根据order_date和update_time这两个字段获取这些数据，所以此时ods_user_order中的数据就是每日的新增和变化了的数据。</span><br><span class="line"></span><br><span class="line">执行代码生成数据：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ods_user_order在前面已经使用过，所以在这只需要将2026-03-01、2026-03-02和2026-03-03的数据加载进去即可</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061542631.png" alt="image-20230406154210239"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061543916.png" alt="image-20230406154346557"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061544315.png" alt="image-20230406154427193"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_order <span class="keyword">partition</span>(dt=<span class="string">'20260301'</span>)  <span class="keyword">select</span> </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time</span><br><span class="line"><span class="keyword">from</span> ods_mall.ods_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260301'</span> <span class="keyword">and</span> order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_order <span class="keyword">partition</span>(dt=<span class="string">'20260302'</span>)  <span class="keyword">select</span> </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time</span><br><span class="line"><span class="keyword">from</span> ods_mall.ods_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260302'</span> <span class="keyword">and</span> order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_order <span class="keyword">partition</span>(dt=<span class="string">'20260303'</span>)  <span class="keyword">select</span> </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time</span><br><span class="line"><span class="keyword">from</span> ods_mall.ods_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260303'</span> <span class="keyword">and</span> order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061552760.png" alt="image-20230406155251457"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061553441.png" alt="image-20230406155334317"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061553044.png" alt="image-20230406155350251"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2：创建拉链表，基于每日更新订单表构建拉链表中的数据</span><br><span class="line">创建拉链表：dws_user_order_zip</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_order_zip(</span><br><span class="line">   order_id    <span class="built_in">bigint</span>,</span><br><span class="line">   order_date    <span class="keyword">string</span>,</span><br><span class="line">   user_id    <span class="built_in">bigint</span>,</span><br><span class="line">   order_money    <span class="keyword">double</span>,</span><br><span class="line">   order_type    <span class="built_in">int</span>,</span><br><span class="line">   order_status    <span class="built_in">int</span>,</span><br><span class="line">   pay_id    <span class="built_in">bigint</span>,</span><br><span class="line">   update_time    <span class="keyword">string</span>,</span><br><span class="line">   start_time    <span class="keyword">string</span>,</span><br><span class="line">   end_time    <span class="keyword">string</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_order_zip/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_order_zip</span><br><span class="line"><span class="keyword">select</span> *</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">       duoz.order_id,</span><br><span class="line">       duoz.order_date,</span><br><span class="line">       duoz.user_id,</span><br><span class="line">       duoz.order_money,</span><br><span class="line">       duoz.order_type,</span><br><span class="line">       duoz.order_status,</span><br><span class="line">       duoz.pay_id,</span><br><span class="line">       duoz.update_time,</span><br><span class="line">       duoz.start_time,</span><br><span class="line">       <span class="keyword">case</span></span><br><span class="line">           <span class="keyword">when</span> duoz.end_time = <span class="string">'9999-12-31'</span> <span class="keyword">and</span> duo.order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">date_add</span>(<span class="string">'2026-03-01'</span>,<span class="number">-1</span>)</span><br><span class="line">           <span class="keyword">else</span> duoz.end_time</span><br><span class="line">       <span class="keyword">end</span> <span class="keyword">as</span> end_time</span><br><span class="line">    <span class="keyword">from</span> dws_mall.dws_user_order_zip <span class="keyword">as</span> duoz</span><br><span class="line">    <span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span> order_id <span class="keyword">from</span> dwd_mall.dwd_user_order</span><br><span class="line">        <span class="keyword">where</span> dt = <span class="string">'20260301'</span></span><br><span class="line">    ) <span class="keyword">as</span> duo</span><br><span class="line">    <span class="keyword">on</span> duoz.order_id = duo.order_id</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">   <span class="keyword">select</span></span><br><span class="line">       duo.order_id,</span><br><span class="line">       duo.order_date,</span><br><span class="line">       duo.user_id,</span><br><span class="line">       duo.order_money,</span><br><span class="line">       duo.order_type,</span><br><span class="line">       duo.order_status,</span><br><span class="line">       duo.pay_id,</span><br><span class="line">       duo.update_time,</span><br><span class="line">       <span class="string">'2026-03-01'</span> <span class="keyword">as</span> start_time,</span><br><span class="line">       <span class="string">'9999-12-31'</span> <span class="keyword">as</span> end_time</span><br><span class="line">   <span class="keyword">from</span> dwd_mall.dwd_user_order <span class="keyword">as</span> duo</span><br><span class="line">   <span class="keyword">where</span> duo.dt = <span class="string">'20260301'</span></span><br><span class="line">) <span class="keyword">as</span> t;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在windows中编写sql语句，回车时可能会有tab制表符，这样的sql复制到hive中执行会报错；对于复制到hive中连成一行无法解析的情况，可以在复制前给每一行的最后添加一个空格</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932%5Cimage-20230406163700653.png" alt="image-20230406163700653"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061642146.png" alt="image-20230406164241911"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql语句里的时间改成20260302后再执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061641683.png" alt="image-20230406164129176"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061642440.png" alt="image-20230406164251390"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061645394.png" alt="image-20230406164509353"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql语句里的时间改成20260303后再执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061648636.png" alt="image-20230406164804417"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061649776.png" alt="image-20230406164904719"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061710378.png" alt="image-20230406171011072"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061710744.png" alt="image-20230406171046707"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061711261.png" alt="image-20230406171146204"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查询有效数据</span><br><span class="line">查询切片数据</span><br><span class="line">查询一条订单历史记录</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061658975.png" alt="image-20230406165823692"></p><h3 id="拉链表的性能问题-面试爱问"><a href="#拉链表的性能问题-面试爱问" class="headerlink" title="拉链表的性能问题(面试爱问)"></a>拉链表的性能问题(面试爱问)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">拉链表也会遇到查询性能的问题，假设我们存放了5年的拉链数据，那么这张表势必会比较大，当查询的时候性能就比较低了</span><br><span class="line">可以用以下思路来解决：</span><br><span class="line">1. 可以尝试对start_time和end_time做索引，这样可以提高一些性能。</span><br><span class="line">2. 保留部分历史数据，我们可以在一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">还有什么是拉链表，如何制作拉链表也爱问</span><br></pre></td></tr></table></figure><h2 id="商品订单数据数仓总结"><a href="#商品订单数据数仓总结" class="headerlink" title="商品订单数据数仓总结"></a>商品订单数据数仓总结</h2><h3 id="数据库和表梳理"><a href="#数据库和表梳理" class="headerlink" title="数据库和表梳理"></a>数据库和表梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062102932.png" alt="image-20230406210206233"></p><h3 id="任务脚本梳理"><a href="#任务脚本梳理" class="headerlink" title="任务脚本梳理"></a>任务脚本梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062103230.png" alt="image-20230406210336400"></p><h2 id="数据可视化和任务调度实现"><a href="#数据可视化和任务调度实现" class="headerlink" title="数据可视化和任务调度实现"></a>数据可视化和任务调度实现</h2><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">数据可视化这块不是项目的重点，不过为了让大家能有一个更加直观的感受，我们可以选择一些现成的数据可视化工具实现。</span><br><span class="line">咱们前面分析过，想要查询hive中的数据可以使用hue，不过hue无法自动生成图表。</span><br><span class="line">所以我们可以考虑使用Zeppelin(还可以操作spark,flink)</span><br><span class="line">针对一些复杂的图表，可以选择定制开发，使用echarts等组件实现</span><br></pre></td></tr></table></figure><h4 id="Zeppelin安装部署"><a href="#Zeppelin安装部署" class="headerlink" title="Zeppelin安装部署"></a>Zeppelin安装部署</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：不要使用Zeppelin0.8.2版本，这个版本有bug，无法使用图形展现数据。</span><br><span class="line">在这我们使用zeppelin-0.9.0-preview1这个版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">把下载好的安装包上传到bigdata04的&#x2F;data&#x2F;soft目录中</span><br><span class="line">1：解压</span><br><span class="line"></span><br><span class="line">2：修改配置</span><br><span class="line">[root@bigdata04 soft]# cd zeppelin-0.9.0-preview1-bin-all&#x2F;conf</span><br><span class="line">[root@bigdata04 conf]# mv zeppelin-env.sh.template zeppelin-env.sh</span><br><span class="line">[root@bigdata04 conf]# mv zeppelin-site.xml.template zeppelin-site.xml</span><br><span class="line">[root@bigdata04 conf]# vi zeppelin-site.xml</span><br><span class="line"># 将默认的127.0.0.1改为0.0.0.0 否则默认情况下只能在本机访问zeppline</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;zeppelin.server.addr&lt;&#x2F;name&gt;</span><br><span class="line"> &lt;value&gt;0.0.0.0&lt;&#x2F;value&gt;</span><br><span class="line"> &lt;description&gt;Server binding address&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3：增加Hive依赖jar包</span><br><span class="line">由于我们需要使用Zepplien连接hive，它里面默认没有集成Hive的依赖jar包，所以最简单的方式就是将Hive的lib目录中的所有jar包全复制到Zeppline中的interpreter&#x2F;jdbc目录中</span><br><span class="line">[root@bigdata04 zeppelin-0.9.0-preview1-bin-all]# cd interpreter&#x2F;jdbc</span><br><span class="line">[root@bigdata04 jdbc]# cp &#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;lib&#x2F;*.jar .</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4：启动</span><br><span class="line">[root@bigdata04 zeppelin-0.9.0-preview1-bin-all]# bin&#x2F;zeppelin-daemon.sh start</span><br><span class="line">5：停止【需要停止的时候传递stop命令即可】</span><br><span class="line">[root@bigdata04 zeppelin-0.9.0-preview1-bin-all]# bin&#x2F;zeppelin-daemon.sh stop</span><br></pre></td></tr></table></figure><h4 id="Zepplin的界面参数配置"><a href="#Zepplin的界面参数配置" class="headerlink" title="Zepplin的界面参数配置"></a>Zepplin的界面参数配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Zepplin启动之后可以通过8080端口进行访问</span><br><span class="line">http:&#x2F;&#x2F;bigdata04:8080&#x2F;</span><br><span class="line"></span><br><span class="line">在使用之前需要先配置hive的基本信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062115317.png" alt="image-20230406211509288"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">搜索jdbc</span><br><span class="line"></span><br><span class="line">点击edit修改里面的参数信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062115096.png" alt="image-20230406211538750"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">修改这四项的内容即可，这里的内容其实就是我们之前学习hive的jdbc操作时指定的参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062118908.png" alt="image-20230406211809935"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：需要在192.168.182.103这台机器上启动hiveserver2服务，否则在zeppline中连不上hive</span><br><span class="line">最后点save按钮即可Zepplin的使用</span><br><span class="line">创建一个note，类似于工作台的概念</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin</span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;hiveserver2</span><br></pre></td></tr></table></figure><h4 id="Zepplin的使用"><a href="#Zepplin的使用" class="headerlink" title="Zepplin的使用"></a>Zepplin的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建一个note，类似于工作台的概念</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062119442.png" alt="image-20230406211917306"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时就可以在里面写SQL了。</span><br><span class="line">如果以图形的形式展示结果，则点击这里面对应图形的图标即可。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062121862.png" alt="image-20230406212125211"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062124210.png" alt="image-20230406212432043"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于想制作一些复杂的图表，可以用sqoop导出到mysql，让前端的人去制作。这里当然也很方便直接与hive交互</span><br></pre></td></tr></table></figure><h3 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">针对数据仓库中的任务脚本我们前面已经整理过了，任务脚本还是比较多的，针对初始化表的脚本只需要</span><br><span class="line">执行一次即可，其它的脚本需要每天都执行一次，这个时候就需要涉及到任务定时调度了。</span><br></pre></td></tr></table></figure><h4 id="Crontab调度器的使用"><a href="#Crontab调度器的使用" class="headerlink" title="Crontab调度器的使用"></a>Crontab调度器的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">咱们前面在学习Linux的时候学过一个crontab调度器，通过它可以实现定时执行指定的脚本。</span><br><span class="line">针对我们这个数据仓库中的这些脚本使用crontab进行调度是可以的</span><br><span class="line">但是需要注意一点：这些任务之间是有一些依赖关系的，从大的层面上来说，dwd层的任务需要等ods层的任务执行成功之后才能开始执行</span><br><span class="line"></span><br><span class="line">那crontab如何知道任务之间的依赖关系呢？</span><br><span class="line">crontab是无法知道任务之间的依赖关系的，我们只能间接实现</span><br><span class="line">举个例子：针对MapReduce任务和Spark任务，任务执行成功之后，在输出目录中会有一个success标记文件，这个文件表示这个任务成功的执行结束了。</span><br><span class="line"></span><br><span class="line">此时如果我们使用crontab调度两个job，一个jobA，一个jobB，先执行jobA，jobA成功执行结束之后才能执行jobB，这个时候我们就需要在脚本中添加一个判断，判断jobA的结果输出目录中是否存在success文件，如果存在则继续执行jobB，否则不执行，并且告警，提示jobA任务执行失败。</span><br><span class="line"></span><br><span class="line">那我们现在执行的是hive的sql任务，sql任务最终不会在输出目录中产生success文件，所以没有办法使用这个标记文件进行判断，不过sql任务会产生数据文件，数据文件的文件名是类似000000_0这样的，可能会有多个，具体的个数是由任务中的reduce的个数决定的，我们也可以选择判断这个数据文件是否存在来判断任务是否成功执行结束。</span><br><span class="line"></span><br><span class="line">注意了：针对SQL任务虽然可以通过判断数据文件来判定任务是否执行成功，不过这种方式不能保证100%的准确率，有可能会存在这种情况，任务确实执行成功了，但是在向结果目录中写数据文件的时候，写了一半，由于网络原因导致数据没有写完，但是我们过来判断000000_0这个文件肯定是存在的，那我们就误以为这个任务执行成功了，其实它的数据是缺失一部分的，不过这种情况的概率极低，可以忽略不计。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时使用crontab调度两个有依赖关系的任务，脚本该如何实现呢？</span><br><span class="line">在bigdata04机器中创建 &#x2F;data&#x2F;soft&#x2F;warehouse_job 目录</span><br><span class="line">创建脚本： crontab_job_schedule.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行jobA</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 先删除jobA的输出目录</span></span><br><span class="line">hdfs dfs -rm -r hdfs://bigdata01:9000/data/dwd/user_addr/dt=$&#123;dt&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dwd_mall.dwd_user_addr partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   addr_id,</span><br><span class="line">   user_id,</span><br><span class="line">   addr_name,</span><br><span class="line">   order_flag,</span><br><span class="line">   user_name,</span><br><span class="line">   mobile</span><br><span class="line">from ods_mall.ods_user_addr</span><br><span class="line">where dt = '$&#123;dt&#125;' and addr_id is not null;</span><br><span class="line">"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">如果jobA执行成功，这条查询命令就可以成功执行，否则在执行的时候会报错</span></span><br><span class="line">hdfs dfs -ls hdfs://bigdata01:9000/data/dwd/user_addr/dt=$&#123;dt&#125;/000000_0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在这里通过$?来判断上一条命令是否成功执行，如果$?的返回值为0，则表示jobA执行成功，否则表示jobA执行失败</span></span><br><span class="line">if [ $? = 0 ]</span><br><span class="line">then</span><br><span class="line">    echo "执行jobB"</span><br><span class="line">else</span><br><span class="line">    # 可以在这里发短息或者发邮件</span><br><span class="line">    echo "jobA执行失败，请处理..."</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果jobA成功执行，则jobB也可以执行，如果jobA执行失败，则jobB不会执行，脚本会生成告警信息。</span><br><span class="line"></span><br><span class="line">这就是使用crontab调度器如何实现任务依赖的功能。</span><br><span class="line">如果项目中的定时任务有很多，使用crontab虽然可以实现任务依赖的功能，但是管理起来不方便，crontab没有提供图形化的界面，使用起来比较麻烦</span><br><span class="line">针对一些简单的定时任务的配置，并且任务比较少的情况下使用crontab是比较方便的。</span><br></pre></td></tr></table></figure><h4 id="Azkaban调度器的使用"><a href="#Azkaban调度器的使用" class="headerlink" title="Azkaban调度器的使用"></a>Azkaban调度器的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于定时执行的脚本很多，可以使用可视化管理界面的任务调度工具进行管理：azkaban(轻量级),Ooize(重量级)</span><br></pre></td></tr></table></figure><h5 id="Azkaban介绍"><a href="#Azkaban介绍" class="headerlink" title="Azkaban介绍"></a>Azkaban介绍</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在实际工作中如果需要配置很多的定时任务，一般会采用一些支持界面操作的调度器，例如：Azkaban、Ooize</span><br><span class="line">Azkaban是一个轻量级的调度器，使用起来比较简单，容易上手。</span><br><span class="line">Ooize是一个重量级的调度器，使用起来相对比较复杂。</span><br><span class="line">在这里我们主要考虑易用性，所以我们选择使用Azkaban。</span><br><span class="line">下面我们来快速了解一下Azkaban，以及它的用法。</span><br><span class="line">Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。</span><br><span class="line">Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">那我们首先把Azkaban安装部署起来</span><br><span class="line">官网下载的Azkaban是源码，需要编译，编译过程会非常慢，主要是由于国外网络原因导致的。</span><br><span class="line">在这我给大家直接提供一个编译好的Azkaban。</span><br><span class="line">为了简化Azakan的安装过程，不让大家在工具的安装上花费太多时间，在这里我们就使用Azkaban的solo模式部署。</span><br><span class="line"></span><br><span class="line">solo模式属于单机模式，那对应的Azkaban也支持在多台机器上运行。</span><br><span class="line">solo模式的优点：</span><br><span class="line">易于安装：无需MySQL示例。它将H2打包为主要的持久存储。</span><br><span class="line">易于启动：Web服务器和执行程序服务器都在同一个进程中运行。</span><br><span class="line">全功能：它包含所有Azkaban功能。</span><br></pre></td></tr></table></figure><h5 id="Azkaban安装部署"><a href="#Azkaban安装部署" class="headerlink" title="Azkaban安装部署"></a>Azkaban安装部署</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">把编译后的Azkaban安装包 azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz 上传到bigdata04的&#x2F;data&#x2F;soft目录下</span><br><span class="line">1：解压</span><br><span class="line"></span><br><span class="line">2：修改Azkaban的时区为上海时区</span><br><span class="line">我们的bigdata04机器在创建的时候已经指定了时区为上海时区</span><br><span class="line">要保证bigdata04机器的时区和Azkaban的时区是一致的。</span><br><span class="line">[root@bigdata04 azkaban-solo-server-0.1.0-SNAPSHOT]# cd conf&#x2F;</span><br><span class="line">[root@bigdata04 conf]# vi azkaban.properties</span><br><span class="line">......</span><br><span class="line">default.timezone.id&#x3D;Asia&#x2F;Shanghai</span><br><span class="line">......</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3：启动</span><br><span class="line">[root@bigdata04 azkaban-solo-server-0.1.0-SNAPSHOT]# bin&#x2F;azkaban-solo-start.sh</span><br><span class="line"></span><br><span class="line">4：停止</span><br><span class="line">[root@bigdata04 azkaban-solo-server-0.1.0-SNAPSHOT]# bin&#x2F;azkaban-solo-stop.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启动成功之后，azkaban会启动一个web界面，监听的端口是8081</span><br><span class="line">http:&#x2F;&#x2F;bigdata04:8081&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062150906.png" alt="image-20230406215038265"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户名和密码默认都是 azkaban</span><br></pre></td></tr></table></figure><h5 id="提交独立任务"><a href="#提交独立任务" class="headerlink" title="提交独立任务"></a>提交独立任务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">如何向Azkaban中提交任务呢？</span><br><span class="line">1：先创建一个Project</span><br><span class="line"></span><br><span class="line">2：向test项目中提交任务</span><br><span class="line">先演示一个独立的任务</span><br><span class="line">创建一个文件hello.job，文件内容如下：</span><br><span class="line"># hello.job</span><br><span class="line">type&#x3D;command</span><br><span class="line">command&#x3D;echo &quot;Hello World!</span><br><span class="line"></span><br><span class="line">这里面的#号开头表示是注释</span><br><span class="line">type：任务类型，这里的command表示这个任务执行的是一个命令</span><br><span class="line">command：这里的command是指具体的命令</span><br><span class="line">将hello.job文件添加到一个zip压缩文件中，hello.zip。</span><br><span class="line"></span><br><span class="line">将hello.zip压缩包提交到刚才创建的test项目中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062155170.png" alt="image-20230406215538918"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时test项目中就包含hello这个任务，点击execute flow执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062156281.png" alt="image-20230406215604662"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062156394.png" alt="image-20230406215635048"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062156493.png" alt="image-20230406215648313"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时这个任务执行一次就结束了，如果想要让任务定时执行，可以在这配置。</span><br><span class="line">先进入test项目中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062200802.png" alt="image-20230406220045801"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062201588.png" alt="image-20230406220101356"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击Schedule，进入配置定时信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062201092.png" alt="image-20230406220133705"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062201869.png" alt="image-20230406220150848"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这就将test项目中的hello.job配置了定时任务。</span><br><span class="line">其实我们会发现这里面的定时任务的配置格式和crontab中的一样。</span><br><span class="line">后期如果想查看这个定时任务的执行情况可以点击hello</span><br><span class="line"></span><br><span class="line">点击executions查看任务的执行列表</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062202682.png" alt="image-20230406220233640"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062202551.png" alt="image-20230406220252870"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果想查看某一次任务的具体执行情况，可以点击execution id</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是一个基本的独立任务的执行，如果是有依赖的多个任务，如何配置呢？</span><br></pre></td></tr></table></figure><h5 id="提交依赖任务"><a href="#提交依赖任务" class="headerlink" title="提交依赖任务"></a>提交依赖任务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">下面再看一个例子：</span><br><span class="line">1：先创建一个project：depen_test</span><br><span class="line"></span><br><span class="line">2：向depen_test项目中提交任务</span><br><span class="line">先创建一个文件first.job</span><br><span class="line"># first.job</span><br><span class="line">type&#x3D;command</span><br><span class="line">command&#x3D;echo &quot;Hello First!&quot;</span><br><span class="line">再创建一个文件second.job</span><br><span class="line"># second.job</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;first</span><br><span class="line">command&#x3D;echo &quot;Hello Second!&quot;</span><br><span class="line"></span><br><span class="line">这里面通过dependencies属性指定了任务的依赖关系，后面的first表示依赖的任务的文件名称</span><br><span class="line"></span><br><span class="line">最后将这两个job文件打成一个zip压缩包</span><br><span class="line">将这个压缩包上传到项目里面</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062206761.png" alt="image-20230406220617754"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">点击Execute Flow，也可以看到任务之间的依赖关系</span><br><span class="line">点击Execute</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062206575.png" alt="image-20230406220640754"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后查看jobList，可以看到这个项目中的两个任务的执行顺序</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062208908.png" alt="image-20230406220807410"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这就是带依赖的任务的任务调度。</span><br></pre></td></tr></table></figure><h5 id="在数仓中使用Azkaban"><a href="#在数仓中使用Azkaban" class="headerlink" title="在数仓中使用Azkaban"></a>在数仓中使用Azkaban</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下针对数仓中的多级任务依赖，如何使用Azkaban实现</span><br><span class="line">以统计电商GMV为例：</span><br><span class="line">这个指标需要依赖于这个流程 MySQL--&gt;HDFS--&gt;ODS--&gt;DWD--&gt;APP</span><br><span class="line">MySQL–&gt;HDFS需要使用Sqoop脚本</span><br><span class="line">HDFS–&gt;ODS需要使用hive alter命令</span><br><span class="line">ODS–&gt;DWD需要使用hive的SQL</span><br><span class="line">DWD–&gt;APP需要使用hive的SQL</span><br><span class="line">接下来开发对应的azkaban job</span><br></pre></td></tr></table></figure><h6 id="collect-job"><a href="#collect-job" class="headerlink" title="collect.job"></a>collect.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># collect.job</span><br><span class="line"># 采集MySQL中的数据至HDFS</span><br><span class="line">type&#x3D;command</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;collect_mysql.sh</span><br></pre></td></tr></table></figure><h6 id="ods-job"><a href="#ods-job" class="headerlink" title="ods.job"></a>ods.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ods.job</span><br><span class="line"># 关联ods层的数据</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;collect</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;ods_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h6 id="dwd-job"><a href="#dwd-job" class="headerlink" title="dwd.job"></a>dwd.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># dwd.job</span><br><span class="line"># 生成dwd层的数据</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;ods</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;dwd_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h6 id="app-job"><a href="#app-job" class="headerlink" title="app.job"></a>app.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># app.job</span><br><span class="line"># 生成app层的数据</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;dwd</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;app_mall_add_partition_2.sh</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062218260.png" alt="image-20230406221808936"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">还要把job文件里涉及的sh脚本上传到linux上指定目录</span><br><span class="line"></span><br><span class="line">然后配置定时任务</span><br></pre></td></tr></table></figure><h2 id="项目优化"><a href="#项目优化" class="headerlink" title="项目优化"></a>项目优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Sqoop数据采集参数调优，它只会生成一个map任务，性能较低</span><br><span class="line"></span><br><span class="line">集群Queue队列优化</span><br><span class="line"></span><br><span class="line">避免任务启动时间过于集中</span><br><span class="line"></span><br><span class="line">Hive on Tez</span><br><span class="line"></span><br><span class="line">Impala提供快速交互查询</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
</feed>
