<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TianYong&#39;s Blog</title>
  
  <subtitle>比你优秀的人都努力，有什么理由不努力！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tianyong.fun/"/>
  <updated>2023-04-17T10:25:28.414Z</updated>
  <id>http://tianyong.fun/</id>
  
  <author>
    <name>TTYONG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>分布式数据库原理与应用-5</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-5.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-5.html</id>
    <published>2023-04-17T08:47:07.000Z</published>
    <updated>2023-04-17T10:25:28.414Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-4</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-4.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-4.html</id>
    <published>2023-04-17T08:47:02.000Z</published>
    <updated>2023-04-17T10:25:25.882Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-3</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-3.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-3.html</id>
    <published>2023-04-17T08:46:58.000Z</published>
    <updated>2023-04-17T15:26:37.989Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第三章-Hbase"><a href="#第三章-Hbase" class="headerlink" title="第三章 Hbase"></a>第三章 Hbase</h1><h2 id="01-Hbase数据模型"><a href="#01-Hbase数据模型" class="headerlink" title="01 Hbase数据模型"></a>01 Hbase数据模型</h2><h3 id="逻辑模型"><a href="#逻辑模型" class="headerlink" title="逻辑模型"></a>逻辑模型</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172154639.png" alt="image-20230417215406265"></p><h3 id="HBase相关概念"><a href="#HBase相关概念" class="headerlink" title="HBase相关概念"></a>HBase相关概念</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">（1）表（table）：HBase采用表来组织数据；</span><br><span class="line">（2）行（row）：每个表都由行组成，每个行由行键（row key）来标识，行键可以是任意字符串；</span><br><span class="line">（3）列族（column family）：一个table有许多个列族，列族是列的集合，属于表结构，也是表的基本访问控制单元；</span><br><span class="line">（4）列标识（column qualifier）：属于某一个Column Family：Column Qualifier形式标识，每条记录可动态添加</span><br><span class="line">（5）时间戳（timestamp）：时间戳用来区分数据的不同版本；</span><br><span class="line">（6）单元格（cell）：在table中，cell中存储的数据没有数据类型，是字节数组byte[] ，通过&lt;RowKey，Column Family: Column Qualifier，Timestamp&gt;元组来访问单元格</span><br></pre></td></tr></table></figure><h3 id="物理模型"><a href="#物理模型" class="headerlink" title="物理模型"></a>物理模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库特点：</span><br><span class="line">表结构预先定义；</span><br><span class="line">每列的数据类型不同；</span><br><span class="line">空值占用存储空间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HBase特点：</span><br><span class="line">只需定义表名和列族，可以动态添加列族和列；</span><br><span class="line">数据都是字符串类型；</span><br><span class="line">空值不占用存储空间；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172203728.png" alt="image-20230417220304381"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172214589.png" alt="image-20230417221413176"></p><h3 id="实际存储方式"><a href="#实际存储方式" class="headerlink" title="实际存储方式"></a>实际存储方式</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172218013.png" alt="image-20230417221843341"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172220818.png" alt="image-20230417222012709"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172220613.png" alt="image-20230417222052208"></p><h2 id="02-Hbase数据定义"><a href="#02-Hbase数据定义" class="headerlink" title="02 Hbase数据定义"></a>02 Hbase数据定义</h2><h3 id="HBase-Shell"><a href="#HBase-Shell" class="headerlink" title="HBase Shell"></a>HBase Shell</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase Shell：HBase的命令行工具，最简单的接口，适合HBase管理使用；</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost bin]# hbase shell</span><br><span class="line">HBase Shell; enter &#39;help&lt;RETURN&gt;&#39; for list of supported commands.</span><br><span class="line">Type &quot;exit&lt;RETURN&gt;&quot; to leave the HBase Shell</span><br><span class="line">Version 1.2.6, rUnknown, Mon May 29 02:25:32 CDT 2017</span><br><span class="line"></span><br><span class="line">hbase(main):001:0&gt; </span><br><span class="line"></span><br><span class="line">命令：help,status,version,exit,quit</span><br></pre></td></tr></table></figure><h3 id="数据定义"><a href="#数据定义" class="headerlink" title="数据定义"></a>数据定义</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172230877.png" alt="image-20230417223050379"></p><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法：creat‘表名’，‘列族名’</span><br><span class="line">描述：</span><br><span class="line">●  必须指定表名和列族；</span><br><span class="line">●  可以创建多个列族；</span><br><span class="line">●  可以对标和列族指明一些参数；</span><br><span class="line">●  参数大小写敏感；</span><br><span class="line">●  字符串参数需要包含在单引号中；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172233954.png" alt="image-20230417223313646"></p><h4 id="表相关操作"><a href="#表相关操作" class="headerlink" title="表相关操作"></a>表相关操作</h4><h5 id="exsit"><a href="#exsit" class="headerlink" title="exsit"></a>exsit</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exsit：查看某个表是否存在</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172239630.png" alt="image-20230417223907126"></p><h5 id="List"><a href="#List" class="headerlink" title="List"></a>List</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List：查看当前所有的表名</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172239913.png" alt="image-20230417223934111"></p><h5 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe：查看选定表的列族及其参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172236116.png" alt="image-20230417223645853"></p><h5 id="Alter"><a href="#Alter" class="headerlink" title="Alter"></a>Alter</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Alter：修改表结构</span><br><span class="line">功能：</span><br><span class="line">修改表中列族的参数信息；</span><br><span class="line">增加列族；</span><br><span class="line">移除或删除已有的列族；</span><br><span class="line"></span><br><span class="line">注意：删除列族时，表中至少有两个列族组成；</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172241746.png" alt="image-20230417224143643"></p><h5 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop：删除表</span><br><span class="line">注意：删除表之前需要先禁用表。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172242902.png" alt="image-20230417224227529"></p><h5 id="truncate"><a href="#truncate" class="headerlink" title="truncate"></a>truncate</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate：删除表中所有数据，想到于对表完成禁用、删除，按原结构重新建立表结构的过程</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172242617.png" alt="image-20230417224251643"></p><h2 id="03-Hbase数据操作"><a href="#03-Hbase数据操作" class="headerlink" title="03 Hbase数据操作"></a>03 Hbase数据操作</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172254865.png" alt="image-20230417225437539"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在HBase中对数据的增删改查命令如表所示，由put命令向表中添加和修改数据，get和scan命令用来查询数据，delete删除列族或列的数据。接下来详细介绍这几个命令的具体用法。</span><br></pre></td></tr></table></figure><h3 id="put"><a href="#put" class="headerlink" title="put"></a>put</h3><h4 id="为单元格插入数据"><a href="#为单元格插入数据" class="headerlink" title="为单元格插入数据"></a>为单元格插入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">语法：put  ‘表名’，‘行键’，‘列族：列限定符’，‘单元格值’，时间戳</span><br><span class="line">描述：必须指定表名、行键、列族、列限定符。</span><br><span class="line">参数区分大小，字符串使用单引号。</span><br><span class="line">只能插入单条数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172255143.png" alt="image-20230417225550657"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在HBase中，更新数据时，不管是添加新的数据还是修改数据都使用put命令，它的语法结构如ppt所示，put命令所带的第一个参数为表名，指定某一张表，第二参数为行键的名称，用来指定某一行，第三个参数是为列族和列的名称，中间用冒号隔开，列族名必须是已经创建的，否则HBase会报错；列名是临时定义的，所以列族里的列是可以随意扩展的。第四个参数为单元格的值，在HBase里，所有数据都是字符串的形式。最后一个参数为时间戳，如果不设置时间戳，系统会自动插入当前时间为时间戳。</span><br><span class="line">HBase中所有命令参数是区分大小写的，字符串是需要包含在单引号中的，这一点在介绍后面操作命令不再提示。</span><br><span class="line">从命令形式来看，put只能插入单元格的数据，如果需要将逻辑表中的一行数据插入到HBase中需要执行几条put命令。</span><br><span class="line">比如，需要将此逻辑表的第一行数据（左边图和红色虚线框）插入HBase中，需要执行5条命令（右图）</span><br></pre></td></tr></table></figure><h4 id="更新单元格数据"><a href="#更新单元格数据" class="headerlink" title="更新单元格数据"></a>更新单元格数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">描述：</span><br><span class="line">如果指定的单元格已经存在，则put为更新数据；</span><br><span class="line">单元格会保存指定version&#x3D;&gt;n的多个版本数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172302591.png" alt="image-20230417230227658"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">另外如果由‘表名’，‘行键’，‘列族：列限定符’指定的单元格已经存在表中，则执行put命令为数据更新操作，</span><br><span class="line"></span><br><span class="line">比如，在执行了左边的5条命令后（左图），再执行这条命令（鼠标指向“put ‘Student’, ‘0001’, ‘StuInfo:Name’,‘Tom Green‘,1），学号为1的学生姓名将改成了tom green。</span><br><span class="line"></span><br><span class="line">默认情况下数据更新后，旧版本的数据将不可见，但如果建表时对列族指定了Version属性值，则旧版数据依然存在，用户查询时可以获得最新的多个版本；</span><br></pre></td></tr></table></figure><h3 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">语法：delete  ‘表名’，‘行键’，‘列族&lt;：列限定符&gt;’，&lt;时间戳&gt;</span><br><span class="line">描述：必须指定表名、行键和列族，列限定符和时间戳是可选参数；</span><br><span class="line">Delete最小删除粒度为单元格，且不能跨列族删除。</span><br><span class="line"></span><br><span class="line">(1)delete ‘Student’, ‘0001’, ‘Grades’</span><br><span class="line">(2)delete ‘Student’, ‘0001’, ‘Grades:Math’ </span><br><span class="line">(3)delete ‘Student’, ‘0001’, ‘Grades:Math’,2</span><br><span class="line">(4)Deleteall ‘Student’, ‘0001’</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase中删除数据采用delete命令，其语法与put命令类似，必须指定表名，行键，和列族。而列限定符和时间戳是可选的</span><br><span class="line">下面通过举例说明delete的使用，如第一条命令，只指定了表名行键和列族，表示删除student表中，学号为0001的学生所有的成绩信息。即将表中第一行grades列族的信息全部删除。</span><br><span class="line">第二条命令，指定了列族和列限定符，表示只删除这个学生的数学成绩。</span><br><span class="line">第三条命令，指定了列族和列限定符的同时，还指定了时间戳，表示所有时间戳小于等于2的数据都会被删掉。注意这里不是只删除时间戳等于2的数据。</span><br><span class="line"></span><br><span class="line">从上面语法和命令来看，delete最小的删除粒度为单元格，而且不能跨列族删除，如果想删除表中所有列族在某个行键上的数据，也就是说想删除一个逻辑行，可以使用deleteall命令，例如第四条命令，则删除0001学号学生的所有信息，包括stuinfo列族中的基本信息和grades列族中的所有成绩信息。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete操作并不会马上删除数据，只是将对应的数据打上删除标记（tombstone），只有在数据产生合并时，数据才会被删除。</span><br></pre></td></tr></table></figure><h3 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h3><h4 id="get"><a href="#get" class="headerlink" title="get"></a>get</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">get：根据行键获取一条数据</span><br><span class="line">scan：扫描一个表，可以指定行键范围，或使用过滤器</span><br><span class="line">语法：get  ‘表名’，‘行键’，&lt;‘列族：列限定符’，时间戳&gt;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172313248.png" alt="image-20230417231312108"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第三条命令指定了列族和时间戳范围，</span><br><span class="line">第四条命令则指定列族和显示的版本数，其结果如图所示（蓝色图），在执行此命令之前先向表的stuinfo列族的name列插入了三个版本的数据，注意这里前提是stuinfo列族在创建时已指定VERSION参数可以保存最近的3个版本的数据。在向同一单元格put三条数据后，再执行第四条命令，显示的结果可以看到，只将最近两个更新的数据显示出来了（红色虚线框）</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172315547.png" alt="image-20230417231510110"></p><h4 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">语法：scan  ‘表名’，&#123;&lt; ‘列族：列限定符’，时间戳&gt;&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172321582.png" alt="image-20230417232102333"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">另外一种数据查询方式使用scan命令进行全表扫描，scan命令必须带的参数是表名，其他参数都可选，还可以指定输出行键范围，以及使用过滤器来对全表数据进行过滤显示。</span><br><span class="line">依然通过举例说明scan命令的方法。</span><br><span class="line">第一条命令指定表名查询全表数据；如图所示将表中所有行和所有列族信息都显示出来了。</span><br><span class="line">第二条命令指定列族名称，显示student表中stuinfo列族的所有数据，注意与get不同的是，get只获得某一行的，而scan获取所有行的stuinfo列族数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172324207.png" alt="image-20230417232442038"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第五条命令指定输出行的范围；显示结果输出起始行和结束行但不包括结束行的数据，如图的命令只显示了001行的数据，并没有显示003行。</span><br><span class="line"></span><br><span class="line">另外这些限定条件可以组合使用，中间使用逗号隔开，如第六条命令所示：查询起始行为001，结束行为002的所有行的stuinfo列族的数据信息。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-2</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.html</id>
    <published>2023-04-17T08:46:52.000Z</published>
    <updated>2023-04-17T13:51:33.733Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第二章-Hbase"><a href="#第二章-Hbase" class="headerlink" title="第二章 Hbase"></a>第二章 Hbase</h1><h2 id="01-Hbase简介"><a href="#01-Hbase简介" class="headerlink" title="01 Hbase简介"></a>01 Hbase简介</h2><h3 id="什么是HBase"><a href="#什么是HBase" class="headerlink" title="什么是HBase"></a>什么是HBase</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HBase是一个开源的NoSQL数据库，参考google的BigTable建模，使用Java语言实现，运行于HDFS文件系统上，为Hadoop提供类似BigTable的服务，可以存储海量稀疏的数据，并具备一定的容错性、高可靠性及伸缩性。</span><br><span class="line"></span><br><span class="line">具备NoSQL数据库的特点：</span><br><span class="line">不支持SQL的跨行事务</span><br><span class="line">不满足完整性约束条件</span><br><span class="line">灵活的数据模型</span><br></pre></td></tr></table></figure><h3 id="HBase的发展历程"><a href="#HBase的发展历程" class="headerlink" title="HBase的发展历程"></a>HBase的发展历程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Apache HBase最初是Powerset公司为了处理自然语言搜索产生的海量数据而开展的项目</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171750807.png" alt="image-20230417175013383"></p><h3 id="HBase特性"><a href="#HBase特性" class="headerlink" title="HBase特性"></a>HBase特性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">容量巨大</span><br><span class="line">列存储</span><br><span class="line">稀疏性</span><br><span class="line">扩展性</span><br><span class="line">高可靠性</span><br></pre></td></tr></table></figure><h4 id="容量巨大"><a href="#容量巨大" class="headerlink" title="容量巨大"></a>容量巨大</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171755056.png" alt="image-20230417175551796"></p><h4 id="列存储"><a href="#列存储" class="headerlink" title="列存储"></a>列存储</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171758370.png" alt="image-20230417175839159"></p><h4 id="稀疏性"><a href="#稀疏性" class="headerlink" title="稀疏性"></a>稀疏性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。</span><br></pre></td></tr></table></figure><h4 id="扩展性"><a href="#扩展性" class="headerlink" title="扩展性"></a>扩展性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">纵向扩展：不断优化主服务器的性能，提高存储空间和性能</span><br><span class="line"></span><br><span class="line">横向扩展：不断向集群添加服务器来提供存储空间和性能</span><br><span class="line"></span><br><span class="line">HBase是横向扩展的，理论上无限横向扩展</span><br></pre></td></tr></table></figure><h4 id="高可靠性"><a href="#高可靠性" class="headerlink" title="高可靠性"></a>高可靠性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于HDFS的多副本机制</span><br><span class="line"></span><br><span class="line">WAL（Write-Ahead-Log）预写机制</span><br><span class="line"></span><br><span class="line">Replication 机制</span><br></pre></td></tr></table></figure><h2 id="02-HDFS原理"><a href="#02-HDFS原理" class="headerlink" title="02 HDFS原理"></a>02 HDFS原理</h2><h3 id="HDFS-分布式文件系统"><a href="#HDFS-分布式文件系统" class="headerlink" title="HDFS- 分布式文件系统"></a>HDFS- 分布式文件系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HDFS即Hadoop分布式文件系统（Hadoop Distributed File System）</span><br><span class="line">提供高可靠性和高吞吐量的文件存储服务</span><br><span class="line"></span><br><span class="line">通过软件设计来保证系统的可靠性</span><br><span class="line"></span><br><span class="line">具有容错性，高可靠性，高可扩展性，高吞吐率。</span><br></pre></td></tr></table></figure><h3 id="HDFS基本架构"><a href="#HDFS基本架构" class="headerlink" title="HDFS基本架构"></a>HDFS基本架构</h3><h3 id="HDFS-块"><a href="#HDFS-块" class="headerlink" title="HDFS- 块"></a>HDFS- 块</h3><h3 id="HDFS-NameNode"><a href="#HDFS-NameNode" class="headerlink" title="HDFS-NameNode"></a>HDFS-NameNode</h3><h3 id="HDFS-SecondaryNameNode"><a href="#HDFS-SecondaryNameNode" class="headerlink" title="HDFS-SecondaryNameNode"></a>HDFS-SecondaryNameNode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">定期的合并edits和fsimage文件</span><br><span class="line">Checkpiont：合并的时间点，默认3600秒，或editlog文件达到64M。</span><br></pre></td></tr></table></figure><p><img src="/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-2%5C202304171810344.png" alt="image-20230417181053843"></p><h3 id="HDFS-DataNode"><a href="#HDFS-DataNode" class="headerlink" title="HDFS-DataNode"></a>HDFS-DataNode</h3><h3 id="HDFS读文件流程"><a href="#HDFS读文件流程" class="headerlink" title="HDFS读文件流程"></a>HDFS读文件流程</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171814207.png" alt="image-20230417181429835"></p><h4 id="HDFS读写机制-读文件机制"><a href="#HDFS读写机制-读文件机制" class="headerlink" title="HDFS读写机制-读文件机制"></a>HDFS读写机制-读文件机制</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171815977.png" alt="image-20230417181523597"></p><h4 id="HDFS读写机制-写文件机制"><a href="#HDFS读写机制-写文件机制" class="headerlink" title="HDFS读写机制-写文件机制"></a>HDFS读写机制-写文件机制</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171816393.png" alt="image-20230417181609459"></p><h3 id="HDFS副本机制"><a href="#HDFS副本机制" class="headerlink" title="HDFS副本机制"></a>HDFS副本机制</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171819478.png" alt="image-20230417181934890"></p><h3 id="HDFS容错"><a href="#HDFS容错" class="headerlink" title="HDFS容错"></a>HDFS容错</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171821864.png" alt="image-20230417182129386"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、NameNode出错：用Secondary NameNode备份的fsimage恢复</span><br><span class="line">2、DataNode出错：DataNode与NameNode通过“心跳”报告状态，当DataNode失效后，副本数减少，而NameNode会定期检查各节点的副本数量， 检查出问题后会启动数据冗余机制。</span><br><span class="line">3、数据出错：数据写入同时保存总和校验码，读取时校验。</span><br></pre></td></tr></table></figure><h2 id="03-Hbase组件和功能"><a href="#03-Hbase组件和功能" class="headerlink" title="03 Hbase组件和功能"></a>03 Hbase组件和功能</h2><h3 id="HBase-架构"><a href="#HBase-架构" class="headerlink" title="HBase 架构"></a>HBase 架构</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172115566.png" alt="image-20230417211518368"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172116455.png" alt="image-20230417211612969"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Client</span><br><span class="line">包含访问HBase的接口并维护cache来加快对HBase的访问</span><br><span class="line"></span><br><span class="line">Zookeeper</span><br><span class="line">保证任何时候，集群中只有一个活跃master</span><br><span class="line">存贮所有Region的寻址入口。</span><br><span class="line">实时监控Region server的上线和下线信息。并实时通知Master</span><br><span class="line">存储HBase的schema和table元数据</span><br><span class="line"></span><br><span class="line">Master</span><br><span class="line">为Region server分配region</span><br><span class="line">负责Region server的负载均衡</span><br><span class="line">发现失效的Region server并重新分配其上的region</span><br><span class="line">管理用户对table的增删改操作</span><br><span class="line"></span><br><span class="line">RegionServer</span><br><span class="line">Region server维护region，处理对这些region的IO请求</span><br><span class="line">Region server负责切分在运行过程中变得过大的region</span><br><span class="line"></span><br><span class="line">Region</span><br><span class="line">HBase自动把表水平划分成多个区域(region)，每个region会保存一个表里面某段连续的数据</span><br><span class="line"></span><br><span class="line">每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变）</span><br><span class="line"></span><br><span class="line">当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上。</span><br><span class="line"></span><br><span class="line">Memstore与storefile</span><br><span class="line">一个region由多个store组成，一个store对应一个CF（列族）</span><br><span class="line"></span><br><span class="line">store包括位于内存中的memstore和位于磁盘的storefile写操作先写入memstore，当memstore中的数据达到某个阈值，hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile</span><br><span class="line"></span><br><span class="line">当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile</span><br><span class="line"></span><br><span class="line">当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡</span><br><span class="line"></span><br><span class="line">客户端检索数据，先在memstore找，找不到再找storefile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。</span><br><span class="line">HRegion由一个或者多个Store组成，每个store保存一个columns family。</span><br><span class="line">每个Strore又由一个memStore和0至多个StoreFile组成。如图：StoreFile以HFile格式保存在HDFS上。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172142998.png" alt="image-20230417214228696"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304172144442.png" alt="image-20230417214420908"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>分布式数据库原理与应用-1</title>
    <link href="http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-1.html"/>
    <id>http://tianyong.fun/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8-1.html</id>
    <published>2023-04-17T08:46:47.000Z</published>
    <updated>2023-04-17T09:43:28.749Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h1><h2 id="数据库基本知识"><a href="#数据库基本知识" class="headerlink" title="数据库基本知识"></a>数据库基本知识</h2><h3 id="什么是数据库？"><a href="#什么是数据库？" class="headerlink" title="什么是数据库？"></a>什么是数据库？</h3><h3 id="什么是数据模型？"><a href="#什么是数据模型？" class="headerlink" title="什么是数据模型？"></a>什么是数据模型？</h3><h4 id="有哪些数据模型？"><a href="#有哪些数据模型？" class="headerlink" title="有哪些数据模型？"></a>有哪些数据模型？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库技术发展至今，传统数据库根据不同的数据模型，主要有以下几种：层次型、网状型和关系型。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171657536.png" alt="image-20230417165719863"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171700245.png" alt="image-20230417170008924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">关系模型要点回顾   </span><br><span class="line">1. 数据结构：</span><br><span class="line">现实世界的实体以及实体之间的各种联系均用关系来表示</span><br><span class="line">数据逻辑结构：二维表</span><br><span class="line">   </span><br><span class="line">   2. 完整性约束条件</span><br><span class="line">域完整性，实体完整性，参照完整性</span><br><span class="line"></span><br><span class="line">    3. 关系操作</span><br><span class="line">选择，投影，连接 等等关系运算；操作对象和结果都是集合</span><br></pre></td></tr></table></figure><h3 id="关系型数据库的优点"><a href="#关系型数据库的优点" class="headerlink" title="关系型数据库的优点"></a>关系型数据库的优点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关系型数据库的特点</span><br><span class="line">（1）容易理解：用二维表表示</span><br><span class="line">（2）使用方便：通用的SQL语言。</span><br><span class="line">（3）易于维护：丰富的完整性约束大大减低了数据冗余和数据不一致的可能性。</span><br></pre></td></tr></table></figure><h3 id="关系型数据库的不足"><a href="#关系型数据库的不足" class="headerlink" title="关系型数据库的不足"></a>关系型数据库的不足</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">对海量数据的读写效率低</span><br><span class="line">    表中有大量数据时，数据的读写速率非常的缓慢</span><br><span class="line">无法适应多变的数据结构</span><br><span class="line">    现代网络中存在大量的半结构化、非结构化数据，针对结构化数据而设计的关系型数据库系统来说，对这些不断变化的数据结构，很难进行高效的处理。</span><br><span class="line">高并发读写的瓶颈</span><br><span class="line">     当数据量达到一定规模时由于关系型数据库的系统逻辑非常复杂，使得在并发处理时非常容易发生死锁，导致其读写速度下滑严重。</span><br><span class="line">可扩展性的限制</span><br><span class="line">由于关系型数据库存在类似的join操作，使得数据库在扩展方面很困难。</span><br></pre></td></tr></table></figure><h2 id="NOSQL数据库理论基础"><a href="#NOSQL数据库理论基础" class="headerlink" title="NOSQL数据库理论基础"></a>NOSQL数据库理论基础</h2><h3 id="什么是NoSQL"><a href="#什么是NoSQL" class="headerlink" title="什么是NoSQL"></a>什么是NoSQL</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171707315.png" alt="image-20230417170659240"></p><h3 id="分布式数据库的特征"><a href="#分布式数据库的特征" class="headerlink" title="分布式数据库的特征"></a>分布式数据库的特征</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">分布式数据库必须具有如下特征，才能应对不断增长的海量数据。</span><br><span class="line">● 高可扩展性：分布式数据库必须具有高可扩展性，能够动态地增添存储节点以实现存储容量的线性扩展</span><br><span class="line">● 高并发性：分布式数据库必须及时响应大规模用户的读&#x2F;写请求，能对海量数据进行随机读写</span><br><span class="line">● 高可用性：分布式数据库必须提供容错机制，能够实现对数据的冗余备份，保证数据和服务的高度可靠性</span><br></pre></td></tr></table></figure><h3 id="NoSQL的特点"><a href="#NoSQL的特点" class="headerlink" title="NoSQL的特点"></a>NoSQL的特点</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171713669.png" alt="image-20230417171305105"></p><h3 id="分布式数据库的数据管理"><a href="#分布式数据库的数据管理" class="headerlink" title="分布式数据库的数据管理"></a>分布式数据库的数据管理</h3><h4 id="什么是数据库系统？"><a href="#什么是数据库系统？" class="headerlink" title="什么是数据库系统？"></a>什么是数据库系统？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库系统  &#x3D;  数据库管理系统     +     数据库</span><br></pre></td></tr></table></figure><h4 id="什么是数据库管理系统？"><a href="#什么是数据库管理系统？" class="headerlink" title="什么是数据库管理系统？"></a>什么是数据库管理系统？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据库管理系统(Database Management System)是一种操纵和管理数据库的大型软件，用于建立、使用和维护数据库，简称DBMS。主要任务就是对外提供数据，对内要管理数据。</span><br></pre></td></tr></table></figure><h4 id="数据处理方式：集中式VS分布式"><a href="#数据处理方式：集中式VS分布式" class="headerlink" title="数据处理方式：集中式VS分布式"></a>数据处理方式：集中式VS分布式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">集中式数据库是指数据库中的数据集中存储在一台计算机上，数据的处理也集中在一台机器上完成。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">分布式数据库是指利用高速计算机网络将物理上分散的多个数据存储单元连接起来组成一个逻辑上统一的数据库。</span><br></pre></td></tr></table></figure><h4 id="CAP理论"><a href="#CAP理论" class="headerlink" title="CAP理论"></a>CAP理论</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">C:一致性（consistency）（强一致性）</span><br><span class="line">它是指任何一个读操作总是能够读到之前完成的写操作的结果。所有节点在同一时间具有相同的数据。</span><br><span class="line"></span><br><span class="line">A:可用性（Availability）（高可用性）</span><br><span class="line">每个请求都能在确定时间内返回一个响应，无论请求是成功或失败。</span><br><span class="line"></span><br><span class="line">P:分区容忍性（Partition Tolerance）</span><br><span class="line">它是指在一个集群，即系统中的一部分节点无法和其他节点进行通信，系统也能正常运行。也就是说，系统中部分信息的丢失或失败不会影响系统的继续运作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">当处理CAP的问题时，可以有几个明显的选择：</span><br><span class="line">CA：也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最简单的做法是把所有与事务相关的内容都放到同一台机器上。</span><br><span class="line">CP：也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务</span><br><span class="line">AP：也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允许系统返回不一致的数据</span><br></pre></td></tr></table></figure><h5 id="设计原则：在C、A、P之中取舍"><a href="#设计原则：在C、A、P之中取舍" class="headerlink" title="设计原则：在C、A、P之中取舍"></a>设计原则：在C、A、P之中取舍</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171721678.png" alt="image-20230417172155421"></p><h2 id="ACID、BASE与一致性"><a href="#ACID、BASE与一致性" class="headerlink" title="ACID、BASE与一致性"></a>ACID、BASE与一致性</h2><h3 id="ACID与BASE"><a href="#ACID与BASE" class="headerlink" title="ACID与BASE"></a>ACID与BASE</h3><h4 id="为什么会出现ACID、BASE-？"><a href="#为什么会出现ACID、BASE-？" class="headerlink" title="为什么会出现ACID、BASE ？"></a>为什么会出现ACID、BASE ？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CAP理论定义了分布式存储的根本问题，但并没有指出一致性和可用性之间到底应该如何权衡。于是出现了ACID、BASE ，给出了权衡A与C的一种可行方案。</span><br><span class="line">ACID和BASE代表了在一致性-可用性两点之间进行选择的设计哲学</span><br><span class="line">ACID强调一致性被关系数据库使用，BASE强调可用性被大多数Nosql使用</span><br></pre></td></tr></table></figure><h4 id="ACID是什么？"><a href="#ACID是什么？" class="headerlink" title="ACID是什么？"></a>ACID是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">指数据库事务正确执行的四个基本要素的缩写。包含：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">原子性：一个事务的所有系列操作步骤被看成是一个动作，所有的步骤要么全部完成要么都不会完成。</span><br><span class="line"></span><br><span class="line">一致性：事务执行前后，数据库的状态都满足所有的完整性约束。不能发生表与表之间存在外键约束，但是有数据却违背这种约束性。</span><br><span class="line"></span><br><span class="line">隔离性：并发执行的事务是隔离的，保证多个事务互不影响，隔离能够确保并发执行的事务能够顺序一个接一个执行，通过隔离，一个未完成事务不会影响另外一个未完成事务。</span><br><span class="line"></span><br><span class="line">持久性：一个事务一旦提交，它对数据库中数据的改变就应该是永久性的，不会因为和其他操作冲突而取消这个事务。</span><br></pre></td></tr></table></figure><h4 id="BASE原则又是什么？"><a href="#BASE原则又是什么？" class="headerlink" title="BASE原则又是什么？"></a>BASE原则又是什么？</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BASE原则 &#x3D;   基本可用性（Basically Available）  +  软状态（Soft state）  +  最终一致性（Eventuallyconsistent）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基本可用性：分布式系统在出现故障的时候，允许损失部分可用性，即保证核心功能或者当前最重要功能可用，但是其他功能会被削弱。</span><br><span class="line"></span><br><span class="line">软状态：允许系统数据存在中间状态，但不会影响到系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步时存在延时。</span><br><span class="line"></span><br><span class="line">最终一致性：要求系统数据副本最终能够一致，而不需要实时保证数据副本一致。最终一致性是弱一致性的一种特殊情况。</span><br></pre></td></tr></table></figure><h2 id="NoSQL数据库分类"><a href="#NoSQL数据库分类" class="headerlink" title="NoSQL数据库分类"></a>NoSQL数据库分类</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171731393.png" alt="image-20230417173132275"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="分布式数据库原理与应用" scheme="http://tianyong.fun/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-5</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-5.html</id>
    <published>2023-04-15T17:08:43.000Z</published>
    <updated>2023-04-17T07:37:40.431Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-5"><a href="#第十四周-消息队列之Kafka从入门到小牛-5" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-5"></a>第十四周 消息队列之Kafka从入门到小牛-5</h1><h2 id="实战：Flume集成Kafka"><a href="#实战：Flume集成Kafka" class="headerlink" title="实战：Flume集成Kafka"></a>实战：Flume集成Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在实际工作中flume和kafka会深度结合使用</span><br><span class="line">1：flume采集数据，将数据实时写入kafka</span><br><span class="line">2：flume从kafka中消费数据，保存到hdfs，做数据备份</span><br><span class="line"></span><br><span class="line">下面我们就来看一个综合案例</span><br><span class="line">使用flume采集日志文件中产生的实时数据，写入到kafka中，然后再使用flume从kafka中将数据消费出来，保存到hdfs上面</span><br><span class="line">那为什么不直接使用flume将采集到的日志数据保存到hdfs上面呢？</span><br><span class="line">因为中间使用kafka进行缓冲之后，后面既可以实现实时计算，又可以实现离线数据备份，最终实现离线计算，所以这一份数据就可以实现两种需求，使用起来很方便，所以在工作中一般都会这样做。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171406839.png" alt="image-20230417140517700"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">下面我们来实现一下这个功能</span><br><span class="line">其实在Flume中，针对Kafka提供的有KafkaSource和KafkaSink</span><br><span class="line">KafkaSource是从kafka中读取数据</span><br><span class="line">KafkaSink是向kafka中写入数据</span><br><span class="line"></span><br><span class="line">所以针对我们目前这个架构，主要就是配置Flume的Agent。</span><br><span class="line">需要配置两个Agent：</span><br><span class="line">第一个Agent负责实时采集日志文件，将采集到的数据写入Kafka中</span><br><span class="line">第二个Agent负责从Kafka中读取数据，将数据写入HDFS中进行备份(落盘)</span><br><span class="line">针对第一个Agent：</span><br><span class="line">source：ExecSource，使用tail -F监控日志文件即可</span><br><span class="line">channel：MemoryChannel</span><br><span class="line">sink：KafkaSink</span><br><span class="line"></span><br><span class="line">针对第二个Agent</span><br><span class="line">Source：KafkaSource</span><br><span class="line">channel：MemoryChannel</span><br><span class="line">sink：HdfsSink</span><br><span class="line"></span><br><span class="line">这里面这些组件其实只有KafkaSource和KafkaSink我们没有使用过，其它的组件都已经用过了。</span><br></pre></td></tr></table></figure><h3 id="配置Agent"><a href="#配置Agent" class="headerlink" title="配置Agent"></a>配置Agent</h3><h4 id="file-to-kafka-conf"><a href="#file-to-kafka-conf" class="headerlink" title="file-to-kafka.conf"></a>file-to-kafka.conf</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来配置第一个Agent：</span><br><span class="line">文件名为： file-to-kafka.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;test.log</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line"></span><br><span class="line"># 指定topic名称</span><br><span class="line">a1.sinks.k1.kafka.topic &#x3D; test_r2p5</span><br><span class="line"># 指定kafka地址，多个节点地址使用逗号分割</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata03</span><br><span class="line"># 一次向kafka中写多少条数据，默认值为100，在这里为了演示方便，改为1</span><br><span class="line"># 在实际工作中这个值具体设置多少需要在传输效率和数据延迟上进行取舍</span><br><span class="line"># 如果kafka后面的实时计算程序对数据的要求是低延迟，那么这个值小一点比较好</span><br><span class="line"># 如果kafka后面的实时计算程序对数据延迟没什么要求，那么就考虑传输性能，一次多传输一些</span><br><span class="line"># 建议这个值的大小和ExecSource每秒钟采集的数据量大致相等，这样不会频繁向kafka中写数</span><br><span class="line">a1.sinks.k1.kafka.flumeBatchSize &#x3D; 1</span><br><span class="line">a1.sinks.k1.kafka.producer.acks &#x3D; 1</span><br><span class="line"># 一个Batch被创建之后，最多过多久，不管这个Batch有没有写满，都必须发送出去</span><br><span class="line"># linger.ms和flumeBatchSize(不积到设置的条数，则一直不写入到topic)，哪个先满足先按哪个规则执行，这个值默认是0，在这设置为1</span><br><span class="line">a1.sinks.k1.kafka.producer.linger.ms &#x3D; 1</span><br><span class="line"># 指定数据传输时的压缩格式，对数据进行压缩，提高传输效率</span><br><span class="line">a1.sinks.k1.kafka.producer.compression.type &#x3D; snappy</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka的producer的相关参数，可以直接在这里设置：a1.sinks.k1.kafka.producer.+。。。</span><br></pre></td></tr></table></figure><h4 id="kafka-to-hdfs-conf"><a href="#kafka-to-hdfs-conf" class="headerlink" title="kafka-to-hdfs.conf"></a>kafka-to-hdfs.conf</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面来配置第二个Agent：</span><br><span class="line">文件名为： kafka-to-hdfs.conf</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"># 一次性向channel中写入的最大数据量，在这为了演示方便，设置为1</span><br><span class="line"># 这个参数的值不要大于MemoryChannel中transactionCapacity的值</span><br><span class="line">a1.sources.r1.batchSize &#x3D; 1</span><br><span class="line"># 最大多长时间向channel写一次数据</span><br><span class="line">a1.sources.r1.batchDurationMillis &#x3D; 2000</span><br><span class="line"># kafka地址</span><br><span class="line">a1.sources.r1.kafka.bootstrap.servers &#x3D; bigdata01:9092,bigdata02:9092,bigdata</span><br><span class="line"># topic名称，可以指定一个或者多个，多个topic之间使用逗号隔开</span><br><span class="line"># 也可以使用正则表达式指定一个topic名称规则</span><br><span class="line">a1.sources.r1.kafka.topics &#x3D; test_r2p5</span><br><span class="line"># 指定消费者组id</span><br><span class="line">a1.sources.r1.kafka.consumer.group.id &#x3D; flume-con1</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;kafkaout</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data-</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04机器的flume目录下复制两个目录</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd &#x2F;data&#x2F;soft&#x2F;apache-flume-1.9.0-bin</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cp -r conf conf-file-to-kafka</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cp -r conf conf-kafka-to-hdfs</span><br><span class="line"></span><br><span class="line">修改 conf_file_to_kafka和conf_kafka_to_hdfs中log4j的配置</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-file-to-kafka</span><br><span class="line">[root@bigdata04 conf_file_to_kafka]# vi log4j.properties </span><br><span class="line">flume.root.logger&#x3D;ERROR,LOGFILE</span><br><span class="line">flume.log.file&#x3D;flume-file-to-kafka.log</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-kafka-to-hdfs</span><br><span class="line">[root@bigdata04 conf_kafka_to_hdfs]# vi log4j.properties </span><br><span class="line">flume.root.logger&#x3D;ERROR,LOGFILE</span><br><span class="line">flume.log.file&#x3D;flume-kafka-to-hdfs.log</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">把刚才配置的两个Agent的配置文件复制到这两个目录下</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-file-to-kafka</span><br><span class="line">[root@bigdata04 conf-file-to-kafka]# vi file-to-kafka.conf</span><br><span class="line">.....把file-to-kafka.conf文件中的内容复制进来即可</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# cd conf-kafka-to-hdfs&#x2F;</span><br><span class="line">[root@bigdata04 conf-kafka-to-hdfs]# vi kafka-to-hdfs.conf</span><br><span class="line">.....把kafka-to-hdfs.conf文件中的内容复制进来即可</span><br></pre></td></tr></table></figure><h3 id="启动Flume-Agent"><a href="#启动Flume-Agent" class="headerlink" title="启动Flume Agent"></a>启动Flume Agent</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">启动这两个Flume Agent</span><br><span class="line">确保zookeeper集群、kafka集群和Hadoop集群是正常运行的</span><br><span class="line">以及Kafka中的topic需要提前创建好</span><br><span class="line"></span><br><span class="line">创建topic</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 -partions 5 --replication-factor 2 --topic test_r2p5</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">先启动第二个Agent，再启动第一个Agent</span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf-kafka-to-hdfs --conf-file conf-kafka-to-hdfs&#x2F;kafka-to-hdfs.conf</span><br><span class="line"></span><br><span class="line">[root@bigdata04 apache-flume-1.9.0-bin]# bin&#x2F;flume-ng agent --name a1 --conf conf-file-to-kafka --conf-file conf-file-to-kafka&#x2F;file-to-kafka.conf</span><br><span class="line"></span><br><span class="line">模拟产生日志数据</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;log&#x2F;</span><br><span class="line">[root@bigdata04 log]# echo hello world &gt;&gt; &#x2F;data&#x2F;log&#x2F;test.log</span><br><span class="line"></span><br><span class="line">到HDFS上查看数据，验证结果：</span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -ls &#x2F;kafkaout</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r-- 2 root supergroup 12 2020-06-09 22:59 &#x2F;kafkaout&#x2F;data-.15</span><br><span class="line">[root@bigdata04 ~]# hdfs dfs -cat &#x2F;kafkaout&#x2F;data-.1591714755267.tmp</span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line">此时Flume可以通过tail -F命令实时监控文件中的新增数据，发现有新数据就写入kafka，然后kafka后面的flume落盘程序，以及kafka后面的实时计算程序就可以使用这份数据了。</span><br></pre></td></tr></table></figure><h2 id="实战：Kafka集群平滑升级"><a href="#实战：Kafka集群平滑升级" class="headerlink" title="实战：Kafka集群平滑升级"></a>实战：Kafka集群平滑升级</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">之前我们在使用 Kafka 0.9.0.0版本的时候，遇到一个比较诡异的问题</span><br><span class="line">（背景：这个版本他们遇到一个问题，官方通过升级kafka版本解决了，但他们之前的版本工作中运用于直播平台，所以不可能将集群停了重新部署一套）</span><br><span class="line">针对消费者组增加消费者的时候可能会导致rebalance，进而导致部分consumer不能再消费分区数据</span><br><span class="line">意思就是之前针对这个topic的5个分区只有2个消费者消费数据，后期我动态的把消费者调整为了5个，这样可能会导致部分消费者无法消费分区中的数据。</span><br><span class="line"></span><br><span class="line">针对这个bug这里有一份详细描述：</span><br><span class="line">https:&#x2F;&#x2F;issues.apache.org&#x2F;jira&#x2F;browse&#x2F;KAFKA-2978</span><br><span class="line">此bug官方在0.9.0.1版本中进行了修复</span><br><span class="line">当时我们的线上集群使用的就是0.9.0.0的版本。</span><br><span class="line"></span><br><span class="line">所以我们需要对线上集群在不影响线上业务的情况下进行升级，称为平滑升级(滚动升级)，也就是升级的时候不影响线上的正常业务运行(但还是要选择在业务低峰期时进行升级)。</span><br><span class="line"></span><br><span class="line">接下来我们就查看了官网文档(0.9.0.0)，上面有针对集群平滑升级的一些信息</span><br><span class="line">http:&#x2F;&#x2F;kafka.apache.org&#x2F;090&#x2F;documentation.html#upgrade</span><br><span class="line">在验证这个升级流程的时候我们是在测试环境下，先模拟线上的集群环境，进行充分测试，可千万不能简单测试一下就直接搞到测试环境去做，这样是很危险的。</span><br><span class="line">由于当时这个kafka集群我们还没有移交给运维负责，并且运维当时对这个框架也不是很熟悉，所以才由我们开发人员来进行平滑升级，否则这种框架升级的事情肯定是交给运维去做的。</span><br><span class="line"></span><br><span class="line">那接下来看一下具体的平滑升级步骤</span><br><span class="line">小版本之间集群升级不需要额外修改集群的配置文件。只需要按照下面步骤去执行即可。</span><br><span class="line">假设kafka0.9.0.0集群在三台服务器上，需要把这三台服务器上的kafka集群升级到0.9.0.1版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：提前在集群的三台机器上把0.9.0.1的安装包，解压、配置好。</span><br><span class="line">主要是log.dirs这个参数，0.9.0.1中的这个参数和0.9.0.0的这个参数一定要保持一致，这样新版本的kafka才可以识别之前的kakfa中的数据。</span><br><span class="line">在集群升级的过程当中建议通过CMAK(kafkamanager)查看集群的状态信息，比较方便</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171529822.png" alt="image-20230417152957587"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1：先stop掉0.9.0.0集群中的第一个节点，然后去CMAK上查看集群的broker信息，确认节点确实已停掉。并且再查看一下，节点的副本下线状态。确认集群是否识别到副本下线状态。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171531285.png" alt="image-20230417153133889"></p><p> <img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171533475.png" alt="image-20230417153328093"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然后在当前节点把kafka0.9.0.1启动起来。再回到CMAK中查看broker信息，确认刚启动的节点是否已正确显示，并且还要确认这个节点是否可以正常接收和发送数据。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304171534542.png" alt="image-20230417153455185"></p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2：按照第一步的流程去依次操作剩余节点即可，就是先把0.9.0.0版本的kafka停掉，再把0.9.0.1版本的kafka启动即可。</span><br><span class="line"></span><br><span class="line">注意：每操作一个节点，需要稍等一下，确认这个节点可以正常接收和发送数据之后，再处理下一个节点。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.html</id>
    <published>2023-04-15T16:08:29.000Z</published>
    <updated>2023-04-16T16:21:23.023Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="Kafka技巧篇"><a href="#Kafka技巧篇" class="headerlink" title="Kafka技巧篇"></a>Kafka技巧篇</h1><h2 id="Kafka集群参数调忧"><a href="#Kafka集群参数调忧" class="headerlink" title="Kafka集群参数调忧"></a>Kafka集群参数调忧</h2><h3 id="JVM参数调忧"><a href="#JVM参数调忧" class="headerlink" title="JVM参数调忧"></a>JVM参数调忧</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">默认启动的Broker进程只会使用1G内存，在实际使用中会导致进程频繁GC，影响Kafka集群的性能和稳</span><br><span class="line">定性</span><br><span class="line">通过 jstat -gcutil &lt;pid&gt; 1000 查看到kafka进程GC情况</span><br><span class="line">主要看 YGC,YGCT,FGC,FGCT 这几个参数，如果这几个值不是很大，就没什么问题</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">YGC：young gc发生的次数</span><br><span class="line">YGCT：young gc消耗的时间</span><br><span class="line">FGC：full gc发生的次数</span><br><span class="line">FGCT：full gc消耗的时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">13248 Kafka</span><br><span class="line">18087 Jps</span><br><span class="line">1679 QuorumPeerMain</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jstat -gcutil 13248 1000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162304926.png" alt="image-20230416230418172"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果你发现YGC很频繁，或者FGC很频繁，就说明内存分配的少了</span><br><span class="line">此时需要修改kafka-server-start.sh中的KAFKA_HEAP_OPTS</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export KAFKA_HEAP_OPTS&#x3D;&quot;-Xmx10g -Xms10g -XX:MetaspaceSize&#x3D;96m -XX:+UseG1GC -XX</span><br><span class="line"></span><br><span class="line">xms:初始化内存</span><br><span class="line">xmx:最大内存</span><br><span class="line">建议设置成一样大，否则可能进行内存交换</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个配置表示给kafka分配了10G内存</span><br></pre></td></tr></table></figure><h3 id="Replication参数调忧"><a href="#Replication参数调忧" class="headerlink" title="Replication参数调忧"></a>Replication参数调忧</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">replica.socket.timeout.ms&#x3D;60000</span><br><span class="line">这个参数的默认值是30秒，它是控制partiton副本之间socket通信的超时时间，如果设置的太小，有可能会由于网络原因导致造成误判，认为某一个partition副本连不上了。</span><br><span class="line"></span><br><span class="line">replica.lag.time.max.ms&#x3D;50000</span><br><span class="line">如果一个副本在指定的时间内没有向leader节点发送任何请求，或者在指定的时间内没有同步完leader中的数据，则leader会将这个节点从Isr列表中移除。</span><br><span class="line"></span><br><span class="line">这个参数的值默认为10秒</span><br><span class="line">如果网络不好，或者kafka压力较大，建议调大该值，否则可能会频繁出现副本丢失，进而导致集群需要频繁复制副本，导致集群压力更大，会陷入一个恶性循环</span><br></pre></td></tr></table></figure><h3 id="Log参数调优"><a href="#Log参数调优" class="headerlink" title="Log参数调优"></a>Log参数调优</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这块是针对Kafka中数据文件的删除时机进行设置，不是对kafka本身的日志参数配置</span><br><span class="line">log.retention.hours&#x3D;24</span><br><span class="line">这个参数默认值为168，单位是小时，就是7天，默认对数据保存7天，可以在这调整数据保存的时间，我们在实际工作中改为了只保存1天，因为kafka中的数据我们会在hdfs中进行备份，保存一份，所以就没有必要在kafka中保留太长时间了。</span><br><span class="line"></span><br><span class="line">在kafka中保留只是为了能够让你在指定的时间内恢复数据，或者重新消费数据，如果没有这种需求，那就没有必要设置太长时间。</span><br><span class="line"></span><br><span class="line">这里分析的Replication的参数和Log参数都是在 server.properties文件中进行配置</span><br><span class="line"></span><br><span class="line">JVM参数是在kafka-server-start.sh脚本中配置</span><br><span class="line"></span><br><span class="line">broker参数调优更多在开发文档里有</span><br></pre></td></tr></table></figure><h2 id="Kafka-Topic命名小技巧"><a href="#Kafka-Topic命名小技巧" class="headerlink" title="Kafka Topic命名小技巧"></a>Kafka Topic命名小技巧</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">针对Kafka中Topic命名的小技巧</span><br><span class="line">建议在给topic命名的时候在后面跟上r2p10之类的内容</span><br><span class="line">r2：表示Partition的副本因子是2</span><br><span class="line">p10：表示这个Topic的分区数是10</span><br><span class="line"></span><br><span class="line">这样的好处是后期我们如果要写消费者消费指定topic的数据，通过topic的名称我们就知道应该设置多少个消费者消费数据效率最高。</span><br><span class="line">因为一个partition同时只能被一个消费者消费，所以效率最高的情况就是消费者的数量和topic的分区数量保持一致。在这里通过topic的名称就可以直接看到，一目了然。</span><br><span class="line"></span><br><span class="line">但是也有一个缺点，就是后期如果我们动态调整了topic的partiton，那么这个topic名称上的partition数量就不准了，针对这个topic，建议大家一开始的时候就提前预估一下，可以多设置一些partition，我们</span><br><span class="line">在工作中的时候针对一些数据量比较大的topic一般会设置40-50个partition，数据量少的topic一般设置5-10个partition，这样后期调整topic partiton数量的场景就比较少了。</span><br></pre></td></tr></table></figure><h2 id="Kafka集群监控管理工具"><a href="#Kafka集群监控管理工具" class="headerlink" title="Kafka集群监控管理工具"></a>Kafka集群监控管理工具</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">现在我们操作Kafka都是在命令行界面中通过脚本操作的，后面需要传很多参数，用起来还是比较麻烦的，那kafka没有提供web界面的支持吗？</span><br><span class="line">很遗憾的告诉你，Apache官方并没有提供，不过好消息是有一个由雅虎开源的一个工具，目前用起来还是不错的。</span><br><span class="line"></span><br><span class="line">它之前的名字叫KafkaManager，后来改名字了，叫CMAK</span><br><span class="line">CMAK是目前最受欢迎的Kafka集群管理工具，最早由雅虎开源，用户可以在Web界面上操作Kafka集群</span><br><span class="line">可以轻松检查集群状态(Topic、Consumer、Offset、Brokers、Replica、Partition)</span><br><span class="line"></span><br><span class="line">那下面我们先去下载这个CMAK</span><br><span class="line">需要到github上面去下载</span><br><span class="line">在github里面搜索CMAK即可</span><br></pre></td></tr></table></figure><h3 id="下载CMAK"><a href="#下载CMAK" class="headerlink" title="下载CMAK"></a>下载CMAK</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162330282.png" alt="image-20230416233039880"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162330225.png" alt="image-20230416233050053"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162331098.png" alt="image-20230416233123771"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162331745.png" alt="image-20230416233141274"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：由于cmak-3.0.0.4.zip版本是在java11这个版本下编译的，所以在运行的时候也需要使用java11这个版本，我们目前服务器上使用的是java8这个版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">我们为什么不使用java11版本呢？因为自2019年1月1日1起，java8之后的更新版本在商业用途的时候就需要收费授权了。</span><br><span class="line">在这针对cmak-3.0.0.4这个版本，如果我们想要使用的话有两种解决办法</span><br><span class="line">1：下载cmak的源码，使用jdk8编译</span><br><span class="line">2：额外安装一个jdk11(自己用不属于商业用途，现实公司很少有用java8以后的)</span><br><span class="line">如果想要编译的话需要安装sbt这个工具对源码进行编译，sbt是Scala 的构建工具, 类似于Maven。</span><br><span class="line"></span><br><span class="line">由于我们在这使用不属于商业用途，所以使用jdk11是没有问题的，那就不用重新编译了。</span><br><span class="line">下载jdk11，jdk-11.0.7_linux-x64_bin.tar.gz</span><br><span class="line">将jdk11的安装包上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">只需要解压即可，不需要配置环境变量，因为只有cmak这个工具才需要使用jdk11</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# tar -zxvf jdk-11.0.7_linux-x64_bin.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">接下来把 cmak-3.0.0.4.zip 上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">1：解压</span><br><span class="line">[root@bigdata01 soft]# unzip cmak-3.0.0.4.zip </span><br><span class="line">-bash: unzip: command not found</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：如果提示-bash: unzip: command not found，则说明目前不支持unzip命令，可以使用yum在线安装</span><br><span class="line">建议先清空一下yum缓存，否则使用yum可能无法安装unzip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 soft]# yum clean all </span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Cleaning repos: base extras updates</span><br><span class="line">Cleaning up list of fastest mirrors</span><br><span class="line"></span><br><span class="line">[root@bigdata01 soft]# yum install -y unzip</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">.....</span><br><span class="line">Running transaction</span><br><span class="line"> Installing : unzip-6.0-21.el7.x86_64 1&#x2F;1 </span><br><span class="line"> Verifying : unzip-6.0-21.el7.x86_64 1&#x2F;1 </span><br><span class="line">Installed:</span><br><span class="line"> unzip.x86_64 0:6.0-21.el7 </span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">再重新解压</span><br><span class="line">[root@bigdata01 soft]# unzip cmak-3.0.0.4.zip</span><br></pre></td></tr></table></figure><h3 id="配置CMAK"><a href="#配置CMAK" class="headerlink" title="配置CMAK"></a>配置CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2：修改CMAK配置</span><br><span class="line">首先修改bin目录下的cmak脚本</span><br><span class="line">在里面配置JAVA_HOME指向jdk11的安装目录，否则默认会使用jdk8</span><br><span class="line">[root@bigdata01 soft]# cd cmak-3.0.0.4</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# cd bin&#x2F;</span><br><span class="line">[root@bigdata01 bin]# vi cmak</span><br><span class="line">....</span><br><span class="line">JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk-11.0.7</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">然后修改conf目录下的application.conf文件</span><br><span class="line">只需要在里面增加一行cmak.zkhosts参数的配置即可，指定zookeeper的地址</span><br><span class="line"></span><br><span class="line">注意：在这里指定zookeeper地址主要是为了让CMAK在里面保存数据，这个zookeeper地址不一定是kafka集群使用的那个zookeeper集群，随便哪个zookeeper集群都可以。(cmak需要报错它自己的东西)</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# cd conf&#x2F;</span><br><span class="line">[root@bigdata01 conf]# vi application.conf </span><br><span class="line">....</span><br><span class="line">cmak.zkhosts&#x3D;&quot;bigdata01:2181,bigdata02:2181,bigdata03:2181&quot;</span><br><span class="line">....</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3：修改kafka启动配置</span><br><span class="line">想要在CMAK中查看kafka的一些指标信息，在启动kafka的时候需要指定JMX_PORT</span><br><span class="line"></span><br><span class="line">停止kafka集群</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh </span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh </span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">重新启动kafka集群，指定JXM_PORT</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# JMX_PORT&#x3D;9988 bin&#x2F;kafka-server-start.sh -d</span><br></pre></td></tr></table></figure><h3 id="启动CMAK"><a href="#启动CMAK" class="headerlink" title="启动CMAK"></a>启动CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">4：启动cmak</span><br><span class="line">[root@bigdata01 cmak-3.0.0.4]# bin&#x2F;cmak -Dconfig.file&#x3D;conf&#x2F;application.conf -</span><br><span class="line"></span><br><span class="line">如果想把cmak放在后台执行的话需要添加上nohup和&amp;</span><br><span class="line">1 [root@bigdata01 cmak-3.0.0.4]# nohup bin&#x2F;cmak -Dconfig.file&#x3D;conf&#x2F;application.conf -Dhttp.port&#x3D;9001 &amp;</span><br><span class="line"></span><br><span class="line">cmak默认监听端口9000，但这样和hdfs的端口重复了</span><br></pre></td></tr></table></figure><h3 id="访问CMAK"><a href="#访问CMAK" class="headerlink" title="访问CMAK"></a>访问CMAK</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5：访问cmak</span><br><span class="line">http:&#x2F;&#x2F;bigdata01:9001&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162356804.png" alt="image-20230416235630637"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6：操作CMAK</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-4%5C202304162355045.png" alt="image-20230416235520700"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162355502.png" alt="image-20230416235548395"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这几个参数配置好了以后还需要配置以下几个线程池相关的参数，这几个参数默认值是1，在保存的时候会提示需要大于1，所以可以都改为10</span><br><span class="line">最后点击Save按钮保存即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brokerViewThreadPoolSize：10</span><br><span class="line">offsetCacheThreadPoolSize：10</span><br><span class="line">kafkaAdminClientThreadPoolSize：10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后进来是这样的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170003610.png" alt="image-20230417000304032"></p><h4 id="查看kafak集群的所有broker信息"><a href="#查看kafak集群的所有broker信息" class="headerlink" title="查看kafak集群的所有broker信息"></a>查看kafak集群的所有broker信息</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170003829.png" alt="image-20230417000358325"></p><h4 id="查看kafak集群的所有topic信息"><a href="#查看kafak集群的所有topic信息" class="headerlink" title="查看kafak集群的所有topic信息"></a>查看kafak集群的所有topic信息</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170008382.png" alt="image-20230417000847024"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170007471.png" alt="image-20230417000718104"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170007023.png" alt="image-20230417000734624"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击topic的消费者信息是可以进来查看的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170016794.png" alt="image-20230417001617419"></p><h4 id="创建一个topic"><a href="#创建一个topic" class="headerlink" title="创建一个topic"></a>创建一个topic</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170016835.png" alt="image-20230417001657421"></p><h4 id="给topic增加分区"><a href="#给topic增加分区" class="headerlink" title="给topic增加分区"></a>给topic增加分区</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304170017969.png" alt="image-20230417001716328"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是CMAK中常见的功能，当然了这里面还要一些我们没有说到的功能就留给大家以后来发掘了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.html</id>
    <published>2023-04-15T14:14:17.000Z</published>
    <updated>2023-04-17T10:41:51.733Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-3"><a href="#第十四周-消息队列之Kafka从入门到小牛-3" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-3"></a>第十四周 消息队列之Kafka从入门到小牛-3</h1><h2 id="Kafka核心之存储和容错机制"><a href="#Kafka核心之存储和容错机制" class="headerlink" title="Kafka核心之存储和容错机制"></a>Kafka核心之存储和容错机制</h2><h3 id="存储策略"><a href="#存储策略" class="headerlink" title="存储策略"></a>存储策略</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在kafka中每个topic包含1到多个partition，每个partition存储一部分Message。每条Message包含三个属性，其中有一个是offset。</span><br><span class="line"></span><br><span class="line">问题来了：offset相当于partition中这个message的唯一id，那么如何通过id高效的找到message？</span><br><span class="line">两大法宝：分段+索引(分段表示一个partition会存储多个文件)</span><br><span class="line"></span><br><span class="line">kafak中数据的存储方式是这样的：</span><br><span class="line">1、每个partition由多个segment【片段】组成，每个segment文件中存储多条消息，</span><br><span class="line">2、每个partition在内存中对应一个index，记录每个segment文件中的第一条消息偏移量。</span><br><span class="line"></span><br><span class="line">Kafka中数据的存储流程是这样的：</span><br><span class="line">生产者生产的消息会被发送到topic的多个partition上，topic收到消息后往对应partition的最后一个segment上添加该消息，segment达到一定的大小后会创建新的segment。</span><br><span class="line">来看这个图，可以认为是针对topic中某个partition的描述</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160035193.png" alt="image-20230416003228787"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">图中左侧就是索引，右边是segment文件，左边的索引里面会存储每一个segment文件中第一条消息的偏移量，由于消息的偏移量都是递增的，这样后期查找起来就方便了，先到索引中判断数据在哪个</span><br><span class="line">segment文件中，然后就可以直接定位到具体的segment文件了，这样再找具体的那一条数据就很快了，因为都是有序的。</span><br></pre></td></tr></table></figure><h3 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h3><h4 id="Broker节点宕机"><a href="#Broker节点宕机" class="headerlink" title="Broker节点宕机"></a>Broker节点宕机</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">当Kafka集群中的一个Broker节点宕机，会出现什么现象？</span><br><span class="line"></span><br><span class="line">下面来演示一下</span><br><span class="line">使用kill -9 杀掉bigdata01中的broker进程测试</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">7522 Jps</span><br><span class="line">2054 Kafka</span><br><span class="line">1679 QuorumPeerMain</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# kill 2054</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">我们可以先通过zookeeper来查看一下，因为当kafka集群中的broker节点启动之后，会自动向zookeeper中进行注册，保存当前节点信息</span><br><span class="line">....]# bin&#x2F;zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers</span><br><span class="line">[ids, seqid, topics]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[1, 2]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160046086.png" alt="image-20230416004647924"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时发现zookeeper的&#x2F;brokers&#x2F;ids下面只有2个节点信息</span><br><span class="line">可以通过get命令查看节点信息，这里面会显示对应的主机名和端口号</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] get &#x2F;brokers&#x2F;ids&#x2F;1</span><br><span class="line">&#123;&quot;listener_security_protocol_map&quot;:&#123;&quot;PLAINTEXT&quot;:&quot;PLAINTEXT&quot;&#125;,&quot;endpoints&quot;:[&quot;PLA</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160050443.png" alt="image-20230416005045245"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">然后再使用describe查询topic的详细信息，会发现此时的分区的leader全部变成了目前存活的另外两个节点</span><br><span class="line"></span><br><span class="line">此时可以发现Isr中的内容和Replicas中的不一样了，因为Isr中显示的是目前正常运行的节点</span><br><span class="line"></span><br><span class="line">所以当Kafka集群中的一个Broker节点宕机之后，对整个集群而言没有什么特别的大影响，此时集群会给partition重新选出来一些新的Leader节点</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160053702.png" alt="image-20230416005338215"></p><h4 id="新增一个Broker节点"><a href="#新增一个Broker节点" class="headerlink" title="新增一个Broker节点"></a>新增一个Broker节点</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当Kafka集群中新增一个Broker节点，会出现什么现象？</span><br><span class="line">新加入一个broker节点，zookeeper会自动识别并在适当的机会选择此节点提供服务</span><br><span class="line"></span><br><span class="line">再次启动bigdata01节点中的broker进程测试</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">此时到zookeeper中查看一下</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line">.....</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;brokers</span><br><span class="line">[ids, seqid, topics]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls &#x2F;brokers&#x2F;ids</span><br><span class="line">[0, 1, 2]</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160058241.png" alt="image-20230416005822131"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">发现broker.id为0的这个节点信息也有了</span><br><span class="line"></span><br><span class="line">在通过describe查看topic的描述信息，Isr中的信息和Replicas中的内容是一样的了</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper localhost:2181 --topic hello</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160059016.png" alt="image-20230416005947958"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">但是启动后有个问题：发现新启动的这个节点不会是任何分区的leader？怎么重新均匀分配呢？</span><br><span class="line">1、Broker中的自动均衡策略（默认已经有）</span><br><span class="line">auto.leader.rebalance.enable&#x3D;true</span><br><span class="line">leader.imbalance.check.interval.seconds 默认值：300</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2、手动执行：</span><br><span class="line">bin&#x2F;kafka-leader-election.sh --bootstrap-server localhost:9092 --election-type pareferred --all-topic-partitions</span><br><span class="line"></span><br><span class="line">Successfully completed leader election (PREFERRED) for partitions hello-4, he</span><br><span class="line"></span><br><span class="line">执行后的效果如下，这样就实现了均匀分配</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160105050.png" alt="image-20230416010500020"></p><h2 id="Kafka生产消费者实战"><a href="#Kafka生产消费者实战" class="headerlink" title="Kafka生产消费者实战"></a>Kafka生产消费者实战</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前面我们使用基于console的生产者和消费者对topic实现了数据的生产和消费，，这个基于控制台的生产者和消费者主要是让我们做测试用的</span><br><span class="line">在实际工作中，我们有时候需要将生产者和消费者功能集成到我们已有的系统中，此时就需要写代码实现生产者和消费者的逻辑了。</span><br><span class="line">在这我们使用java代码来实现生产者和消费者的功能</span><br></pre></td></tr></table></figure><h3 id="Kafka-Java代码编程"><a href="#Kafka-Java代码编程" class="headerlink" title="Kafka Java代码编程"></a>Kafka Java代码编程</h3><h4 id="Java代码实现生产者代码"><a href="#Java代码实现生产者代码" class="headerlink" title="Java代码实现生产者代码"></a>Java代码实现生产者代码</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160114267.png" alt="image-20230416011434393"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">先创建maven项目， db_kafka</span><br><span class="line"></span><br><span class="line">添加kafka的maven依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.kafka&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;kafka-clients&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;2.4.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">开发生产者代码</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.kafka;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Java代码实现生产者代码</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerDemo</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">         <span class="comment">//指定kafka的broker地址</span></span><br><span class="line">         prop.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">         <span class="comment">//指定key-value数据的序列化格式(key就是之前讲的，如果指定了数据有key，则可以根据它来将数据放入哪一个partition，一般用不到；但这里要知道不然要报错)</span></span><br><span class="line">         prop.put(<span class="string">"key.serializer"</span>, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         prop.put(<span class="string">"value.serializer"</span>, StringSerializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定topic</span></span><br><span class="line">         String topic = <span class="string">"hello"</span>; </span><br><span class="line">         <span class="comment">//创建kafka生产者</span></span><br><span class="line">         KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String,String&gt;(prop);</span><br><span class="line">         <span class="comment">//向topic中生产数据(这里也没有传入key，只传入了value)</span></span><br><span class="line">         producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="string">"hello kafka"</span>))</span><br><span class="line">         <span class="comment">//关闭链接</span></span><br><span class="line">         producer.close();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Java代码实现消费者代码"><a href="#Java代码实现消费者代码" class="headerlink" title="Java代码实现消费者代码"></a>Java代码实现消费者代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.kafka;</span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Collection;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：Java代码实现消费者代码</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerDemo</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">         <span class="comment">//指定kafka的broker地址(之前控制台那里server没s)</span></span><br><span class="line">         prop.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"bigdata01:9092,bigdata02:9092,bigdata03:9092"</span>);</span><br><span class="line">         <span class="comment">//指定key-value的反序列化类型</span></span><br><span class="line">         prop.put(<span class="string">"key.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         prop.put(<span class="string">"value.deserializer"</span>, StringDeserializer<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定消费者组(之前控制台那里，会自动生成)</span></span><br><span class="line">         prop.put(<span class="string">"group.id"</span>, <span class="string">"con-1"</span>);</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//创建消费者</span></span><br><span class="line">         KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String,String&gt;(prop);</span><br><span class="line">        Collection&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">         topics.add(<span class="string">"hello"</span>);</span><br><span class="line">         <span class="comment">//订阅指定的topic</span></span><br><span class="line">         consumer.subscribe(topics);</span><br><span class="line">         <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">             <span class="comment">//消费数据【注意：需要修改jdk编译级别为1.8，否则Duration.ofSeconds(1)会语法报错</span></span><br><span class="line">             ConsumerRecords&lt;String, String&gt; poll = consumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">             <span class="keyword">for</span> (ConsumerRecord&lt;String,String&gt; consumerRecord : poll) &#123;</span><br><span class="line">             System.out.println(consumerRecord);</span><br><span class="line">             &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1. 关闭kafka服务器的防火墙</span><br><span class="line">2. 配置windows的hosts文件 添加kafka节点的hostname和ip的映射关系。[如果我们的hosts文件中没有对kafka节点的hostnam和ip的映射关系做配置，在这经过多次尝试连接不上就会报错]</span><br><span class="line"></span><br><span class="line">先开启消费者。</span><br><span class="line">发现没有消费到数据，这个topic中是有数据的，为什么之前的数据没有消费出来呢？(就是前面讲的，默认会从consumer生成后生成的数据读取)不要着急，先带着这个问题往下面看</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-3%5Cimage-20230416014022148.png" alt="image-20230416014022148"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再开启生产者，生产者会生产一条数据，然后就结束</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160143273.png" alt="image-20230416014143241"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时回到kafka的消费者端就可以看到消费出来的数据了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160142342.png" alt="image-20230416014214263"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以这个时候我们发现，新产生的数据我们是可以消费到的，但是之前的数据我们就无法消费了，那下面我们来分析一下这个问题</span><br></pre></td></tr></table></figure><h4 id="消费者代码扩展"><a href="#消费者代码扩展" class="headerlink" title="消费者代码扩展"></a>消费者代码扩展</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x2F;&#x2F;开启消费者自动提交offset功能，默认就是开启的</span><br><span class="line">prop.put(&quot;enable.auto.commit&quot;,&quot;true&quot;);</span><br><span class="line">&#x2F;&#x2F;自动提交offset的时间间隔，单位是毫秒(在开启自动提交时，它默认开启，且默认值是5000)</span><br><span class="line">prop.put(&quot;auto.commit.interval.ms&quot;,&quot;5000&quot;);</span><br><span class="line"></span><br><span class="line">&#x2F;*</span><br><span class="line">注意：正常情况下，kafka消费数据的流程是这样的</span><br><span class="line">先根据group.id指定的消费者组到kafka中查找之前保存的offset信息</span><br><span class="line"></span><br><span class="line">如果查找到了，说明之前使用这个消费者组消费过数据，则根据之前保存的offset继续进行消费</span><br><span class="line"></span><br><span class="line">如果没查找到(说明第一次消费)，或者查找到了，但是查找到的那个offset对应的数据已经不存</span><br><span class="line"></span><br><span class="line">这个时候消费者该如何消费数据？</span><br><span class="line">(因为kafka默认只会保存7天的数据，超过时间数据会被删除)</span><br><span class="line"></span><br><span class="line">此时会根据auto.offset.reset的值执行不同的消费逻辑</span><br><span class="line"></span><br><span class="line">这个参数的值有三种:[earliest,latest,none]</span><br><span class="line">earliest：表示从最早的数据开始消费(从头消费)</span><br><span class="line">latest【默认】：表示从最新的数据开始消费</span><br><span class="line">none：如果根据指定的group.id没有找到之前消费的offset信息，就会抛异常</span><br><span class="line"></span><br><span class="line">(工作中earliest和latest常用)</span><br><span class="line"></span><br><span class="line">解释：【查找到了，但是查找到的那个offset对应的数据已经不存在了】 </span><br><span class="line">假设你第一天使用一个消费者去消费了一条数据，然后就把消费者停掉了，等了7天之后，你又使用这个消费者去消费数据</span><br><span class="line">这个时候，这个消费者启动的时候会到kafka里面查询它之前保存的offset信息</span><br><span class="line">但是那个offset对应的数据已经被删了，所以此时再根据这个offset去消费是消费不到数据的</span><br><span class="line"></span><br><span class="line">总结，一般在实时计算的场景下，这个参数的值建议设置为latest，消费最新的数据</span><br><span class="line"></span><br><span class="line">这个参数只有在消费者第一次消费数据，或者之前保存的offset信息已过期的情况下才会生效</span><br><span class="line"> *&#x2F;</span><br><span class="line"></span><br><span class="line">prop.put(&quot;auto.offset.reset&quot;,&quot;latest&quot;);</span><br><span class="line">&#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时我们来验证一下，</span><br><span class="line">先启动一次生产者，再启动一次消费者，看看消费者能不能消费到这条数据，如果能消费到，就说明此时是根据上次保存的offset信息进行消费了。结果发现是可以消费到的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：消费者消费到数据之后，不要立刻关闭程序，要至少等5秒，因为自动提交offset的时机是5秒提交一次</span><br><span class="line"></span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 4, leaderEpoch &#x3D; 5, offset &#x3D; 0, Cre</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">将auto.offset.reset置为earliest，修改一下group.id的值，相当于使用一个新的消费者，验证一下，看是否能把这个topic中的所有数据都取出来，因为新的消费者第一次肯定是获取不到offset信息的，</span><br><span class="line">所以就会根据auto.offset.reset的值来消费数据</span><br><span class="line"></span><br><span class="line">prop.put(&quot;group.id&quot;, &quot;con-2&quot;);</span><br><span class="line">prop.put(&quot;auto.offset.reset&quot;,&quot;earliest&quot;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 2, leaderEpoch &#x3D; 0, offset &#x3D; 0, Cre</span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 3, leaderEpoch &#x3D; 3, offset &#x3D; 0, Cre</span><br><span class="line">ConsumerRecord(topic &#x3D; hello, partition &#x3D; 4, leaderEpoch &#x3D; 5, offset &#x3D; 0, Cre</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162130163.png" alt="image-20230416213026828"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时，关闭消费者(需要等待5秒，这样才会提交offset)，再重新启动，发现没有消费到数据，说明此时就</span><br><span class="line">根据上次保存的offset来消费数据了，因为没有新数据产生，所以就消费不到了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">最后来处理一下程序输出的日志警告信息，这里其实示因为缺少依赖日志依赖</span><br><span class="line">在pom文件中添加log4j的依赖，然后将 log4j.properties 添加到 resources目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;info,stdout</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout &#x3D; org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Target &#x3D; System.out</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br></pre></td></tr></table></figure><h3 id="Consumer消费offset查询"><a href="#Consumer消费offset查询" class="headerlink" title="Consumer消费offset查询"></a>Consumer消费offset查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka0.9版本以前，消费者的offset信息保存在 zookeeper中</span><br><span class="line">从kafka0.9开始，使用了新的消费API，消费者的信息会保存在kafka里面的__consumer_offsets这个topic中</span><br><span class="line"></span><br><span class="line">因为频繁操作zookeeper性能不高，所以kafka在自己的topic中负责维护消费者的offset信息。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162135924.png" alt="image-20230416213511507"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如何查询保存在kafka中的Consumer的offset信息呢？</span><br><span class="line">使用kafka-consumer-groups.sh这个脚本可以查看目前所有的consumer group</span><br><span class="line"></span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-consumer-groups.sh --list --bootstrap-server localhost:9092</span><br><span class="line"></span><br><span class="line">con-1</span><br><span class="line">con-2 (前面视频里修改过)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">具体查看某一个consumer group的信息</span><br><span class="line">GROUP：当前消费者组，通过group.id指定的值</span><br><span class="line">TOPIC：当前消费的topic</span><br><span class="line">PARTITION：消费的分区</span><br><span class="line">CURRENT-OFFSET：消费者消费到这个分区的offset</span><br><span class="line">LOG-END-OFFSET：当前分区中数据的最大offset</span><br><span class="line">LAG：当前分区未消费数据量</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group con-1</span><br><span class="line">GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG</span><br><span class="line">con-1 hello 4 1 1 0 </span><br><span class="line">con-1 hello 2 1 1 0 </span><br><span class="line">con-1 hello 3 1 1 0</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162141088.png" alt="image-20230416214127550"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">partition:是指消费者消费了哪些分区</span><br><span class="line">current-offset:当前消费了的数据的offset</span><br><span class="line">log-end-offset:最新数据的offset</span><br><span class="line">lag:还有多少条数据没消费</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时再执行一次生产者代码，生产一条数据，重新查看一下这个消费者的offset情况</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162148346.png" alt="image-20230416214839816"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如何分析生产的数据能不能及时消费掉：查看lag</span><br><span class="line">如果lag值比较大：就需要增加消费者个数，同一个代码执行多次(但group.id不能变)</span><br></pre></td></tr></table></figure><h3 id="Consumer消费顺序"><a href="#Consumer消费顺序" class="headerlink" title="Consumer消费顺序"></a>Consumer消费顺序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当一个消费者消费一个partition时候，消费的数据顺序和此partition数据的生产顺序是一致的</span><br><span class="line"></span><br><span class="line">当一个消费者消费多个partition时候，消费者按照partition的顺序，首先消费一个partition，当消费完一个partition最新的数据后再消费其它partition中的数据</span><br><span class="line"></span><br><span class="line">总之：如果一个消费者消费多个partiton，只能保证消费的数据顺序在一个partition内是有序的</span><br><span class="line"></span><br><span class="line">也就是说消费kafka中的数据只能保证消费partition内的数据是有序的，多个partition之间是无序的。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162201198.png" alt="image-20230416220156464"></p><h3 id="Kafka的三种语义"><a href="#Kafka的三种语义" class="headerlink" title="Kafka的三种语义"></a>Kafka的三种语义</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka可以实现以下三种语义，这三种语义是针对消费者而言的：</span><br></pre></td></tr></table></figure><h4 id="至少一次：at-least-once"><a href="#至少一次：at-least-once" class="headerlink" title="至少一次：at-least-once"></a>至少一次：at-least-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这种语义有可能会对数据重复处理</span><br><span class="line">实现至少一次消费语义的消费者也很简单。</span><br><span class="line">1: 设置enable.auto.commit为false，禁用自动提交offset</span><br><span class="line">2: 消息处理完之后手动调用consumer.commitSync()提交offset</span><br><span class="line">这种方式是在消费数据之后，手动调用函数consumer.commitSync()异步提交offset，有可能处理多次的场景是消费者的消息处理完并输出到结果库，但是offset还没提交，这个时候消费者挂掉了，再重启的时候会重新消费并处理消息，所以至少会处理一次</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304162213700.png" alt="image-20230416221328144"></p><h4 id="至多一次：at-most-once"><a href="#至多一次：at-most-once" class="headerlink" title="至多一次：at-most-once"></a>至多一次：at-most-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">这种语义有可能会丢失数据</span><br><span class="line">至多一次消费语义是kafka消费者的默认实现。配置这种消费者最简单的方式是</span><br><span class="line">1: enable.auto.commit设置为true。</span><br><span class="line">2: auto.commit.interval.ms设置为一个较低的时间范围。</span><br><span class="line">由于上面的配置，此时kafka会有一个独立的线程负责按照指定间隔提交offset。</span><br><span class="line"></span><br><span class="line">消费者的offset已经提交，但是消息还在处理中(还没有处理完)，这个时候程序挂了，导致数据没有被成功处理，再重启的时候会从上次提交的offset处消费，导致上次没有被成功处理的消息就丢失了。</span><br></pre></td></tr></table></figure><h4 id="仅一次：exactly-once"><a href="#仅一次：exactly-once" class="headerlink" title="仅一次：exactly-once"></a>仅一次：exactly-once</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这种语义可以保证数据只被消费处理一次。</span><br><span class="line">实现仅一次语义的思路如下：</span><br><span class="line">1: 将enable.auto.commit设置为false，禁用自动提交offset</span><br><span class="line">2: 使用consumer.seek(topicPartition，offset)来指定offset</span><br><span class="line">3: 在处理消息的时候，要同时保存住每个消息的offset。以原子事务的方式保存offset和处理的消息结果，这个时候相当于自己保存offset信息了，把offset和具体的数据绑定到一块，数据真正处理成功的时候才会保存offset信息</span><br><span class="line">这样就可以保证数据仅被处理一次了。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.html</id>
    <published>2023-04-15T14:13:26.000Z</published>
    <updated>2023-04-16T03:01:48.465Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="大数据开发工程师-第十四周-消息队列之Kafka从入门到小牛-2"><a href="#大数据开发工程师-第十四周-消息队列之Kafka从入门到小牛-2" class="headerlink" title="大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2"></a>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-2</h1><h2 id="Kafka使用初体验"><a href="#Kafka使用初体验" class="headerlink" title="Kafka使用初体验"></a>Kafka使用初体验</h2><h3 id="Kafka中Topic的操作"><a href="#Kafka中Topic的操作" class="headerlink" title="Kafka中Topic的操作"></a>Kafka中Topic的操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka集群安装好了以后我们就想向kafka中添加一些数据</span><br><span class="line">想要添加数据首先需要创建topic</span><br><span class="line">那接下来看一下针对topic的一些操作</span><br></pre></td></tr></table></figure><h4 id="新增Topic"><a href="#新增Topic" class="headerlink" title="新增Topic"></a>新增Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">指定2个分区，2个副本，注意：副本数不能大于集群中Broker的数量</span><br><span class="line"></span><br><span class="line">因为每个partition的副本必须保存在不同的broker，否则没有意义，如果partition的副本都保存在同一个broker，那么这个broker挂了，则partition数据依然会丢失</span><br><span class="line"></span><br><span class="line">在这里我使用的是3个节点的kafka集群，所以副本数我就暂时设置为2，最大可以设置为3</span><br><span class="line"></span><br><span class="line">如果你们用的是单机kafka的话，这里的副本数就只能设置为1了，这个需要注意一下</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --partitions 2 --replication-factor 2</span><br><span class="line">--topic hello</span><br><span class="line">Created topic hello.</span><br></pre></td></tr></table></figure><h4 id="查询Topic"><a href="#查询Topic" class="headerlink" title="查询Topic"></a>查询Topic</h4><h5 id="查询Kafka中的所有Topic列表"><a href="#查询Kafka中的所有Topic列表" class="headerlink" title="查询Kafka中的所有Topic列表"></a>查询Kafka中的所有Topic列表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">查询Kafka中的所有Topic列表以及查看指定Topic的详细信息</span><br><span class="line">查询kafka中所有的topic列表</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line">hello</span><br><span class="line"></span><br><span class="line">topic数据是存在zookeeper中，所以直接指定zookeeper地址就可以了(有些地方需要指定kafka地址)</span><br></pre></td></tr></table></figure><h5 id="查看指定Topic的详细信息"><a href="#查看指定Topic的详细信息" class="headerlink" title="查看指定Topic的详细信息"></a>查看指定Topic的详细信息</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看指定topic的详细信息</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper localhost:2181 --topic hello</span><br><span class="line">Topic: hello PartitionCount: 2 ReplicationFactor: 2 Configs: </span><br><span class="line">Topic: hello Partition: 0 Leader: 2 Replicas: 2,0 Is</span><br><span class="line">Topic: hello Partition: 1 Leader: 0 Replicas: 0,1 Is</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152231741.png" alt="image-20230415223043384"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">第一个行显示指定topic所有partitions的一个总结</span><br><span class="line">PartitionCount：表示这个Topic一共有多少个partition</span><br><span class="line">ReplicationFactor：表示这个topic中partition的副本因子是几个</span><br><span class="line">Config：这个表示创建Topic时动态指定的配置信息，在这我们没有额外指定配置信息</span><br><span class="line"></span><br><span class="line">下面每一行给出的是一个partition的信息，如果只有一个partition，则只显示一行。</span><br><span class="line">Topic：显示当前的topic名称</span><br><span class="line">Partition：显示当前topic的partition编号</span><br><span class="line">Leader：Leader partition所在的节点编号，这个编号其实就是broker.id的值，</span><br><span class="line">来看这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152234654.png" alt="image-20230415223420514"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这个图里面的hello这个topic有两个partition，其中partition1的leader所在的节点是broker1，partition2的leader所在的节点是broker2</span><br><span class="line"></span><br><span class="line">Replicas：当前partition所有副本所在的节点编号【包含Leader所在的节点】，如果设置多个副本的话，这里会显示多个，不管该节点是否是Leader以及是否存活。</span><br><span class="line"></span><br><span class="line">Isr：当前partition处于同步状态的所有节点，这里显示的所有节点都是存活状态的，并且跟Leader同步的(包含Leader所在的节点)</span><br><span class="line"></span><br><span class="line">所以说Replicas和Isr的区别就是</span><br><span class="line">如果某个partition的副本所在的节点宕机了，在Replicas中还是会显示那个节点，但是在Isr中就不会显示了，Isr中显示的都是处于正常状态的节点。</span><br></pre></td></tr></table></figure><h4 id="修改Topic"><a href="#修改Topic" class="headerlink" title="修改Topic"></a>修改Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">修改一般是修改Topic的partition数量，只能增加</span><br><span class="line"></span><br><span class="line">为什么partition只能增加？</span><br><span class="line">因为数据是存储在partition中的，如果可以减少partition的话，那么partition中的数据就丢了</span><br><span class="line"></span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --alter --zookeeper localhost:2181 --partitions 5 --topic hello </span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partitio</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line">修改之后再来查看一下topic的详细信息</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --describe --zookeeper </span><br><span class="line">Topic: hello PartitionCount: 5 ReplicationFactor: 2 Configs: </span><br><span class="line"> Topic: hello Partition: 0 Leader: 2 Replicas: 2,0 I</span><br><span class="line"> Topic: hello Partition: 1 Leader: 0 Replicas: 0,1 I</span><br><span class="line"> Topic: hello Partition: 2 Leader: 1 Replicas: 1,2 I</span><br><span class="line"> Topic: hello Partition: 3 Leader: 2 Replicas: 2,1 I</span><br><span class="line"> Topic: hello Partition: 4 Leader: 0 Replicas: 0,2</span><br></pre></td></tr></table></figure><h4 id="删除Topic"><a href="#删除Topic" class="headerlink" title="删除Topic"></a>删除Topic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">删除Kafka中的指定Topic</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --delete --zookeeper localhost 2181 --topic hello</span><br><span class="line">Topic hello is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br><span class="line"></span><br><span class="line">删除操作是不可逆的，删除Topic会删除它里面的所有数据</span><br><span class="line"></span><br><span class="line">注意：Kafka从1.0.0开始默认开启了删除操作，之前的版本只会把Topic标记为删除状态，需要设置delete.topic.enable为true才可以真正删除</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果不想开启删除功能，可以设置delete.topic.enable为false，这样删除topic的时候只会把它标记为删除状态，此时这个topic依然可以正常使用。</span><br><span class="line">delete.topic.enable可以配置在server.properties文件中</span><br></pre></td></tr></table></figure><h3 id="Kafka中的生产者和消费者"><a href="#Kafka中的生产者和消费者" class="headerlink" title="Kafka中的生产者和消费者"></a>Kafka中的生产者和消费者</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">前面我们学习了Kafka中的topic的创建方式，下面我们可以向topic中生产数据以及消费数据了</span><br><span class="line">生产数据需要用到生产者</span><br><span class="line">消费数据需要用到消费者</span><br><span class="line"></span><br><span class="line">kafka默认提供了基于控制台的生产者和消费者，方便测试使用</span><br><span class="line">生产者： bin&#x2F;kafka-console-producer.sh</span><br><span class="line">消费者： bin&#x2F;kafka-console-consumer.sh</span><br></pre></td></tr></table></figure><h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">先来看一下如何向里面生产数据</span><br><span class="line">直接使用kafka提供的基于控制台的生产者</span><br><span class="line">先创建一个topic【5个分区，2个副本】：</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost 2181 --partitions 5 --replication-factor 2 --topic hello</span><br><span class="line">Created topic hello.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">向这个topic中生产数据</span><br><span class="line">broker-list：kafka的服务地址[多个用逗号隔开](这里需要用到kafka地址，上面创建topic时指定的是zookeeper，这里用本地地址和使用bigdata01:9092,bigdata02:9092,bigdata03:9092是一样的)</span><br><span class="line">topic：topic名称</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-producer.sh --broker-list localhost:9092 --topic hello</span><br><span class="line">&gt;hehe</span><br></pre></td></tr></table></figure><h4 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">下面来创建一个消费者消费topic中的数据</span><br><span class="line">bootstrap-server：kafka的服务地址</span><br><span class="line">topic:具体的topic下面来创建一个消费者消费topic中的数据</span><br><span class="line">1 [root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hello</span><br><span class="line"></span><br><span class="line">发现消费不到刚才生产的数据，为什么呢？</span><br><span class="line">因为kafka的消费者默认是消费最新生产的数据，如果想消费之前生产的数据需要添加一个参数--from-beginning，表示从头消费的意思</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hello --from-beginning</span><br><span class="line"></span><br><span class="line">hehe</span><br><span class="line"></span><br><span class="line">这里创建消费者的机器01、02、03都可以</span><br></pre></td></tr></table></figure><h4 id="案例：QQ群聊天"><a href="#案例：QQ群聊天" class="headerlink" title="案例：QQ群聊天"></a>案例：QQ群聊天</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">通过kafka可以模拟QQ群聊天的功能，我们来看一下</span><br><span class="line">首先在kafka中创建一个新的topic，可以认为是我们在QQ里面创建了一个群，群号是88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-topics.sh --create --zookeeper localhost 2181 --partitions 5 --replication-factor 2 --topic 88888888</span><br><span class="line">Created topic 88888888.</span><br><span class="line"></span><br><span class="line">然后我把你们都拉到这个群里面，这样我在群里面发消息你们就都能收到了</span><br><span class="line">在bigdata02和bigdata03上开启消费者，可以认为是把这两个人拉到群里面了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic 88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic 88888888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">然后我在bigdata01上开启生产者发消息，这样bigdata02和bigdata03都是可以收到的。</span><br><span class="line">这样就可以认为在群里的人都能收到我发的消息，类似于发广播。</span><br><span class="line">这个其实主要利用了kafka中的多消费者的特性，每个消费者都可以消费到相同的数据</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-console-producer.sh --broker-list localhost:9092 --topic 88888888 </span><br><span class="line">&gt;hello everyone</span><br></pre></td></tr></table></figure><h2 id="Kafka核心扩展内容"><a href="#Kafka核心扩展内容" class="headerlink" title="Kafka核心扩展内容"></a>Kafka核心扩展内容</h2><h3 id="Broker扩展"><a href="#Broker扩展" class="headerlink" title="Broker扩展"></a>Broker扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Broker的参数可以配置在server.properties这个配置文件中，Broker中支持的完整参数在官方文档中有体现</span><br><span class="line">具体链接为：</span><br><span class="line">http:&#x2F;&#x2F;kafka.apache.org&#x2F;24&#x2F;documentation.html#brokerconfigs</span><br><span class="line">针对Broker的参数，我们主要分析两块</span><br></pre></td></tr></table></figure><h4 id="Log-Flush-Policy"><a href="#Log-Flush-Policy" class="headerlink" title="Log Flush Policy"></a>Log Flush Policy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1：Log Flush Policy：设置数据flush到磁盘的时机</span><br><span class="line">为了减少磁盘写入的次数,broker会将消息暂时缓存起来,当消息的个数达到一定阀值或者过了一定的时间间隔后,再flush到磁盘,这样可以减少磁盘IO调用的次数。</span><br><span class="line"></span><br><span class="line">这块主要通过两个参数控制</span><br><span class="line">log.flush.interval.messages 一个分区的消息数阀值，达到该阈值则将该分区的数据flush到磁盘，注意这里是针对分区，因为topic是一个逻辑概念，分区是真实存在的，每个分区会在磁盘上产生一个目录</span><br><span class="line">[root@bigdata01 kafka-logs]# ll</span><br><span class="line">total 20</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:12 88888888-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:12 88888888-3</span><br><span class="line">-rw-r--r--. 1 root root 4 Jun 8 15:23 cleaner-offset-checkpoint</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-12</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-15</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-18</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-21</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-24</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-27</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-3</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-30</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-33</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-36</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-39</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-42</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-45</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-48</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-6</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:08 __consumer_offsets-9</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-0</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-1</span><br><span class="line">drwxr-xr-x. 2 root root 141 Jun 8 17:04 hello-4 </span><br><span class="line">hello topic有5个分区，但这里只有2个目录的原因是：没写几条数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160022785.png" alt="image-20230416002248398"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个参数的默认值为9223372036854775807，long的最大值</span><br><span class="line">默认值太大了，所以建议修改，可以使用server.properties中针对这个参数指定的值10000，需要去掉注释之后这个参数才生效。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">log.flush.interval.ms间隔指定时间</span><br><span class="line">默认间隔指定的时间将内存中缓存的数据flush到磁盘中，由文档可知，这个参数的默认值为null，此时会使用log.flush.scheduler.interval.ms参数的值，log.flush.scheduler.interval.ms参数的值默认是 9223372036854775807，long的最大值</span><br><span class="line"></span><br><span class="line">所以这个值也建议修改，可以使用server.properties中针对这个参数指定的值1000，单位是毫秒，表示每1秒写一次磁盘，这个参数也需要去掉注释之后才生效</span><br></pre></td></tr></table></figure><h4 id="Log-Retention-Policy"><a href="#Log-Retention-Policy" class="headerlink" title="Log Retention Policy"></a>Log Retention Policy</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">设置数据保存周期，默认7天</span><br><span class="line">kafka中的数据默认会保存7天，如果kafka每天接收的数据量过大，这样是很占磁盘空间的，建议修改数据保存周期，我们之前在实际工作中是将数据保存周期改为了1天。</span><br><span class="line"></span><br><span class="line">数据保存周期主要通过这几个参数控制</span><br><span class="line">log.retention.hours，这个参数默认值为168，单位是小时，就是7天，可以在这调整数据保存的时间，超过这个时间数据会被自动删除</span><br><span class="line">log.retention.bytes，这个参数表示当分区的文件达到一定大小的时候会删除它，如果设置了按照指定周期删除数据文件，这个参数不设置也可以，这个参数默认是没有开启的</span><br><span class="line">log.retention.check.interval.ms，这个参数表示检测的间隔时间，单位是毫秒，默认值是300000，就是5分钟，表示每5分钟检测一次文件看是否满足删除的时机</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">默认根据时间就行了，后期在时间范围内还可以从kafka中恢复数据</span><br></pre></td></tr></table></figure><h3 id="Producer扩展"><a href="#Producer扩展" class="headerlink" title="Producer扩展"></a>Producer扩展</h3><h4 id="producer发送数据到partition的方式"><a href="#producer发送数据到partition的方式" class="headerlink" title="producer发送数据到partition的方式"></a>producer发送数据到partition的方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Producer默认是随机将数据发送到topic的不同分区中，也可以根据用户设置的算法来根据消息的key来计算输入到哪个partition里面</span><br><span class="line"></span><br><span class="line">此时需要通过partitioner来控制，这个知道就行了，因为在实际工作中一般在向kafka中生产数据的都是不带key的，只有数据内容，所以一般都是使用随机的方式发送数据</span><br></pre></td></tr></table></figure><h4 id="producer的数据通讯方式"><a href="#producer的数据通讯方式" class="headerlink" title="producer的数据通讯方式"></a>producer的数据通讯方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">在这里有一个需要注意的内容就是</span><br><span class="line">针对producer的数据通讯方式：同步发送和异步发送</span><br><span class="line"></span><br><span class="line">同步是指：生产者发出数据后，等接收方发回响应以后再发送下个数据的通讯方式。</span><br><span class="line">异步是指：生产者发出数据后，不等接收方发回响应，接着发送下个数据的通讯方式。</span><br><span class="line"></span><br><span class="line">具体的数据通讯策略是由acks参数控制的</span><br><span class="line">acks默认为1，表示需要Leader节点回复收到消息，这样生产者才会发送下一条数据</span><br><span class="line">acks：all，表示需要所有Leader+副本节点回复收到消息（acks&#x3D;-1），这样生产者才会发送下一条数据</span><br><span class="line">acks：0，表示不需要任何节点回复，生产者会继续发送下一条数据</span><br><span class="line">再来看一下这个图：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304152350669.png" alt="image-20230415235025189"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">我们在向hello这个topic生产数据的时候，可以在生产者中设置acks参数，</span><br><span class="line">acks设置为1，表示我们在向hello这个topic的partition1这个分区写数据的时候，只需要让leader所在的broker1这个节点回复确认收到的消息就可以了，这样生产者就可以发送下一条数据了</span><br><span class="line"></span><br><span class="line">如果acks设置为all，则需要partition1的这两个副本所在的节点(包含Leader)都回复收到消息，生产者才会发送下一条数据</span><br><span class="line"></span><br><span class="line">如果acks设置为0，表示生产者不会等待任何partition所在节点的回复，它只管发送数据，不管你有没有收到，所以这种情况丢失数据的概率比较高。</span><br></pre></td></tr></table></figure><h5 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对这块在面试的时候会有一个面试题：Kafka如何保证数据不丢？</span><br><span class="line">其实就是通过acks机制保证的，如果设置acks为all，则可以保证数据不丢，因为此时把数据发送给kafka之后，会等待对应partition所在的所有leader和副本节点都确认收到消息之后才会认为数据发送成功了，所以在这种策略下，只要把数据发送给kafka之后就不会丢了。</span><br><span class="line"></span><br><span class="line">如果acks设置为1，则当我们把数据发送给partition之后，partition的leader节点也确认收到了，但是leader回复完确认消息之后，leader对应的节点就宕机了，副本partition还没来得及将数据同步过去，所以会存在丢失的可能性。</span><br><span class="line">不过如果宕机的是副本partition所在的节点，则数据是不会丢的</span><br><span class="line"></span><br><span class="line">如果acks设置为0的话就表示是顺其自然了，只管发送，不管kafka有没有收到，这种情况表示对数据丢不丢都无所谓了。</span><br></pre></td></tr></table></figure><h3 id="Consumer扩展"><a href="#Consumer扩展" class="headerlink" title="Consumer扩展"></a>Consumer扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">在消费者中还有一个消费者组的概念</span><br><span class="line">每个consumer属于一个消费者组，通过group.id指定消费者组</span><br><span class="line"></span><br><span class="line">那组内消费和组间消费有什么区别吗？</span><br><span class="line">组内：消费者组内的所有消费者消费同一份数据；</span><br><span class="line"></span><br><span class="line">注意：在同一个消费者组中，一个partition同时只能有一个消费者消费数据</span><br><span class="line">如果消费者的个数小于分区的个数，一个消费者会消费多个分区的数据。</span><br><span class="line">如果消费者的个数大于分区的个数，则多余的消费者不消费数据</span><br><span class="line">所以，对于一个topic,同一个消费者组中推荐不能有多于分区个数的消费者,否则将意味着某些消费者将无法获得消息。</span><br><span class="line"></span><br><span class="line">组间：多个消费者组消费相同的数据，互不影响。</span><br><span class="line">来看下面这个图，加深一下理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160000840.png" alt="image-20230416000001364"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Kafka集群有两个节点，Broker1和Broker2</span><br><span class="line">集群内有一个topic，这个topic有4个分区，P0,P1,P2,P3</span><br><span class="line"></span><br><span class="line">下面有两个消费者组</span><br><span class="line">Consumer Group A和Consumer Group B</span><br><span class="line">其中Consumer Group A中有两个消费者C1和C2，由于这个topic有4个分区，所以，C1负责消费两个分区的数据，C2负责消费两个分区的数据，这个属于组内消费</span><br><span class="line">Consumer Group B有5个消费者，C3~C7，其中C3,C4,C5,C6分别消费一个分区的数据，而C7就是多余出来的了，因为现在这个消费者组内的消费者的数量比对应的topic的分区数量还多，但是一个分区同时只能被一个消费者消费，所以就会有一个消费者处于空闲状态。这个也属于组内消费</span><br><span class="line">Consumer Group A和Consumer Group B这两个消费者组属于组间消费，互不影响。</span><br></pre></td></tr></table></figure><h3 id="Topic、Partition扩展"><a href="#Topic、Partition扩展" class="headerlink" title="Topic、Partition扩展"></a>Topic、Partition扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">每个partition在存储层面是append log文件。</span><br><span class="line">新消息都会被直接追加到log文件的尾部，每条消息在log文件中的位置称为offset(偏移量)。</span><br><span class="line">越多partitions可以容纳更多的consumer,有效提升并发消费的能力。</span><br><span class="line"></span><br><span class="line">具体什么时候增加topic的数量？什么时候增加partition的数量呢？</span><br><span class="line"></span><br><span class="line">业务类型增加需要增加topic、数据量大需要增加partition</span><br></pre></td></tr></table></figure><h3 id="Message扩展"><a href="#Message扩展" class="headerlink" title="Message扩展"></a>Message扩展</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每条Message包含了以下三个属性：</span><br><span class="line">1. offset对应类型：long，表示此消息在一个partition中的起始的位置。可以认为offset是partition中Message的id，自增的</span><br><span class="line">2. MessageSize 对应类型：int32 此消息的字节大小。</span><br><span class="line">3. data，类型为bytes,是message的具体内容。</span><br><span class="line">看这个图，加深对Topic、Partition、Message的理解</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304160016235.png" alt="image-20230416001618719"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里也体现了kafka高吞吐量的原因：磁盘顺序读写由于内存随机访问</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-2%5C202304160022785.png" alt></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BTableAPI%E5%92%8CSQL-5.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BTableAPI%E5%92%8CSQL-5.html</id>
    <published>2023-04-08T15:22:12.000Z</published>
    <updated>2023-04-15T05:13:18.839Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-Flink核心API之TableAPI和SQL-5"><a href="#第十六周-Flink极速上手篇-Flink核心API之TableAPI和SQL-5" class="headerlink" title="第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5"></a>第十六周 Flink极速上手篇-Flink核心API之TableAPI和SQL-5</h1><h2 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意:Table API和SQL现在还处于活跃开发阶段，还没有完全实现Flink中所有的特性。不是所有的[Table API，SQL]和[流，批]的组合都是支持的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL的由来：</span><br><span class="line">Flink针对标准的流处理和批处理提供了两种关系型API，Table API和SQL。Table API允许用户以一种很直观的方式进行select、filter和join操作。Flink SQL基于Apache Calcite实现标准SQL。针对批处理和流处理可以提供相同的处理语义和结果。</span><br><span class="line">Flink Table API、SQL和Flink的DataStream API、DataSet API是紧密联系在一起的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL是一种关系型API，用户可以像操作Mysql数据库表一样的操作数据，而不需要写代码，更不需要手工的对代码进行调优。另外，SQL作为一个非程序员可操作的语言，学习成本很低，如果一个系统提供 SQL支持，将很容易被用户接受。</span><br><span class="line"></span><br><span class="line">如果你想要使用Table API和SQL的话，需要添加下面的依赖</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-api-scala-bridge_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如果你想在 本地 IDE中运行程序，还需要添加下面的依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-planner-blink_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">如果你用到了老的执行引擎，还需要添加下面这个依赖</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-table-planner_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注意：由于部分table相关的代码是用Scala实现的，所以，这个依赖也是必须的。【这个依赖我们在前面开发DataStream程序的时候已经添加过了】</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.11.0&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL通过join API集成在一起，这个join API的核心概念是Table，Table可以作为查询的输入和输出。</span><br><span class="line">针对Table API和SQL我们主要讲解以下内容</span><br><span class="line">1：Table API和SQL的使用</span><br><span class="line">2：DataStream、DataSet和Table之间的互相转换</span><br></pre></td></tr></table></figure><h2 id="Table-API-和SQL的使用"><a href="#Table-API-和SQL的使用" class="headerlink" title="Table API 和SQL的使用"></a>Table API 和SQL的使用</h2><h3 id="创建TableEnvironment对象"><a href="#创建TableEnvironment对象" class="headerlink" title="创建TableEnvironment对象"></a>创建TableEnvironment对象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">想要使用Table API 和SQL，首先要创建一个TableEnvironment对象。</span><br><span class="line">下面我们来创建一个TableEnvironment对象</span><br></pre></td></tr></table></figure><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.&#123;<span class="type">BatchTableEnvironment</span>, <span class="type">Stream</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">EnvironmentSettings</span>, <span class="type">TableEnvironment</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建TableEnvironment对象</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CreateTableEnvironmentScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL不需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 则针对stream和batch都可以使用TableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定底层使用Blink引擎，以及数据处理模式-stream</span></span><br><span class="line">         <span class="comment">//从1.11版本开始，Blink引擎成为Table API和SQL的默认执行引擎，在生产环境下面，推</span></span><br><span class="line">         <span class="keyword">val</span> sSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build()</span><br><span class="line">         <span class="comment">//创建TableEnvironment对象</span></span><br><span class="line">         <span class="keyword">val</span> sTableEnv = <span class="type">TableEnvironment</span>.create(sSettings)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定底层使用Blink引擎，以及数据处理模式-batch</span></span><br><span class="line">         <span class="keyword">val</span> bSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inBatchMode().build()</span><br><span class="line">         <span class="comment">//创建TableEnvironment对象</span></span><br><span class="line">         <span class="keyword">val</span> bTableEnv = <span class="type">TableEnvironment</span>.create(bSettings)</span><br><span class="line">         </span><br><span class="line">         </span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 针对stream需要使用StreamTableEnvironment</span></span><br><span class="line"><span class="comment">         * 针对batch需要使用BatchTableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="comment">//创建StreamTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> ssEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> ssSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build()</span><br><span class="line">         <span class="keyword">val</span> ssTableEnv = <span class="type">StreamTableEnvironment</span>.create(ssEnv, ssSettings)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//创建BatchTableEnvironment</span></span><br><span class="line">         <span class="comment">//注意：此时只能使用旧的执行引擎，新的Blink执行引擎不支持和DataSet转换</span></span><br><span class="line">         <span class="keyword">val</span> bbEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> bbTableEnv = <span class="type">BatchTableEnvironment</span>.create(bbEnv)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建TableEnvironment对象</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CreateTableEnvironmentJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL不需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 则针对stream和batch都可以使用TableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="comment">//创建TableEnvironment对象-stream</span></span><br><span class="line">         EnvironmentSettings sSettings = EnvironmentSettings.newInstance().use</span><br><span class="line">         TableEnvironment sTableEnv = TableEnvironment.create(sSettings);</span><br><span class="line">         <span class="comment">//创建TableEnvironment对象-batch</span></span><br><span class="line">         EnvironmentSettings bSettings = EnvironmentSettings.newInstance().use</span><br><span class="line">         TableEnvironment bTableEnv = TableEnvironment.create(bSettings);</span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 注意：如果Table API和SQL需要和DataStream或者DataSet互相转换</span></span><br><span class="line"><span class="comment">         * 针对stream需要使用StreamTableEnvironment</span></span><br><span class="line"><span class="comment">         * 针对batch需要使用BatchTableEnvironment</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         <span class="comment">//创建StreamTableEnvironment</span></span><br><span class="line">         StreamExecutionEnvironment ssEnv = StreamExecutionEnvironment.getExec</span><br><span class="line">         EnvironmentSettings ssSettings = EnvironmentSettings.newInstance().us</span><br><span class="line">         StreamTableEnvironment ssTableEnv = StreamTableEnvironment.create(ssE</span><br><span class="line">         <span class="comment">//创建BatchTableEnvironment</span></span><br><span class="line">         <span class="comment">//注意：此时只能使用旧的执行引擎，新的Blink执行引擎不支持和DataSet转换</span></span><br><span class="line">         ExecutionEnvironment bbEnv = ExecutionEnvironment.getExecutionEnviron</span><br><span class="line">         BatchTableEnvironment bbTableEnv = BatchTableEnvironment.create(bbEnv</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Table-API和-SQL的使用"><a href="#Table-API和-SQL的使用" class="headerlink" title="Table API和 SQL的使用"></a>Table API和 SQL的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面我们来演示一下Table API和SQL的使用</span><br><span class="line">目前创建Table的很多方法都过时了，都不推荐使用了，例如：registerTableSource、connect等方法</span><br><span class="line">目前官方推荐使用executeSql的方式，executeSql里面支持</span><br><span class="line">1DDL&#x2F;DML&#x2F;DQL&#x2F;SHOW&#x2F;DESCRIBE&#x2F;EXPLAIN&#x2F;USE等语法</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121052838.png" alt="image-20230412105247826"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">EnvironmentSettings</span>, <span class="type">TableEnvironment</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TableAPI 和 SQL的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableAPIAndSQLOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取TableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> sSettings = <span class="type">EnvironmentSettings</span>.newInstance.useBlinkPlanner.inStreami</span><br><span class="line">         <span class="keyword">val</span> sTableEnv = <span class="type">TableEnvironment</span>.create(sSettings)</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * connector.type：指定connector的类型</span></span><br><span class="line"><span class="comment">         * connector.path：指定文件或者目录地址</span></span><br><span class="line"><span class="comment">         * format.type：文件数据格式化类型</span></span><br><span class="line"><span class="comment">         * 注意：SQL语句如果出现了换行，行的末尾可以添加空格或者\n都可以，最后一行不用添</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">         sTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//使用Table API实现数据查询和过滤等操作</span></span><br><span class="line">         <span class="comment">/*import org.apache.flink.table.api._</span></span><br><span class="line"><span class="comment">         val result = sTableEnv.from("myTable")</span></span><br><span class="line"><span class="comment">         .select($"id",$"name")</span></span><br><span class="line"><span class="comment">         .filter($"id" &gt; 1)*/</span></span><br><span class="line">        </span><br><span class="line">         <span class="comment">//使用SQL实现数据查询和过滤等操作</span></span><br><span class="line">         <span class="keyword">val</span> result = sTableEnv.sqlQuery(<span class="string">"select id,name from myTable where id &gt; 1"</span> )</span><br><span class="line">         <span class="comment">//输出结果到控制台</span></span><br><span class="line">         result.execute.print()</span><br><span class="line">          <span class="comment">//创建输出表</span></span><br><span class="line">         sTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table newTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\res',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">         <span class="comment">//输出结果到表newTable中</span></span><br><span class="line">         result.executeInsert(<span class="string">"newTable"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121053478.png" alt="image-20230412105345232"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">注意：针对SQL建表语句的写法还有一种比较清晰的写法</span><br><span class="line"></span><br><span class="line">sTableEnv.executeSql(</span><br><span class="line"> &quot;&quot;&quot;</span><br><span class="line"> |create table myTable(</span><br><span class="line"> |id int,</span><br><span class="line"> |name string</span><br><span class="line"> |) with (</span><br><span class="line"> |&#39;connector.type&#39; &#x3D; &#39;filesystem&#39;,</span><br><span class="line"> |&#39;connector.path&#39; &#x3D; &#39;D:\data\source&#39;,</span><br><span class="line"> |&#39;format.type&#39; &#x3D; &#39;csv&#39;</span><br><span class="line"> |)</span><br><span class="line"> |&quot;&quot;&quot;.stripMargin)</span><br></pre></td></tr></table></figure><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * TableAPI 和 SQL的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableAPIAndSQLOpJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//获取TableEnvironment</span></span><br><span class="line">         EnvironmentSettings sSettings = EnvironmentSettings.newInstance().use</span><br><span class="line">         TableEnvironment sTableEnv = TableEnvironment.create(sSettings);</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         sTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>);</span><br><span class="line">         <span class="comment">//使用Table API实现数据查询和过滤等操作</span></span><br><span class="line">         <span class="comment">/*Table result = sTableEnv.from("myTable")</span></span><br><span class="line"><span class="comment">         .select($("id"), $("name"))</span></span><br><span class="line"><span class="comment">         .filter($("id").isGreater(1));*/</span></span><br><span class="line">             <span class="comment">//使用SQL实现数据查询和过滤等操作</span></span><br><span class="line">         Table result = sTableEnv.sqlQuery(<span class="string">"select id,name from myTable where </span></span><br><span class="line"><span class="string">         //输出结果到控制台</span></span><br><span class="line"><span class="string">         result.execute().print();</span></span><br><span class="line"><span class="string">         //创建输出表</span></span><br><span class="line"><span class="string">         sTableEnv.executeSql("</span><span class="string">" +</span></span><br><span class="line"><span class="string">         "</span><span class="function">create table <span class="title">newTable</span><span class="params">(\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>id <span class="keyword">int</span>,\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>name string\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>)</span> <span class="title">with</span> <span class="params">(\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span><span class="string">'connector.type'</span> = <span class="string">'filesystem'</span>,\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span><span class="string">'connector.path'</span> = <span class="string">'D:\\data\\res'</span>,\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span><span class="string">'format.type'</span> = <span class="string">'csv'</span>\n<span class="string">" +</span></span></span></span><br><span class="line"><span class="function"><span class="params"><span class="string">         "</span>)</span>")</span>;</span><br><span class="line">         <span class="comment">//输出结果到表newTable中</span></span><br><span class="line">         result.executeInsert(<span class="string">"newTable"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DataStream、DataSet和Table之间的互相转换"><a href="#DataStream、DataSet和Table之间的互相转换" class="headerlink" title="DataStream、DataSet和Table之间的互相转换"></a>DataStream、DataSet和Table之间的互相转换</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Table API和SQL可以很容易的和DataStream和DataSet程序集成到一块。通过TableEnvironment，可以把DataStream或者DataSet注册为 Table，这样就可以使用Table API和SQL查询了。通过</span><br><span class="line">TableEnvironment也可以把Table对象转换为DataStream或者DataSet，这样就可以使用DataStream或者DataSet中的相关API了。</span><br></pre></td></tr></table></figure><h3 id="使用DataStream创建表"><a href="#使用DataStream创建表" class="headerlink" title="使用DataStream创建表"></a>使用DataStream创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1：使用DataStream创建表，主要包含下面这两种情况</span><br><span class="line">使用DataStream创建view视图</span><br><span class="line">使用DataStream创建table对象</span><br></pre></td></tr></table></figure><h4 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataStream转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataStreamToTableScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> ssEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> ssSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build()</span><br><span class="line">         <span class="keyword">val</span> ssTableEnv = <span class="type">StreamTableEnvironment</span>.create(ssEnv, ssSettings)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//获取DataStream</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> stream = ssEnv.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mack"</span>)))</span><br><span class="line">         <span class="comment">//第一种：将DataStream转换为view视图</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line">   ssTableEnv.createTemporaryView(<span class="string">"myTable"</span>,stream,<span class="symbol">'id</span>,<span class="symbol">'name</span>)</span><br><span class="line">         ssTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().print()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//第二种：将DataStream转换为table对象</span></span><br><span class="line">         <span class="keyword">val</span> table = ssTableEnv.fromDataStream(stream, $<span class="string">"id"</span>, $<span class="string">"name"</span>)</span><br><span class="line">         table.select($<span class="string">"id"</span>,$<span class="string">"name"</span>)</span><br><span class="line">         .filter($<span class="string">"id"</span> &gt; <span class="number">1</span>)</span><br><span class="line">         .execute()</span><br><span class="line">         .print()</span><br><span class="line">         <span class="comment">//注意：'id,'name 和 $"id", $"name" 这两种写法是一样的效果</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataStream转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataStreamToTableJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         StreamExecutionEnvironment ssEnv = StreamExecutionEnvironment.getExec</span><br><span class="line">         EnvironmentSettings ssSettings = EnvironmentSettings.newInstance().us</span><br><span class="line">         StreamTableEnvironment ssTableEnv = StreamTableEnvironment.create(ssE</span><br><span class="line">         <span class="comment">//获取DataStream</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataStreamSource&lt;Tuple2&lt;Integer, String&gt;&gt; stream = ssEnv.fromCollecti</span><br><span class="line">         <span class="comment">//第一种：将DataStream转换为view视图</span></span><br><span class="line">         ssTableEnv.createTemporaryView(<span class="string">"myTable"</span>,stream,$(<span class="string">"id"</span>),$(<span class="string">"name"</span>));</span><br><span class="line">         ssTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().p</span><br><span class="line">         <span class="comment">//第二种：将DataStream转换为table对象</span></span><br><span class="line">         Table table = ssTableEnv.fromDataStream(stream, $(<span class="string">"id"</span>), $(<span class="string">"name"</span>));</span><br><span class="line">         table.select($(<span class="string">"id"</span>), $(<span class="string">"name"</span>))</span><br><span class="line">         .filter($(<span class="string">"id"</span>).isGreater(<span class="number">1</span>))</span><br><span class="line">         .execute()</span><br><span class="line">         .print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用DataSet创建表"><a href="#使用DataSet创建表" class="headerlink" title="使用DataSet创建表"></a>使用DataSet创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：此时只能使用旧的执行引擎，新的Blink执行引擎不支持和DataSet转换</span><br></pre></td></tr></table></figure><h4 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.&#123;<span class="type">BatchTableEnvironment</span>, <span class="type">Stream</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataSet转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSetToTableScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> bbEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> bbTableEnv = <span class="type">BatchTableEnvironment</span>.create(bbEnv)</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//获取DataSet</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> set = bbEnv.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mack"</span>)))</span><br><span class="line">         <span class="comment">//第一种：将DataSet转换为view视图</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line">         bbTableEnv.createTemporaryView(<span class="string">"myTable"</span>,set,<span class="symbol">'id</span>,<span class="symbol">'name</span>)</span><br><span class="line">         bbTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().print</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//第二种：将DataSet转换为table对象</span></span><br><span class="line">         <span class="keyword">val</span> table = bbTableEnv.fromDataSet(set, $<span class="string">"id"</span>, $<span class="string">"name"</span>)</span><br><span class="line">         table.select($<span class="string">"id"</span>,$<span class="string">"name"</span>)</span><br><span class="line">         .filter($<span class="string">"id"</span> &gt; <span class="number">1</span>)</span><br><span class="line">         .execute()</span><br><span class="line">         .print()</span><br><span class="line">         <span class="comment">//注意：'id,'name 和 $"id", $"name" 这两种写法是一样的效果</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> <span class="keyword">static</span> org.apache.flink.table.api.Expressions.$;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将DataSet转换成表</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DataSetToTableJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         ExecutionEnvironment bbEnv = ExecutionEnvironment.getExecutionEnviron</span><br><span class="line">         BatchTableEnvironment bbTableEnv = BatchTableEnvironment.create(bbEnv</span><br><span class="line">         <span class="comment">//获取DataSet</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; set = bbEnv.fromCollection(data);</span><br><span class="line">         <span class="comment">//第一种：将DataSet转换为view视图</span></span><br><span class="line">         bbTableEnv.createTemporaryView(<span class="string">"myTable"</span>,set,$(<span class="string">"id"</span>),$(<span class="string">"name"</span>));</span><br><span class="line">         bbTableEnv.sqlQuery(<span class="string">"select * from myTable where id &gt; 1"</span>).execute().p</span><br><span class="line">         <span class="comment">//第二种：将DataSet转换为table对象</span></span><br><span class="line">         Table table = bbTableEnv.fromDataSet(set, $(<span class="string">"id"</span>), $(<span class="string">"name"</span>));</span><br><span class="line">         table.select($(<span class="string">"id"</span>), $(<span class="string">"name"</span>))</span><br><span class="line">         .filter($(<span class="string">"id"</span>).isGreater(<span class="number">1</span>))</span><br><span class="line">         .execute()</span><br><span class="line">         .print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">将Table转换为DataStream或者DataSet时，你需要指定生成的 DataStream或者DataSet的数据类型，即，Table 的每行数据要转换成的数据类型。通常最方便的选择是转换成Row。以下列表概述了不同选项的功能：</span><br><span class="line">Row: 通过角标映射字段，支持任意数量的字段，支持null值，无类型安全（type-safe）检查。</span><br><span class="line">POJO: Java中的实体类，这个实体类中的字段名称需要和Table中的字段名称保持一致，支持任意数量的字段，支持null值，有类型安全检查。</span><br><span class="line">Case Class: 通过角标映射字段，不支持null值，有类型安全检查。</span><br><span class="line">Tuple: 通过角标映射字段，Scala中限制22个字段，Java中限制25个字段，不支持null值，有类型安全检查。</span><br><span class="line">Atomic Type: Table必须有一个字段，不支持null值，有类型安全检查。</span><br></pre></td></tr></table></figure><h3 id="将Table转换为DataStream"><a href="#将Table转换为DataStream" class="headerlink" title="将Table转换为DataStream"></a>将Table转换为DataStream</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3：将表转换成DataStream流式查询的结果Table会被动态地更新，即每个新的记录到达输入流时结果就会发生变化。因此，转换此动态查询的DataStream需要对表的更新进行编码。</span><br><span class="line"></span><br><span class="line">有几种模式可以将Table转换为DataStream。</span><br><span class="line">Append Mode:这种模式只适用于当动态表仅由INSERT更改修改时(仅附加)，之前添加的数据不会被更新。</span><br><span class="line">Retract Mode:可以始终使用此模式，它使用一个Boolean标识来编码INSERT和DELETE更改。</span><br></pre></td></tr></table></figure><h4 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将table转换成 DataStream</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableToDataStreamScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> ssEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> ssSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inSt</span><br><span class="line">         <span class="keyword">val</span> ssTableEnv = <span class="type">StreamTableEnvironment</span>.create(ssEnv, ssSettings)</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         ssTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         <span class="keyword">val</span> table = ssTableEnv.from(<span class="string">"myTable"</span>)</span><br><span class="line">         <span class="comment">//将table转换为DataStream</span></span><br><span class="line">         <span class="comment">//如果只有新增(追加)操作，可以使用toAppendStream</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> appStream = ssTableEnv.toAppendStream[<span class="type">Row</span>](table)</span><br><span class="line">         appStream.map(row=&gt;(row.getField(<span class="number">0</span>).toString.toInt,row.getField(<span class="number">1</span>).toString))</span><br><span class="line">         .print()</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//如果有增加操作，还有删除操作，则使用toRetractStream</span></span><br><span class="line">         <span class="keyword">val</span> retStream = ssTableEnv.toRetractStream[<span class="type">Row</span>](table)</span><br><span class="line">         retStream.map(tup=&gt;&#123;</span><br><span class="line">         <span class="keyword">val</span> flag = tup._1</span><br><span class="line">         <span class="keyword">val</span> row = tup._2</span><br><span class="line">         <span class="keyword">val</span> id = row.getField(<span class="number">0</span>).toString.toInt</span><br><span class="line">         <span class="keyword">val</span> name = row.getField(<span class="number">1</span>).toString</span><br><span class="line">         (flag,id,name)</span><br><span class="line">         &#125;).print()</span><br><span class="line">         <span class="comment">//注意：将table对象转换为DataStream之后，就需要调用StreamExecutionEnvironment</span></span><br><span class="line">         ssEnv.execute(<span class="string">"TableToDataStreamScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121150371.png" alt="image-20230412115003343"></p><h4 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将table转换成 DataStream</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableToDataStreamJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取StreamTableEnvironment</span></span><br><span class="line">         StreamExecutionEnvironment ssEnv = StreamExecutionEnvironment.getExec</span><br><span class="line">         EnvironmentSettings ssSettings = EnvironmentSettings.newInstance().us</span><br><span class="line">         StreamTableEnvironment ssTableEnv = StreamTableEnvironment.create(ssE</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         ssTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>);</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         Table table = ssTableEnv.from(<span class="string">"myTable"</span>);</span><br><span class="line">         <span class="comment">//将table转换为DataStream</span></span><br><span class="line">         <span class="comment">//如果只有新增(追加)操作，可以使用toAppendStream</span></span><br><span class="line">                                                                 DataStream&lt;Row&gt; appStream = ssTableEnv.toAppendStream(table, Row.clas</span><br><span class="line">         appStream.map(<span class="keyword">new</span> MapFunction&lt;Row, Tuple2&lt;Integer,String&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, String&gt; <span class="title">map</span><span class="params">(Row row)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">int</span> id = Integer.parseInt(row.getField(<span class="number">0</span>).toString());</span><br><span class="line">         String name = row.getField(<span class="number">1</span>).toString();</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, String&gt;(id, name);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         <span class="comment">//如果有增加操作，还有删除操作，则使用toRetractStream</span></span><br><span class="line">         DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retStream = ssTableEnv.toRetractStre</span><br><span class="line">         retStream.map(<span class="keyword">new</span> MapFunction&lt;Tuple2&lt;Boolean, Row&gt;, Tuple3&lt;Boolean,Int</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Boolean, Integer, String&gt; <span class="title">map</span><span class="params">(Tuple2&lt;Boolean, Row&gt; t</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         Boolean flag = tup.f0;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">int</span> id = Integer.parseInt(tup.f1.getField(<span class="number">0</span>)</span>.<span class="title">toString</span><span class="params">()</span>)</span>;</span><br><span class="line">         String name = tup.f1.getField(<span class="number">1</span>).toString();</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Boolean, Integer, String&gt;(flag, id, name);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         ssEnv.execute(<span class="string">"TableToDataStreamJava"</span>);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="将表转换成DataSet"><a href="#将表转换成DataSet" class="headerlink" title="将表转换成DataSet"></a>将表转换成DataSet</h3><h4 id="scala-5"><a href="#scala-5" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.tablesql</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">BatchTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将table转换成 DataSet</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableToDataSetScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         <span class="keyword">val</span> bbEnv = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> bbTableEnv = <span class="type">BatchTableEnvironment</span>.create(bbEnv)</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         bbTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>)</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         <span class="keyword">val</span> table = bbTableEnv.from(<span class="string">"myTable"</span>)</span><br><span class="line">         <span class="comment">//将table转换为DataSet</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> set = bbTableEnv.toDataSet[<span class="type">Row</span>](table)</span><br><span class="line">         set.map(row=&gt;(row.getField(<span class="number">0</span>).toString.toInt,row.getField(<span class="number">1</span>).toString))</span><br><span class="line">         .print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304121153273.png" alt="image-20230412115320971"></p><h4 id="java-5"><a href="#java-5" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.tablesql;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.BatchTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 将table转换成 DataSet</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableToDataSetJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取BatchTableEnvironment</span></span><br><span class="line">         ExecutionEnvironment bbEnv = ExecutionEnvironment.getExecutionEnviron</span><br><span class="line">         BatchTableEnvironment bbTableEnv = BatchTableEnvironment.create(bbEnv</span><br><span class="line">         <span class="comment">//创建输入表</span></span><br><span class="line">         bbTableEnv.executeSql(<span class="string">""</span> +</span><br><span class="line">         <span class="string">"create table myTable(\n"</span> +</span><br><span class="line">         <span class="string">"id int,\n"</span> +</span><br><span class="line">         <span class="string">"name string\n"</span> +</span><br><span class="line">         <span class="string">") with (\n"</span> +</span><br><span class="line">         <span class="string">"'connector.type' = 'filesystem',\n"</span> +</span><br><span class="line">         <span class="string">"'connector.path' = 'D:\\data\\source',\n"</span> +</span><br><span class="line">         <span class="string">"'format.type' = 'csv'\n"</span> +</span><br><span class="line">         <span class="string">")"</span>);</span><br><span class="line">         <span class="comment">//获取table</span></span><br><span class="line">         Table table = bbTableEnv.from(<span class="string">"myTable"</span>);</span><br><span class="line">         <span class="comment">//将table转换为DataSet</span></span><br><span class="line">         DataSet&lt;Row&gt; set = bbTableEnv.toDataSet(table, Row<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">         set.map(<span class="keyword">new</span> MapFunction&lt;Row, Tuple2&lt;Integer,String&gt;&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, String&gt; <span class="title">map</span><span class="params">(Row row)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">int</span> id = Integer.parseInt(row.getField(<span class="number">0</span>).toString());</span><br><span class="line">         String name = row.getField(<span class="number">1</span>).toString();</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, String&gt;(id, name);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataSetAPI-4.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataSetAPI-4.html</id>
    <published>2023-04-08T15:21:40.000Z</published>
    <updated>2023-04-10T07:20:55.895Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-Flink核心API之DataSetAPI-4"><a href="#第十六周-Flink极速上手篇-Flink核心API之DataSetAPI-4" class="headerlink" title="第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4"></a>第十六周 Flink极速上手篇-Flink核心API之DataSetAPI-4</h1><h2 id="DataSet-API"><a href="#DataSet-API" class="headerlink" title="DataSet API"></a>DataSet API</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataSet API主要可以分为3块来分析：DataSource、Transformation、Sink。</span><br><span class="line"></span><br><span class="line">DataSource是程序的数据源输入。</span><br><span class="line">Transformation是具体的操作，它对一个或多个输入数据源进行计算处理，例如map、flatMap、filter等操作。</span><br><span class="line"></span><br><span class="line">DataSink是程序的输出，它可以把Transformation处理之后的数据输出到指定的存储介质中。</span><br></pre></td></tr></table></figure><h3 id="DataSet-API之DataSource"><a href="#DataSet-API之DataSource" class="headerlink" title="DataSet API之DataSource"></a>DataSet API之DataSource</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对DataSet批处理而言，其实最多的就是读取HDFS中的文件数据，所以在这里我们主要介绍两个DataSource组件。</span><br><span class="line"></span><br><span class="line">基于集合</span><br><span class="line">fromCollection(Collection)，主要是为了方便测试使用。它的用法和DataStreamAPI中的用法一样，我们已经用过很多次了。</span><br><span class="line"></span><br><span class="line">基于文件</span><br><span class="line">readTextFile(path)，读取hdfs中的数据文件。这个前面我们也使用过了。</span><br></pre></td></tr></table></figure><h3 id="DataSet-API之Transformation"><a href="#DataSet-API之Transformation" class="headerlink" title="DataSet API之Transformation"></a>DataSet API之Transformation</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">算子 解释</span><br><span class="line">map 输入一个元素进行处理，返回一个元素</span><br><span class="line">mapPartition 类似map，一次处理一个分区的数据</span><br><span class="line">flatMap 输入一个元素进行处理，可以返回多个元素</span><br><span class="line">filter 对数据进行过滤，符合条件的数据会被留下</span><br><span class="line">reduce 对当前元素和上一次的结果进行聚合操作</span><br><span class="line">aggregate sum(),min(),max()等</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里面的算子我们都是比较熟悉的，在前面DatatreamAPI中都用过，用法都是一样的，所以在这就不再演示了</span><br><span class="line">mapPartition这个算子我们在Flink中还没用过，不过在Spark中是用过的，用法也是一样的</span><br><span class="line"></span><br><span class="line">其实mapPartition就是一次处理一批数据，如果在处理数据的时候想要获取第三方资源连接，建议使用mapPartition，这样可以一批数据获取一次连接，提高性能。</span><br><span class="line">下面来演示一下Flink中mapPartition的使用</span><br></pre></td></tr></table></figure><h4 id="mapPartition"><a href="#mapPartition" class="headerlink" title="mapPartition"></a>mapPartition</h4><h5 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MapPartition的使用：一次处理一个分区的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchMapPartitionScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//生成数据源数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="string">"hello you"</span>, <span class="string">"hello me"</span>))</span><br><span class="line">         <span class="comment">//每次处理一个分区的数据</span></span><br><span class="line">         text.mapPartition(it=&gt;&#123;</span><br><span class="line">         <span class="comment">//可以在此处创建数据库连接，建议把这块代码放到try-catch代码块中</span></span><br><span class="line">         <span class="comment">//注意：此时是每个分区获取一个数据库连接，不需要每处理一条数据就获取一次连接，</span></span><br><span class="line">         <span class="keyword">val</span> res = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">         it.foreach(line=&gt;&#123;</span><br><span class="line">         <span class="keyword">val</span> words = line.split(<span class="string">" "</span>)</span><br><span class="line">         <span class="keyword">for</span>(word &lt;- words)&#123;</span><br><span class="line">         res.append(word)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         res</span><br><span class="line">         <span class="comment">//关闭数据库连接</span></span><br><span class="line">         &#125;).print()</span><br><span class="line">         <span class="comment">//No new data sinks have been defined since the last execution.</span></span><br><span class="line">         <span class="comment">//The last execution refers to the latest call to 'execute()', 'count()', </span></span><br><span class="line">         <span class="comment">//注意：针对DataSetAPI，如果在后面调用的是count、collect、print，则最后不需要指定execute即可</span></span><br><span class="line">         <span class="comment">//env.execute("BatchMapPartitionScala")</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapPartitionFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MapPartition的使用：一次处理一个分区的数据</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchMapPartitionJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//生成数据源数据</span></span><br><span class="line">         DataSource&lt;String&gt; text = env.fromCollection(Arrays.asList(<span class="string">"hello you"</span>,<span class="string">"hello me"</span>));</span><br><span class="line">         <span class="comment">//每次处理一个分区的数据</span></span><br><span class="line">         text.mapPartition(<span class="keyword">new</span> MapPartitionFunction&lt;String, String&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mapPartition</span><span class="params">(Iterable&lt;String&gt; iterable, Collector&lt;String&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">//可以在此处创建数据库连接，建议把这块代码放到try-catch代码块中</span></span><br><span class="line">             Iterator&lt;String&gt; it = iterable.iterator();</span><br><span class="line">         <span class="keyword">while</span>(it.hasNext())&#123;</span><br><span class="line">             String line = it.next();</span><br><span class="line">             String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">             <span class="keyword">for</span>(String word: words)&#123;</span><br><span class="line">             out.collect(word);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//关闭数据库连接</span></span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">下面还有一些transformation算子</span><br><span class="line">算子 解释</span><br><span class="line">distinct 返回数据集中去重之后的元素</span><br><span class="line">join 内连接</span><br><span class="line">outerJoin 外连接</span><br><span class="line">cross 获取两个数据集的笛卡尔积</span><br><span class="line">union 返回多个数据集的总和，数据类型需要一致</span><br><span class="line">first-n 获取集合中的前N个元素</span><br><span class="line">distinct算子比较简单，就是对数据进行全局去重。</span><br><span class="line">join：内连接，可以连接两份数据集</span><br></pre></td></tr></table></figure><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><h5 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * join：内连接</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchJoinScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mick"</span>)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"bj"</span>), (<span class="number">2</span>, <span class="string">"sh"</span>), (<span class="number">4</span>, <span class="string">"gz"</span>)))</span><br><span class="line">         <span class="comment">//对两份数据集执行join操作</span></span><br><span class="line">         text1.join(text2)</span><br><span class="line">         <span class="comment">//注意：这里的where和equalTo实现了类似于on fieldA=fieldB的效果</span></span><br><span class="line">         <span class="comment">//where：指定左边数据集中参与比较的元素角标</span></span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         <span class="comment">//equalTo指定右边数据集中参与比较的元素角标</span></span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;(first,second)=&gt;&#123;</span><br><span class="line">             (first._1,first._2,second._2)</span><br><span class="line">         &#125;&#125;.print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101353385.png" alt="image-20230410135332014"></p><h5 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.JoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * join：内连接</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchJoinJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data1 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text1 = env.fromCollection(data1)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data2 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"bj"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"sh"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">4</span>,<span class="string">"gz"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text2 = env.fromCollection(data2)</span><br><span class="line">         <span class="comment">//对两份数据集执行join操作</span></span><br><span class="line">         text1.join(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         <span class="comment">//三个输入参数：</span></span><br><span class="line">         <span class="comment">//第一个tuple2是左边数据集的类型，</span></span><br><span class="line">         <span class="comment">//第二个tuple2是右边数据集的类型，</span></span><br><span class="line">         <span class="comment">//第三个tuple3是此函数返回的数据集类型</span></span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Integer,String&gt;,Tuple3&lt;Integer,String,String&gt;&gt;()</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Integer,String&gt; secondfirst,Tuple2&lt;Integer,String</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         return new Tuple3&lt;Integer, String, String&gt;(first.f0,f</span></span></span><br><span class="line"><span class="function"><span class="params">         &#125;</span></span></span><br><span class="line"><span class="function"><span class="params">         &#125;)</span>.<span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outerJoin：外连接</span><br></pre></td></tr></table></figure><h4 id="outerJoin"><a href="#outerJoin" class="headerlink" title="outerJoin"></a>outerJoin</h4><h5 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * outerJoin：外连接</span></span><br><span class="line"><span class="comment"> * 一共有三种情况</span></span><br><span class="line"><span class="comment"> * 1：leftOuterJoin</span></span><br><span class="line"><span class="comment"> * 2：rightOuterJoin</span></span><br><span class="line"><span class="comment"> * 3：fullOuterJoin</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchOuterJoinScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"jack"</span>), (<span class="number">2</span>, <span class="string">"tom"</span>), (<span class="number">3</span>, <span class="string">"mick"</span>)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">"bj"</span>), (<span class="number">2</span>, <span class="string">"sh"</span>), (<span class="number">4</span>, <span class="string">"gz"</span>)))</span><br><span class="line">         <span class="comment">//对两份数据集执行leftOuterJoin操作</span></span><br><span class="line">         text1.leftOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;</span><br><span class="line">         (first,second)=&gt;&#123;</span><br><span class="line">         <span class="comment">//注意：second中的元素可能为null</span></span><br><span class="line">         <span class="keyword">if</span>(second==<span class="literal">null</span>)&#123;</span><br><span class="line">         (first._1,first._2,<span class="string">"null"</span>)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         (first._1,first._2,second._2)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;.print()</span><br><span class="line">         println(<span class="string">"========================================"</span>)</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.rightOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;</span><br><span class="line">         (first,second)=&gt;&#123;</span><br><span class="line">         <span class="comment">//注意：first中的元素可能为null</span></span><br><span class="line">         <span class="keyword">if</span>(first==<span class="literal">null</span>)&#123;</span><br><span class="line">             (second._1,<span class="string">"null"</span>,second._2)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         (first._1,first._2,second._2)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;.print()</span><br><span class="line">         println(<span class="string">"========================================"</span>)</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.fullOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)&#123;</span><br><span class="line">         (first,second)=&gt;&#123;</span><br><span class="line">         <span class="comment">//注意：first和second中的元素都有可能为null</span></span><br><span class="line">             <span class="keyword">if</span>(first==<span class="literal">null</span>)&#123;</span><br><span class="line">         (second._1,<span class="string">"null"</span>,second._2)</span><br><span class="line">         &#125;<span class="keyword">else</span> <span class="keyword">if</span>(second==<span class="literal">null</span>)&#123;</span><br><span class="line">         (first._1,first._2,<span class="string">"null"</span>)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         (first._1,first._2,second._2)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;.print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101516270.png" alt="image-20230410151623671"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101516094.png" alt="image-20230410151648029"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101517219.png" alt="image-20230410151719016"></p><h5 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.JoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * outerJoin：外连接</span></span><br><span class="line"><span class="comment"> * 一共有三种情况</span></span><br><span class="line"><span class="comment"> * 1：leftOuterJoin</span></span><br><span class="line"><span class="comment"> * 2：rightOuterJoin</span></span><br><span class="line"><span class="comment"> * 3：fullOuterJoin</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchOuterJoinJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironme</span><br><span class="line">         <span class="comment">//初始化第一份数据 Tuple2&lt;用户id,用户姓名&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data1 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"jack"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"tom"</span>));</span><br><span class="line">         data1.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"mick"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text1 = env.fromCollection(data1)</span><br><span class="line">         <span class="comment">//初始化第二份数据 Tuple2&lt;用户id,用户所在城市&gt;</span></span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data2 = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"bj"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"sh"</span>));</span><br><span class="line">         data2.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">4</span>,<span class="string">"gz"</span>));</span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text2 = env.fromCollection(data2)</span><br><span class="line">         <span class="comment">//对两份数据集执行leftOuterJoin操作</span></span><br><span class="line">         text1.leftOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Intege</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Intege</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">if</span>(second==<span class="keyword">null</span>)</span></span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">                                                    &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         System.out.println(<span class="string">"=============================================="</span>);</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.rightOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Intege</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Intege</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">if</span>(first==<span class="keyword">null</span>)</span></span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(second</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">         System.out.println(<span class="string">"=============================================="</span>);</span><br><span class="line">         <span class="comment">//对两份数据集执行rightOuterJoin操作</span></span><br><span class="line">         text1.fullOuterJoin(text2)</span><br><span class="line">         .where(<span class="number">0</span>)</span><br><span class="line">         .equalTo(<span class="number">0</span>)</span><br><span class="line">         .with(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;Integer, String&gt;, Tuple2&lt;Intege</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, String, String&gt; <span class="title">join</span><span class="params">(Tuple2&lt;Intege</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">throws</span> Exception &#123;</span></span></span><br><span class="line"><span class="function"><span class="params">         <span class="keyword">if</span>(first==<span class="keyword">null</span>)</span></span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(second</span><br><span class="line">         &#125;<span class="keyword">else</span> <span class="keyword">if</span>(second==<span class="keyword">null</span>)&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, String, String&gt;(first.f</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="cross"><a href="#cross" class="headerlink" title="cross"></a>cross</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross：获取两个数据集的笛卡尔积</span><br></pre></td></tr></table></figure><h5 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * cross：获取两个数据集的笛卡尔积</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchCrossScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化第一份数据</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">         <span class="comment">//初始化第二份数据</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>(<span class="string">"a"</span>, <span class="string">"b"</span>))</span><br><span class="line">         <span class="comment">//执行cross操作</span></span><br><span class="line">             text1.cross(text2).print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101509307.png" alt="image-20230410150858841"></p><h5 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * cross：获取两个数据集的笛卡尔积</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchCrossJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironme</span><br><span class="line">         <span class="comment">//初始化第一份数据</span></span><br><span class="line">         DataSource&lt;Integer&gt; text1 = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>));</span><br><span class="line">         <span class="comment">//初始化第二份数据</span></span><br><span class="line">         DataSource&lt;String&gt; text2 = env.fromCollection(Arrays.asList(<span class="string">"a"</span>, <span class="string">"b"</span>)</span><br><span class="line">         <span class="comment">//执行cross操作</span></span><br><span class="line">         text1.cross(text2).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">union：返回两个数据集的总和，数据类型需要一致</span><br><span class="line">和DataStreamAPI中的union操作功能一样</span><br><span class="line">first-n：获取集合中的前N个元素</span><br></pre></td></tr></table></figure><h4 id="first-n"><a href="#first-n" class="headerlink" title="first-n"></a>first-n</h4><h5 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.batch.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.<span class="type">Order</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * first-n：获取集合中的前N个元素</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchFirstNScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">val</span> data = <span class="type">ListBuffer</span>[<span class="type">Tuple2</span>[<span class="type">Int</span>,<span class="type">String</span>]]()</span><br><span class="line">         data.append((<span class="number">2</span>,<span class="string">"zs"</span>))</span><br><span class="line">         data.append((<span class="number">4</span>,<span class="string">"ls"</span>))</span><br><span class="line">         data.append((<span class="number">3</span>,<span class="string">"ww"</span>))</span><br><span class="line">         data.append((<span class="number">1</span>,<span class="string">"aw"</span>))</span><br><span class="line">         data.append((<span class="number">1</span>,<span class="string">"xw"</span>))</span><br><span class="line">         data.append((<span class="number">1</span>,<span class="string">"mw"</span>))</span><br><span class="line">        <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//初始化数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(data)</span><br><span class="line">         <span class="comment">//获取前3条数据，按照数据插入的顺序</span></span><br><span class="line">         text.first(<span class="number">3</span>).print()</span><br><span class="line">         println(<span class="string">"=================================="</span>)</span><br><span class="line">         <span class="comment">//根据数据中的第一列进行分组，获取每组的前2个元素</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).first(<span class="number">2</span>).print()</span><br><span class="line">         println(<span class="string">"=================================="</span>)</span><br><span class="line">         <span class="comment">//根据数据中的第一列分组，再根据第二列进行组内排序[倒序],获取每组的前2个元素</span></span><br><span class="line">         <span class="comment">//分组排序取TopN</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).sortGroup(<span class="number">1</span>,<span class="type">Order</span>.<span class="type">DESCENDING</span>).first(<span class="number">2</span>).print()</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.batch.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.operators.Order;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * first-n：获取集合中的前N个元素</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchFirstNJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironme</span><br><span class="line">         ArrayList&lt;Tuple2&lt;Integer, String&gt;&gt; data = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">2</span>,<span class="string">"zs"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">4</span>,<span class="string">"ls"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">3</span>,<span class="string">"ww"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"aw"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"xw"</span>));</span><br><span class="line">         data.add(<span class="keyword">new</span> Tuple2&lt;Integer,String&gt;(<span class="number">1</span>,<span class="string">"mw"</span>));</span><br><span class="line">         <span class="comment">//初始化数据</span></span><br><span class="line">         DataSource&lt;Tuple2&lt;Integer, String&gt;&gt; text = env.fromCollection(data);</span><br><span class="line">         <span class="comment">//获取前3条数据，按照数据插入的顺序</span></span><br><span class="line">         text.first(<span class="number">3</span>).print();</span><br><span class="line">         System.out.println(<span class="string">"===================================="</span>);</span><br><span class="line">         <span class="comment">//根据数据中的第一列进行分组，获取每组的前2个元素</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).first(<span class="number">2</span>).print();</span><br><span class="line">         System.out.println(<span class="string">"===================================="</span>);</span><br><span class="line">         <span class="comment">//根据数据中的第一列分组，再根据第二列进行组内排序[倒序],获取每组的前2个元素</span></span><br><span class="line">         <span class="comment">//分组排序取TopN</span></span><br><span class="line">         text.groupBy(<span class="number">0</span>).sortGroup(<span class="number">1</span>, Order.DESCENDING).first(<span class="number">2</span>).print();</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="DataSet-API之DataSink"><a href="#DataSet-API之DataSink" class="headerlink" title="DataSet API之DataSink"></a>DataSet API之DataSink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Flink针对DataSet提供了一些已经实现好的数据目的地</span><br><span class="line">其中最常见的是向HDFS中写入数据</span><br><span class="line"></span><br><span class="line">writeAsText()：将元素以字符串形式逐行写入，这些字符串通过调用每个元素的toString()方法来获取</span><br><span class="line">writeAsCsv()：将元组以逗号分隔写入文件中，行及字段之间的分隔是可配置的，每个字段的值来自对象的toString()方法</span><br><span class="line"></span><br><span class="line">还有一个是print：打印每个元素的toString()方法的值</span><br><span class="line">这个print是测试的时候使用的。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataStreamAPI-3.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-Flink%E6%A0%B8%E5%BF%83API%E4%B9%8BDataStreamAPI-3.html</id>
    <published>2023-04-08T15:20:20.000Z</published>
    <updated>2023-04-10T04:47:08.995Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-Flink核心API之DataStreamAPI-3"><a href="#第十六周-Flink极速上手篇-Flink核心API之DataStreamAPI-3" class="headerlink" title="第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3"></a>第十六周 Flink极速上手篇-Flink核心API之DataStreamAPI-3</h1><h2 id="Flink核心API"><a href="#Flink核心API" class="headerlink" title="Flink核心API"></a>Flink核心API</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091728382.png" alt="image-20230409172217160"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Flink中提供了4种不同层次的API，每种API在简洁和易表达之间有自己的权衡，适用于不同的场景。目前上面3个会用得比较多。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">低级API(Stateful Stream Processing)：提供了对时间和状态的细粒度控制，简洁性和易用性较差，主要应用在一些复杂事件处理逻辑上。</span><br><span class="line"></span><br><span class="line">核心API(DataStream&#x2F;DataSet API)：主要提供了针对流数据和批数据的处理，是对低级API进行了一些封装，提供了filter、sum、max、min等高级函数，简单易用，所以这些API在工作中应用还是比较广泛的。</span><br><span class="line"></span><br><span class="line">Table API：一般与DataSet或者DataStream紧密关联，可以通过一个DataSet或DataStream创建出一个Table，然后再使用类似于filter, join,或者select这种操作。最后还可以将一个Table对象转成</span><br><span class="line">DataSet或DataStream。</span><br><span class="line"></span><br><span class="line">SQL：Flink的SQL底层是基于Apache Calcite，Apache Calcite实现了标准的SQL，使用起来比其他API更加灵活，因为可以直接使用SQL语句。Table API和SQL可以很容易地结合在一块使用，因为它们都返回Table对象。</span><br><span class="line"></span><br><span class="line">针对这些API我们主要学习下面这些</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091729767.png" alt="image-20230409172651151"></p><h3 id="DataStream-API"><a href="#DataStream-API" class="headerlink" title="DataStream API"></a>DataStream API</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataStream API主要分为3块：DataSource、Transformation、DataSink。</span><br><span class="line"></span><br><span class="line">DataSource是程序的输入数据源。</span><br><span class="line">Transformation是具体的操作，它对一个或多个输入数据源进行计算处理，例如map、flatMap和filter等操作。</span><br><span class="line"></span><br><span class="line">DataSink是程序的输出，它可以把Transformation处理之后的数据输出到指定的存储介质中。</span><br></pre></td></tr></table></figure><h4 id="DataStream-API之DataSoure"><a href="#DataStream-API之DataSoure" class="headerlink" title="DataStream API之DataSoure"></a>DataStream API之DataSoure</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DataSource是程序的输入数据源，Flink提供了大量内置的DataSource，也支持自定义DataSource，不过目前Flink提供的这些已经足够我们正常使用了。</span><br><span class="line">Flink提供的内置输入数据源：包括基于socket、基于Collection</span><br><span class="line">还有就是Flink还提供了一批Connectors，可以实现读取第三方数据源，</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091734753.png" alt="image-20230409173429387"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Flink内置：表示Flink中默认自带的。</span><br><span class="line"></span><br><span class="line">Apache Bahir：表示需要添加这个依赖包之后才能使用的。</span><br><span class="line"></span><br><span class="line">针对source的这些Connector，我们在实际工作中最常用的就是Kafka</span><br><span class="line">当程序出现错误的时候，Flink的容错机制能恢复并继续运行程序，这种错误包括机器故障、网络故障、程序故障等</span><br><span class="line"></span><br><span class="line">针对Flink提供的常用数据源接口，如果程序开启了checkpoint快照机制，Flink可以提供这些容错性保证</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091735114.png" alt="image-20230409173548905"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对这些常用的DataSouce，基于socket的我们之前已经用过了，下面我们来看一下基于Collection集合的。</span><br><span class="line"></span><br><span class="line">针对Kafka的这个我们在后面会详细分析，在这里先不讲。</span><br><span class="line"></span><br><span class="line">由于我们后面还会学到批处理的功能，所以在项目里面创建几个包，把流处理和批处理的代码分开，后期看起来比较清晰。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091750998.png" alt="image-20230409174935218"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来在 com.imooc.scala.stream 里面创建一个包：source，将代码放到source包里面</span><br></pre></td></tr></table></figure><h5 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.source</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于collection的source的使用</span></span><br><span class="line"><span class="comment"> * 注意：这个source的主要应用场景是模拟测试代码流程的时候使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamCollectionSourceScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//使用collection集合生成DataStream</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">         text.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamCollectionSource"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="java"><a href="#java" class="headerlink" title="java"></a>java</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.source;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基于collection的source的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamCollectionSourceJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">             StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//使用collection集合生成DataStream</span></span><br><span class="line">         DataStreamSource&lt;Integer&gt; text = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>));</span><br><span class="line">         text.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">     env.execute(<span class="string">"StreamCollectionSourceJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="DataStream-API之Transformation"><a href="#DataStream-API之Transformation" class="headerlink" title="DataStream API之Transformation"></a>DataStream API之Transformation</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transformation是Flink程序的计算算子，负责对数据进行处理，Flink提供了大量的算子，其实Flink中的大部分算子的使用和spark中算子的使用是一样的，下面我们来看一下：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">算子 解释</span><br><span class="line">map 输入一个元素进行处理，返回一个元素</span><br><span class="line">flatMap 输入一个元素进行处理，可以返回多个元素</span><br><span class="line">filter 对数据进行过滤，符合条件的数据会被留下</span><br><span class="line">keyBy 根据key分组，相同key的数据会进入同一个分区</span><br><span class="line">reduce 对当前元素和上一次的结果进行聚合操作</span><br><span class="line">aggregations sum(),min(),max()等</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这里面的算子的用法其实和spark中对应算子的用法是一致的，这里面的map、flatmap、keyBy、reduce、sum这些算子我们都用过了。</span><br><span class="line">所以这里面的算子就不再单独演示了。</span><br><span class="line">往下面看。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">算子 解释</span><br><span class="line">union 合并多个流，多个流的数据类型必须一致</span><br><span class="line">connect 只能连接两个流，两个流的数据类型可以不同</span><br><span class="line">split 根据规则把一个数据流切分为多个流</span><br><span class="line">shuffle 随机分区</span><br><span class="line">rebalance 对数据集进行再平衡，重分区，消除数据倾斜</span><br><span class="line">rescale 重分区</span><br><span class="line">partitionCustom 自定义分区</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这里面的算子我们需要分析一下。</span><br><span class="line">union：表示合并多个流，但是多个流的数据类型必须一致</span><br><span class="line">多个流join之后，就变成了一个流</span><br><span class="line">应用场景：多种数据源的数据类型一致，数据处理规则也一致</span><br></pre></td></tr></table></figure><h5 id="union"><a href="#union" class="headerlink" title="union"></a>union</h5><h6 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 合并多个流，多个流的数据类型必须一致</span></span><br><span class="line"><span class="comment"> * 应用场景：多种数据源的数据类型一致，数据处理规则也一致</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamUnionScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromCollection(<span class="type">Array</span>(<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br><span class="line">         <span class="comment">//合并流</span></span><br><span class="line">         <span class="keyword">val</span> unionStream = text1.union(text2)</span><br><span class="line">         <span class="comment">//打印流中的数据</span></span><br><span class="line">         unionStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamUnionScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 合并多个流，多个流的数据类型必须一致</span></span><br><span class="line"><span class="comment"> * 应用场景：多种数据源的数据类型一致，数据处理规则也一致</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamUnionJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         DataStreamSource&lt;Integer&gt; text1 = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         DataStreamSource&lt;Integer&gt; text2 = env.fromCollection(Arrays.asList(<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>));</span><br><span class="line">         <span class="comment">//合并流</span></span><br><span class="line">         DataStream&lt;Integer&gt; unionStream = text1.union(text2);</span><br><span class="line">         <span class="comment">//打印流中的数据</span></span><br><span class="line">         unionStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamUnionJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">connect：只能连接两个流，两个流的数据类型可以不同</span><br><span class="line"></span><br><span class="line">两个流被connect之后，只是被放到了同一个流中，它们内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。</span><br><span class="line"></span><br><span class="line">connect方法会返回connectedStream，在connectedStream中需要使用CoMap、CoFlatMap这种函数，类似于map和flatmap</span><br></pre></td></tr></table></figure><h6 id="scala-2"><a href="#scala-2" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.<span class="type">CoMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 只能连接两个流，两个流的数据类型可以不同</span></span><br><span class="line"><span class="comment"> * 应用：可以将两种不同格式的数据统一成一种格式</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamConnectScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text1 = env.fromElements(<span class="string">"user:tom,age:18"</span>)</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         <span class="keyword">val</span> text2 = env.fromElements(<span class="string">"user:jack_age:20"</span>)</span><br><span class="line">         <span class="comment">//连接两个流</span></span><br><span class="line">         <span class="keyword">val</span> connectStream = text1.connect(text2)</span><br><span class="line">         connectStream.map(<span class="keyword">new</span> <span class="type">CoMapFunction</span>[<span class="type">String</span>,<span class="type">String</span>,<span class="type">String</span>] &#123;</span><br><span class="line">         <span class="comment">//处理第1份数据流中的数据</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map1</span></span>(value: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">         value.replace(<span class="string">","</span>,<span class="string">"-"</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//处理第2份数据流中的数据</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map2</span></span>(value: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">         value.replace(<span class="string">"_"</span>,<span class="string">"-"</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamConnectScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304091821183.png" alt="image-20230409182145286"></p><h6 id="java-2"><a href="#java-2" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.ConnectedStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.CoMapFunction;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 只能连接两个流，两个流的数据类型可以不同</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamConnectJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//第1份数据流</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text1 = env.fromElements(<span class="string">"user:tom,age:18"</span>);</span><br><span class="line">         <span class="comment">//第2份数据流</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text2 = env.fromElements(<span class="string">"user:jack_age:20"</span>);</span><br><span class="line">         <span class="comment">//连接两个流</span></span><br><span class="line">         ConnectedStreams&lt;String, String&gt; connectStream = text1.connect(text2);</span><br><span class="line">         connectStream.map(<span class="keyword">new</span> CoMapFunction&lt;String, String, String&gt;() &#123;</span><br><span class="line">         <span class="comment">//处理第1份数据流中的数据</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">map1</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> value.replace(<span class="string">","</span>,<span class="string">"-"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//处理第2份数据流中的数据</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">map2</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> value.replace(<span class="string">"_"</span>,<span class="string">"-"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamConnectJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="split"><a href="#split" class="headerlink" title="split"></a>split</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">split：根据规则把一个数据流切分为多个流</span><br><span class="line"></span><br><span class="line">注意：split只能分一次流，切分出来的流不能继续分流</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">split需要和select配合使用，选择切分后的流</span><br><span class="line">应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑</span><br></pre></td></tr></table></figure><h6 id="scala-3"><a href="#scala-3" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation</span><br><span class="line"><span class="keyword">import</span> java.&#123;lang, util&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.collector.selector.<span class="type">OutputSelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据规则把一个数据流切分为多个流</span></span><br><span class="line"><span class="comment"> * 注意：split只能分一次流，切分出来的流不能继续分流</span></span><br><span class="line"><span class="comment"> * split需要和select配合使用，选择切分后的流</span></span><br><span class="line"><span class="comment"> * 应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamSplitScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         <span class="keyword">val</span> splitStream = text.split(<span class="keyword">new</span> <span class="type">OutputSelector</span>[<span class="type">Int</span>] &#123;</span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(value: <span class="type">Int</span>): lang.<span class="type">Iterable</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">         <span class="keyword">val</span> list = <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">String</span>]()</span><br><span class="line">         <span class="keyword">if</span>(value % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">         list.add(<span class="string">"even"</span>)<span class="comment">//偶数</span></span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         list.add(<span class="string">"odd"</span>)<span class="comment">//奇数</span></span><br><span class="line">         &#125;</span><br><span class="line">         list</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//选择流</span></span><br><span class="line">         <span class="keyword">val</span> evenStream = splitStream.select(<span class="string">"even"</span>)</span><br><span class="line">         evenStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//二次切分流会报错</span></span><br><span class="line">         <span class="comment">//Consecutive multiple splits are not supported. Splits are deprecated. Pl</span></span><br><span class="line">         <span class="comment">/*val lowHighStream = evenStream.split(new OutputSelector[Int] &#123;</span></span><br><span class="line"><span class="comment">         override def select(value: Int): lang.Iterable[String] = &#123;</span></span><br><span class="line"><span class="comment">         val list = new util.ArrayList[String]()</span></span><br><span class="line"><span class="comment">         if(value &lt;= 5)&#123;</span></span><br><span class="line"><span class="comment">         list.add("low");</span></span><br><span class="line"><span class="comment">         &#125;else&#123;</span></span><br><span class="line"><span class="comment">         list.add("high")</span></span><br><span class="line"><span class="comment">         &#125;</span></span><br><span class="line"><span class="comment">         list</span></span><br><span class="line"><span class="comment">         &#125;</span></span><br><span class="line"><span class="comment">         &#125;)</span></span><br><span class="line"><span class="comment">         val lowStream = lowHighStream.select("low")</span></span><br><span class="line"><span class="comment">         lowStream.print().setParallelism(1)*/</span></span><br><span class="line">         env.execute(<span class="string">"StreamSplitScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="java-3"><a href="#java-3" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.collector.selector.OutputSelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SplitStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据规则把一个数据流切分为多个流</span></span><br><span class="line"><span class="comment"> * 注意：split只能分一次流，切分出来的流不能继续分流</span></span><br><span class="line"><span class="comment"> * split需要和select配合使用，选择切分后的流</span></span><br><span class="line"><span class="comment"> * 应用场景：将一份数据流切分为多份，便于针对每一份数据使用不同的处理逻辑</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamSplitJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">             DataStreamSource&lt;Integer&gt; text = env.fromCollection(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>));</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         SplitStream&lt;Integer&gt; splitStream = text.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">         ArrayList&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">         <span class="keyword">if</span> (value % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">         list.add(<span class="string">"even"</span>);<span class="comment">//偶数</span></span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         list.add(<span class="string">"odd"</span>);<span class="comment">//奇数</span></span><br><span class="line">         &#125;</span><br><span class="line">         <span class="keyword">return</span> list;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//选择流</span></span><br><span class="line">         DataStream&lt;Integer&gt; evenStream = splitStream.select(<span class="string">"even"</span>);</span><br><span class="line">         evenStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamSplitJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">目前split切分的流无法进行二次切分，并且split方法已经标记为过时了，官方不推荐使用，现在官方推荐使用side output的方式实现。</span><br></pre></td></tr></table></figure><h5 id="side-output"><a href="#side-output" class="headerlink" title="side output"></a>side output</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我来看一下使用side output如何实现流的多次切分</span><br></pre></td></tr></table></figure><h6 id="scala-4"><a href="#scala-4" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation</span><br><span class="line"><span class="keyword">import</span> java.&#123;lang, util&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.collector.selector.<span class="type">OutputSelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">ProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">OutputTag</span>, <span class="type">StreamExecutionEnviron</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用sideoutput切分流</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamSideOutputScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         <span class="comment">//首先定义两个sideoutput来准备保存切分出来的数据</span></span><br><span class="line">         <span class="keyword">val</span> outputTag1 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"even"</span>)&#123;&#125;<span class="comment">//保存偶数</span></span><br><span class="line">         <span class="keyword">val</span> outputTag2 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"odd"</span>)&#123;&#125;<span class="comment">//保存奇数</span></span><br><span class="line">         <span class="comment">//注意：process属于Flink中的低级api</span></span><br><span class="line">         <span class="keyword">val</span> outputStream = text.process(<span class="keyword">new</span> <span class="type">ProcessFunction</span>[<span class="type">Int</span>,<span class="type">Int</span>] &#123;</span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">Int</span>, ctx: <span class="type">ProcessFunction</span>[<span class="type">Int</span>, <span class="type">Int</span>]#<span class="type">Context</span>,out:<span class="type">Collector</span>[<span class="type">Int</span>]):</span><br><span class="line">         <span class="keyword">if</span>(value % <span class="number">2</span> == <span class="number">0</span> )&#123;</span><br><span class="line">         ctx.output(outputTag1,value)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         ctx.output(outputTag2,value)</span><br><span class="line">         &#125; </span><br><span class="line">                                     &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//获取偶数数据流</span></span><br><span class="line">         <span class="keyword">val</span> evenStream = outputStream.getSideOutput(outputTag1)</span><br><span class="line">         <span class="comment">//获取奇数数据流</span></span><br><span class="line">         <span class="keyword">val</span> oddStream = outputStream.getSideOutput(outputTag2)</span><br><span class="line">         <span class="comment">//evenStream.print().setParallelism(1)</span></span><br><span class="line">         <span class="comment">//对evenStream流进行二次切分</span></span><br><span class="line">         <span class="keyword">val</span> outputTag11 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"low"</span>)&#123;&#125;<span class="comment">//保存小于等五5的数字</span></span><br><span class="line">         <span class="keyword">val</span> outputTag12 = <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">Int</span>](<span class="string">"high"</span>)&#123;&#125;<span class="comment">//保存大于5的数字</span></span><br><span class="line">         <span class="keyword">val</span> subOutputStream = evenStream.process(<span class="keyword">new</span> <span class="type">ProcessFunction</span>[<span class="type">Int</span>,<span class="type">Int</span>] &#123;</span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">Int</span>, ctx: <span class="type">ProcessFunction</span>[<span class="type">Int</span>, <span class="type">Int</span>]#<span class="type">Context</span>,<span class="type">Collector</span>[<span class="type">Int</span>]):</span><br><span class="line">         <span class="keyword">if</span>(value&lt;=<span class="number">5</span>)&#123;</span><br><span class="line">          ctx.output(outputTag11,value)</span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         ctx.output(outputTag12,value)</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         <span class="comment">//获取小于等于5的数据流</span></span><br><span class="line">         <span class="keyword">val</span> lowStream = subOutputStream.getSideOutput(outputTag11)</span><br><span class="line">         <span class="comment">//获取大于5的数据流</span></span><br><span class="line">         <span class="keyword">val</span> highStream = subOutputStream.getSideOutput(outputTag12)</span><br><span class="line">         lowStream.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         env.execute(<span class="string">"StreamSideOutputScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092310468.png" alt="image-20230409231015815"></p><h6 id="java-4"><a href="#java-4" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.transformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.OutputTag;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用sideoutput切分流</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamSideoutputJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecuti</span><br><span class="line">         DataStreamSource&lt;Integer&gt; text = env.fromCollection(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>));</span><br><span class="line">         <span class="comment">//按照数据的奇偶性对数据进行分流</span></span><br><span class="line">         <span class="comment">//首先定义两个sideoutput来准备保存切分出来的数据</span></span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag1 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"even"</span>) &#123;&#125;;</span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag2 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"odd"</span>) &#123;&#125;;</span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; outputStream = text.process(<span class="keyword">new</span> ProcessFunction&lt;Integer,Integer&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer value, Context ctx, Collector&lt;Integer&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (value % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">         ctx.output(outputTag1, value);</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         ctx.output(outputTag2, value);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//获取偶数数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; evenStream = outputStream.getSideOutput(outputTag1);</span><br><span class="line">         <span class="comment">//获取奇数数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; oddStream = outputStream.getSideOutput(outputTag2);</span><br><span class="line">         <span class="comment">//对evenStream流进行二次切分</span></span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag11 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"low"</span>) &#123;&#125;;</span><br><span class="line">         OutputTag&lt;Integer&gt; outputTag12 = <span class="keyword">new</span> OutputTag&lt;Integer&gt;(<span class="string">"high"</span>) &#123;&#125;;</span><br><span class="line">         SingleOutputStreamOperator&lt;Integer&gt; subOutputStream = evenStream.process(<span class="keyword">new</span> ProcessFunction&lt;Integer,Integer&gt;()&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer value, Context ctx, Collector&lt;Integer&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (value &lt;= <span class="number">5</span>) &#123;</span><br><span class="line">         ctx.output(outputTag11, value);</span><br><span class="line">         &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         ctx.output(outputTag12, value);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//获取小于等于5的数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; lowStream = subOutputStream.getSideOutput(outputTag11);</span><br><span class="line">         <span class="comment">//获取大于5的数据流</span></span><br><span class="line">         DataStream&lt;Integer&gt; highStream = subOutputStream.getSideOutput(outputTag12);</span><br><span class="line">         lowStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         env.execute(<span class="string">"StreamSideoutputJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">其实想要实现多级流切分，使用filter算子也是可以实现的，给大家留一个作业，大家可以下去实验一下。</span><br><span class="line">最后针对这几个算子总结一下：</span><br><span class="line">首先是union和connect的区别，如图所示：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092319871.png" alt="image-20230409231907397"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">union可以连接多个流，最后汇总成一个流，流里面的数据使用相同的计算规则</span><br><span class="line">connect值可以连接2个流，最后汇总成一个流，但是流里面的两份数据相互还是独立的，每一份数据使用一个计算规则</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后是流切分</span><br><span class="line">如果是只需要切分一次的话使用split或者side output都可以</span><br><span class="line">如果想要切分多次，就不能使用split了，需要使用side output</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092319304.png" alt="image-20230409231939157"></p><h5 id="分区相关算子"><a href="#分区相关算子" class="headerlink" title="分区相关算子"></a>分区相关算子</h5><h6 id="random"><a href="#random" class="headerlink" title="random"></a>random</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">接下来看一下这几个和分区相关的算子</span><br><span class="line">算子 解释</span><br><span class="line">random 随机分区</span><br><span class="line">rebalance对数据集进行再平衡，重分区，消除数据倾斜|</span><br><span class="line">rescale重分区</span><br><span class="line">custom partition 自定义分区</span><br><span class="line"></span><br><span class="line">random：随机分区，它表示将上游数据随机分发到下游算子实例的每个分区中，在代码层面体现是调用shuffle()函数</span><br><span class="line"></span><br><span class="line">查看源码，shuffle底层对应的是ShufflePartitioner这个类</span><br><span class="line">这个类里面有一个selectChannel函数，这个函数会计算数据将会被发送给哪个分区，里面使用的是random.nextInt，所以说是随机的。</span><br><span class="line"></span><br><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line"> return random.nextInt(numberOfChannels);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="rebalance"><a href="#rebalance" class="headerlink" title="rebalance"></a>rebalance</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rebalance：重新平衡分区(循环分区)，我觉得叫循环分区更好理解，它表示对数据集进行再平衡，消除数据倾斜，为每个分区创建相同的负载，其实就是通过循环的方式给下游算子实例的每个分区分配数据，在代码层面体现是调用rebalance()函数</span><br><span class="line"></span><br><span class="line">查看源码，rebalance底层对应的是RebalancePartitioner这个类</span><br><span class="line">这个类里面有一个setup和selectChannel函数，setup函数会根据分 区数初始化一个随机值nextChannelToSendTo，然后selectChannel函数会使用nextChannelToSendTo加1和分区数取模，把计算的值再赋给nextChannelToSendTo，后面以此类推，其实就可以实现向下游算子实例的多个分区循环发送数据了，这样每个分区获取到的数据基本一致。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public void setup(int numberOfChannels) &#123;</span><br><span class="line"> super.setup(numberOfChannels);</span><br><span class="line"> nextChannelToSendTo &#x3D; ThreadLocalRandom.current().nextInt(numberOfChannels)</span><br><span class="line">&#125;</span><br><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line">     nextChannelToSendTo &#x3D; (nextChannelToSendTo + 1) % numberOfChannels;</span><br><span class="line">     return nextChannelToSendTo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="rescale"><a href="#rescale" class="headerlink" title="rescale"></a>rescale</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rescale：重分区</span><br><span class="line">查看源码，rescale底层对应的是RescalePartitioner这个类</span><br><span class="line">这个类里面有一个selectChannel函数，这里面的numberOfChannels是分区数量，其实也可以认为是我们所说的算子的并行度，因为一个分区是由一个线程负责处理的，它们两个是一一对应的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line"> if (++nextChannelToSendTo &gt;&#x3D; numberOfChannels) &#123;</span><br><span class="line">  nextChannelToSendTo &#x3D; 0;</span><br><span class="line"> &#125;</span><br><span class="line"> return nextChannelToSendTo;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">详细的解释在这个类的注释中也是有的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">* The subset of downstream operations to which the upstream operation sends</span><br><span class="line">* elements depends on the degree of parallelism of both the upstream and downs</span><br><span class="line">* For example, if the upstream operation has parallelism 2 and the downstream </span><br><span class="line">* has parallelism 4, then one upstream operation would distribute elements to </span><br><span class="line">* downstream operations while the other upstream operation would distribute to</span><br><span class="line">* two downstream operations. If, on the other hand, the downstream operation h</span><br><span class="line">* 2 while the upstream operation has parallelism 4 then two upstream operation</span><br><span class="line">* distribute to one downstream operation while the other two upstream operatio</span><br><span class="line">* distribute to the other downstream operations.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果上游操作有2个并发，而下游操作有4个并发，那么上游的1个并发结果循环分配给下游的2个并发操作，上游的另外1个并发结果循环分配给下游的另外2个并发操作。</span><br><span class="line">另一种情况，如果上游有4个并发操作，而下游有2个并发操作，那么上游的其中2个并发操作的结果会分配给下游的一个并发操作，而上游的另</span><br><span class="line">外2个并发操作的结果则分配给下游的另外1个并发操作。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：rescale与rebalance的区别是rebalance会产生全量重分区，而rescale不会。</span><br></pre></td></tr></table></figure><h6 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">broadcast：广播分区，将上游算子实例中的数据输出到下游算子实例的每个分区中，适合用于大数据集Join小数据集的场景，可以提高性能。</span><br><span class="line"></span><br><span class="line">查看源码，broadcast底层对应的是BroadcastPartitioner这个类</span><br><span class="line">看这个类中的selectChannel函数代码的注释，提示广播分区不支持选择Channel,因为会输出数据到下游的每个Channel中，就是发送到下游算子实例的每个分区中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Note: Broadcast mode could be handled directly for all the output channels</span><br><span class="line"> * in record writer, so it is no need to select channels via this method.</span><br><span class="line"> *&#x2F;</span><br><span class="line">@Override</span><br><span class="line">public int selectChannel(SerializationDelegate&lt;StreamRecord&lt;T&gt;&gt; record) &#123;</span><br><span class="line"> throw new UnsupportedOperationException(&quot;Broadcast partitioner does not sup</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="custom-partition"><a href="#custom-partition" class="headerlink" title="custom partition"></a>custom partition</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">custom partition：自定义分区，可以按照自定义规则实现</span><br><span class="line">自定义分区需要实现Partitioner接口</span><br><span class="line"></span><br><span class="line">这是针对这几种分区的解释，下面来通过这个图总结一下，加深理解</span><br></pre></td></tr></table></figure><h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092345507.png" alt="image-20230409234549859"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092348975.png" alt="image-20230409234806351"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304092350119.png" alt="image-20230409235046659"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后使用代码演示一下它们具体的用法</span><br></pre></td></tr></table></figure><h6 id="scala-5"><a href="#scala-5" class="headerlink" title="scala"></a>scala</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnviro</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 分区规则的使用</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamPartitionOpScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="comment">//注意：默认情况下Fink任务中算子的并行度会读取当前机器的CPU个数</span></span><br><span class="line">         <span class="comment">//但fromCollection的并行度为1，由源码可知</span></span><br><span class="line">         <span class="keyword">val</span> text = env.fromCollection(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//注意：在这里建议将这个隐式转换代码放到类上面</span></span><br><span class="line">         <span class="comment">//因为默认它只在main函数生效，针对下面提取的shuffleOp是无效的，否则也需要在shuffleOp添加这行代码</span></span><br><span class="line">         <span class="comment">//import org.apache.flink.api.scala._</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用shuffle分区规则</span></span><br><span class="line">         <span class="comment">//shuffleOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用rebalance分区规则</span></span><br><span class="line">         <span class="comment">//rebalanceOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用rescale分区规则</span></span><br><span class="line">         <span class="comment">//rescaleOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//使用broadcast分区规则，此代码一共会打印40条数据，因为print的并行度为4</span></span><br><span class="line">         <span class="comment">//broadcastOp(text)</span></span><br><span class="line">         </span><br><span class="line">         <span class="comment">//自定义分区规则：根据数据的奇偶性进行分区</span></span><br><span class="line">         <span class="comment">//注意：此时虽然print算子的并行度是4，但是自定义的分区规则只会把数据分发给2个并行度，所以有两个不干活</span></span><br><span class="line">         <span class="comment">//custormPartitionOp(text)</span></span><br><span class="line">         env.execute(<span class="string">"StreamPartitionOpScala"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">custormPartitionOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         <span class="comment">//.partitionCustom(new MyPartitionerScala,0)//这种写法已经过期</span></span><br><span class="line">         .partitionCustom(<span class="keyword">new</span> <span class="type">MyPartitionerScala</span>, num =&gt; num)<span class="comment">//官方建议使用keyselector</span></span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">broadcastOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .broadcast</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">rescaleOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .rescale</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">rebalanceOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .rebalance</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">shuffleOp</span></span>(text: <span class="type">DataStream</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">         <span class="comment">//由于fromCollection已经设置了并行度为1，所以需要再接一个算子之后才能修改并行度</span></span><br><span class="line">         text.map(num =&gt; num)</span><br><span class="line">         .setParallelism(<span class="number">2</span>) <span class="comment">//设置map算子的并行度为2</span></span><br><span class="line">         .shuffle  </span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(<span class="number">4</span>) <span class="comment">//设置print算子的并行度为4</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.transformation</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">Partitioner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义分区规则：按照数字的奇偶性进行分区</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitionerScala</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>[<span class="type">Int</span>]</span>&#123;</span><br><span class="line">     <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">partition</span></span>(key: <span class="type">Int</span>, numPartitions: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">         println(<span class="string">"分区总数："</span>+numPartitions)</span><br><span class="line">         <span class="keyword">if</span>(key % <span class="number">2</span> == <span class="number">0</span>)&#123;<span class="comment">//偶数分到0号分区</span></span><br><span class="line">         <span class="number">0</span></span><br><span class="line">         &#125;<span class="keyword">else</span>&#123;<span class="comment">//奇数分到1号分区</span></span><br><span class="line">         <span class="number">1</span></span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">custom partition</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100026329.png" alt="image-20230410002655126"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">broadcast 此时每个数据都有四次输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100014718.png" alt="image-20230410001436246"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rescale  四个分区都有输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100013065.png" alt="image-20230410001313667"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rebalance 四个分区都有数据输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100010193.png" alt="image-20230410000959762"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shuffle  不一定每个分区都有数据输出</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304100010853.png" alt="image-20230410001045259"></p><h6 id="java-5"><a href="#java-5" class="headerlink" title="java"></a>java</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">package com.imooc.java.stream.transformation;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line">&#x2F;**</span><br><span class="line"> * 分区规则的使用</span><br><span class="line"> * Created by xuwei</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class StreamPartitionOpJava &#123;</span><br><span class="line">     public static void main(String[] args) throws Exception&#123;</span><br><span class="line">         StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecuti</span><br><span class="line">         DataStreamSource&lt;Integer&gt; text &#x3D; env.fromCollection(Arrays.asList(1, 2</span><br><span class="line">         &#x2F;&#x2F;使用shuffle分区规则</span><br><span class="line">         &#x2F;&#x2F;shuffleOp(text);</span><br><span class="line">         &#x2F;&#x2F;使用rebalance分区规则</span><br><span class="line">         &#x2F;&#x2F;rebalanceOp(text);</span><br><span class="line">         &#x2F;&#x2F;使用rescale分区规则</span><br><span class="line">         &#x2F;&#x2F;rescaleOp(text);</span><br><span class="line">         &#x2F;&#x2F;使用broadcast分区规则</span><br><span class="line">         &#x2F;&#x2F;broadcastOp(text);</span><br><span class="line">         &#x2F;&#x2F;自定义分区规则</span><br><span class="line">         &#x2F;&#x2F;custormPartitionOp(text);</span><br><span class="line">         env.execute(&quot;StreamPartitionOpJava&quot;);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void custormPartitionOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .partitionCustom(new MyPartitionerJava(), new KeySelector&lt;Inte</span><br><span class="line">         @Override</span><br><span class="line">         public Integer getKey(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void broadcastOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .broadcast()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void rescaleOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .rescale()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void rebalanceOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .rebalance()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">         &#125;</span><br><span class="line">         private static void shuffleOp(DataStreamSource&lt;Integer&gt; text) &#123;</span><br><span class="line">         text.map(new MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">         @Override</span><br><span class="line">         public Integer map(Integer integer) throws Exception &#123;</span><br><span class="line">         return integer;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).setParallelism(2)</span><br><span class="line">         .shuffle()</span><br><span class="line">         .print()</span><br><span class="line">         .setParallelism(4);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="DataStream-API之DataSink"><a href="#DataStream-API之DataSink" class="headerlink" title="DataStream API之DataSink"></a>DataStream API之DataSink</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DataSink是 输出组件，负责把计算好的数据输出到其它存储介质中</span><br><span class="line">Flink支持把流数据输出到文件中，不过在实际工作中这种场景不多，因为流数据处理之后一般会存储到一些消息队列里面，或者数据库里面，很少会保存到文件中的。</span><br><span class="line"></span><br><span class="line">还有就是print，直接打印，这个其实我们已经用了很多次了，这种用法主要是在测试的时候使用的，方便查看输出的结果信息</span><br><span class="line"></span><br><span class="line">Flink提供了一批Connectors，可以实现输出到第三方目的地</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Flink内置 Apache Bahir</span><br><span class="line">Kafka ActiveMQ</span><br><span class="line">Cassandra Flume</span><br><span class="line">Kinesis Streams Redis</span><br><span class="line">Elasticsearch Akka</span><br><span class="line">Hadoop FileSysterm</span><br><span class="line">RabbitMQ</span><br><span class="line">NiFi</span><br><span class="line">JDBC</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">针对sink的这些connector，我们在实际工作中最常用的是kafka、redis</span><br><span class="line">针对Flink提供的常用sink组件，可以提供这些容错性保证</span><br><span class="line"></span><br><span class="line">DataSink 容错保证 备注</span><br><span class="line">Redis at least once</span><br><span class="line">Kafka    at least once&#x2F;exactlyonceKafka0.9和0.10提供at least once，Kafka0.11及以上提供</span><br><span class="line">exactly once</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101205370.png" alt="image-20230410120538984"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">针对kafka这个sink组件的使用，我们在后面会统一分析，现在我们来使用一下redis这个sink组件</span><br><span class="line">需求：接收Socket传输过来的数据，把数据保存到Redis的list队列中。</span><br></pre></td></tr></table></figure><h5 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">先到Flink官网，文档，connector,datasteam,redis,添加对应的依赖(一般不是正确的依赖，把名字复制到maven官网，查找)</span><br></pre></td></tr></table></figure><h6 id="scala-6"><a href="#scala-6" class="headerlink" title="scala"></a>scala</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：redis sink是在Bahir这个依赖包中，所以在pom.xml中需要添加对应的依赖</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.bahir&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;flink-connector-redis_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;1.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala.stream.sink</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.<span class="type">RedisSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.config.<span class="type">FlinkJedisPoo</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.&#123;<span class="type">RedisCommand</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：接收Socket传输过来的数据，把数据保存到Redis的list队列中。</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamRedisSinkScala</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//组装数据，这里组装的是tuple2类型</span></span><br><span class="line">         <span class="comment">//第一个元素是指list队列的key名称</span></span><br><span class="line">         <span class="comment">//第二个元素是指需要向list队列中添加的元素</span></span><br><span class="line">         <span class="keyword">val</span> listData = text.map(word =&gt; (<span class="string">"l_words_scala"</span>, word))</span><br><span class="line">         </span><br><span class="line">         <span class="comment">//指定redisSink</span></span><br><span class="line">         <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">FlinkJedisPoolConfig</span>.<span class="type">Builder</span>().setHost(<span class="string">"bigdata04"</span>).setPort(<span class="number">6379</span>).build()</span><br><span class="line">         <span class="keyword">val</span> redisSink = <span class="keyword">new</span> <span class="type">RedisSink</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">String</span>]](conf, <span class="keyword">new</span> <span class="type">MyRedisMapper</span>)</span><br><span class="line">                                                                listData.addSink(redisSink)</span><br><span class="line">         env.execute(<span class="string">"StreamRedisSinkScala"</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">    </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">extends</span> <span class="title">RedisMapper</span>[<span class="type">Tuple2</span>[<span class="type">String</span>,<span class="type">String</span>]]</span>&#123;</span><br><span class="line">         <span class="comment">//指定具体的操作命令</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getCommandDescription</span></span>: <span class="type">RedisCommandDescription</span> = &#123;</span><br><span class="line">         <span class="keyword">new</span> <span class="type">RedisCommandDescription</span>(<span class="type">RedisCommand</span>.<span class="type">LPUSH</span>)</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取key</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKeyFromData</span></span>(data: (<span class="type">String</span>, <span class="type">String</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">         data._1</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取value</span></span><br><span class="line">         <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValueFromData</span></span>(data: (<span class="type">String</span>, <span class="type">String</span>)): <span class="type">String</span> = &#123;</span><br><span class="line">         data._2</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">注意：针对List数据类型，我们在定义getCommandDescription方法的时候，使用new</span><br><span class="line">RedisCommandDescription(RedisCommand.LPUSH);。</span><br><span class="line"></span><br><span class="line">如果是Hash数据类型，在定义getCommandDescription方法的时候，需要使用new</span><br><span class="line">RedisCommandDescription(RedisCommand.HSET,“hashKey”);，在构造函数中需要直接指定Hash数据类型的key的名称。</span><br><span class="line"></span><br><span class="line">注意：执行代码之前，需要先开启socket和redis服务</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">通过socket传递单词</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line"></span><br><span class="line">最终到redis中查看结果</span><br><span class="line">[root@bigdata04 redis-5.0.9]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; lrange l_words_scala 0 -1</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101233738.png" alt="image-20230410123339761"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101234121.png" alt="image-20230410123420552"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最终到redis中查看结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304101234377.png" alt="image-20230410123448085"></p><h6 id="java-6"><a href="#java-6" class="headerlink" title="java"></a>java</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java.stream.sink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.RedisSink;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoo</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommand;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommandD</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.redis.common.mapper.RedisMapper;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：接收Socket传输过来的数据，把数据保存到Redis的list队列中。</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamRedisSinkJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line">         <span class="comment">//组装数据</span></span><br><span class="line">         SingleOutputStreamOperator&lt;Tuple2&lt;String, String&gt;&gt; listData = text.map(<span class="keyword">new</span> MapFunction&lt;String,String&gt;&#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">map</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="string">"l_words_java"</span>, word);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;);</span><br><span class="line">         <span class="comment">//指定redisSink</span></span><br><span class="line">         FlinkJedisPoolConfig conf = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder().setHost(<span class="string">"bigdata04"</span>).setPort(<span class="number">6379</span>).build();</span><br><span class="line">        </span><br><span class="line">         RedisSink&lt;Tuple2&lt;String, String&gt;&gt; redisSink = <span class="keyword">new</span> RedisSink&lt;&gt;(conf, <span class="keyword">new</span> MyRedisMapper()) </span><br><span class="line">         listData.addSink(redisSink);</span><br><span class="line">         </span><br><span class="line">                                                                             env.execute(<span class="string">"StreamRedisSinkJava"</span>);</span><br><span class="line">         &#125;</span><br><span class="line">        </span><br><span class="line">                                                      <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">implements</span> <span class="title">RedisMapper</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>,<span class="title">String</span>&gt;&gt;</span>&#123;</span><br><span class="line">         <span class="comment">//指定具体的操作命令</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> RedisCommandDescription(RedisCommand.LPUSH);</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取key</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getKeyFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> data.f0;</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">//获取value</span></span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">getValueFromData</span><span class="params">(Tuple2&lt;String, String&gt; data)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> data.f1;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-实战：流处理和批处理程序开发-2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%AE%9E%E6%88%98%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%92%8C%E6%89%B9%E5%A4%84%E7%90%86%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91-2.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%AE%9E%E6%88%98%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%92%8C%E6%89%B9%E5%A4%84%E7%90%86%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91-2.html</id>
    <published>2023-04-07T17:41:10.000Z</published>
    <updated>2023-04-08T16:44:33.313Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-实战：流处理和批处理程序开发-2"><a href="#第十六周-Flink极速上手篇-实战：流处理和批处理程序开发-2" class="headerlink" title="第十六周 Flink极速上手篇-实战：流处理和批处理程序开发-2"></a>第十六周 Flink极速上手篇-实战：流处理和批处理程序开发-2</h1><h2 id="Flink快速上手使用"><a href="#Flink快速上手使用" class="headerlink" title="Flink快速上手使用"></a>Flink快速上手使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">创建maven项目，因为要使用scala编写代码，在src里main里除了java目录，还要创建scala目录，再创建包</span><br><span class="line"></span><br><span class="line">setting里的module里的scala sdk要导入</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304081548060.png" alt="image-20230408154825118"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来在 pom.xml 中引入flink相关依赖，前面两个是针对java代码的，后面两个是针对scala代码的，最后一个依赖是这对flink1.11这个版本需要添加的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-java&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-streaming-java_2.12&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-scala_2.12&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt; </span><br><span class="line">     &lt;artifactId&gt;flink-streaming-scala_2.12&lt;&#x2F;artifactId&gt; </span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt; </span><br><span class="line">     &lt;scope&gt;provided&lt;&#x2F;scope&gt; </span><br><span class="line">&lt;&#x2F;dependency&gt; </span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;flink-clients_2.12&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;1.11.1&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h2 id="Flink-Job开发步骤"><a href="#Flink-Job开发步骤" class="headerlink" title="Flink Job开发步骤"></a>Flink Job开发步骤</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在开发Flink程序之前，我们先来梳理一下开发一个Flink程序的步骤</span><br><span class="line">1：获得一个执行环境</span><br><span class="line">2：加载&#x2F;创建 初始化数据</span><br><span class="line">3：指定操作数据的transaction算子</span><br><span class="line">4：指定数据目的地</span><br><span class="line">5：调用execute()触发执行程序</span><br><span class="line"></span><br><span class="line">注意：Flink程序是延迟计算的，只有最后调用execute()方法的时候才会真正触发执行程序和Spark类似，Spark中是必须要有action算子才会真正执行。</span><br></pre></td></tr></table></figure><h3 id="Streaming-WordCount"><a href="#Streaming-WordCount" class="headerlink" title="Streaming WordCount"></a>Streaming WordCount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">需求</span><br><span class="line">通过socket实时产生一些单词，使用flink实时接收数据，对指定时间窗口内(例如：2秒)的数据进行聚合统计，并且把时间窗口内计算的结果打印出来</span><br><span class="line">代码开发</span><br><span class="line">下面我们就来开发第一个Flink程序。</span><br><span class="line">先使用scala代码开发</span><br></pre></td></tr></table></figure><h4 id="scala"><a href="#scala" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.<span class="type">KeySelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过Socket实时产生一些单词，</span></span><br><span class="line"><span class="comment"> * 使用Flink实时接收数据</span></span><br><span class="line"><span class="comment"> * 对指定时间窗口内(例如：2秒)的数据进行聚合统计</span></span><br><span class="line"><span class="comment"> * 并且把时间窗口内计算的结果打印出来</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SocketWindowWordCountScala</span> </span>&#123;</span><br><span class="line">     <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意：在执行代码之前，需要先在bigdata04机器上开启socket，端口为9001</span></span><br><span class="line"><span class="comment">     * @param args</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">         <span class="comment">//获取运行环境</span></span><br><span class="line">         <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         <span class="keyword">val</span> text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>)</span><br><span class="line">         <span class="comment">//处理数据</span></span><br><span class="line">         <span class="comment">//注意：必须要添加这一行隐式转换的代码，否则下面的flatMap方法会报错</span></span><br><span class="line">         <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">         <span class="keyword">val</span> wordCount = text.flatMap(_.split(<span class="string">" "</span>))<span class="comment">//将每一行数据根据空格切分单词</span></span><br><span class="line">         .map((_,<span class="number">1</span>))<span class="comment">//每一个单词转换为tuple2的形式(单词,1)</span></span><br><span class="line">         <span class="comment">//.keyBy(0)//根据tuple2中的第一列进行分组</span></span><br><span class="line">         .keyBy(tup=&gt;tup._1)<span class="comment">//官方推荐使用keyselector选择器选择数据</span></span><br><span class="line">         .timeWindow(<span class="type">Time</span>.seconds(<span class="number">2</span>))<span class="comment">//时间窗口为2秒，表示每隔2秒钟计算一次接收到的数据</span></span><br><span class="line">         .sum(<span class="number">1</span>)<span class="comment">// 使用sum或者reduce都可以</span></span><br><span class="line">         <span class="comment">//.reduce((t1,t2)=&gt;(t1._1,t1._2+t2._2))</span></span><br><span class="line">             <span class="comment">//使用一个线程执行打印操作</span></span><br><span class="line">         wordCount.print().setParallelism(<span class="number">1</span>)</span><br><span class="line">         <span class="comment">//执行程序</span></span><br><span class="line">         env.execute(<span class="string">"SocketWindowWordCountScala"</span>)</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：在idea等开发工具里面运行代码的时候需要把pom.xml中的scope配置注释掉</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082223638.png" alt="image-20230408222314468"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04上面开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001</span><br><span class="line">hello you</span><br><span class="line">hello me</span><br><span class="line">hello you hello me</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082225596.png" alt="image-20230408222535380"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">注意：此时代码执行的时候下面会显示一些红色的log4j的警告信息，提示缺少相关依赖和配置</span><br><span class="line"></span><br><span class="line">将log4j.properties配置文件和log4j的相关maven配置添加到pom.xml文件中</span><br><span class="line"></span><br><span class="line">&lt;!-- log4j的依赖 --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;slf4j-api&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;slf4j-log4j12&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;1.7.10&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082228288.png" alt="image-20230408222823964"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时再执行就没有红色的警告信息了，但是使用info日志级别打印的信息太多了，所以将log4j中的日志级别配置改为error级别</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082229674.png" alt="image-20230408222903540"></p><h4 id="java"><a href="#java" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：通过socket实时产生一些单词</span></span><br><span class="line"><span class="comment"> * 使用Flink实时接收数据</span></span><br><span class="line"><span class="comment"> * 对指定时间窗口内(例如：2秒)的数据进行聚合统计</span></span><br><span class="line"><span class="comment"> * 并且把时间窗口内计算的结果打印出来</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketWindowWordCountJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取运行环境</span></span><br><span class="line">         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">         <span class="comment">//连接socket获取输入数据</span></span><br><span class="line">         DataStreamSource&lt;String&gt; text = env.socketTextStream(<span class="string">"bigdata04"</span>, <span class="number">9001</span>);</span><br><span class="line">         <span class="comment">//处理数据</span></span><br><span class="line">         SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordCount = text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String,String&gt;()&#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">         <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">         out.collect(word);</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).map(<span class="keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span></span><br><span class="line"><span class="function">         return new Tuple2&lt;String, Integer&gt;<span class="params">(word, <span class="number">1</span>)</span></span>;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; tup)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> tup.f0;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;)<span class="comment">//.keyBy(0)</span></span><br><span class="line">         .timeWindow(Time.seconds(<span class="number">2</span>))</span><br><span class="line">         .sum(<span class="number">1</span>);</span><br><span class="line">         <span class="comment">//使用一个线程执行打印操作</span></span><br><span class="line">         wordCount.print().setParallelism(<span class="number">1</span>);</span><br><span class="line">         <span class="comment">//执行程序</span></span><br><span class="line">         env.execute(<span class="string">"SocketWindowWordCountJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Batch-WordCount"><a href="#Batch-WordCount" class="headerlink" title="Batch WordCount"></a>Batch WordCount</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需求：统计指定文件中单词出现的总次数</span><br><span class="line">下面来开发Flink的批处理代码</span><br></pre></td></tr></table></figure><h4 id="scala-1"><a href="#scala-1" class="headerlink" title="scala"></a>scala</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.scala</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：统计指定文件中单词出现的总次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchWordCountScala</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"> <span class="comment">//获取执行环境</span></span><br><span class="line"> <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"> <span class="keyword">val</span> inputPath = <span class="string">"hdfs://bigdata01:9000/hello.txt"</span></span><br><span class="line"> <span class="keyword">val</span> outPath = <span class="string">"hdfs://bigdata01:9000/out"</span></span><br><span class="line"> <span class="comment">//读取文件中的数据</span></span><br><span class="line"> <span class="keyword">val</span> text = env.readTextFile(inputPath)</span><br><span class="line"> <span class="comment">//处理数据</span></span><br><span class="line"> <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"> <span class="keyword">val</span> wordCount = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"> .map((_, <span class="number">1</span>))</span><br><span class="line"> .groupBy(<span class="number">0</span>)</span><br><span class="line"> .sum(<span class="number">1</span>)</span><br><span class="line"> .setParallelism(<span class="number">1</span>) <span class="comment">// 这里设置并行度是为了将所有数据写到一个文件里，查看比较方便</span></span><br><span class="line"> <span class="comment">//将结果数据保存到文件中</span></span><br><span class="line"> wordCount.writeAsCsv(outPath,<span class="string">"\n"</span>,<span class="string">" "</span>)</span><br><span class="line"> <span class="comment">//执行程序</span></span><br><span class="line"> env.execute(<span class="string">"BatchWordCountScala"</span>)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：这里面执行setParallelism(1)设置并行度为1是为了将所有数据写到一个文件里面，我们查看结果的时候比较方便(此时输出路径会变成一个文件)</span><br><span class="line">还有就是flink在windows中执行代码，使用到hadoop的时候，需要将hadoop-client的依赖添加到项目中，否则会提示不支持hdfs这种文件系统。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在pom.xml文件中增加</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line"> &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line"> &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line"> &lt;version&gt;3.2.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">此时执行代码就可以正常执行了。</span><br><span class="line">执行成功之后到hdfs上查看结果</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082256026.png" alt="image-20230408225649895"></p><h4 id="java-1"><a href="#java-1" class="headerlink" title="java"></a>java</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.java;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.DataSet;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.AggregateOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：统计指定文件中单词出现的总次数</span></span><br><span class="line"><span class="comment"> * Created by xuwei</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchWordCountJava</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         <span class="comment">//获取执行环境</span></span><br><span class="line">         ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">         String inputPath = <span class="string">"hdfs://bigdata01:9000/hello.txt"</span>;</span><br><span class="line">         String outPath = <span class="string">"hdfs://bigdata01:9000/out2"</span>;</span><br><span class="line">         <span class="comment">//读取文件中的数据</span></span><br><span class="line">         DataSource&lt;String&gt; text = env.readTextFile(inputPath);</span><br><span class="line">         <span class="comment">//处理数据(这里flatmap之后，没写map，直接一步到位)</span></span><br><span class="line">         DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; wordCount = text.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String,Tuple2&lt;String,Integer&gt;&gt;()&#123;</span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span></span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">         <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">         out.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, <span class="number">1</span>));</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         &#125;).groupBy(<span class="number">0</span>)</span><br><span class="line">         .sum(<span class="number">1</span>)</span><br><span class="line">         .setParallelism(<span class="number">1</span>);</span><br><span class="line">         <span class="comment">//将结果数据保存到文件中</span></span><br><span class="line">         wordCount.writeAsCsv(outPath,<span class="string">"\n"</span>,<span class="string">" "</span>);</span><br><span class="line">         <span class="comment">//执行程序</span></span><br><span class="line">         env.execute(<span class="string">"BatchWordCountJava"</span>);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-Streaming和Batch的区别"><a href="#Flink-Streaming和Batch的区别" class="headerlink" title="Flink Streaming和Batch的区别"></a>Flink Streaming和Batch的区别</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">流处理Streaming</span><br><span class="line">执行环境：StreamExecutionEnvironment</span><br><span class="line">数据类型：DataStream</span><br><span class="line"></span><br><span class="line">批处理Batch</span><br><span class="line">执行环境：ExecutionEnvironment</span><br><span class="line">数据类型：DataSet</span><br></pre></td></tr></table></figure><h2 id="Flink集群安装部署"><a href="#Flink集群安装部署" class="headerlink" title="Flink集群安装部署"></a>Flink集群安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Flink支持多种安装部署方式</span><br><span class="line">Standalone</span><br><span class="line">ON YARN</span><br><span class="line">Mesos、Kubernetes、AWS…</span><br><span class="line">这些安装方式我们主要讲一下standalone和on yarn。</span><br><span class="line">如果是一个独立环境的话，可能会用到standalone集群模式。</span><br><span class="line">在生产环境下一般还是用on yarn 这种模式比较多，因为这样可以综合利用集群资源。和我们之前讲的spark on yarn是一样的效果</span><br><span class="line">这个时候我们的Hadoop集群上面既可以运行MapReduce任务，Spark任务，还可以运行Flink任务，一举三得。</span><br></pre></td></tr></table></figure><h3 id="standalone"><a href="#standalone" class="headerlink" title="standalone"></a>standalone</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下standalone模式</span><br><span class="line">它的架构是这样的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082323175.png" alt="image-20230408232348677"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">依赖环境</span><br><span class="line">jdk1.8及以上【配置JAVA_HOME环境变量】</span><br><span class="line">ssh免密码登录</span><br><span class="line">在这我们使用bigdata01、02、03这三台机器，这几台机器的基础环境都是ok的，可以直接使用。</span><br><span class="line"></span><br><span class="line">集群规划如下：</span><br><span class="line">master：bigdata01</span><br><span class="line">slave：bigdata02、bigdata03</span><br><span class="line">接下来我们需要先去下载Flink的安装包。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082326209.png" alt="image-20230408232658079"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">由于目前Flink各个版本之间差异比较大，属于快速迭代阶段，所以在这我们就使用最新版本了，使用Flink1.11.1版本。</span><br><span class="line">安装包下载好以后上传到bigdata01的&#x2F;data&#x2F;soft目录中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">下面开始安装Flink集群</span><br><span class="line">1：解压</span><br><span class="line"></span><br><span class="line">2：修改配置</span><br><span class="line">[root@bigdata01 soft]# cd flink-1.11.1</span><br><span class="line">[root@bigdata01 flink-1.11.1]# cd conf&#x2F;</span><br><span class="line">[root@bigdata01 conf]# vi flink-conf.yaml </span><br><span class="line">......</span><br><span class="line">jobmanager.rpc.address: bigdata01</span><br><span class="line">......</span><br><span class="line">[root@bigdata01 conf]# vi masters </span><br><span class="line">bigdata01:8081</span><br><span class="line">[root@bigdata01 conf]# vi workers</span><br><span class="line">bigdata02</span><br><span class="line">bigdata03</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">3：将修改完配置的flink目录拷贝到其它两个从节点</span><br><span class="line">[root@bigdata01 soft]# scp -rq flink-1.11.1 bigdata02:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">[root@bigdata01 soft]# scp -rq flink-1.11.1 bigdata03:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line"></span><br><span class="line">4：启动Flink集群</span><br><span class="line">[root@bigdata01 soft]# cd flink-1.11.1</span><br><span class="line">[root@bigdata01 flink-1.11.1]# bin&#x2F;start-cluster.sh </span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host bigdata01.</span><br><span class="line">Starting taskexecutor daemon on host bigdata02.</span><br><span class="line">Starting taskexecutor daemon on host bigdata03.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5：验证一下进程</span><br><span class="line">在bigdata01上执行jps</span><br><span class="line">[root@bigdata01 flink-1.11.1]# jps</span><br><span class="line">3986 StandaloneSessionClusterEntrypoint</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在bigdata02上执行jps</span><br><span class="line">在bigdata03上执行jps</span><br><span class="line">[root@bigdata02 ~]# jps</span><br><span class="line">2149 TaskManagerRunner</span><br><span class="line">[root@bigdata03 ~]# jps</span><br><span class="line">2150 TaskManagerRunner</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">6：访问Flink的web界面</span><br><span class="line">http:&#x2F;&#x2F;bigdata01:8081</span><br><span class="line">7：停止集群，在主节点上执行停止集群脚本</span><br><span class="line">[root@bigdata01 flink-1.11.1]# bin&#x2F;stop-cluster.sh </span><br><span class="line">Stopping taskexecutor daemon (pid: 2149) on host bigdata02.</span><br><span class="line">Stopping taskexecutor daemon (pid: 2150) on host bigdata03.</span><br><span class="line">Stopping standalonesession daemon (pid: 3986) on host bigdata01.</span><br></pre></td></tr></table></figure><h4 id="Standalone集群核心参数"><a href="#Standalone集群核心参数" class="headerlink" title="Standalone集群核心参数"></a>Standalone集群核心参数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">参数 解释</span><br><span class="line">jobmanager.memory .process.size 主节点可用内存大小</span><br><span class="line">taskmanager.memory.process.size 从节点可用内存大小</span><br><span class="line">taskmanager.numberOfTaskSlots 从节点可以启动的进程数量，建议设置为从节可用的cpu数量</span><br><span class="line">parallelism.default Flink任务的默认并行度</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1：slot是静态的概念，是指taskmanager具有的并发执行能力</span><br><span class="line">2：parallelism是动态的概念，是指程序运行时实际使用的并发能力</span><br><span class="line">3：设置合适的parallelism能提高程序计算效率，太多了和太少了都不好</span><br></pre></td></tr></table></figure><h3 id="Flink-ON-YARN"><a href="#Flink-ON-YARN" class="headerlink" title="Flink ON YARN"></a>Flink ON YARN</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink ON YARN模式就是使用客户端的方式，直接向Hadoop集群提交任务即可。不需要单独启动Flink进程。</span><br><span class="line">注意：</span><br><span class="line">1：Flink ON YARN 模式依赖Hadoop 2.4.1及以上版本</span><br><span class="line">2：Flink ON YARN支持两种使用方式</span><br></pre></td></tr></table></figure><h4 id="Flink-ON-YARN两种使用方式"><a href="#Flink-ON-YARN两种使用方式" class="headerlink" title="Flink ON YARN两种使用方式"></a>Flink ON YARN两种使用方式</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082341963.png" alt="image-20230408234110607"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在工作中建议使用第二种方式。</span><br></pre></td></tr></table></figure><h5 id="Flink-ON-YARN第一种方式"><a href="#Flink-ON-YARN第一种方式" class="headerlink" title="Flink ON YARN第一种方式"></a>Flink ON YARN第一种方式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下第一种方式</span><br><span class="line">第一步：在集群中初始化一个长时间运行的Flink集群</span><br><span class="line">使用yarn-session.sh脚本</span><br><span class="line">第二步：使用flink run命令向Flink集群中提交任务</span><br><span class="line"></span><br><span class="line">注意：使用flink on yarn需要确保hadoop集群已经启动成功</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">下面来具体演示一下</span><br><span class="line">首先在bigdata04机器上安装一个Flink客户端，其实就是把Flink的安装包上传上去解压即可，不需要启动</span><br><span class="line"></span><br><span class="line">接下来在执行yarn-session.sh 脚本之前我们需要先设置 HADOOP_CLASSPATH这个环境变量，否则，执行</span><br><span class="line">yarn-session.sh 是会报错的，提示找不到hadoop的一些依赖。</span><br><span class="line"></span><br><span class="line">在&#x2F;etc&#x2F;profile中配置HADOOP_CLASSPATH</span><br><span class="line">[root@bigdata04 flink-1.11.1]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;jdk1.8</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;hadoop-3.2.0</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin</span><br><span class="line">export SPARK_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;spark-2.4.3-bin-hadoop2.7</span><br><span class="line">export SQOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;sqoop-1.4.7.bin__hadoop-2.6.0</span><br><span class="line">export HADOOP_CLASSPATH&#x3D;&#96;$&#123;HADOOP_HOME&#125;&#x2F;bin&#x2F;hadoop classpath&#96;</span><br><span class="line">export PATH&#x3D;.:$JAVA_HOME&#x2F;bin:$HADOOP_HOME&#x2F;bin:$HIVE_HOME&#x2F;bin:$SPARK_HO</span><br><span class="line">ME&#x2F;bin:$SQOOP_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082349229.png" alt="image-20230408234907861"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来，使用 yarn-session.sh在YARN中创建一个长时间运行的Flink集群</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;yarn-session.sh -jm 1024m -tm 1024m -d</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个表示创建一个Flink集群，-jm 是指定主节点的内存，-tm是指定从节点的内存，-d是表示把这个进程放到后台去执行。</span><br><span class="line">启动之后，会看到类似这样的日志信息，这里面会显示flink web界面的地址，以及这个flink集群在yarn中对应的applicationid。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082347701.png" alt="image-20230408234705575"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时到YARN的web界面中确实可以看到这个flink集群。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082353556.png" alt="image-20230408235222041"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082354623.png" alt="image-20230408235427696"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以使用屏幕中显示的flink的web地址或者yarn中这个链接都是可以进入这个flink的web界面的</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082353738.png" alt="image-20230408235310889"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来向这个Flink集群中提交任务，此时使用Flink中的内置案例</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink run .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这个时候我们使用flink run的时候，它会默认找这个文件，然后根据这个文件找到刚才我们创建的那个永久的Flink集群，这个文件里面保存的就是刚才启动的那个Flink集群在YARN中对应的applicationid。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082357866.png" alt="image-20230408235700425"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082357811.png" alt="image-20230408235714610"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">任务提交上去执行完成之后，再来看flink的web界面，发现这里面有一个已经执行结束的任务了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304082359725.png" alt="image-20230408235924462"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这个任务在执行的时候，会动态申请一些资源执行任务，任务执行完毕之后，对应的资源会自动释放掉。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">最后把这个Flink集群停掉，使用yarn的kill命令</span><br><span class="line">[root@bigdata04 flink-1.11.1]# yarn application -kill application_17689063095</span><br><span class="line">2026-01-20 23:25:22,548 INFO client.RMProxy: Connecting to ResourceManager at </span><br><span class="line">Killing application application_1768906309581_0005</span><br><span class="line">2026-01-20 23:25:23,239 INFO impl.YarnClientImpl: Killed application_17689063</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对 yarn-session命令，它后面还支持一些其它参数，可以在后面传一个-help参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090002237.png" alt="image-20230409000251119"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：这里的-j 是指定Flink任务的jar包，此参数可以省略不写也可以</span><br></pre></td></tr></table></figure><h5 id="Flink-ON-YARN第二种方式"><a href="#Flink-ON-YARN第二种方式" class="headerlink" title="Flink ON YARN第二种方式"></a>Flink ON YARN第二种方式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">flink run -m yarn-cluster(创建Flink集群+提交任务)</span><br><span class="line">使用flink run直接创建一个临时的Flink集群，并且提交任务</span><br><span class="line">此时这里面的参数前面加上了一个y参数</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink run -m yarn-cluster -yjm 1024 -ytm 1024 .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar</span><br><span class="line"></span><br><span class="line">提交上去之后，会先创建一个Flink集群，然后在这个Flink集群中执行任务。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090009823.png" alt="image-20230409000904789"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对Flink命令的一些用法汇总</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090010917.png" alt="image-20230409001026836"></p><h4 id="Flink-ON-YARN的好处"><a href="#Flink-ON-YARN的好处" class="headerlink" title="Flink ON YARN的好处"></a>Flink ON YARN的好处</h4> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1：提高大数据集群机器的利用率</span><br><span class="line">2：一套集群，可以执行MR任务，Spark任务，Flink任务等</span><br></pre></td></tr></table></figure><h3 id="向集群中提交Flink任务"><a href="#向集群中提交Flink任务" class="headerlink" title="向集群中提交Flink任务"></a>向集群中提交Flink任务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">接下来我们希望把前面我们自己开发的Flink任务提交到集群上面，在这我就使用flink on yarn的第二种方式来向集群提交一个Flink任务。</span><br><span class="line"></span><br><span class="line">第一步：在pom.xml中添加打包配置</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;!-- 编译插件 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.6.0&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;!-- scala编译插件 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;net.alchim31.maven&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;scala-maven-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.1.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;scalaCompatVersion&gt;2.12&lt;&#x2F;scalaCompatVersion&gt;</span><br><span class="line">                &lt;scalaVersion&gt;2.12.11&lt;&#x2F;scalaVersion&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;compile&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;goal&gt;compile&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;test-compile-scala&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;test-compile&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;add-source&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;goal&gt;testCompile&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;!-- 打jar包插件(会包含所有依赖) --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;descriptorRefs&gt;</span><br><span class="line">                    &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class="line">                &lt;&#x2F;descriptorRefs&gt;</span><br><span class="line">                &lt;archive&gt;</span><br><span class="line">                    &lt;manifest&gt;</span><br><span class="line">                        &lt;!-- 可以设置jar包的入口类(可选) --&gt;</span><br><span class="line">                        &lt;mainClass&gt;&lt;&#x2F;mainClass&gt;</span><br><span class="line">                    &lt;&#x2F;manifest&gt;</span><br><span class="line">                &lt;&#x2F;archive&gt;</span><br><span class="line">            &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                    &lt;&#x2F;goals&gt;</span><br><span class="line">                &lt;&#x2F;execution&gt;</span><br><span class="line">            &lt;&#x2F;executions&gt;</span><br><span class="line">        &lt;&#x2F;plugin&gt;</span><br><span class="line">    &lt;&#x2F;plugins&gt;</span><br><span class="line">&lt;&#x2F;build&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：需要将Flink和Hadoop的相关依赖的score属性设置为provided，这些依赖不需要打进jar包里面。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">第二步：生成jar包： mvn clean package -DskipTests</span><br><span class="line">第三步：将 db_flink-1.0-SNAPSHOT-jar-with-dependencies.jar 上传到bigdata04机器上的 &#x2F;data&#x2F;sof</span><br><span class="line">t&#x2F;flink-1.11.1 目录中(上传到哪个目录都可以)</span><br><span class="line"></span><br><span class="line">第四步：提交Flink任务</span><br><span class="line">注意：提交任务之前，先开启socket</span><br><span class="line">[root@bigdata04 ~]# nc -l 9001 </span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink run -m yarn-cluster -c com.imooc.scala.SocketWindowWordCountScala -yjm 1024 -ytm 1024 db_flink-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时到yarn上面可以看到确实新增了一个任务，点击进去可以看到flink的web界面</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090028085.png" alt="image-20230409002811696"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通过socket输入一串内容</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090028348.png" alt="image-20230409002838306"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090029943.png" alt="image-20230409002909775"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090029419.png" alt="image-20230409002934711"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090030195.png" alt="image-20230409003014134"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090030583.png" alt="image-20230409003040435"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090031669.png" alt="image-20230409003059283"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们希望把这个任务停掉，因为这个任务是一个流处理的任务，提交成功之后，它会一直运行。</span><br><span class="line">注意：此时如果我们使用ctrl+c关掉之前提交任务的那个进程，这里的flink任务是不会有任何影响的，可以一直运行，因为flink任务已经提交到hadoop集群里面了。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090032922.png" alt="image-20230409003221865"></p><h4 id="停止任务"><a href="#停止任务" class="headerlink" title="停止任务"></a>停止任务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">此时如果想要停止Flink任务，有两种方式</span><br><span class="line"></span><br><span class="line">1：停止yarn中任务(这种是直接停止yarn中flink集群，任务也就停止了)</span><br><span class="line">[root@bigdata04 flink-1.11.1]# yarn application -kill application_17689629561</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2：停止flink任务</span><br><span class="line">可以在界面上点击这个按钮，或者在命令行中执行flink cancel停止都可以</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090035939.png" alt="image-20230409003528258"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">或者</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;flink cancel -yid application_176896295613 d7bo35cf4co10xxxxxxxxxxx(具体任务ID)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304090039108.png" alt="image-20230409003917743"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">这个flink任务停止之后，对应的那个yarn-session（Flink集群）也就停止了。</span><br><span class="line"></span><br><span class="line">注意：此时flink任务停止之后就无法再查看flink的web界面了，如果想看查看历史任务的执行信息就看不了了，怎么办呢？</span><br><span class="line">咱们之前在学习spark的时候其实也遇到过这种问题，当时是通过启动spark的historyserver进程解决</span><br><span class="line">的。</span><br><span class="line">flink也有historyserver进程，也是可以解决这个问题的。</span><br><span class="line">historyserver进程可以在任意一台机器上启动，在这我们选择在bigdata04机器上启动</span><br><span class="line">在启动historyserver进程之前，需要先修改bigdata04中的flink-conf.yaml配置文件</span><br><span class="line"></span><br><span class="line">[root@bigdata04 flink-1.11.1]# vi conf&#x2F;flink-conf.yaml </span><br><span class="line">......</span><br><span class="line">jobmanager.archive.fs.dir: hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;completed-jobs&#x2F;</span><br><span class="line">historyserver.web.address: 192.168.182.103</span><br><span class="line">historyserver.web.port: 8082</span><br><span class="line">historyserver.archive.fs.dir: hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;completed-jobs&#x2F;</span><br><span class="line">historyserver.archive.fs.refresh-interval: 10000</span><br><span class="line">......</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">然后启动flink的historyserver进程</span><br><span class="line">[root@bigdata04 flink-1.11.1]# bin&#x2F;historyserver.sh start</span><br><span class="line"></span><br><span class="line">验证进程</span><br><span class="line">[root@bigdata04 flink-1.11.1]# jps</span><br><span class="line">5894 HistoryServer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：hadoop集群中的historyserver进程也需要启动</span><br><span class="line">在bigdata01、bigdata02、bigdata03节点上启动hadoop的historyserver进程</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br><span class="line">[root@bigdata02 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br><span class="line">[root@bigdata03 hadoop-3.2.0]# bin&#x2F;mapred --daemon start historyserver</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时Flink任务停止之后也是可以访问flink的web界面的。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十六周 Flink极速上手篇-初识Flink</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%88%9D%E8%AF%86Flink-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%85%AD%E5%91%A8-Flink%E6%9E%81%E9%80%9F%E4%B8%8A%E6%89%8B%E7%AF%87-%E5%88%9D%E8%AF%86Flink-1.html</id>
    <published>2023-04-07T16:52:17.000Z</published>
    <updated>2023-04-08T13:46:48.682Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十六周-Flink极速上手篇-初识Flink"><a href="#第十六周-Flink极速上手篇-初识Flink" class="headerlink" title="第十六周 Flink极速上手篇-初识Flink"></a>第十六周 Flink极速上手篇-初识Flink</h1><h2 id="初识Flink"><a href="#初识Flink" class="headerlink" title="初识Flink"></a>初识Flink</h2><h3 id="什么是Flink"><a href="#什么是Flink" class="headerlink" title="什么是Flink"></a>什么是Flink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Apache Flink 是一个开源的分布式，高性能，高可用，准确的流处理框架。</span><br><span class="line">分布式：表示flink程序可以运行在很多台机器上，</span><br><span class="line">高性能：表示Flink处理性能比较高</span><br><span class="line">高可用：表示flink支持程序的自动重启机制。</span><br><span class="line">准确的：表示flink可以保证处理数据的准确性。</span><br><span class="line">Flink支持流处理和批处理，虽然我们刚才说了flink是一个流处理框架，但是它也支持批处理。</span><br><span class="line">其实对于flink而言，它是一个流处理框架，批处理只是流处理的一个极限特例而已。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080056558.png" alt="image-20230408005641755"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">左边是数据源，从这里面可以看出来，这些数据是实时产生的一些日志，或者是数据库、文件系统、kv存储系统中的数据。</span><br><span class="line">中间是Flink，负责对数据进行处理。</span><br><span class="line">右边是目的地，Flink可以将计算好的数据输出到其它应用中，或者存储系统中。</span><br></pre></td></tr></table></figure><h3 id="Flink架构图"><a href="#Flink架构图" class="headerlink" title="Flink架构图"></a>Flink架构图</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080059947.png" alt="image-20230408005900310"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">首先图片最下面表示是flink的一些部署模式，支持local，和集群(standalone，yarn)，也支持在云上部署。</span><br><span class="line"></span><br><span class="line">往上一层是flink的核心，分布式的流处理引擎。</span><br><span class="line"></span><br><span class="line">再往上面是flink的API和类库</span><br><span class="line">主要有两大块API，DataStream API和DataSet API，分别做流处理和批处理。</span><br><span class="line">针对DataStream API这块，支持复杂事件处理，和table操作，其实也是支持SQL操作的。</span><br><span class="line">针对Dataset API这块，支持flinkML机器学习，Gelly图计算，table操作，这块也是支持SQL操作的。</span><br><span class="line"></span><br><span class="line">其实从这可以看出来，Flink也是有自己的生态圈的，里面包含了实时计算、离线计算、机器学习、图计算、Table和SQL计算等等</span><br><span class="line">所以说它和Spark还是有点像的，不过它们两个的底层计算引擎是有本质区别的，一会我们会详细分析。</span><br></pre></td></tr></table></figure><h3 id="Flink三大核心组件"><a href="#Flink三大核心组件" class="headerlink" title="Flink三大核心组件"></a>Flink三大核心组件</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080113253.png" alt="image-20230408011332076"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink包含三大核心组件</span><br><span class="line">Data Source，数据源(负责接收数据)，</span><br><span class="line">Transformations 算子(负责对数据进行处理)</span><br><span class="line">Data Sink 输出组件(负责把计算好的数据输出到其它存储介质中)</span><br></pre></td></tr></table></figure><h3 id="Flink的流处理与批处理"><a href="#Flink的流处理与批处理" class="headerlink" title="Flink的流处理与批处理"></a>Flink的流处理与批处理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来分析一下Flink这个计算引擎的核心内容</span><br><span class="line"></span><br><span class="line">在大数据处理领域，批处理和流处理一般被认为是两种不同的任务，一个大数据框架一般会被设计为只能处理其中一种任务</span><br><span class="line"></span><br><span class="line">例如Storm只支持流处理任务，而MapReduce、Spark只支持批处理任务。Spark Streaming是Spark之上支持流处理任务的子系统，看似是一个特例，其实并不是——Spark Streaming采用了一种micro-batch的架构，就是把输入的数据流切分成细粒度的batch，并为每一个batch提交一个批处理的Spark任务，所以Spark Streaming本质上执行的还是批处理任务，和Storm这种流式的数据处理方式是完全不同的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Flink通过灵活的执行引擎，能够同时支持批处理和流处理</span><br><span class="line"></span><br><span class="line">在执行引擎这一层，流处理系统与批处理系统最大的不同在于节点之间的数据传输方式。</span><br><span class="line">对于一个流处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理</span><br><span class="line">这就是典型的一条一条处理</span><br><span class="line">而对于一个批处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满的时候，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点</span><br><span class="line">这两种数据传输模式是两个极端，对应的是流处理系统对低延迟的要求和批处理系统对高吞吐量的要求</span><br><span class="line">Flink的执行引擎采用了一种十分灵活的方式，同时支持了这两种数据传输模型</span><br><span class="line">Flink以固定的缓存块为单位进行网络数据传输，用户可以通过缓存块超时值指定缓存块的传输时机。如果缓存块的超时值为0，则Flink的数据传输方式类似前面所说的流处理系统的标准模型，此时系统可以获得最低的处理延迟</span><br><span class="line"></span><br><span class="line">如果缓存块的超时值为无限大，则Flink的数据传输方式类似前面所说的批处理系统的标准模型，此时系统可以获得最高的吞吐量</span><br><span class="line"></span><br><span class="line">这样就比较灵活了，其实底层还是流式计算模型，批处理只是一个极限特例而已。</span><br><span class="line">看一下这个图中显示的三种数据传输模型</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080127620.png" alt="image-20230408012754579"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个：一条一条处理</span><br><span class="line">第二个：一批一批处理</span><br><span class="line">第三个：按照缓存块进行处理，缓存块可以无限小，也可以无限大，这样就可以同时支持流处理和批处理了。</span><br></pre></td></tr></table></figure><h3 id="Storm-vs-SparkStreaming-vs-Flink"><a href="#Storm-vs-SparkStreaming-vs-Flink" class="headerlink" title="Storm vs SparkStreaming vs Flink"></a>Storm vs SparkStreaming vs Flink</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来对比一下目前大数据领域中的三种实时计算引擎</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304080133033.png" alt="image-20230408013308451"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">Native：表示来一条数据处理一条数据</span><br><span class="line">Mirco-Batch：表示划分小批，一小批一小批的处理数据</span><br><span class="line">组合式：表示是基础API，例如实现一个求和操作都需要写代码实现，比较麻烦，代码量会比较多。</span><br><span class="line">声明式：表示提供的是封装后的高阶函数，例如filter、count等函数，可以直接使用，比较方便，代码量比较少。</span><br></pre></td></tr></table></figure><h3 id="实时计算框架如何选择"><a href="#实时计算框架如何选择" class="headerlink" title="实时计算框架如何选择"></a>实时计算框架如何选择</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1：需要关注流数据是否需要进行状态管理</span><br><span class="line">2：消息语义是否有特殊要求At-least-once或者Exectly-once</span><br><span class="line">3：小型独立的项目，需要低延迟的场景，建议使用Storm</span><br><span class="line">4：如果项目中已经使用了Spark，并且秒级别的实时处理可以满足需求，建议使用SparkStreaming</span><br><span class="line">5：要求消息语义为Exectly-once，数据量较大，要求高吞吐低延迟，需要进行状态管理，建议选择Flink</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十四周 消息队列之Kafka从入门到小牛-1</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-1.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%91%A8-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B9%8BKafka%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%B0%8F%E7%89%9B-1.html</id>
    <published>2023-04-07T16:50:41.000Z</published>
    <updated>2023-04-15T17:17:52.503Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十四周-消息队列之Kafka从入门到小牛-1"><a href="#第十四周-消息队列之Kafka从入门到小牛-1" class="headerlink" title="第十四周 消息队列之Kafka从入门到小牛-1"></a>第十四周 消息队列之Kafka从入门到小牛-1</h1><h2 id="初识Kafka"><a href="#初识Kafka" class="headerlink" title="初识Kafka"></a>初识Kafka</h2><h3 id="什么是消息队列"><a href="#什么是消息队列" class="headerlink" title="什么是消息队列"></a>什么是消息队列</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在学习Kafka之前我们先来看一下什么是消息队列</span><br><span class="line">消息队列(Message Queue)：可以简称为MQ</span><br><span class="line">例如：Java中的Queue队列，也可以认为是一个消息队列</span><br><span class="line"></span><br><span class="line">消息队列：顾名思义，消息+队列，其实就是保存消息的队列，属于消息传输过程中的容器。</span><br><span class="line"></span><br><span class="line">消息队列主要提供生产、消费接口供外部调用，做数据的存储和读取</span><br></pre></td></tr></table></figure><h3 id="消息队列分类"><a href="#消息队列分类" class="headerlink" title="消息队列分类"></a>消息队列分类</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">消息队列大致可以分为两种：点对点(P2P)、发布订阅(Pub&#x2F;Sub)</span><br><span class="line">共同点：</span><br><span class="line">针对数据的处理流程是一样的</span><br><span class="line">消息生产者生产消息发送到queue中，然后消息消费者从queue中读取并且消费消息。</span><br><span class="line"></span><br><span class="line">不同点：</span><br><span class="line">点对点(p2p)模型包含：消息队列(Queue)、发送者(Sender)、接收者(Receiver)</span><br><span class="line">一个生产者生产的消息只有一个消费者(Consumer)（消息一旦被消费，就不在消息队列中）消费。</span><br><span class="line">例如QQ中的私聊，我发给你的消息只有你能看到，别人是看不到的</span><br><span class="line"></span><br><span class="line">发布订阅(Pub&#x2F;Sub)模型包含：消息队列(Queue)、主题（Topic）、发布者（Publisher）、订阅者（Subscriber）</span><br><span class="line">每个消息可以有多个消费者，彼此互不影响。比如我发布一个微博：关注我的人都能够看到，或者QQ中的群聊，我在群里面发一条消息，群里面所有人都能看到</span><br><span class="line"></span><br><span class="line">这就是这两种消息队列的区别</span><br><span class="line">我们接下来要学习的Kafka这个消息队列是属于发布订阅模型的</span><br></pre></td></tr></table></figure><h2 id="什么是Kafka"><a href="#什么是Kafka" class="headerlink" title="什么是Kafka"></a>什么是Kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Kafka 是一个高吞吐量的、持久性的、分布式发布订阅消息系统</span><br><span class="line">高吞吐量：可以满足每秒百万级别消息的生产和消费。</span><br><span class="line"></span><br><span class="line">为什么这么快？</span><br><span class="line">难道Kafka的数据是放在内存里面的吗？</span><br><span class="line">不是的，Kafka的数据还是放在磁盘里面的</span><br><span class="line">主要是Kafka利用了磁盘顺序读写速度超过内存随机读写速度这个特性。所以说它的吞吐量才这么高</span><br><span class="line"></span><br><span class="line">持久性：有一套完善的消息存储机制，确保数据高效安全的持久化。</span><br><span class="line">分布式：它是基于分布式的扩展、和容错机制；Kafka的数据都会复制到几台服务器上。当某一台机器故障失效时，生产者和消费者切换使用其它的机器。</span><br></pre></td></tr></table></figure><h3 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Kafka的数据时存储是磁盘中的，为什么可以满足每秒百万级别消息的生产和消费？</span><br><span class="line">这是一个面试题，其实就是我们刚才针对高吞吐量的解释：kafka利用了磁盘顺序读写速度超过内存随机读写速度这个特性</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Kafka主要应用在实时计算领域，可以和Flume、Spark、Flink等框架结合在一块使用</span><br><span class="line">例如：我们使用Flume采集网站产生的日志数据，将数据写入到Kafka中，然后通过Spark或者Flink从Kafka中消费数据进行计算，这其实是一个典型的实时计算案例的架构</span><br></pre></td></tr></table></figure><h2 id="Kafka组件介绍"><a href="#Kafka组件介绍" class="headerlink" title="Kafka组件介绍"></a>Kafka组件介绍</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来分析一下Kafka中的组件，加深对kafka的理解</span><br><span class="line">看这个图</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304151537112.png" alt="image-20230415153737183"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">先看中间的Kafka Cluster</span><br><span class="line">这个Kafka集群内有两个节点，这些节点在这里我们称之为Broker</span><br><span class="line">Broker：消息的代理，Kafka集群中的一个节点称为一个broker</span><br><span class="line"></span><br><span class="line">在Kafka中有Topic的概念</span><br><span class="line">Topic：称为主题，Kafka处理的消息的不同分类(是一个逻辑概念)。</span><br><span class="line">如果把Kafka认为是一个数据库的话，那么Kafka中的Topic就可以认为是一张表</span><br><span class="line">不同的topic中存储不同业务类型的数据，方便使用</span><br><span class="line"></span><br><span class="line">在Topic内部有partition的概念</span><br><span class="line">Partition：是Topic物理上的分组，一个Topic会被分为1个或者多个partition(分区)，分区个数是在创建topic的时候指定。每个topic都是有分区的，至少1个。</span><br><span class="line"></span><br><span class="line">注意：这里面针对partition其实还有副本的概念，主要是为了提供数据的容错性，我们可以在创建Topic的时候指定partition的副本因子是几个。</span><br><span class="line">在这里面副本因子其实就是2了，其中一个是Leader，另一个是真正的副本</span><br><span class="line">Leader中的这个partition负责接收用户的读写请求，副本partition负责从Leader里面的partiton中同步数据，这样的话，如果后期leader对应的节点宕机了，副本可以切换为leader顶上来。</span><br><span class="line"></span><br><span class="line">在partition内部还有一个message的概念</span><br><span class="line">Message：我们称之为消息，代表的就是一条数据，它是通信的基本单位，每个消息都属于一个partition。</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">在这里总结一下</span><br><span class="line">Broker&gt;Topic&gt;Partition&gt;Message</span><br><span class="line"></span><br><span class="line">接下来还有两个组件，看图中的最左边和最右边</span><br><span class="line">Producer：消息和数据的生产者，向Kafka的topic生产数据。</span><br><span class="line">Consumer：消息和数据的消费者，从kafka的topic中消费数据。</span><br><span class="line"></span><br><span class="line">这里的消费者可以有多个，每个消费者可以消费到相同的数据</span><br><span class="line">最后还有一个Zookeeper服务，Kafka的运行是需要依赖于Zookeeper的，Zookeeper负责协调Kafka集群的正常运行。</span><br></pre></td></tr></table></figure><h2 id="Kafka集群安装部署"><a href="#Kafka集群安装部署" class="headerlink" title="Kafka集群安装部署"></a>Kafka集群安装部署</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">前面我们对Kafka有了一个基本的认识，下面我们就想使用一下Kafka</span><br><span class="line"></span><br><span class="line">在使用之前，需要先把Kafka安装部署起来</span><br><span class="line">Kafka是支持单机和集群模式的，建议大家在学习阶段使用单机模式即可，单机和集群在操作上没有任何区别</span><br><span class="line"></span><br><span class="line">注意：由于Kafka需要依赖于Zookeeper，所以在这我们需要先把Zookeeper安装部署起来</span><br></pre></td></tr></table></figure><h3 id="zookeeper安装部署"><a href="#zookeeper安装部署" class="headerlink" title="zookeeper安装部署"></a>zookeeper安装部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">针对Zookeeper前期不需要掌握太多，只需要掌握Zookeeper的安装部署以及它的基本操作即可。</span><br><span class="line">Zookeeper也支持单机和集群安装，建议大家在学习阶段使用单机即可，单机和集群在操作上没有任何区别。</span><br><span class="line">在这里我们会针对单机和集群这两种方式分别演示一下</span><br></pre></td></tr></table></figure><h4 id="zookeeper单机安装"><a href="#zookeeper单机安装" class="headerlink" title="zookeeper单机安装"></a>zookeeper单机安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">zookeeper需要依赖于jdk，只要保证jdk已经正常安装即可。</span><br><span class="line">具体安装步骤如下：</span><br><span class="line">1：下载zookeeper的安装包</span><br><span class="line">进入Zookeeper的官网 </span><br><span class="line"></span><br><span class="line">2：把安装包上传到bigdata01机器的&#x2F;data&#x2F;soft目录下</span><br><span class="line">[root@bigdata01 soft]# ll</span><br><span class="line">-rw-r--r--. 1 root root 9394700 Jun 2 21:33 apache-zookeeper-3.5.8-bin.t</span><br><span class="line">3：解压安装包</span><br><span class="line">[root@bigdata01 soft]# tar -zxvf apache-zookeeper-3.5.8-bin.tar.gz</span><br><span class="line">4：修改配置文件</span><br><span class="line">首先将zoo_sample.cfg重命名为zoo.cfg</span><br><span class="line">然后修改 zoo.cfg中的dataDir参数的值，dataDir指向的目录存储的是zookeeper的核心数据，所以这个目录不能使用tmp目录</span><br><span class="line">[root@bigdata01 soft]# cd apache-zookeeper-3.5.8-bin&#x2F;conf</span><br><span class="line">[root@bigdata01 conf]# mv zoo_sample.cfg zoo.cfg</span><br><span class="line">[root@bigdata01 conf]# vi zoo.cfg </span><br><span class="line">dataDir&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;data</span><br><span class="line"></span><br><span class="line">5：启动zookeeper服务</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">6：验证</span><br><span class="line">如果能看到QuorumPeerMain进程就说明zookeeper启动成功</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# jps</span><br><span class="line">1701 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">注意：如果执行jps命令发现没有QuorumPeerMain进程，则需要到logs目录下去查看zookeeper-*.out这个日志文件</span><br><span class="line">也可以通过zkServer.sh脚本查看当前机器中zookeeper服务的状态</span><br><span class="line">注意：使用zkServer.sh默认会连接本机2181端口的zookeeper服务，默认情况下zookeeper会监听2181端口，这个需要注意一下，因为后面我们在使用zookeeper的时候需要知道它监听的端口是哪个。</span><br><span class="line">最下面显示的Mode信息，表示当前是一个单机独立集群</span><br><span class="line"></span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: standalone</span><br><span class="line"></span><br><span class="line">如果没有启动成功的话则会提示连不上服务not running</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">7：操作zookeeper</span><br><span class="line">首先使用zookeeper的客户端工具连接到zookeeper里面，使用bin目录下面的zkCli.sh脚本，默认会连接本机的zookeeper服务</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkCli.sh</span><br><span class="line">Connecting to localhost:2181</span><br><span class="line">.....</span><br><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:None path:null</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">这样就进入zookeeper的命令行了。</span><br><span class="line">在这里面可以操作Zookeeper中的目录结构</span><br><span class="line">zookeeper中的目录结构和Linux文件系统的目录结构类似</span><br><span class="line">zookeeper里面的每一个目录我们称之为节点(ZNode)</span><br><span class="line">正常情况下我们可以把ZNode认为和文件系统中的目录类似，但是有一点需要注意：ZNode节点本身是可以存储数据的。</span><br><span class="line"></span><br><span class="line">zookeeper中提供了一些命令可以对它进行一些操作</span><br><span class="line">在命令行下随便输入一个字符，按回车就会提示出zookeeper支持的所有命令</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 8] aa</span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">     addauth scheme auth</span><br><span class="line">     close </span><br><span class="line">     config [-c] [-w] [-s]</span><br><span class="line">     connect host:port</span><br><span class="line">     create [-s] [-e] [-c] [-t ttl] path [data] [acl]</span><br><span class="line">     delete [-v version] path</span><br><span class="line">     deleteall path</span><br><span class="line">     delquota [-n|-b] path</span><br><span class="line">     get [-s] [-w] path</span><br><span class="line">     getAcl [-s] path</span><br><span class="line">     history </span><br><span class="line">    listquota path</span><br><span class="line">     ls [-s] [-w] [-R] path</span><br><span class="line">     ls2 path [watch]</span><br><span class="line">     printwatches on|off</span><br><span class="line">     quit </span><br><span class="line">     reconfig [-s] [-v version] [[-file path] | [-members serverID&#x3D;host:po</span><br><span class="line">     redo cmdno</span><br><span class="line">     removewatches path [-c|-d|-a] [-l]</span><br><span class="line">     rmr path</span><br><span class="line">     set [-s] [-v version] path data</span><br><span class="line">     setAcl [-s] [-v version] [-R] path acl</span><br><span class="line">     setquota -n|-b val path</span><br><span class="line">     stat [-w] path</span><br><span class="line">     sync path</span><br><span class="line">Command not found: Command not found aa</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们来具体看一些比较常用的功能：</span><br></pre></td></tr></table></figure><h5 id="查看根节点下面有什么内容-常用"><a href="#查看根节点下面有什么内容-常用" class="headerlink" title="查看根节点下面有什么内容(常用)"></a>查看根节点下面有什么内容(常用)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls &#x2F;</span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure><h5 id="创建节点"><a href="#创建节点" class="headerlink" title="创建节点"></a>创建节点</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在根节点下面创建一个test节点，在test节点上存储数据hello</span><br><span class="line">[zk: localhost:2181(CONNECTED) 9] create &#x2F;test hello</span><br><span class="line">Created &#x2F;test</span><br></pre></td></tr></table></figure><h5 id="查看节点中的信息-常用"><a href="#查看节点中的信息-常用" class="headerlink" title="查看节点中的信息(常用)"></a>查看节点中的信息(常用)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查看&#x2F;test节点中的内容</span><br><span class="line">[zk: localhost:2181(CONNECTED) 10] get &#x2F;test</span><br><span class="line">hello</span><br></pre></td></tr></table></figure><h5 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个删除命令可以递归删除，这里面还有一个delete命令，也可以删除节点，但是只能删除空节点，如果节点下面还有子节点，想一次性全部删除建议使用deleteall(递归删除，适用于这个节点下还有节点；这里用delete也可以)</span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] deleteall &#x2F;test</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">直接按 ctrl+c 就可以退出这个操作界面，想优雅一些的话可以输入quit退出</span><br><span class="line"></span><br><span class="line">或者quit</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">8：停止zookeeper服务</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh stop</span><br></pre></td></tr></table></figure><h4 id="zookeeper集群安装"><a href="#zookeeper集群安装" class="headerlink" title="zookeeper集群安装"></a>zookeeper集群安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1：集群节点规划，使用三个节点搭建一个zookeeper集群</span><br><span class="line">bigdata01</span><br><span class="line">bigdata02</span><br><span class="line">bigdata03</span><br><span class="line"></span><br><span class="line">2：首先在bigdata01节点上配置zookeeper</span><br><span class="line">解压</span><br><span class="line">修改配置(2888:集群节点进行通信时的端口；3888:集群节点进行选举时用的端口)</span><br><span class="line">[root@bigdata01 soft]# cd apache-zookeeper-3.5.8-bin&#x2F;conf&#x2F;</span><br><span class="line">[root@bigdata01 conf]# mv zoo_sample.cfg zoo.cfg </span><br><span class="line">dataDir&#x3D;&#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;data</span><br><span class="line">server.0&#x3D;bigdata01:2888:3888</span><br><span class="line">server.1&#x3D;bigdata02:2888:3888</span><br><span class="line">server.2&#x3D;bigdata03:2888:3888</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">创建目录保存myid文件，并且向myid文件中写入内容</span><br><span class="line">myid中的值其实是和zoo.cfg中server后面指定的编号是一一对应的</span><br><span class="line">编号0对应的是bigdata01这台机器，所以在这里指定0</span><br><span class="line">在这里使用echo和重定向实现数据写入</span><br><span class="line">[root@bigdata01 conf]#cd &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# mkdir data</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# cd data</span><br><span class="line">[root@bigdata01 data]# echo 0 &gt; myid</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3：把修改好配置的zookeeper拷贝到其它两个节点</span><br><span class="line">[root@bigdata01 soft]# scp -rq apache-zookeeper-3.5.8-bin bigdata02:&#x2F;data&#x2F;soft</span><br><span class="line">[root@bigdata01 soft]# scp -rq apache-zookeeper-3.5.8-bin bigdata03:&#x2F;data&#x2F;soft</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">4：修改bigdata02和bigdata03上zookeeper中myid文件的内容</span><br><span class="line">首先修改bigdata02节点上的myid文件</span><br><span class="line">[root@bigdata02 ~]# cd &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;data&#x2F;</span><br><span class="line">[root@bigdata02 data]# echo 1 &gt; myid</span><br><span class="line"></span><br><span class="line">然后修改bigdata03节点上的myid文件</span><br><span class="line">[root@bigdata03 ~]# cd &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;data&#x2F;</span><br><span class="line">[root@bigdata03 data]# echo 2 &gt; myid</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">5：启动zookeeper集群</span><br><span class="line">分别在 bigdata01、bigdata02、bigdata03 上启动zookeeper进程</span><br><span class="line">在bigdata01上启动</span><br><span class="line">[root@bigdata01 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">在bigdata02上启动</span><br><span class="line">[root@bigdata02 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">在bigdata03上启动</span><br><span class="line">[root@bigdata03 apache-zookeeper-3.5.8-bin]# bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;data&#x2F;soft&#x2F;apache-zookeeper-3.5.8-bin&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">6：验证</span><br><span class="line">分别在bigdata01、bigdata02、bigdata03上执行jps命令验证是否有 QuorumPeerMain 进程</span><br><span class="line">如果都有就说明zookeeper集群启动正常了</span><br><span class="line"></span><br><span class="line">如果没有就到对应的节点的logs目录下查看zookeeper*-*.out日志文件</span><br><span class="line">执行bin&#x2F;zkServer.sh status命令会发现有一个节点显示为leader，其他两个节点为follower</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">7：操作zookeeper</span><br><span class="line">和上面单机的操作方式一样</span><br><span class="line">8：停止zookeeper集群</span><br><span class="line">在bigdata01、bigdata02、bigdata03三台机器上分别执行bin&#x2F;zkServer.sh stop命令</span><br></pre></td></tr></table></figure><h3 id="kafka安装部署"><a href="#kafka安装部署" class="headerlink" title="kafka安装部署"></a>kafka安装部署</h3><h4 id="kafka单机安装"><a href="#kafka单机安装" class="headerlink" title="kafka单机安装"></a>kafka单机安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zookeeper集群安装好了以后就可以开始安装kafka了。</span><br><span class="line"></span><br><span class="line">注意：在安装kafka之前需要先确保zookeeper集群是启动状态。</span><br><span class="line"></span><br><span class="line">kafka还需要依赖于基础环境jdk，需要确保jdk已经安装到位。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1：下载kafka安装包</span><br><span class="line"></span><br><span class="line">注意：kafka在启动的时候不需要安装scala环境，只有在编译源码的时候才需要，因为运行的时候是在jvm虚拟机上运行的，只需要有jdk环境就可以了</span><br><span class="line"></span><br><span class="line">2：把kafka安装包上传到bigdata01的&#x2F;data&#x2F;soft目录下</span><br><span class="line">3：解压</span><br><span class="line">4：修改配置文件(config&#x2F;server.properties)</span><br><span class="line">主要参数：</span><br><span class="line">broker.id：集群节点id编号，单机模式不用修改</span><br><span class="line">listeners：默认监听9092端口</span><br><span class="line">log.dirs：注意：这个目录不是存储日志的，是存储Kafka中核心数据的目录，这个目录默认是指zookeeper.connect：kafka依赖的zookeeper</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对单机模式，如果kafka和zookeeper在同一台机器上，并且zookeeper监听的端口就是那个默认的2181端口，则zookeeper.connect这个参数就不需要修改了。</span><br><span class="line">只需要修改一下log.dirs即可 </span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# cd kafka_2.12-2.4.1&#x2F;config&#x2F;</span><br><span class="line">[root@bigdata01 config]# vi server.properties</span><br><span class="line">log.dirs&#x3D;&#x2F;data&#x2F;soft&#x2F;kafka_2.12-2.4.1&#x2F;kafka-logs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5：启动kafka</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line"></span><br><span class="line">-daemon的作用是让进程后台运行</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">6：验证</span><br><span class="line">启动成功之后会产生一个kafka进程</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# jps</span><br><span class="line">2230 QuorumPeerMain</span><br><span class="line">3117 Kafka</span><br><span class="line">3182 Jps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7：停止kafka</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-stop.sh</span><br></pre></td></tr></table></figure><h4 id="kafka集群安装"><a href="#kafka集群安装" class="headerlink" title="kafka集群安装"></a>kafka集群安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：集群节点规划，使用三个节点搭建一个kafka集群</span><br><span class="line">bigdata01</span><br><span class="line">bigdata02</span><br><span class="line">bigdata03</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">注意：针对Kafka集群而言，没有主从之分，所有节点都是一样的。</span><br><span class="line">2：首先在bigdata01节点上配置kafka</span><br><span class="line">解压：</span><br><span class="line">[root@bigdata01 soft]# tar -zxvf kafka_2.12-2.4.1.tgz</span><br><span class="line"></span><br><span class="line">修改配置文件</span><br><span class="line"></span><br><span class="line">注意：此时针对集群模式需要修改broker.id、log.dirs、以及 zookeeper.connect</span><br><span class="line"></span><br><span class="line">broker.id的值默认是从0开始的，集群中所有节点的broker.id从0开始递增即可</span><br><span class="line">所以bigdata01节点的broker.id值为0</span><br><span class="line"></span><br><span class="line">log.dirs的值建议指定到一块存储空间比较大的磁盘上面，因为在实际工作中kafka中会存储很多数据，我这个虚拟机里面就一块磁盘，所以就指定到&#x2F;data目录下面了</span><br><span class="line"></span><br><span class="line">zookeeper.connect的值是zookeeper集群的地址，可以指定集群中的一个节点或者多个节点地址，多个节点地址之间使用逗号隔开即可(避免zookeeper当前这个节点挂了，kafka识别不出来，建议写多个)</span><br><span class="line">[root@bigdata01 soft]# cd kafka_2.12-2.4.1&#x2F;config&#x2F;</span><br><span class="line">broker.id&#x3D;0</span><br><span class="line">log.dirs&#x3D;&#x2F;data&#x2F;kafka-logs</span><br><span class="line">zookeeper.connect&#x3D;bigdata01:2181,bigdata02:2181,bigdata03:2181</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">3：将修改好配置的kafka安装包拷贝到其它两个节点</span><br><span class="line">[root@bigdata01 soft]# scp -rq kafka_2.12-2.4.1 bigdata02:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">[root@bigdata01 soft]# scp -rq kafka_2.12-2.4.1 bigdata03:&#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">4：修改bigdata02和bigdata03上kafka中broker.id的值</span><br><span class="line">首先修改bigdata02节点上的broker.id的值为1</span><br><span class="line">[root@bigdata02 ~]# cd &#x2F;data&#x2F;soft&#x2F;kafka_2.12-2.4.1&#x2F;config&#x2F;</span><br><span class="line">[root@bigdata02 config]# vi server.properties </span><br><span class="line">broker.id&#x3D;1</span><br><span class="line">然后修改bigdata03节点上的broker.id的值为2</span><br><span class="line">[root@bigdata03 ~]# cd &#x2F;data&#x2F;soft&#x2F;kafka_2.12-2.4.1&#x2F;config&#x2F;</span><br><span class="line">[root@bigdata03 config]# vi server.properties </span><br><span class="line">broker.id&#x3D;2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">5：启动集群</span><br><span class="line">分别在bigdata01、bigdata02、bigdata03上启动kafka进程</span><br><span class="line">在bigdata01上启动</span><br><span class="line">[root@bigdata01 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line"></span><br><span class="line">-daemon的作用是让进程后台运行</span><br><span class="line">在bigdata02上启动</span><br><span class="line">[root@bigdata02 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line">在bigdata03上启动</span><br><span class="line">[root@bigdata03 kafka_2.12-2.4.1]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">6：验证</span><br><span class="line">分别在bigdata01、bigdata02、bigdata03上执行jps命令验证是否有kafka进程</span><br><span class="line">如果都有就说明kafka集群启动正常了</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304151651403.png" alt="image-20230415165152051"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十三周 综合项目:电商数据仓库之商品订单数仓2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.html</id>
    <published>2023-04-05T14:30:33.000Z</published>
    <updated>2023-04-06T16:35:47.609Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十三周-综合项目-电商数据仓库之商品订单数仓2"><a href="#第十三周-综合项目-电商数据仓库之商品订单数仓2" class="headerlink" title="第十三周 综合项目:电商数据仓库之商品订单数仓2"></a>第十三周 综合项目:电商数据仓库之商品订单数仓2</h1><h2 id="拉链表"><a href="#拉链表" class="headerlink" title="拉链表"></a>拉链表</h2><h3 id="什么是拉链表"><a href="#什么是拉链表" class="headerlink" title="什么是拉链表"></a>什么是拉链表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对订单表、订单商品表，流水表，这些表中的数据是比较多的，如果使用全量的方式，会造成大量的数据冗余，浪费磁盘空间。</span><br><span class="line">所以这种表，一般使用增量的方式，每日采集新增的数据。</span><br><span class="line"></span><br><span class="line">在这注意一点：针对订单表，如果单纯的按照订单产生时间增量采集数据，是有问题的，因为用户可能今天下单，明天才支付，但是Hive是不支持数据更新的，这样虽然MySQL中订单的状态改变了，但是Hive中订单的状态还是之前的状态。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">想要解决这个问题，一般有这么几种方案：</span><br><span class="line">第一种：每天全量导入订单表的数据，这种方案在项目启动初期是没有多大问题的，因为前期数据量不大，但是随着项目的运营，订单量暴增，假设每天新增1亿订单，之前已经累积了100亿订单，如果每天都是全量导入的话，那也就意味着每天都需要把数据库中的100多亿订单数据导入到HDFS中保存一份，这样会极大的造成数据冗余，太浪费磁盘空间了。</span><br><span class="line">第二种：只保存当天的全量订单表数据，每次在导入之前，删除前一天保存的全量订单数据，这种方式虽然不会造成数据冗余，但是无法查询订单的历史状态，只有当前的最新状态，也不太好。</span><br><span class="line">第三种：拉链表，这种方式在普通增量导入方式的基础之上进行完善，把变化的数据也导入进来，这样既不会造成大量的数据冗余，还可以查询订单的历史状态。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">拉链表是针对数据仓库设计中表存储数据的方式而定义的，顾名思义，所谓拉链，就是记录历史。记录一个事物从开始，一直到当前状态的所有历史变化的信息。</span><br><span class="line"></span><br><span class="line">下面就是一张拉链表，存储的是用户的最基本信息以及每条记录的生命周期。我们可以使用这张表拿到当天的最新数据以及之前的历史数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061515178.png" alt="image-20230406151532551"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">说明：</span><br><span class="line">start_time表示该条记录的生命周期开始时间，end_time 表示该条记录的生命周期结束时间；</span><br><span class="line">end_time &#x3D; &#39;9999-12-31&#39;表示该条记录目前处于有效状态；</span><br><span class="line"></span><br><span class="line">如果查询当前所有有效的记录，则使用 SQL</span><br><span class="line">select * from user where end_time &#x3D;&#39;9999-12-31&#39;</span><br><span class="line"></span><br><span class="line">如果查询2026-01-02的历史快照【获取指定时间内的有效数据】，则使用SQL</span><br><span class="line">select * from user where start_time &lt;&#x3D; &#39;2026-01-02&#39; and end_time &gt;&#x3D; &#39;2026-01-02&#39;</span><br><span class="line"></span><br><span class="line">这就是拉链表。</span><br></pre></td></tr></table></figure><h3 id="如何制作拉链表"><a href="#如何制作拉链表" class="headerlink" title="如何制作拉链表"></a>如何制作拉链表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">那针对我们前面分析的订单表，希望使用拉链表的方式实现数据采集，因为每天都保存全量订单数据比较浪费磁盘空间，但是只采集增量的话无法反应订单的状态变化。</span><br><span class="line">所以需要既采集增量，还要采集订单状态变化了的数据。</span><br><span class="line">针对订单表中的订单状态字段有这么几个阶段</span><br><span class="line">未支付</span><br><span class="line">已支付</span><br><span class="line">未发货</span><br><span class="line">已发货</span><br><span class="line">在这我们先分析两种状态：未支付和已支付。</span><br><span class="line"></span><br><span class="line">我们先举个例子：</span><br><span class="line">假设我们的系统是2026年3月1日开始运营的</span><br><span class="line">那么到3月1日结束订单表所有数据如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061527784.png" alt="image-20230406152722507"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061528829.png" alt="image-20230406152831907"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">基于订单表中的这些数据如何制作拉链表？</span><br><span class="line"></span><br><span class="line">实现思路</span><br><span class="line">1：首先针对3月1号中的订单数据构建初始的拉链表，拉链表中需要有一个start_time(数据生效开始时间)和end_time(数据生效结束时间)，默认情况下start_time等于表中的创建时间，end_time初始化为一个无限大的日期9999-12-31</span><br><span class="line">将3月1号的订单数据导入到拉链表中。</span><br><span class="line">此时拉链表中数据如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061531144.png" alt="image-20230406153103346"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2：在3月2号的时候，需要将订单表中发生了变化的数据和新增的订单数据 整合到之前的拉链表中</span><br><span class="line">此时需要先创建一个每日更新表，将每日新增和变化了的数据保存到里面</span><br><span class="line"></span><br><span class="line">然后基于拉链表和这个每日更新表进行left join，根据订单id进行关联，如果可以关联上，就说明这个订单的状态发生了变化，然后将订单状态发生了变化的数据的end_time改为2026-03-01(当天日期-1天)</span><br><span class="line">然后再和每日更新表中的数据执行union all操作，将结果重新insert到拉链表中</span><br><span class="line"></span><br><span class="line">最终拉链表中的数据如下：</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932%5Cimage-20230406153453757.png" alt="image-20230406153453757"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">因为在3月2号的时候，订单id为001的数据的订单状态发生了变化，所以拉链表中订单id为001的原始数据的end_time需要修改为2026-03-01，</span><br><span class="line">然后需要新增一条订单id为001的数据，订单状态为已支付，start_time为2026-03-02，end_time为9999-12-31。</span><br><span class="line">还需要将3月2号新增的订单id为003的数据也添加进来。</span><br></pre></td></tr></table></figure><h3 id="【实战】基于订单表的拉链表实现"><a href="#【实战】基于订单表的拉链表实现" class="headerlink" title="【实战】基于订单表的拉链表实现"></a>【实战】基于订单表的拉链表实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">下面我们开始实现：</span><br><span class="line">1：首先初始化2026-03-01、2026-03-02和2026-03-03的订单表新增和变化的数据，ods_user_order(直接将数据初始化到HDFS中)，这个表其实就是前面我们所说的每日更新表</span><br><span class="line"></span><br><span class="line">注意：这里模拟使用sqoop从mysql中抽取新增和变化的数据，根据order_date和update_time这两个字段获取这些数据，所以此时ods_user_order中的数据就是每日的新增和变化了的数据。</span><br><span class="line"></span><br><span class="line">执行代码生成数据：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ods_user_order在前面已经使用过，所以在这只需要将2026-03-01、2026-03-02和2026-03-03的数据加载进去即可</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061542631.png" alt="image-20230406154210239"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061543916.png" alt="image-20230406154346557"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061544315.png" alt="image-20230406154427193"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_order <span class="keyword">partition</span>(dt=<span class="string">'20260301'</span>)  <span class="keyword">select</span> </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time</span><br><span class="line"><span class="keyword">from</span> ods_mall.ods_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260301'</span> <span class="keyword">and</span> order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_order <span class="keyword">partition</span>(dt=<span class="string">'20260302'</span>)  <span class="keyword">select</span> </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time</span><br><span class="line"><span class="keyword">from</span> ods_mall.ods_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260302'</span> <span class="keyword">and</span> order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_order <span class="keyword">partition</span>(dt=<span class="string">'20260303'</span>)  <span class="keyword">select</span> </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time</span><br><span class="line"><span class="keyword">from</span> ods_mall.ods_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260303'</span> <span class="keyword">and</span> order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061552760.png" alt="image-20230406155251457"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061553441.png" alt="image-20230406155334317"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061553044.png" alt="image-20230406155350251"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2：创建拉链表，基于每日更新订单表构建拉链表中的数据</span><br><span class="line">创建拉链表：dws_user_order_zip</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_order_zip(</span><br><span class="line">   order_id    <span class="built_in">bigint</span>,</span><br><span class="line">   order_date    <span class="keyword">string</span>,</span><br><span class="line">   user_id    <span class="built_in">bigint</span>,</span><br><span class="line">   order_money    <span class="keyword">double</span>,</span><br><span class="line">   order_type    <span class="built_in">int</span>,</span><br><span class="line">   order_status    <span class="built_in">int</span>,</span><br><span class="line">   pay_id    <span class="built_in">bigint</span>,</span><br><span class="line">   update_time    <span class="keyword">string</span>,</span><br><span class="line">   start_time    <span class="keyword">string</span>,</span><br><span class="line">   end_time    <span class="keyword">string</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_order_zip/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_order_zip</span><br><span class="line"><span class="keyword">select</span> *</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">       duoz.order_id,</span><br><span class="line">       duoz.order_date,</span><br><span class="line">       duoz.user_id,</span><br><span class="line">       duoz.order_money,</span><br><span class="line">       duoz.order_type,</span><br><span class="line">       duoz.order_status,</span><br><span class="line">       duoz.pay_id,</span><br><span class="line">       duoz.update_time,</span><br><span class="line">       duoz.start_time,</span><br><span class="line">       <span class="keyword">case</span></span><br><span class="line">           <span class="keyword">when</span> duoz.end_time = <span class="string">'9999-12-31'</span> <span class="keyword">and</span> duo.order_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">date_add</span>(<span class="string">'2026-03-01'</span>,<span class="number">-1</span>)</span><br><span class="line">           <span class="keyword">else</span> duoz.end_time</span><br><span class="line">       <span class="keyword">end</span> <span class="keyword">as</span> end_time</span><br><span class="line">    <span class="keyword">from</span> dws_mall.dws_user_order_zip <span class="keyword">as</span> duoz</span><br><span class="line">    <span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span> order_id <span class="keyword">from</span> dwd_mall.dwd_user_order</span><br><span class="line">        <span class="keyword">where</span> dt = <span class="string">'20260301'</span></span><br><span class="line">    ) <span class="keyword">as</span> duo</span><br><span class="line">    <span class="keyword">on</span> duoz.order_id = duo.order_id</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line">   <span class="keyword">select</span></span><br><span class="line">       duo.order_id,</span><br><span class="line">       duo.order_date,</span><br><span class="line">       duo.user_id,</span><br><span class="line">       duo.order_money,</span><br><span class="line">       duo.order_type,</span><br><span class="line">       duo.order_status,</span><br><span class="line">       duo.pay_id,</span><br><span class="line">       duo.update_time,</span><br><span class="line">       <span class="string">'2026-03-01'</span> <span class="keyword">as</span> start_time,</span><br><span class="line">       <span class="string">'9999-12-31'</span> <span class="keyword">as</span> end_time</span><br><span class="line">   <span class="keyword">from</span> dwd_mall.dwd_user_order <span class="keyword">as</span> duo</span><br><span class="line">   <span class="keyword">where</span> duo.dt = <span class="string">'20260301'</span></span><br><span class="line">) <span class="keyword">as</span> t;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在windows中编写sql语句，回车时可能会有tab制表符，这样的sql复制到hive中执行会报错；对于复制到hive中连成一行无法解析的情况，可以在复制前给每一行的最后添加一个空格</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%932%5Cimage-20230406163700653.png" alt="image-20230406163700653"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061642146.png" alt="image-20230406164241911"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql语句里的时间改成20260302后再执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061641683.png" alt="image-20230406164129176"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061642440.png" alt="image-20230406164251390"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061645394.png" alt="image-20230406164509353"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql语句里的时间改成20260303后再执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061648636.png" alt="image-20230406164804417"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061649776.png" alt="image-20230406164904719"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061710378.png" alt="image-20230406171011072"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061710744.png" alt="image-20230406171046707"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061711261.png" alt="image-20230406171146204"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查询有效数据</span><br><span class="line">查询切片数据</span><br><span class="line">查询一条订单历史记录</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304061658975.png" alt="image-20230406165823692"></p><h3 id="拉链表的性能问题-面试爱问"><a href="#拉链表的性能问题-面试爱问" class="headerlink" title="拉链表的性能问题(面试爱问)"></a>拉链表的性能问题(面试爱问)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">拉链表也会遇到查询性能的问题，假设我们存放了5年的拉链数据，那么这张表势必会比较大，当查询的时候性能就比较低了</span><br><span class="line">可以用以下思路来解决：</span><br><span class="line">1. 可以尝试对start_time和end_time做索引，这样可以提高一些性能。</span><br><span class="line">2. 保留部分历史数据，我们可以在一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">还有什么是拉链表，如何制作拉链表也爱问</span><br></pre></td></tr></table></figure><h2 id="商品订单数据数仓总结"><a href="#商品订单数据数仓总结" class="headerlink" title="商品订单数据数仓总结"></a>商品订单数据数仓总结</h2><h3 id="数据库和表梳理"><a href="#数据库和表梳理" class="headerlink" title="数据库和表梳理"></a>数据库和表梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062102932.png" alt="image-20230406210206233"></p><h3 id="任务脚本梳理"><a href="#任务脚本梳理" class="headerlink" title="任务脚本梳理"></a>任务脚本梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062103230.png" alt="image-20230406210336400"></p><h2 id="数据可视化和任务调度实现"><a href="#数据可视化和任务调度实现" class="headerlink" title="数据可视化和任务调度实现"></a>数据可视化和任务调度实现</h2><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">数据可视化这块不是项目的重点，不过为了让大家能有一个更加直观的感受，我们可以选择一些现成的数据可视化工具实现。</span><br><span class="line">咱们前面分析过，想要查询hive中的数据可以使用hue，不过hue无法自动生成图表。</span><br><span class="line">所以我们可以考虑使用Zeppelin(还可以操作spark,flink)</span><br><span class="line">针对一些复杂的图表，可以选择定制开发，使用echarts等组件实现</span><br></pre></td></tr></table></figure><h4 id="Zeppelin安装部署"><a href="#Zeppelin安装部署" class="headerlink" title="Zeppelin安装部署"></a>Zeppelin安装部署</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意：不要使用Zeppelin0.8.2版本，这个版本有bug，无法使用图形展现数据。</span><br><span class="line">在这我们使用zeppelin-0.9.0-preview1这个版本</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">把下载好的安装包上传到bigdata04的&#x2F;data&#x2F;soft目录中</span><br><span class="line">1：解压</span><br><span class="line"></span><br><span class="line">2：修改配置</span><br><span class="line">[root@bigdata04 soft]# cd zeppelin-0.9.0-preview1-bin-all&#x2F;conf</span><br><span class="line">[root@bigdata04 conf]# mv zeppelin-env.sh.template zeppelin-env.sh</span><br><span class="line">[root@bigdata04 conf]# mv zeppelin-site.xml.template zeppelin-site.xml</span><br><span class="line">[root@bigdata04 conf]# vi zeppelin-site.xml</span><br><span class="line"># 将默认的127.0.0.1改为0.0.0.0 否则默认情况下只能在本机访问zeppline</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;zeppelin.server.addr&lt;&#x2F;name&gt;</span><br><span class="line"> &lt;value&gt;0.0.0.0&lt;&#x2F;value&gt;</span><br><span class="line"> &lt;description&gt;Server binding address&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3：增加Hive依赖jar包</span><br><span class="line">由于我们需要使用Zepplien连接hive，它里面默认没有集成Hive的依赖jar包，所以最简单的方式就是将Hive的lib目录中的所有jar包全复制到Zeppline中的interpreter&#x2F;jdbc目录中</span><br><span class="line">[root@bigdata04 zeppelin-0.9.0-preview1-bin-all]# cd interpreter&#x2F;jdbc</span><br><span class="line">[root@bigdata04 jdbc]# cp &#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin&#x2F;lib&#x2F;*.jar .</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4：启动</span><br><span class="line">[root@bigdata04 zeppelin-0.9.0-preview1-bin-all]# bin&#x2F;zeppelin-daemon.sh start</span><br><span class="line">5：停止【需要停止的时候传递stop命令即可】</span><br><span class="line">[root@bigdata04 zeppelin-0.9.0-preview1-bin-all]# bin&#x2F;zeppelin-daemon.sh stop</span><br></pre></td></tr></table></figure><h4 id="Zepplin的界面参数配置"><a href="#Zepplin的界面参数配置" class="headerlink" title="Zepplin的界面参数配置"></a>Zepplin的界面参数配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Zepplin启动之后可以通过8080端口进行访问</span><br><span class="line">http:&#x2F;&#x2F;bigdata04:8080&#x2F;</span><br><span class="line"></span><br><span class="line">在使用之前需要先配置hive的基本信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062115317.png" alt="image-20230406211509288"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">搜索jdbc</span><br><span class="line"></span><br><span class="line">点击edit修改里面的参数信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062115096.png" alt="image-20230406211538750"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">修改这四项的内容即可，这里的内容其实就是我们之前学习hive的jdbc操作时指定的参数</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062118908.png" alt="image-20230406211809935"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：需要在192.168.182.103这台机器上启动hiveserver2服务，否则在zeppline中连不上hive</span><br><span class="line">最后点save按钮即可Zepplin的使用</span><br><span class="line">创建一个note，类似于工作台的概念</span><br><span class="line">[root@bigdata04 ~]# cd &#x2F;data&#x2F;soft&#x2F;apache-hive-3.1.2-bin</span><br><span class="line">[root@bigdata04 apache-hive-3.1.2-bin]# bin&#x2F;hiveserver2</span><br></pre></td></tr></table></figure><h4 id="Zepplin的使用"><a href="#Zepplin的使用" class="headerlink" title="Zepplin的使用"></a>Zepplin的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建一个note，类似于工作台的概念</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062119442.png" alt="image-20230406211917306"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时就可以在里面写SQL了。</span><br><span class="line">如果以图形的形式展示结果，则点击这里面对应图形的图标即可。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062121862.png" alt="image-20230406212125211"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062124210.png" alt="image-20230406212432043"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于想制作一些复杂的图表，可以用sqoop导出到mysql，让前端的人去制作。这里当然也很方便直接与hive交互</span><br></pre></td></tr></table></figure><h3 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">针对数据仓库中的任务脚本我们前面已经整理过了，任务脚本还是比较多的，针对初始化表的脚本只需要</span><br><span class="line">执行一次即可，其它的脚本需要每天都执行一次，这个时候就需要涉及到任务定时调度了。</span><br></pre></td></tr></table></figure><h4 id="Crontab调度器的使用"><a href="#Crontab调度器的使用" class="headerlink" title="Crontab调度器的使用"></a>Crontab调度器的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">咱们前面在学习Linux的时候学过一个crontab调度器，通过它可以实现定时执行指定的脚本。</span><br><span class="line">针对我们这个数据仓库中的这些脚本使用crontab进行调度是可以的</span><br><span class="line">但是需要注意一点：这些任务之间是有一些依赖关系的，从大的层面上来说，dwd层的任务需要等ods层的任务执行成功之后才能开始执行</span><br><span class="line"></span><br><span class="line">那crontab如何知道任务之间的依赖关系呢？</span><br><span class="line">crontab是无法知道任务之间的依赖关系的，我们只能间接实现</span><br><span class="line">举个例子：针对MapReduce任务和Spark任务，任务执行成功之后，在输出目录中会有一个success标记文件，这个文件表示这个任务成功的执行结束了。</span><br><span class="line"></span><br><span class="line">此时如果我们使用crontab调度两个job，一个jobA，一个jobB，先执行jobA，jobA成功执行结束之后才能执行jobB，这个时候我们就需要在脚本中添加一个判断，判断jobA的结果输出目录中是否存在success文件，如果存在则继续执行jobB，否则不执行，并且告警，提示jobA任务执行失败。</span><br><span class="line"></span><br><span class="line">那我们现在执行的是hive的sql任务，sql任务最终不会在输出目录中产生success文件，所以没有办法使用这个标记文件进行判断，不过sql任务会产生数据文件，数据文件的文件名是类似000000_0这样的，可能会有多个，具体的个数是由任务中的reduce的个数决定的，我们也可以选择判断这个数据文件是否存在来判断任务是否成功执行结束。</span><br><span class="line"></span><br><span class="line">注意了：针对SQL任务虽然可以通过判断数据文件来判定任务是否执行成功，不过这种方式不能保证100%的准确率，有可能会存在这种情况，任务确实执行成功了，但是在向结果目录中写数据文件的时候，写了一半，由于网络原因导致数据没有写完，但是我们过来判断000000_0这个文件肯定是存在的，那我们就误以为这个任务执行成功了，其实它的数据是缺失一部分的，不过这种情况的概率极低，可以忽略不计。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">此时使用crontab调度两个有依赖关系的任务，脚本该如何实现呢？</span><br><span class="line">在bigdata04机器中创建 &#x2F;data&#x2F;soft&#x2F;warehouse_job 目录</span><br><span class="line">创建脚本： crontab_job_schedule.sh</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行jobA</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 先删除jobA的输出目录</span></span><br><span class="line">hdfs dfs -rm -r hdfs://bigdata01:9000/data/dwd/user_addr/dt=$&#123;dt&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dwd_mall.dwd_user_addr partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   addr_id,</span><br><span class="line">   user_id,</span><br><span class="line">   addr_name,</span><br><span class="line">   order_flag,</span><br><span class="line">   user_name,</span><br><span class="line">   mobile</span><br><span class="line">from ods_mall.ods_user_addr</span><br><span class="line">where dt = '$&#123;dt&#125;' and addr_id is not null;</span><br><span class="line">"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">如果jobA执行成功，这条查询命令就可以成功执行，否则在执行的时候会报错</span></span><br><span class="line">hdfs dfs -ls hdfs://bigdata01:9000/data/dwd/user_addr/dt=$&#123;dt&#125;/000000_0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在这里通过$?来判断上一条命令是否成功执行，如果$?的返回值为0，则表示jobA执行成功，否则表示jobA执行失败</span></span><br><span class="line">if [ $? = 0 ]</span><br><span class="line">then</span><br><span class="line">    echo "执行jobB"</span><br><span class="line">else</span><br><span class="line">    # 可以在这里发短息或者发邮件</span><br><span class="line">    echo "jobA执行失败，请处理..."</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">如果jobA成功执行，则jobB也可以执行，如果jobA执行失败，则jobB不会执行，脚本会生成告警信息。</span><br><span class="line"></span><br><span class="line">这就是使用crontab调度器如何实现任务依赖的功能。</span><br><span class="line">如果项目中的定时任务有很多，使用crontab虽然可以实现任务依赖的功能，但是管理起来不方便，crontab没有提供图形化的界面，使用起来比较麻烦</span><br><span class="line">针对一些简单的定时任务的配置，并且任务比较少的情况下使用crontab是比较方便的。</span><br></pre></td></tr></table></figure><h4 id="Azkaban调度器的使用"><a href="#Azkaban调度器的使用" class="headerlink" title="Azkaban调度器的使用"></a>Azkaban调度器的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于定时执行的脚本很多，可以使用可视化管理界面的任务调度工具进行管理：azkaban(轻量级),Ooize(重量级)</span><br></pre></td></tr></table></figure><h5 id="Azkaban介绍"><a href="#Azkaban介绍" class="headerlink" title="Azkaban介绍"></a>Azkaban介绍</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在实际工作中如果需要配置很多的定时任务，一般会采用一些支持界面操作的调度器，例如：Azkaban、Ooize</span><br><span class="line">Azkaban是一个轻量级的调度器，使用起来比较简单，容易上手。</span><br><span class="line">Ooize是一个重量级的调度器，使用起来相对比较复杂。</span><br><span class="line">在这里我们主要考虑易用性，所以我们选择使用Azkaban。</span><br><span class="line">下面我们来快速了解一下Azkaban，以及它的用法。</span><br><span class="line">Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。</span><br><span class="line">Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">那我们首先把Azkaban安装部署起来</span><br><span class="line">官网下载的Azkaban是源码，需要编译，编译过程会非常慢，主要是由于国外网络原因导致的。</span><br><span class="line">在这我给大家直接提供一个编译好的Azkaban。</span><br><span class="line">为了简化Azakan的安装过程，不让大家在工具的安装上花费太多时间，在这里我们就使用Azkaban的solo模式部署。</span><br><span class="line"></span><br><span class="line">solo模式属于单机模式，那对应的Azkaban也支持在多台机器上运行。</span><br><span class="line">solo模式的优点：</span><br><span class="line">易于安装：无需MySQL示例。它将H2打包为主要的持久存储。</span><br><span class="line">易于启动：Web服务器和执行程序服务器都在同一个进程中运行。</span><br><span class="line">全功能：它包含所有Azkaban功能。</span><br></pre></td></tr></table></figure><h5 id="Azkaban安装部署"><a href="#Azkaban安装部署" class="headerlink" title="Azkaban安装部署"></a>Azkaban安装部署</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">把编译后的Azkaban安装包 azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz 上传到bigdata04的&#x2F;data&#x2F;soft目录下</span><br><span class="line">1：解压</span><br><span class="line"></span><br><span class="line">2：修改Azkaban的时区为上海时区</span><br><span class="line">我们的bigdata04机器在创建的时候已经指定了时区为上海时区</span><br><span class="line">要保证bigdata04机器的时区和Azkaban的时区是一致的。</span><br><span class="line">[root@bigdata04 azkaban-solo-server-0.1.0-SNAPSHOT]# cd conf&#x2F;</span><br><span class="line">[root@bigdata04 conf]# vi azkaban.properties</span><br><span class="line">......</span><br><span class="line">default.timezone.id&#x3D;Asia&#x2F;Shanghai</span><br><span class="line">......</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3：启动</span><br><span class="line">[root@bigdata04 azkaban-solo-server-0.1.0-SNAPSHOT]# bin&#x2F;azkaban-solo-start.sh</span><br><span class="line"></span><br><span class="line">4：停止</span><br><span class="line">[root@bigdata04 azkaban-solo-server-0.1.0-SNAPSHOT]# bin&#x2F;azkaban-solo-stop.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启动成功之后，azkaban会启动一个web界面，监听的端口是8081</span><br><span class="line">http:&#x2F;&#x2F;bigdata04:8081&#x2F;</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062150906.png" alt="image-20230406215038265"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户名和密码默认都是 azkaban</span><br></pre></td></tr></table></figure><h5 id="提交独立任务"><a href="#提交独立任务" class="headerlink" title="提交独立任务"></a>提交独立任务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">如何向Azkaban中提交任务呢？</span><br><span class="line">1：先创建一个Project</span><br><span class="line"></span><br><span class="line">2：向test项目中提交任务</span><br><span class="line">先演示一个独立的任务</span><br><span class="line">创建一个文件hello.job，文件内容如下：</span><br><span class="line"># hello.job</span><br><span class="line">type&#x3D;command</span><br><span class="line">command&#x3D;echo &quot;Hello World!</span><br><span class="line"></span><br><span class="line">这里面的#号开头表示是注释</span><br><span class="line">type：任务类型，这里的command表示这个任务执行的是一个命令</span><br><span class="line">command：这里的command是指具体的命令</span><br><span class="line">将hello.job文件添加到一个zip压缩文件中，hello.zip。</span><br><span class="line"></span><br><span class="line">将hello.zip压缩包提交到刚才创建的test项目中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062155170.png" alt="image-20230406215538918"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时test项目中就包含hello这个任务，点击execute flow执行</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062156281.png" alt="image-20230406215604662"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062156394.png" alt="image-20230406215635048"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062156493.png" alt="image-20230406215648313"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">此时这个任务执行一次就结束了，如果想要让任务定时执行，可以在这配置。</span><br><span class="line">先进入test项目中</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062200802.png" alt="image-20230406220045801"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062201588.png" alt="image-20230406220101356"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">点击Schedule，进入配置定时信息</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062201092.png" alt="image-20230406220133705"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062201869.png" alt="image-20230406220150848"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这就将test项目中的hello.job配置了定时任务。</span><br><span class="line">其实我们会发现这里面的定时任务的配置格式和crontab中的一样。</span><br><span class="line">后期如果想查看这个定时任务的执行情况可以点击hello</span><br><span class="line"></span><br><span class="line">点击executions查看任务的执行列表</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062202682.png" alt="image-20230406220233640"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062202551.png" alt="image-20230406220252870"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果想查看某一次任务的具体执行情况，可以点击execution id</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是一个基本的独立任务的执行，如果是有依赖的多个任务，如何配置呢？</span><br></pre></td></tr></table></figure><h5 id="提交依赖任务"><a href="#提交依赖任务" class="headerlink" title="提交依赖任务"></a>提交依赖任务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">下面再看一个例子：</span><br><span class="line">1：先创建一个project：depen_test</span><br><span class="line"></span><br><span class="line">2：向depen_test项目中提交任务</span><br><span class="line">先创建一个文件first.job</span><br><span class="line"># first.job</span><br><span class="line">type&#x3D;command</span><br><span class="line">command&#x3D;echo &quot;Hello First!&quot;</span><br><span class="line">再创建一个文件second.job</span><br><span class="line"># second.job</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;first</span><br><span class="line">command&#x3D;echo &quot;Hello Second!&quot;</span><br><span class="line"></span><br><span class="line">这里面通过dependencies属性指定了任务的依赖关系，后面的first表示依赖的任务的文件名称</span><br><span class="line"></span><br><span class="line">最后将这两个job文件打成一个zip压缩包</span><br><span class="line">将这个压缩包上传到项目里面</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062206761.png" alt="image-20230406220617754"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">点击Execute Flow，也可以看到任务之间的依赖关系</span><br><span class="line">点击Execute</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062206575.png" alt="image-20230406220640754"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最后查看jobList，可以看到这个项目中的两个任务的执行顺序</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062208908.png" alt="image-20230406220807410"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这就是带依赖的任务的任务调度。</span><br></pre></td></tr></table></figure><h5 id="在数仓中使用Azkaban"><a href="#在数仓中使用Azkaban" class="headerlink" title="在数仓中使用Azkaban"></a>在数仓中使用Azkaban</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">下面我们来看一下针对数仓中的多级任务依赖，如何使用Azkaban实现</span><br><span class="line">以统计电商GMV为例：</span><br><span class="line">这个指标需要依赖于这个流程 MySQL--&gt;HDFS--&gt;ODS--&gt;DWD--&gt;APP</span><br><span class="line">MySQL–&gt;HDFS需要使用Sqoop脚本</span><br><span class="line">HDFS–&gt;ODS需要使用hive alter命令</span><br><span class="line">ODS–&gt;DWD需要使用hive的SQL</span><br><span class="line">DWD–&gt;APP需要使用hive的SQL</span><br><span class="line">接下来开发对应的azkaban job</span><br></pre></td></tr></table></figure><h6 id="collect-job"><a href="#collect-job" class="headerlink" title="collect.job"></a>collect.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># collect.job</span><br><span class="line"># 采集MySQL中的数据至HDFS</span><br><span class="line">type&#x3D;command</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;collect_mysql.sh</span><br></pre></td></tr></table></figure><h6 id="ods-job"><a href="#ods-job" class="headerlink" title="ods.job"></a>ods.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ods.job</span><br><span class="line"># 关联ods层的数据</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;collect</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;ods_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h6 id="dwd-job"><a href="#dwd-job" class="headerlink" title="dwd.job"></a>dwd.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># dwd.job</span><br><span class="line"># 生成dwd层的数据</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;ods</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;dwd_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h6 id="app-job"><a href="#app-job" class="headerlink" title="app.job"></a>app.job</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># app.job</span><br><span class="line"># 生成app层的数据</span><br><span class="line">type&#x3D;command</span><br><span class="line">dependencies&#x3D;dwd</span><br><span class="line">command&#x3D;sh &#x2F;data&#x2F;soft&#x2F;warehouse_job&#x2F;app_mall_add_partition_2.sh</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304062218260.png" alt="image-20230406221808936"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">还要把job文件里涉及的sh脚本上传到linux上指定目录</span><br><span class="line"></span><br><span class="line">然后配置定时任务</span><br></pre></td></tr></table></figure><h2 id="项目优化"><a href="#项目优化" class="headerlink" title="项目优化"></a>项目优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Sqoop数据采集参数调优，它只会生成一个map任务，性能较低</span><br><span class="line"></span><br><span class="line">集群Queue队列优化</span><br><span class="line"></span><br><span class="line">避免任务启动时间过于集中</span><br><span class="line"></span><br><span class="line">Hive on Tez</span><br><span class="line"></span><br><span class="line">Impala提供快速交互查询</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十三周 综合项目:电商数据仓库之商品订单数仓</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%93.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%B8%89%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E5%95%86%E5%93%81%E8%AE%A2%E5%8D%95%E6%95%B0%E4%BB%93.html</id>
    <published>2023-04-05T14:30:13.000Z</published>
    <updated>2023-04-05T17:36:04.999Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十三周-综合项目-电商数据仓库之商品订单数仓"><a href="#第十三周-综合项目-电商数据仓库之商品订单数仓" class="headerlink" title="第十三周 综合项目:电商数据仓库之商品订单数仓"></a>第十三周 综合项目:电商数据仓库之商品订单数仓</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">和之前用户行为数仓构建步骤一样，先对下面两层进行构建，上面两层基于业务需求来构建这两层的表</span><br><span class="line"></span><br><span class="line">服务端数据在mysql中的表如下：(已通过sqoop抽取到hdfs上，只需要创建表将数据关联起来就可以了)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052249257.png" alt="image-20230405224934100"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs上商品订单数据相关目录</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052309451.png" alt="image-20230405230954102"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052310213.png" alt="image-20230405231024155"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052312876.png" alt="image-20230405231205937"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052313424.png" alt="image-20230405231347893"></p><h2 id="ods层"><a href="#ods层" class="headerlink" title="ods层"></a>ods层</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052251060.png" alt="image-20230405225123910"></p><h3 id="建表脚本"><a href="#建表脚本" class="headerlink" title="建表脚本"></a>建表脚本</h3><h4 id="ods-mall-init-table-sh"><a href="#ods-mall-init-table-sh" class="headerlink" title="ods_mall_init_table.sh"></a>ods_mall_init_table.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ods层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists ods_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists ods_mall.ods_user(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   user_gender          tinyint,</span><br><span class="line">   user_birthday        string,</span><br><span class="line">   e_mail               string,</span><br><span class="line">   mobile               string,</span><br><span class="line">   register_time        string,</span><br><span class="line">   is_blacklist         tinyint</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/user/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_user_extend(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   is_pregnant_woman    tinyint,</span><br><span class="line">   is_have_children     tinyint,</span><br><span class="line">   is_have_car          tinyint,</span><br><span class="line">   phone_brand          string,</span><br><span class="line">   phone_cnt            int,</span><br><span class="line">   change_phone_cnt     int,</span><br><span class="line">   weight               int,</span><br><span class="line">   height               int</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/user_extend/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_user_addr(</span><br><span class="line">   addr_id              bigint,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   addr_name            string,</span><br><span class="line">   order_flag           tinyint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   mobile               string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/user_addr/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_goods_info(</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_no             string,</span><br><span class="line">   goods_name           string,</span><br><span class="line">   curr_price           double,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   goods_desc           string,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/goods_info/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_category_code(</span><br><span class="line">   first_category_id    int,</span><br><span class="line">   first_category_name  string,</span><br><span class="line">   second_category_id   int,</span><br><span class="line">   second_catery_name   string,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   third_category_name  string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/category_code/';</span><br><span class="line"></span><br><span class="line">create external table if not exists ods_mall.ods_user_order(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   order_date           string,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   order_money          double,</span><br><span class="line">   order_type           int,</span><br><span class="line">   order_status         int,</span><br><span class="line">   pay_id               bigint,</span><br><span class="line">   update_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/user_order/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_order_item(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_amount         int,</span><br><span class="line">   curr_price           double,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/order_item/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_order_delivery(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   addr_id              bigint,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   carriage_money       double,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/order_delivery/';</span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_payment_flow(</span><br><span class="line">   pay_id               bigint,</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   trade_no             bigint,</span><br><span class="line">   pay_money            double,</span><br><span class="line">   pay_type             int,</span><br><span class="line">   pay_time             string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/ods/payment_flow/';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h4 id="ods-mall-add-partition-sh"><a href="#ods-mall-add-partition-sh" class="headerlink" title="ods_mall_add_partition.sh"></a>ods_mall_add_partition.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 给ods层的表添加分区，这个脚本后期每天执行一次</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨，添加昨天的分区，添加完分区之后，再执行后面的计算脚本</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_user add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_user_extend add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_user_addr add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_goods_info add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_category_code add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_user_order add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_order_item add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_order_delivery add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">alter table ods_mall.ods_payment_flow add <span class="keyword">if</span> not exists partition(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101'</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里用了开发用户行为数仓时用的添加分区通用脚本</span></span><br><span class="line">sh add_partition.sh ods_mall.ods_user $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_user_extend $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_user_addr $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_goods_info $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_category_code $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_user_order $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_order_item $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_order_delivery $&#123;dt&#125; $&#123;dt&#125;</span><br><span class="line">sh add_partition.sh ods_mall.ods_payment_flow $&#123;dt&#125; $&#123;dt&#125;</span><br></pre></td></tr></table></figure><h2 id="dwd层"><a href="#dwd层" class="headerlink" title="dwd层"></a>dwd层</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其实可以直接对ods中对应的表进行抽取字段，但为了保险起见，对各个表中的id字段(是各个表的主键)进行过滤，因为数据采集过程中可能id字段会出现问题</span><br></pre></td></tr></table></figure><h3 id="建表脚本-1"><a href="#建表脚本-1" class="headerlink" title="建表脚本"></a>建表脚本</h3><h4 id="dwd-mall-init-table-sh"><a href="#dwd-mall-init-table-sh" class="headerlink" title="dwd_mall_init_table.sh"></a>dwd_mall_init_table.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dwd层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dwd_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_user(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   user_gender          tinyint,</span><br><span class="line">   user_birthday        string,</span><br><span class="line">   e_mail               string,</span><br><span class="line">   mobile               string,</span><br><span class="line">   register_time        string,</span><br><span class="line">   is_blacklist         tinyint</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/user/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists dwd_mall.dwd_user_extend(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   is_pregnant_woman    tinyint,</span><br><span class="line">   is_have_children     tinyint,</span><br><span class="line">   is_have_car          tinyint,</span><br><span class="line">   phone_brand          string,</span><br><span class="line">   phone_cnt            int,</span><br><span class="line">   change_phone_cnt     int,</span><br><span class="line">   weight               int,</span><br><span class="line">   height               int</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/user_extend/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_user_addr(</span><br><span class="line">   addr_id              bigint,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   addr_name            string,</span><br><span class="line">   order_flag           tinyint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   mobile               string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/user_addr/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists dwd_mall.dwd_goods_info(</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_no             string,</span><br><span class="line">   goods_name           string,</span><br><span class="line">   curr_price           double,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   goods_desc           string,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/goods_info/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists dwd_mall.dwd_category_code(</span><br><span class="line">   first_category_id    int,</span><br><span class="line">   first_category_name  string,</span><br><span class="line">   second_category_id   int,</span><br><span class="line">   second_catery_name   string,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   third_category_name  string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/category_code/';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_user_order(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   order_date           string,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   order_money          double,</span><br><span class="line">   order_type           int,</span><br><span class="line">   order_status         int,</span><br><span class="line">   pay_id               bigint,</span><br><span class="line">   update_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/user_order/';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_order_item(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_amount         int,</span><br><span class="line">   curr_price           double,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/order_item/';</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists dwd_mall.dwd_order_delivery(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   addr_id              bigint,</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   carriage_money       double,</span><br><span class="line">   create_time          string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/order_delivery/';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dwd_mall.dwd_payment_flow(</span><br><span class="line">   pay_id               bigint,</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   trade_no             bigint,</span><br><span class="line">   pay_money            double,</span><br><span class="line">   pay_type             int,</span><br><span class="line">   pay_time             string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited   </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dwd/payment_flow/';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h4 id="dwd-mall-add-partition-sh"><a href="#dwd-mall-add-partition-sh" class="headerlink" title="dwd_mall_add_partition.sh"></a>dwd_mall_add_partition.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 基于ods层的表进行清洗，将清洗之后的数据添加到dwd层对应表的对应分区中</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_user partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   user_id,</span><br><span class="line">   user_name,</span><br><span class="line">   user_gender,</span><br><span class="line">   user_birthday,</span><br><span class="line">   e_mail,</span><br><span class="line">   mobile,</span><br><span class="line">   register_time,</span><br><span class="line">   is_blacklist</span><br><span class="line">from ods_mall.ods_user</span><br><span class="line">where dt = '$&#123;dt&#125;' and user_id is not null;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_user_extend partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   user_id,</span><br><span class="line">   is_pregnant_woman,</span><br><span class="line">   is_have_children,</span><br><span class="line">   is_have_car,</span><br><span class="line">   phone_brand,</span><br><span class="line">   phone_cnt,</span><br><span class="line">   change_phone_cnt,</span><br><span class="line">   weight,</span><br><span class="line">   height</span><br><span class="line">from ods_mall.ods_user_extend</span><br><span class="line">where dt = '$&#123;dt&#125;' and user_id is not null;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_user_addr partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   addr_id,</span><br><span class="line">   user_id,</span><br><span class="line">   addr_name,</span><br><span class="line">   order_flag,</span><br><span class="line">   user_name,</span><br><span class="line">   mobile</span><br><span class="line">from ods_mall.ods_user_addr</span><br><span class="line">where dt = '$&#123;dt&#125;' and addr_id is not null;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_goods_info partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   goods_id,</span><br><span class="line">   goods_no,</span><br><span class="line">   goods_name,</span><br><span class="line">   curr_price,</span><br><span class="line">   third_category_id,</span><br><span class="line">   goods_desc,</span><br><span class="line">   create_time</span><br><span class="line">from ods_mall.ods_goods_info</span><br><span class="line">where dt = '$&#123;dt&#125;' and goods_id is not null;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_category_code partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   first_category_id,</span><br><span class="line">   first_category_name,</span><br><span class="line">   second_category_id,</span><br><span class="line">   second_catery_name,</span><br><span class="line">   third_category_id,</span><br><span class="line">   third_category_name</span><br><span class="line">from ods_mall.ods_category_code</span><br><span class="line">where dt = '$&#123;dt&#125;' and first_category_id is not null;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">insert overwrite table dwd_mall.dwd_user_order partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   order_id,</span><br><span class="line">   order_date,</span><br><span class="line">   user_id,</span><br><span class="line">   order_money,</span><br><span class="line">   order_type,</span><br><span class="line">   order_status,</span><br><span class="line">   pay_id,</span><br><span class="line">   update_time </span><br><span class="line">from ods_mall.ods_user_order</span><br><span class="line">where dt = '$&#123;dt&#125;' and order_id is not null;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_order_item partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   order_id,</span><br><span class="line">   goods_id,</span><br><span class="line">   goods_amount,</span><br><span class="line">   curr_price,</span><br><span class="line">   create_time</span><br><span class="line">from ods_mall.ods_order_item</span><br><span class="line">where dt = '$&#123;dt&#125;' and order_id is not null;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite t able dwd_mall.dwd_order_delivery partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   order_id,</span><br><span class="line">   addr_id,</span><br><span class="line">   user_id,</span><br><span class="line">   carriage_money,</span><br><span class="line">   create_time</span><br><span class="line">from ods_mall.ods_order_delivery</span><br><span class="line">where dt = '$&#123;dt&#125;' and order_id is not null;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_payment_flow partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   pay_id,</span><br><span class="line">   order_id,</span><br><span class="line">   trade_no,</span><br><span class="line">   pay_money,</span><br><span class="line">   pay_type,</span><br><span class="line">   pay_time</span><br><span class="line">from ods_mall.ods_payment_flow</span><br><span class="line">where dt = '$&#123;dt&#125;' and order_id is not null;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052333331.png" alt="image-20230405233300223"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052335705.png" alt="image-20230405233549286"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052336364.png" alt="image-20230405233613163"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052337003.png" alt="image-20230405233743838"></p><h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">需求一：用户信息宽表(后期查询用户信息时只需在一张表里查询，不需要关联多张表)</span><br><span class="line">需求二：电商GMV(电商领域很常见，表示一段时间内的交易总金额)</span><br><span class="line">需求三：商品相关指标(商品销售情况，比如哪些受用户喜欢)</span><br><span class="line">需求四：用户行为漏斗分析</span><br><span class="line"></span><br><span class="line">为了保证大家在练习的时候计算的需求结果和我的保持一致，所以针对后面的测试数据就不再随机生成了，而是生成固定的数据，一共1个月的数据</span><br><span class="line">从2026-02-01到2026-02-28的数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052344522.png" alt="image-20230405234430452"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052346838.png" alt="image-20230405234656212"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052349025.png" alt="image-20230405234927830"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">执行上面的代码，此时数据就会被上传到HDFS上面。</span><br><span class="line">然后执行ods层和dwd层的脚本，重新加载计算2026-02月份的数据。</span><br><span class="line">1：执行ods层的脚本</span><br><span class="line">写一个临时脚本，在脚本中写一个for循环，循环加载数据</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 加载ods层的数据</span></span><br><span class="line"></span><br><span class="line">for((i=1;i&lt;=28;i++))</span><br><span class="line">do</span><br><span class="line">if [ $i -lt 10 ]</span><br><span class="line">then</span><br><span class="line">dt="2026020"$i</span><br><span class="line">else</span><br><span class="line">dt="202602"$i </span><br><span class="line">fi</span><br><span class="line">echo "ods_mall_add_partition.sh" $&#123;dt&#125;</span><br><span class="line">sh ods_mall_add_partition.sh $&#123;dt&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 计算dwd层的数据</span></span><br><span class="line"></span><br><span class="line">for((i=1;i&lt;=28;i++))</span><br><span class="line">do</span><br><span class="line">if [ $i -lt 10 ]</span><br><span class="line">then</span><br><span class="line">dt="2026020"$i</span><br><span class="line">else</span><br><span class="line">dt="202602"$i</span><br><span class="line">fi</span><br><span class="line">echo "dwd_mall_add_partition.sh" $&#123;dt&#125;</span><br><span class="line">sh  dwd_mall_add_partition.sh $&#123;dt&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">等脚本执行完毕之后，验证一下结果，随便找一个日期的数据验证即可，能查到数据就说明是OK的。最好是再查看一下 partition的信息。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052358185.png" alt="image-20230405235827525"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052353292.png" alt="image-20230405235349443"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052354916.png" alt="image-20230405235406590"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052355506.png" alt="image-20230405235510464"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052356274.png" alt="image-20230405235640110"></p><h3 id="需求一：用户信息宽表"><a href="#需求一：用户信息宽表" class="headerlink" title="需求一：用户信息宽表"></a>需求一：用户信息宽表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">宽表主要是便于使用，在使用的时候不至于每次都要关联很多张表</span><br><span class="line">用户信息宽表包括服务端中的user表、user_extend表</span><br><span class="line">如果有需求的话其实还可以把用户的一些其它维度的数据关联过来，例如：当日的下单数量、消费金额等等指标，</span><br><span class="line">实现思路如下：</span><br><span class="line">对dwd_user表和dwd_user_extend表执行left join操作，通过user_id进行关联即可，将结果数据保存到 dws_user_info_all表中</span><br></pre></td></tr></table></figure><h4 id="dws层"><a href="#dws层" class="headerlink" title="dws层"></a>dws层</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_info_all(</span><br><span class="line">   user_id              <span class="built_in">bigint</span>,</span><br><span class="line">   user_name            <span class="keyword">string</span>,</span><br><span class="line">   user_gender          <span class="built_in">tinyint</span>,</span><br><span class="line">   user_birthday        <span class="keyword">string</span>,</span><br><span class="line">   e_mail               <span class="keyword">string</span>,</span><br><span class="line">   mobile               <span class="keyword">string</span>,</span><br><span class="line">   register_time        <span class="keyword">string</span>,</span><br><span class="line">   is_blacklist         <span class="built_in">tinyint</span>,</span><br><span class="line">   is_pregnant_woman    <span class="built_in">tinyint</span>,</span><br><span class="line">   is_have_children     <span class="built_in">tinyint</span>,</span><br><span class="line">   is_have_car          <span class="built_in">tinyint</span>,</span><br><span class="line">   phone_brand          <span class="keyword">string</span>,</span><br><span class="line">   phone_cnt            <span class="built_in">int</span>,</span><br><span class="line">   change_phone_cnt     <span class="built_in">int</span>,</span><br><span class="line">   weight               <span class="built_in">int</span>,</span><br><span class="line">   height               <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_info_all/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_info_all <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>)  <span class="keyword">select</span> </span><br><span class="line">   du.user_id,</span><br><span class="line">   du.user_name,</span><br><span class="line">   du.user_gender,</span><br><span class="line">   du.user_birthday,</span><br><span class="line">   du.e_mail,</span><br><span class="line">   du.mobile,</span><br><span class="line">   du.register_time,</span><br><span class="line">   du.is_blacklist,</span><br><span class="line">   due.is_pregnant_woman,</span><br><span class="line">   due.is_have_children,</span><br><span class="line">   due.is_have_car,</span><br><span class="line">   due.phone_brand,</span><br><span class="line">   due.phone_cnt,</span><br><span class="line">   due.change_phone_cnt,</span><br><span class="line">   due.weight,</span><br><span class="line">   due.height</span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user <span class="keyword">as</span> du</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dwd_mall.dwd_user_extend <span class="keyword">as</span> due</span><br><span class="line"><span class="keyword">on</span> du.user_id = due.user_id</span><br><span class="line"><span class="keyword">where</span> du.dt = <span class="string">'20260201'</span> <span class="keyword">and</span> due.dt = <span class="string">'20260201'</span>;</span><br></pre></td></tr></table></figure><h5 id="建表脚本-2"><a href="#建表脚本-2" class="headerlink" title="建表脚本"></a>建表脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_1.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_1.sh</span><br></pre></td></tr></table></figure><h5 id="dws-mall-init-table-1-sh"><a href="#dws-mall-init-table-1-sh" class="headerlink" title="dws_mall_init_table_1.sh"></a>dws_mall_init_table_1.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：用户信息宽表</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_info_all(</span><br><span class="line">   user_id              bigint,</span><br><span class="line">   user_name            string,</span><br><span class="line">   user_gender          tinyint,</span><br><span class="line">   user_birthday        string,</span><br><span class="line">   e_mail               string,</span><br><span class="line">   mobile               string,</span><br><span class="line">   register_time        string,</span><br><span class="line">   is_blacklist         tinyint,</span><br><span class="line">   is_pregnant_woman    tinyint,</span><br><span class="line">   is_have_children     tinyint,</span><br><span class="line">   is_have_car          tinyint,</span><br><span class="line">   phone_brand          string,</span><br><span class="line">   phone_cnt            int,</span><br><span class="line">   change_phone_cnt     int,</span><br><span class="line">   weight               int,</span><br><span class="line">   height               int</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_info_all/';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="dws-mall-add-partition-1-sh"><a href="#dws-mall-add-partition-1-sh" class="headerlink" title="dws_mall_add_partition_1.sh"></a>dws_mall_add_partition_1.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：用户信息宽表</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_user_info_all partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   du.user_id,</span><br><span class="line">   du.user_name,</span><br><span class="line">   du.user_gender,</span><br><span class="line">   du.user_birthday,</span><br><span class="line">   du.e_mail,</span><br><span class="line">   du.mobile,</span><br><span class="line">   du.register_time,</span><br><span class="line">   du.is_blacklist,</span><br><span class="line">   due.is_pregnant_woman,</span><br><span class="line">   due.is_have_children,</span><br><span class="line">   due.is_have_car,</span><br><span class="line">   due.phone_brand,</span><br><span class="line">   due.phone_cnt,</span><br><span class="line">   due.change_phone_cnt,</span><br><span class="line">   due.weight,</span><br><span class="line">   due.height</span><br><span class="line">from dwd_mall.dwd_user as du</span><br><span class="line">left join dwd_mall.dwd_user_extend as due</span><br><span class="line">on du.user_id = due.user_id</span><br><span class="line">where du.dt = '$&#123;dt&#125;' and due.dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060014790.png" alt="image-20230406001420794"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060014322.png" alt="image-20230406001456982"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060015364.png" alt="image-20230406001541136"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问题：如何将服务端的数据和客户端的数据通过用户维度关联起来？</span><br><span class="line">之前统计的客户端数据(用户行为数据)针对用户相关的指标都是使用的xaid(因为用户在使用app时，不一定登陆了，所以没有用user_id)。</span><br><span class="line">但是在服务端数据中用户信息只有user_id。</span><br><span class="line">这两份数据如果先要关联起来，还需要在用户行为数仓中提取一个表，表里面只需要有两列：</span><br><span class="line">user_id和xaid。这样就可以把客户端数据和服务端数据中的用户关联起来了。</span><br></pre></td></tr></table></figure><h3 id="需求二：电商-GMV"><a href="#需求二：电商-GMV" class="headerlink" title="需求二：电商 GMV"></a>需求二：电商 GMV</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GMV：Gross Merchandise Volume，是指一定时间段内的成交总金额</span><br><span class="line">GMV多用于电商行业，这个实际指的是拍下的订单总金额，包含付款和未付款的部分。</span><br><span class="line">我们在统计的时候就可以将订单表中的每天的所有订单金额全部都累加起来就可以获取到当天的GMV了</span><br><span class="line"></span><br><span class="line">实现思路：</span><br><span class="line">对dwd_user_order表中的数据进行统计即可，通过order_money字段可以计算出来GMV，将结果数据保存到表 app_gmv中</span><br></pre></td></tr></table></figure><h4 id="app层"><a href="#app层" class="headerlink" title="app层"></a>app层</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：gmv 字段的类型可以使用double或者decimal(10,2)都可以</span><br><span class="line">decimal(10,2)，可以更方便的控制小数位数，数据看起来更加清晰，所以建议使用decimal(10,2)。</span><br><span class="line">其实针对金钱相关的字段类型建议使用decimal(10,2)，使用double也不错，也是可以的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists app_mall.app_gmv(</span><br><span class="line">    gmv   decimal(10,2) </span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;app&#x2F;gmv&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_gmv partition(dt&#x3D;&#39;20260201&#39;)  select </span><br><span class="line">sum(order_money) as gmv</span><br><span class="line">from dwd_mall.dwd_user_order</span><br><span class="line">where dt &#x3D; &#39;20260201&#39;;</span><br></pre></td></tr></table></figure><h5 id="建表脚本-3"><a href="#建表脚本-3" class="headerlink" title="建表脚本"></a>建表脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_2.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_2.sh</span><br></pre></td></tr></table></figure><h5 id="app-mall-init-table-2-sh"><a href="#app-mall-init-table-2-sh" class="headerlink" title="app_mall_init_table_2.sh"></a>app_mall_init_table_2.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求二：电商GMV</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_gmv(</span><br><span class="line">    gmv   decimal(10,2) </span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/gmv/';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-mall-add-partition-2-sh"><a href="#app-mall-add-partition-2-sh" class="headerlink" title="app_mall_add_partition_2.sh"></a>app_mall_add_partition_2.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求二：电商GMV</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_gmv partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">sum(order_money) as gmv</span><br><span class="line">from dwd_mall.dwd_user_order</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">问题：有了每天的GMV，后期能不能计算月度GMV，季度GMV，以及年度GMV？以及GMV的同比，环比？</span><br><span class="line">可以的，有了每日的GMV之后，按照月、季度、年对GMV进行聚合即可。</span><br><span class="line">同比环比，我们在前面已经实现过类似的需求。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060037068.png" alt="image-20230406003729623"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060037204.png" alt="image-20230406003754317"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060038053.png" alt="image-20230406003824047"></p><h3 id="需求三：商品相关指标"><a href="#需求三：商品相关指标" class="headerlink" title="需求三：商品相关指标"></a>需求三：商品相关指标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">针对这个需求我们主要统计两个指标</span><br><span class="line">1：商品销售情况（商品名称，一级类目，订单总量，销售额）</span><br><span class="line">2：商品品类偏好Top10（商品一级类目，订单总量）</span><br><span class="line"></span><br><span class="line">首先看第一个指标：商品销售情况</span><br><span class="line">这个指标主要统计商品名称，一级类目，订单总量，销售额这些字段信息</span><br><span class="line">订单中的详细信息是在dwd_order_item表中，需要关联dwd_goods_info和</span><br><span class="line">dwd_category_code获取商品名称和商品一级类目信息</span><br><span class="line"></span><br><span class="line">在这最好是基于这些表先构建一个商品订单信息的宽表dws_order_goods_all_info，便于后期</span><br><span class="line">其它需求复用。</span><br><span class="line">然后基于这个宽表统计出来这个指标需要的信息，保存到表app_goods_sales_item</span><br><span class="line"></span><br><span class="line">接着看第二个指标：商品品类偏好Top10</span><br><span class="line">这个指标可以在第一个指标的基础之上，根据一级类目进行分组，按照类目下的订单总量排序，取Top10，保存到表 app_category_top10中</span><br></pre></td></tr></table></figure><h4 id="dws层-1"><a href="#dws层-1" class="headerlink" title="dws层"></a>dws层</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists dws_mall.dws_order_goods_all_info(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_amount         int,</span><br><span class="line">   order_curr_price           double,</span><br><span class="line">   order_create_time          string,</span><br><span class="line">   goods_no             string,</span><br><span class="line">   goods_name           string,</span><br><span class="line">   goods_curr_price           double,</span><br><span class="line">   goods_desc           string,</span><br><span class="line">   goods_create_time          string,</span><br><span class="line">   first_category_id    int,</span><br><span class="line">   first_category_name  string,</span><br><span class="line">   second_category_id   int,</span><br><span class="line">   second_catery_name   string,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   third_category_name  string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/order_goods_all_info/';</span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_order_goods_all_info partition(dt='20260201')  select </span><br><span class="line">   doi.order_id,</span><br><span class="line">   doi.goods_id,</span><br><span class="line">   doi.goods_amount,</span><br><span class="line">   doi.curr_price as order_curr_price,</span><br><span class="line">   doi.create_time as order_create_time,</span><br><span class="line">   dgi.goods_no,</span><br><span class="line">   dgi.goods_name,</span><br><span class="line">   dgi.curr_price as goods_curr_price,</span><br><span class="line">   dgi.goods_desc,</span><br><span class="line">   dgi.create_time as goods_create_time,</span><br><span class="line">   dcc.first_category_id,</span><br><span class="line">   dcc.first_category_name,</span><br><span class="line">   dcc.second_category_id,</span><br><span class="line">   dcc.second_catery_name,</span><br><span class="line">   dcc.third_category_id,</span><br><span class="line">   dcc.third_category_name</span><br><span class="line">from dwd_mall.dwd_order_item as doi</span><br><span class="line">left join dwd_mall.dwd_goods_info as dgi</span><br><span class="line">on doi.goods_id = dgi.goods_id</span><br><span class="line">left join dwd_mall.dwd_category_code as dcc</span><br><span class="line">on dgi.third_category_id = dcc.third_category_id</span><br><span class="line">where doi.dt = '20260201' and dgi.dt = '20260201' and dcc.dt = '20260201';</span><br></pre></td></tr></table></figure><h5 id="开发脚本"><a href="#开发脚本" class="headerlink" title="开发脚本"></a>开发脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_3.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_3.sh</span><br></pre></td></tr></table></figure><h6 id="dws-mall-init-table-3-sh"><a href="#dws-mall-init-table-3-sh" class="headerlink" title="dws_mall_init_table_3.sh"></a>dws_mall_init_table_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：商品相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_order_goods_all_info(</span><br><span class="line">   order_id             bigint,</span><br><span class="line">   goods_id             bigint,</span><br><span class="line">   goods_amount         int,</span><br><span class="line">   order_curr_price           double,</span><br><span class="line">   order_create_time          string,</span><br><span class="line">   goods_no             string,</span><br><span class="line">   goods_name           string,</span><br><span class="line">   goods_curr_price           double,</span><br><span class="line">   goods_desc           string,</span><br><span class="line">   goods_create_time          string,</span><br><span class="line">   first_category_id    int,</span><br><span class="line">   first_category_name  string,</span><br><span class="line">   second_category_id   int,</span><br><span class="line">   second_catery_name   string,</span><br><span class="line">   third_category_id    int,</span><br><span class="line">   third_category_name  string</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/order_goods_all_info/';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-3-sh"><a href="#dws-mall-add-partition-3-sh" class="headerlink" title="dws_mall_add_partition_3.sh"></a>dws_mall_add_partition_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：商品相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_order_goods_all_info partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">   doi.order_id,</span><br><span class="line">   doi.goods_id,</span><br><span class="line">   doi.goods_amount,</span><br><span class="line">   doi.curr_price as order_curr_price,</span><br><span class="line">   doi.create_time as order_create_time,</span><br><span class="line">   dgi.goods_no,</span><br><span class="line">   dgi.goods_name,</span><br><span class="line">   dgi.curr_price as goods_curr_price,</span><br><span class="line">   dgi.goods_desc,</span><br><span class="line">   dgi.create_time as goods_create_time,</span><br><span class="line">   dcc.first_category_id,</span><br><span class="line">   dcc.first_category_name,</span><br><span class="line">   dcc.second_category_id,</span><br><span class="line">   dcc.second_catery_name,</span><br><span class="line">   dcc.third_category_id,</span><br><span class="line">   dcc.third_category_name</span><br><span class="line">from dwd_mall.dwd_order_item as doi</span><br><span class="line">left join dwd_mall.dwd_goods_info as dgi</span><br><span class="line">on doi.goods_id = dgi.goods_id</span><br><span class="line">left join dwd_mall.dwd_category_code as dcc</span><br><span class="line">on dgi.third_category_id = dcc.third_category_id</span><br><span class="line">where doi.dt = '$&#123;dt&#125;' and dgi.dt = '$&#123;dt&#125;' and dcc.dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h4 id="app层-1"><a href="#app层-1" class="headerlink" title="app层"></a>app层</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_goods_sales_item(</span><br><span class="line">    goods_name    <span class="keyword">string</span>,</span><br><span class="line">first_category_name     <span class="keyword">string</span>,</span><br><span class="line">order_total    <span class="built_in">bigint</span>,</span><br><span class="line">price_total    <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/goods_sales_item/'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_goods_sales_item <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>)  <span class="keyword">select</span> </span><br><span class="line">goods_name,</span><br><span class="line">first_category_name,</span><br><span class="line"><span class="keyword">count</span>(order_id) <span class="keyword">as</span> order_total,</span><br><span class="line"><span class="keyword">sum</span>(goods_amount * order_curr_price) <span class="keyword">as</span> price_total</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_order_goods_all_info</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> goods_name,first_category_name;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_category_top10(</span><br><span class="line">first_category_name     <span class="keyword">string</span>,</span><br><span class="line">order_total    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/category_top10/'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_category_top10 <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>)  <span class="keyword">select</span> </span><br><span class="line">first_category_name,</span><br><span class="line"><span class="keyword">sum</span>(order_total) <span class="keyword">as</span> order_total</span><br><span class="line"><span class="keyword">from</span> app_mall.app_goods_sales_item </span><br><span class="line"><span class="keyword">where</span> dt =<span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> first_category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> order_total <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><h5 id="开发脚本-1"><a href="#开发脚本-1" class="headerlink" title="开发脚本"></a>开发脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_3.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_3.sh</span><br></pre></td></tr></table></figure><h5 id="app-mall-init-table-3-sh"><a href="#app-mall-init-table-3-sh" class="headerlink" title="app_mall_init_table_3.sh"></a>app_mall_init_table_3.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：商品相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_goods_sales_item(</span><br><span class="line">    goods_name    string,</span><br><span class="line">first_category_name     string,</span><br><span class="line">order_total    bigint,</span><br><span class="line">price_total    decimal(10,2)</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/goods_sales_item/';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_category_top10(</span><br><span class="line">first_category_name     string,</span><br><span class="line">order_total    bigint</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/category_top10/';</span><br><span class="line"> </span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-mall-add-partition-3-sh"><a href="#app-mall-add-partition-3-sh" class="headerlink" title="app_mall_add_partition_3.sh"></a>app_mall_add_partition_3.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：商品相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_goods_sales_item partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">goods_name,</span><br><span class="line">first_category_name,</span><br><span class="line">count(order_id) as order_total,</span><br><span class="line">sum(goods_amount * order_curr_price) as price_total</span><br><span class="line">from dws_mall.dws_order_goods_all_info</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by goods_name,first_category_name;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_category_top10 partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">first_category_name,</span><br><span class="line">sum(order_total) as order_total</span><br><span class="line">from app_mall.app_goods_sales_item </span><br><span class="line">where dt ='$&#123;dt&#125;'</span><br><span class="line">group by first_category_name</span><br><span class="line">order by order_total desc</span><br><span class="line">limit 10;</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060107837.png" alt="image-20230406010738922"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060107081.png" alt="image-20230406010754961"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060108980.png" alt="image-20230406010814759"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060108252.png" alt="image-20230406010840827"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060109152.png" alt="image-20230406010927249"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060116924.png" alt="image-20230406011604503"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">问题：如果想要统计商品品类内最受用户喜欢的Top10商品，如何统计？</span><br><span class="line">基于dws_order_goods_all_info表中的数据，根据商品品类进行分区，根据商品的订单总量</span><br><span class="line">进行排序，获取每个品类中用户喜欢的Top10商品</span><br></pre></td></tr></table></figure><h3 id="需求四：漏斗分析"><a href="#需求四：漏斗分析" class="headerlink" title="需求四：漏斗分析"></a>需求四：漏斗分析</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060119866.png" alt="image-20230406011942433"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">其实就是根据用户的行为一层一层分析用户的转化率。</span><br><span class="line">活跃--&gt;商品详情页--&gt;下单--&gt;支付</span><br><span class="line">每一个指标对应的表：</span><br><span class="line">活跃：dws_user_active_history</span><br><span class="line">商品详情页：dwd_good_item</span><br><span class="line">下单：dwd_user_order</span><br><span class="line">支付：dwd_user_order</span><br><span class="line">实现思路：</span><br><span class="line">首先统计当天活跃用户数量</span><br><span class="line">接着统计当天进入了多少个商品详情页</span><br><span class="line">接着统计当天下单的数量</span><br><span class="line">最后统计当天支付的数量</span><br><span class="line">并且计算每一层的转化率。</span><br><span class="line">最终把结果数据保存到表 app_user_conver_funnel中</span><br></pre></td></tr></table></figure><h4 id="app-层"><a href="#app-层" class="headerlink" title="app 层"></a>app 层</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_conver_funnel(</span><br><span class="line">    active_num    <span class="built_in">int</span>,</span><br><span class="line">    item_num     <span class="built_in">int</span>,</span><br><span class="line">    order_num    <span class="built_in">int</span>,</span><br><span class="line">    pay_num    <span class="built_in">int</span>,</span><br><span class="line">    active_to_item_ratio    <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">    item_to_order_ratio    <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>),</span><br><span class="line">    order_to_pay_ratio    <span class="built_in">decimal</span>(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_conver_funnel/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_conver_funnel <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>)  <span class="keyword">select</span> </span><br><span class="line">duah.active_num,</span><br><span class="line">dgi.item_num,</span><br><span class="line">duo.order_num,</span><br><span class="line">duo.pay_num,</span><br><span class="line">dgi.item_num/duah.active_num <span class="keyword">as</span> active_to_item_ratio,</span><br><span class="line">duo.order_num/dgi.item_num <span class="keyword">as</span> item_to_order_ratio,</span><br><span class="line">duo.pay_num/duo.order_num <span class="keyword">as</span> order_to_pay_ratio</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> </span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> active_num</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line">) <span class="keyword">as</span> duah</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> </span><br><span class="line"><span class="keyword">count</span>(<span class="keyword">distinct</span> goods_id) <span class="keyword">as</span> item_num</span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_good_item</span><br><span class="line">    <span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line">)<span class="keyword">as</span> dgi</span><br><span class="line"><span class="keyword">on</span> <span class="number">1</span>=<span class="number">1</span></span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> order_num,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">case</span> <span class="keyword">when</span> order_status != <span class="number">0</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> pay_num</span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_order</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line">) <span class="keyword">as</span> duo</span><br><span class="line"><span class="keyword">on</span> <span class="number">1</span>=<span class="number">1</span>;</span><br></pre></td></tr></table></figure><h5 id="开发脚本-2"><a href="#开发脚本-2" class="headerlink" title="开发脚本"></a>开发脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_4.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_4.sh</span><br></pre></td></tr></table></figure><h5 id="app-mall-init-table-4-sh"><a href="#app-mall-init-table-4-sh" class="headerlink" title="app_mall_init_table_4.sh"></a>app_mall_init_table_4.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求四：漏斗分析</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_conver_funnel(</span><br><span class="line">    active_num    int,</span><br><span class="line">    item_num     int,</span><br><span class="line">    order_num    int,</span><br><span class="line">    pay_num    int,</span><br><span class="line">    active_to_item_ratio    decimal(10,2),</span><br><span class="line">    item_to_order_ratio    decimal(10,2),</span><br><span class="line">    order_to_pay_ratio    decimal(10,2)</span><br><span class="line">)partitioned by(dt string) </span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_conver_funnel/';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-mall-add-partition-4-sh"><a href="#app-mall-add-partition-4-sh" class="headerlink" title="app_mall_add_partition_4.sh"></a>app_mall_add_partition_4.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求四：漏斗分析</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_conver_funnel partition(dt='$&#123;dt&#125;')  select </span><br><span class="line">duah.active_num,</span><br><span class="line">dgi.item_num,</span><br><span class="line">duo.order_num,</span><br><span class="line">duo.pay_num,</span><br><span class="line">dgi.item_num/duah.active_num as active_to_item_ratio,</span><br><span class="line">duo.order_num/dgi.item_num as item_to_order_ratio,</span><br><span class="line">duo.pay_num/duo.order_num as order_to_pay_ratio</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">    select </span><br><span class="line">count(*) as active_num</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">) as duah</span><br><span class="line">join</span><br><span class="line">(</span><br><span class="line">    select </span><br><span class="line">count(distinct goods_id) as item_num</span><br><span class="line">from dwd_mall.dwd_good_item</span><br><span class="line">    where dt = '$&#123;dt&#125;'</span><br><span class="line">)as dgi</span><br><span class="line">on 1=1</span><br><span class="line">join</span><br><span class="line">(</span><br><span class="line">    select</span><br><span class="line">count(*) as order_num,</span><br><span class="line">sum(case when order_status != 0 then 1 else 0 end) as pay_num</span><br><span class="line">from dwd_mall.dwd_user_order</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">) as duo</span><br><span class="line">on 1=1;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060133819.png" alt="image-20230406013300841"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060133557.png" alt="image-20230406013314309"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304060134761.png" alt="image-20230406013432629"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库之用户行为数仓4</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%934.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%934.html</id>
    <published>2023-04-04T08:46:35.000Z</published>
    <updated>2023-04-15T02:41:44.022Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="综合项目-电商数据仓库之用户行为数仓4"><a href="#综合项目-电商数据仓库之用户行为数仓4" class="headerlink" title="综合项目:电商数据仓库之用户行为数仓4"></a>综合项目:电商数据仓库之用户行为数仓4</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">截止目前hdfs上，用户行为数据目录结构的样子</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304042036477.png" alt="image-20230404203633888"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304042038941.png" alt="image-20230404203822647"></p><h2 id="用户行为数据数仓开发"><a href="#用户行为数据数仓开发" class="headerlink" title="用户行为数据数仓开发"></a>用户行为数据数仓开发</h2><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304042029511.png" alt="image-20230404202837476"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据仓库分为 4层：ods层、dwd层、dws层、app层，</span><br><span class="line">我们先来构建第一层：ods层</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1：由于在构建数据仓库的时候我们会创建多个数据库，所以在创建以及使用表的时候最好</span><br><span class="line">都在表名前面带上对应的数据库名称，否则可能会出现一些不必要的问题，可能会把ods层的表建到dwd层。</span><br><span class="line">2：考虑到SQL重跑的情况，需要在SQL语句中添加if not exists</span><br><span class="line">3：hive中可以用string、date和timestamp表示日期时间，date用yyyy-MM-dd的形式表示，timestamp用yyyy-MM-dd hh:mm:ss 的形式表示，string 可以表示 yyyy-MM-dd和yyyy-MM-dd hh:mm:ss</span><br><span class="line">这三种格式之间可以互相转换，不过在用的时候建议格式统一，String可以表示另外两种格式，并且也支持日期的大小比较，所以在这里针对时间统一使用String表示。</span><br></pre></td></tr></table></figure><h3 id="ods-层"><a href="#ods-层" class="headerlink" title="ods 层"></a>ods 层</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">创建ods层的表</span><br><span class="line">表名对应的建表语句</span><br><span class="line"></span><br><span class="line">注意：在使用这里的建表语句的时候注意里面的日期目录，需要和你 HDFS 中生成的日期目录保持一致</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_user_active( <span class="comment"># 都使用的数据库.表名，这样后期就不用来回切换当前数据库</span></span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_user_active <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/1'</span>; <span class="comment"># 这里写的相对路径，前面最后有/，则这里头不需要/，且相对路径，这里不能使用/开头；使用绝对路径一定正确</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_click_good(</span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_click_good <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/2'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_good_item(</span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_good_item <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/3'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_good_list(</span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_good_list <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/4'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> ods_mall.ods_app_close(</span><br><span class="line">    <span class="keyword">log</span>    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/ods/user_action/'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> ods_mall.ods_app_close <span class="keyword">add</span>  <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) location <span class="string">'20260101/5'</span>;</span><br></pre></td></tr></table></figure><h4 id="ods层抽取脚本"><a href="#ods层抽取脚本" class="headerlink" title="ods层抽取脚本"></a>ods层抽取脚本</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">针对ods层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">ods_mall_init_table.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">add_partition.sh：这个脚本是通用的，所有添加分区的地方都可以使用。</span><br><span class="line">ods_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h5 id="ods-mall-init-table-sh"><a href="#ods-mall-init-table-sh" class="headerlink" title="ods_mall_init_table.sh"></a>ods_mall_init_table.sh</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># ods层数据库和表初始化脚本，只需要执行一次</span><br><span class="line"></span><br><span class="line">hive -e &quot;</span><br><span class="line">create database if not exists ods_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists ods_mall.ods_user_active(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_click_good(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_good_item(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">create external table if not exists ods_mall.ods_good_list(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists ods_mall.ods_app_close(</span><br><span class="line">    log    string</span><br><span class="line">)partitioned by (dt string)</span><br><span class="line"> row format delimited</span><br><span class="line"> fields terminated by &#39;\t&#39;</span><br><span class="line"> location &#39;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;&#39;;</span><br><span class="line"></span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure><h5 id="add-partition-sh"><a href="#add-partition-sh" class="headerlink" title="add_partition.sh"></a>add_partition.sh</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 给外部分区表添加分区</span><br><span class="line"># 接收三个参数</span><br><span class="line">#1：表名</span><br><span class="line">#2：分区字段dt的值：格式20260101</span><br><span class="line">#3：分区路径(相对路径或者绝对路径都可以)</span><br><span class="line"></span><br><span class="line">if [ $# !&#x3D; 3 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;参数异常：add_partition.sh &lt;tabkle_name&gt; &lt;dt&gt; &lt;path&gt;&quot;</span><br><span class="line">exit 100</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">table_name&#x3D;$1</span><br><span class="line">dt&#x3D;$2</span><br><span class="line">path&#x3D;$3</span><br><span class="line"></span><br><span class="line">hive -e &quot;</span><br><span class="line">alter table $&#123;table_name&#125; add  if not exists partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;) location &#39;$&#123;path&#125;&#39;;</span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure><h5 id="ods-mall-add-partition-sh"><a href="#ods-mall-add-partition-sh" class="headerlink" title="ods_mall_add_partition.sh"></a>ods_mall_add_partition.sh</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 给ods层的表添加分区，这个脚本后期每天执行一次</span><br><span class="line"># 每天凌晨，添加昨天的分区，添加完分区之后，再执行后面的计算脚本</span><br><span class="line"></span><br><span class="line"># 默认获取昨天的日期，也支持传参指定一个日期</span><br><span class="line">if [ &quot;z$1&quot; &#x3D; &quot;z&quot; ]</span><br><span class="line">then</span><br><span class="line">dt&#x3D;&#96;date +%Y%m%d --date&#x3D;&quot;1 days ago&quot;&#96;</span><br><span class="line">else</span><br><span class="line">dt&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#alter table ods_mall.ods_user_active add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;1&#39;;</span><br><span class="line">#alter table ods_mall.ods_click_good add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;2&#39;;</span><br><span class="line">#alter table ods_mall.ods_good_item add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;3&#39;;</span><br><span class="line">#alter table ods_mall.ods_good_list add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;4&#39;;</span><br><span class="line">#alter table ods_mall.ods_app_close add  if not exists partition(dt&#x3D;&#39;20260101&#39;) location &#39;20260101&#x2F;5&#39;;</span><br><span class="line">sh add_partition.sh ods_mall.ods_user_active $&#123;dt&#125; $&#123;dt&#125;&#x2F;1</span><br><span class="line">sh add_partition.sh ods_mall.ods_click_good $&#123;dt&#125; $&#123;dt&#125;&#x2F;2</span><br><span class="line">sh add_partition.sh ods_mall.ods_good_item $&#123;dt&#125; $&#123;dt&#125;&#x2F;3</span><br><span class="line">sh add_partition.sh ods_mall.ods_good_list $&#123;dt&#125; $&#123;dt&#125;&#x2F;4</span><br><span class="line">sh add_partition.sh ods_mall.ods_app_close $&#123;dt&#125; $&#123;dt&#125;&#x2F;5</span><br></pre></td></tr></table></figure><h3 id="dwd层"><a href="#dwd层" class="headerlink" title="dwd层"></a>dwd层</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">针对 ods层表中的数据进行清洗，参考数据清洗规则，按照实际情况对数据进行清洗</span><br><span class="line">注意：如果清洗规则使用SQL可以实现，那么就使用SQL实现数据清洗，如果清洗的规则</span><br><span class="line">使用SQL实现起来非常麻烦，或者使用SQL压根无法实现，此时就可以考虑需要使用MapReduce代码或者 Spark代码对数据进行清洗了。</span><br><span class="line">由于我们这里采集的数据还是比较规整的，可以使用SQL实现，所以我们就直接使用SQL实现数据清洗了。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">创建dwd层的表</span><br><span class="line">注意：</span><br><span class="line">1：原始json数据中的用户 id字段名称为 uid，但是在商品订单数据中用户id字段名称为user_id，这块需要注意一下，在实际工作中会有这种情况，客户端数据和服务端数据的个别字段名称不一致，所以我们在使用的时候最好是统一一下，后期使用起来比较方便，所以在这里我会通过 uid解析数据，解析之后，给字段起别名为 user_id</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2：hive中的timestamp只能解析yyyy-MM-dd HH:MM:SS格式的数据，所以针对这里面的acttime字段我们使用bigint类型</span><br><span class="line">3：为了考虑到SQL重跑的情况，在使用insert into table(追加)的时候最好改为insert overwrite table(覆盖)，否则SQL重复执行的时候会重复写入数据</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_user_active(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    ad_status    <span class="built_in">tinyint</span>,</span><br><span class="line">    loading_time    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/user_active/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_user_active <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>)  <span class="keyword">select</span></span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ad_status'</span>) <span class="keyword">as</span> ad_status,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.loading_time'</span>) <span class="keyword">as</span> loading_time</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_user_active <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span> <span class="comment"># group by在这里的作用是去重，因为flume采集数据可能会重复，distinct也可以但效率不高</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_click_good(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    goods_id    <span class="built_in">bigint</span>,</span><br><span class="line">    location    <span class="built_in">tinyint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/click_good/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_click_good <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>)  <span class="keyword">select</span> </span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.goods_id'</span>) <span class="keyword">as</span> goods_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.location'</span>) <span class="keyword">as</span> location</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_click_good <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>; <span class="comment"># 为空则是pc端生成的数据直接过滤掉，只保留app端生成的数据；且pc端数据实际生活中很少</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_good_item(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    goods_id    <span class="built_in">bigint</span>,</span><br><span class="line">    stay_time    <span class="built_in">bigint</span>,</span><br><span class="line">loading_time    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/good_item/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_good_item <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) <span class="keyword">select</span> </span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.goods_id'</span>) <span class="keyword">as</span> goods_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.stay_time'</span>) <span class="keyword">as</span> stay_time,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.loading_time'</span>) <span class="keyword">as</span> loading_time</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_good_item <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_good_list(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    loading_time    <span class="built_in">bigint</span>,</span><br><span class="line">    loading_type    <span class="built_in">tinyint</span>,</span><br><span class="line">goods_num    <span class="built_in">tinyint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/good_list/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_good_list <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) <span class="keyword">select</span> </span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.loading_time'</span>) <span class="keyword">as</span> loading_time,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.loading_type'</span>) <span class="keyword">as</span> loading_type,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.goods_num'</span>) <span class="keyword">as</span> goods_num</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_good_list <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_app_close(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/app_close/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_mall.dwd_app_close <span class="keyword">partition</span>(dt=<span class="string">'20260101'</span>) <span class="keyword">select</span> </span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.uid'</span>) <span class="keyword">as</span> user_id,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) <span class="keyword">as</span> xaid,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.platform'</span>) <span class="keyword">as</span> platform,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.ver'</span>) <span class="keyword">as</span> ver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.vercode'</span>) <span class="keyword">as</span> vercode,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.net'</span>) <span class="keyword">as</span> net,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.brand'</span>) <span class="keyword">as</span> brand,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.model'</span>) <span class="keyword">as</span> <span class="keyword">model</span>,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.display'</span>) <span class="keyword">as</span> display,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.osver'</span>) <span class="keyword">as</span> osver,</span><br><span class="line">get_json_object(<span class="keyword">log</span>,<span class="string">'$.acttime'</span>) <span class="keyword">as</span> acttime</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">log</span> <span class="keyword">from</span> ods_mall.ods_app_close <span class="keyword">where</span> dt = <span class="string">'20260101'</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">log</span></span><br><span class="line">) <span class="keyword">as</span> tmp</span><br><span class="line"><span class="keyword">where</span> get_json_object(<span class="keyword">log</span>,<span class="string">'$.xaid'</span>) !=<span class="string">''</span>;</span><br></pre></td></tr></table></figure><h4 id="dwd层抽取脚本"><a href="#dwd层抽取脚本" class="headerlink" title="dwd层抽取脚本"></a>dwd层抽取脚本</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dwd_mall_init_table.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dwd_mall_add_partition.sh</span><br></pre></td></tr></table></figure><h5 id="dwd-mall-init-table-sh"><a href="#dwd-mall-init-table-sh" class="headerlink" title="dwd_mall_init_table.sh"></a>dwd_mall_init_table.sh</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment"># dwd层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_user_active(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    ad_status    <span class="built_in">tinyint</span>,</span><br><span class="line">    loading_time    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/user_active/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_click_good(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    goods_id    <span class="built_in">bigint</span>,</span><br><span class="line">    location    <span class="built_in">tinyint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/click_good/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_good_item(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    goods_id    <span class="built_in">bigint</span>,</span><br><span class="line">    stay_time    <span class="built_in">bigint</span>,</span><br><span class="line">loading_time    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/good_item/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_good_list(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span>,</span><br><span class="line">    loading_time    <span class="built_in">bigint</span>,</span><br><span class="line">    loading_type    <span class="built_in">tinyint</span>,</span><br><span class="line">goods_num    <span class="built_in">tinyint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/good_list/'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dwd_mall.dwd_app_close(</span><br><span class="line">    user_id    <span class="built_in">bigint</span>,</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">    platform    <span class="built_in">tinyint</span>,</span><br><span class="line">    ver    <span class="keyword">string</span>,</span><br><span class="line">    vercode    <span class="keyword">string</span>,</span><br><span class="line">    net    <span class="built_in">bigint</span>,</span><br><span class="line">    brand    <span class="keyword">string</span>,</span><br><span class="line">    <span class="keyword">model</span>    <span class="keyword">string</span>,</span><br><span class="line">    display    <span class="keyword">string</span>,</span><br><span class="line">    osver    <span class="keyword">string</span>,</span><br><span class="line">    acttime    <span class="built_in">bigint</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>) </span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dwd/app_close/'</span>;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="dwd-mall-add-partition-sh"><a href="#dwd-mall-add-partition-sh" class="headerlink" title="dwd_mall_add_partition.sh"></a>dwd_mall_add_partition.sh</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 基于ods层的表进行清洗，将清洗之后的数据添加到dwd层对应表的对应分区中</span><br><span class="line"># 每天凌晨执行一次</span><br><span class="line"></span><br><span class="line"># 默认获取昨天的日期，也支持传参指定一个日期</span><br><span class="line">if [ &quot;z$1&quot; &#x3D; &quot;z&quot; ]</span><br><span class="line">then</span><br><span class="line">dt&#x3D;&#96;date +%Y%m%d --date&#x3D;&quot;1 days ago&quot;&#96;</span><br><span class="line">else</span><br><span class="line">dt&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e &quot;</span><br><span class="line">insert overwrite table dwd_mall.dwd_user_active partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;)  select</span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime,</span><br><span class="line">get_json_object(log,&#39;$.ad_status&#39;) as ad_status,</span><br><span class="line">get_json_object(log,&#39;$.loading_time&#39;) as loading_time</span><br><span class="line">from </span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_user_active where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_click_good partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;)  select </span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime,</span><br><span class="line">get_json_object(log,&#39;$.goods_id&#39;) as goods_id,</span><br><span class="line">get_json_object(log,&#39;$.location&#39;) as location</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_click_good where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_good_item partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;) select </span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime,</span><br><span class="line">get_json_object(log,&#39;$.goods_id&#39;) as goods_id,</span><br><span class="line">get_json_object(log,&#39;$.stay_time&#39;) as stay_time,</span><br><span class="line">get_json_object(log,&#39;$.loading_time&#39;) as loading_time</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_good_item where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_good_list partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;) select </span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime,</span><br><span class="line">get_json_object(log,&#39;$.loading_time&#39;) as loading_time,</span><br><span class="line">get_json_object(log,&#39;$.loading_type&#39;) as loading_type,</span><br><span class="line">get_json_object(log,&#39;$.goods_num&#39;) as goods_num</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_good_list where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dwd_mall.dwd_app_close partition(dt&#x3D;&#39;$&#123;dt&#125;&#39;) select </span><br><span class="line">get_json_object(log,&#39;$.uid&#39;) as user_id,</span><br><span class="line">get_json_object(log,&#39;$.xaid&#39;) as xaid,</span><br><span class="line">get_json_object(log,&#39;$.platform&#39;) as platform,</span><br><span class="line">get_json_object(log,&#39;$.ver&#39;) as ver,</span><br><span class="line">get_json_object(log,&#39;$.vercode&#39;) as vercode,</span><br><span class="line">get_json_object(log,&#39;$.net&#39;) as net,</span><br><span class="line">get_json_object(log,&#39;$.brand&#39;) as brand,</span><br><span class="line">get_json_object(log,&#39;$.model&#39;) as model,</span><br><span class="line">get_json_object(log,&#39;$.display&#39;) as display,</span><br><span class="line">get_json_object(log,&#39;$.osver&#39;) as osver,</span><br><span class="line">get_json_object(log,&#39;$.acttime&#39;) as acttime</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select log from ods_mall.ods_app_close where dt &#x3D; &#39;$&#123;dt&#125;&#39; group by log</span><br><span class="line">) as tmp</span><br><span class="line">where get_json_object(log,&#39;$.xaid&#39;) !&#x3D;&#39;&#39;;</span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">前面的两层中的表和需求一般没什么关系，就是把已有的数据接入进来，然后对数据进行清洗处理</span><br><span class="line">但是后面的dws层和app层是和业务有关联的，所以在构建这两层中的表的时候，我们需要根据一些典型的业务场景来进行分析，在根据具体业务建表的时候尽可能把表设计的更加通用，可以满足后期一些类似业务需求</span><br><span class="line">就是说我们在基于业务构建表的时候，不要直接一个SQL搞定，可以把一些复杂的SQL基于一些维度进行拆分，拆分出来一些中间表，再基于这些中间表统计最终的结果。</span><br><span class="line">这样这个中间表里面的数据，我们后期针对一些类似的业务需求还是可以服用的。</span><br><span class="line">需求一：每日新增用户相关指标</span><br><span class="line">需求二：每日活跃用户相关指标(主活)</span><br><span class="line">需求三：用户7日流失push提醒</span><br><span class="line">需求四：每日启动App次数相关指标</span><br><span class="line">需求五：操作系统活跃用户相关指标</span><br><span class="line">需求六：APP崩溃相关指标</span><br><span class="line"></span><br><span class="line">在计算这些需求的时候，为了保证大家在下面练习时计算的结果和我这边计算的结果保持一致，所以针对后面的测试数据就不再随机生成了，而是生成固定的数据，一共1个月的数据</span><br><span class="line">从2026-02-01到2026-02-2的8数据</span><br><span class="line"></span><br><span class="line">然后执行 ods层和 dwd层的脚本，重新加载计算 2026-02月份的数据。</span><br></pre></td></tr></table></figure><h4 id="tmp-load-ods-data-sh"><a href="#tmp-load-ods-data-sh" class="headerlink" title="tmp_load_ods_data.sh"></a>tmp_load_ods_data.sh</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1：执行 ods层的脚本</span><br><span class="line">写一个临时脚本，在脚本中写一个 for循环，循环加载数据</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 加载ods层的数据</span></span><br><span class="line"></span><br><span class="line">for((i=1;i&lt;=28;i++))</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ $i -lt <span class="number">10</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    dt=<span class="string">"2026020"</span>$i</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">        dt=<span class="string">"202602"</span>$i</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">"ods_mall_add_partition.sh"</span> $&#123;dt&#125;</span><br><span class="line">sh ods_mall_add_partition.sh $&#123;dt&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h4 id="tmp-load-dwd-data-sh"><a href="#tmp-load-dwd-data-sh" class="headerlink" title="tmp_load_dwd_data.sh"></a>tmp_load_dwd_data.sh</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 加载dwd层的数据</span></span><br><span class="line"></span><br><span class="line">for((i=1;i&lt;=28;i++))</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ $i -lt <span class="number">10</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    dt=<span class="string">"2026020"</span>$i</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">        dt=<span class="string">"202602"</span>$i</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">"dwd_mall_add_partition.sh"</span> $&#123;dt&#125;</span><br><span class="line">sh dwd_mall_add_partition.sh $&#123;dt&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051440087.png" alt="image-20230405144005173"></p><h4 id="需求一：每日新增用户相关指标"><a href="#需求一：每日新增用户相关指标" class="headerlink" title="需求一：每日新增用户相关指标"></a>需求一：每日新增用户相关指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在统计新增用户时，用户是以设备标识(xaid 字段)来判断的，每一个设备都有一个唯一设备码，因为会存在用户不登录的情况，以及多人共用一个账号的情况，所以根据用户 id 进行统计是不准确的。</span><br><span class="line">新增用户是指第一次安装并且使用 app 的用户，后期卸载之后再使用就不算新用户了</span><br><span class="line">这个新增用户其实也可以称为新增设备，一个设备对应一个用户。</span><br><span class="line">1：每日新增用户量</span><br><span class="line">2：每日新增用户量的日环比和周同比</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> 先详细分析第 1 个指标，每日新增用户量</span><br><span class="line">在实际工作中通过这个指标可以衡量我们产品的用户增长速度，如果每日新增用户量一直是</span><br><span class="line">上升的，说明我们的产品势头正好，如果在一段时间内增速减缓或者下降，这个时候需要考</span><br><span class="line">虑如何获取新用户。</span><br><span class="line">咱们前面分析了，新增用户是指第一次安装并且使用 APP 的用户，咱们有一个埋点会上报</span><br><span class="line">用户打开 APP 这个行为，所以计算新增用户量就使用这一份数据</span><br><span class="line">ods 层的表名为：ods_user_active</span><br><span class="line">dwd 层的表名为：dwd_user_active</span><br><span class="line">实现思路如下：</span><br><span class="line">1：我们基于清洗之后的打开 app 上报的数据创建一个历史表，这个表里面包含的有 xaid 字</span><br><span class="line">段，针对每天的数据基于 xaid 进行去重</span><br><span class="line">2：如果我们要计算 2026 年 02 月 1 日的新增用户量的话，就拿这一天上报的打开 app 的数</span><br><span class="line">据，和前面的历史表进行 left join，使用 xaid 进行关联，关联不上的数据则为新增数据。</span><br></pre></td></tr></table></figure><h5 id="每日新增用户量"><a href="#每日新增用户量" class="headerlink" title="每日新增用户量"></a>每日新增用户量</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">举个例子：</span><br><span class="line">(1)：第一步会产生一个历史表：dws_user_active_history，这个表中有一个 xaid 字段</span><br><span class="line">dws_user_active_history</span><br><span class="line">xaid</span><br><span class="line">a1</span><br><span class="line">b1</span><br><span class="line">c1</span><br><span class="line">d1</span><br><span class="line">(2)：第二步会产生一个临时表，表里面包含的是那一天上报的打开 app的数据</span><br><span class="line">dws_user_active_20260201_tmp</span><br><span class="line">xaid</span><br><span class="line">a1</span><br><span class="line">b1</span><br><span class="line">x1</span><br><span class="line">y1</span><br><span class="line">z1</span><br><span class="line">(3)：对这两个表进行left join</span><br><span class="line">dws_user_active_20260201_tmp dws_user_active_history</span><br><span class="line">xaid xaid</span><br><span class="line">a1 a1</span><br><span class="line">b1 b1</span><br><span class="line">x1 null</span><br><span class="line">y1 null</span><br><span class="line">z1 null</span><br><span class="line">此时，dws_user_active_history.xaid为null的数据条数即为当日新增用户数</span><br><span class="line"></span><br><span class="line">3：将计算出来的每日新增用户信息保存到表 dws_user_new_item 表中，这个表按照天作为分区，便于后期其它需求使用这个表</span><br><span class="line"></span><br><span class="line">4：基于 dws_user_new_item 对数据进行聚合计算，将计算出来的新增用户数量保存到结果表 app_user_new_count 中。</span><br><span class="line"></span><br><span class="line">注意：在这里处理完之后，还需要将 dws_user_active_20260201_tmp 这个临时表中的数据</span><br><span class="line">insert 到 dws_user_active_history 这个历史表中。</span><br><span class="line">最后删除这个临时表即可</span><br></pre></td></tr></table></figure><h5 id="每日新增用户量的日环比和周同比"><a href="#每日新增用户量的日环比和周同比" class="headerlink" title="每日新增用户量的日环比和周同比"></a>每日新增用户量的日环比和周同比</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">接下来是第2个指标，每日新增用户量的日环比和周同比</span><br><span class="line">同比一般是指本期统计数据和往年的同时期的统计数据比较，例如 2026 年2月和2025年2月相比较；这个统计周期也可以是按月或者周</span><br><span class="line"></span><br><span class="line">环比一般是指本期统计数据和上一期的统计数据作比较，例如 2026 年 2月和2026年1月相比较；这个统计周期也可以是按周或者日</span><br><span class="line"></span><br><span class="line">在实际工作中通过同比和环比是可以衡量某一个指标的变化速度，供产品经理做一些决策的时候使用。</span><br><span class="line">日环比&#x3D;(本期的数据-上一期的数据)&#x2F;上一期的数据</span><br><span class="line">日环比中的时间单位是天</span><br><span class="line">周同比&#x3D;(本期的数据-上一期的数据)&#x2F;上一期的数据</span><br><span class="line">周同比中的时间单位是周(7天)</span><br><span class="line"></span><br><span class="line">实现思路</span><br><span class="line">直接基于 app_user_new_count 进行统计即可，可以统计出来某一天的日环比和周同比</span><br><span class="line">生成一个新表 app_user_new_count_ratio</span><br><span class="line">里面包含日期、新增用户量、日环比、周同比</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">针对前面对需求的分析，我们最终在dws层需要创建三个表</span><br><span class="line">dws_user_active_20260201_tmp</span><br><span class="line">和</span><br><span class="line">dws_user_new_item</span><br><span class="line">和</span><br><span class="line">dws_user_active_history</span><br><span class="line"></span><br><span class="line">在app层需要创建两个表</span><br><span class="line">app_user_new_count</span><br><span class="line">和</span><br><span class="line">app_user_new_count_ratio</span><br><span class="line">下面开始在数据仓库中构建这些表</span><br></pre></td></tr></table></figure><h5 id="dws"><a href="#dws" class="headerlink" title="dws"></a>dws</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_active_20260201_tmp(</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">times   <span class="built_in">int</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_active_20260201_tmp <span class="keyword">select</span></span><br><span class="line">xaid,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> times</span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> xaid;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_active_history(</span><br><span class="line">    xaid    <span class="keyword">string</span>,</span><br><span class="line">times   <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_active_history'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_new_item(</span><br><span class="line">    xaid    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_new_item'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_new_item <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">duat.xaid</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_20260201_tmp duat</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> (<span class="keyword">select</span> xaid <span class="keyword">from</span> dws_mall.dws_user_active_history <span class="keyword">group</span> <span class="keyword">by</span> xaid) duah</span><br><span class="line"><span class="keyword">on</span> duat.xaid = duah.xaid</span><br><span class="line"><span class="keyword">where</span> duah.xaid <span class="keyword">is</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_active_history <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">xaid,</span><br><span class="line">times</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_20260201_tmp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_lost_item(</span><br><span class="line">    xaid    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_lost_item'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_lost_item <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">xaid</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt &gt;= regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> xaid</span><br><span class="line"><span class="keyword">having</span> <span class="keyword">max</span>(dt) = regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_platform_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_platform_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_android_osver_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">osver <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_ios_osver_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">osver <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_brand_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">brand <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> brand;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_model_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_model_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">model</span> <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">model</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_net_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> net</span><br><span class="line"><span class="keyword">when</span> <span class="number">0</span> <span class="keyword">then</span> <span class="string">'未知'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'WIFI'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'2G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">3</span> <span class="keyword">then</span> <span class="string">'3G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">4</span> <span class="keyword">then</span> <span class="string">'4G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">5</span> <span class="keyword">then</span> <span class="string">'5G'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> net;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_app_close_platform_vercode(</span><br><span class="line">    platform    <span class="keyword">string</span>,</span><br><span class="line">vercode    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/app_close_platform_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_app_close_platform_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">platform,</span><br><span class="line">vercode,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_app_close</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform,vercode;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_1.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_1.sh</span><br></pre></td></tr></table></figure><h6 id="dws-mall-init-table-1-sh"><a href="#dws-mall-init-table-1-sh" class="headerlink" title="dws_mall_init_table_1.sh"></a>dws_mall_init_table_1.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日新增用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 由于这个表需要每天创建一个，用完之后删除，所以选择把这个建表语句放到添加分区数据的脚本中</span></span><br><span class="line"><span class="meta">#</span><span class="bash">create table <span class="keyword">if</span> not exists dws_mall.dws_user_active_20260201_tmp(</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    xaid    string,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">times</span>   int</span></span><br><span class="line"><span class="meta">#</span><span class="bash">);</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_active_history(</span><br><span class="line">    xaid    string,</span><br><span class="line">times   int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_active_history';</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">create external table if not exists dws_mall.dws_user_new_item(</span><br><span class="line">    xaid    string</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_new_item';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-1-sh"><a href="#dws-mall-add-partition-1-sh" class="headerlink" title="dws_mall_add_partition_1.sh"></a>dws_mall_add_partition_1.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日新增用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create table if not exists dws_mall.dws_user_active_$&#123;dt&#125;_tmp(</span><br><span class="line">    xaid    string,</span><br><span class="line">    times   int</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_active_$&#123;dt&#125;_tmp select</span><br><span class="line">xaid,</span><br><span class="line">count(*) as times</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by xaid;</span><br><span class="line"></span><br><span class="line">--注意：考虑到脚本重跑的情况，所以在这开面每次执行的时候都会先删除dws_user_active_history表中指定分区的数据</span><br><span class="line">--因为在计算每日新增用户的时候需要和dws_user_active_history进行关联查询</span><br><span class="line">alter table dws_mall.dws_user_active_history drop partition(dt='$&#123;dt&#125;');</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_new_item partition(dt='$&#123;dt&#125;') select</span><br><span class="line">duat.xaid</span><br><span class="line">from dws_mall.dws_user_active_$&#123;dt&#125;_tmp duat</span><br><span class="line">left join (select xaid from dws_mall.dws_user_active_history group by xaid) duah</span><br><span class="line">on duat.xaid = duah.xaid</span><br><span class="line">where duah.xaid is null;</span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_active_history partition(dt='$&#123;dt&#125;') select</span><br><span class="line">xaid,</span><br><span class="line">times</span><br><span class="line">from dws_mall.dws_user_active_$&#123;dt&#125;_tmp;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app"><a href="#app" class="headerlink" title="app"></a>app</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_new_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_new_count'</span>;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_new_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_new_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_new_count_ratio(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span>,</span><br><span class="line">day_ratio    <span class="keyword">double</span>,</span><br><span class="line">week_ratio    <span class="keyword">double</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_new_count_ratio'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 日环比，周同比</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_new_count_ratio <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">num</span>,</span><br><span class="line">(<span class="keyword">num</span>-num_1)/num_1 <span class="keyword">as</span> day_ratio,</span><br><span class="line">(<span class="keyword">num</span>-num_7)/num_7 <span class="keyword">as</span> week_ratio</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">    dt,</span><br><span class="line">    <span class="keyword">num</span>,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_1,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">7</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_7</span><br><span class="line">    <span class="keyword">from</span> app_mall.app_user_new_count</span><br><span class="line"><span class="keyword">where</span> dt &gt;=regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line">) <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_active_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_active_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_active_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_active_count_ratio(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span>,</span><br><span class="line">day_ratio    <span class="keyword">double</span>,</span><br><span class="line">week_ratio    <span class="keyword">double</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_active_count_ratio'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_active_count_ratio <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">num</span>,</span><br><span class="line">(<span class="keyword">num</span>-num_1)/num_1 <span class="keyword">as</span> day_ratio,</span><br><span class="line">(<span class="keyword">num</span>-num_7)/num_7 <span class="keyword">as</span> week_ratio</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">    dt,</span><br><span class="line">    <span class="keyword">num</span>,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_1,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">7</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_7</span><br><span class="line">    <span class="keyword">from</span> app_mall.app_user_active_count</span><br><span class="line"><span class="keyword">where</span> dt &gt;=regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line">) <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_lost_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_lost_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_lost_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_lost_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_count(</span><br><span class="line">    pv    <span class="built_in">int</span>,</span><br><span class="line">uv    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_count'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>(times) <span class="keyword">as</span> pv,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_distrib(</span><br><span class="line">    ts_1    <span class="built_in">int</span>,</span><br><span class="line">ts_2    <span class="built_in">int</span>,</span><br><span class="line">ts_3_m    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">1</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_1,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">2</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_2,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times &gt;= <span class="number">3</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_3_m</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_platform_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_platform_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_platform_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_android_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_android_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_ios_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_ios_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_brand_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_brand_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_model_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_model_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_model_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_net_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_net_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_platform_all(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_platform_all'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_platform_all <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_android_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_android_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_android_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_ios_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_ios_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_ios_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br></pre></td></tr></table></figure><h6 id="app-mall-init-table-1-sh"><a href="#app-mall-init-table-1-sh" class="headerlink" title="app_mall_init_table_1.sh"></a>app_mall_init_table_1.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日新增用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_new_count(</span><br><span class="line">    num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_new_count';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_new_count_ratio(</span><br><span class="line">    num    int,</span><br><span class="line">day_ratio    double,</span><br><span class="line">week_ratio    double</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_new_count_ratio';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-1-sh"><a href="#app-mall-add-partition-1-sh" class="headerlink" title="app_mall_add_partition_1.sh"></a>app_mall_add_partition_1.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日新增用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 转换日期格式，20260201改为<span class="variable">$&#123;dt_new&#125;</span></span></span><br><span class="line">dt_new=`date +%Y-%m-%d --date="$&#123;dt&#125;"`</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_new_count partition(dt='$&#123;dt&#125;') select</span><br><span class="line">count(*) as num</span><br><span class="line">from dws_mall.dws_user_new_item</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_new_count_ratio partition(dt='$&#123;dt&#125;') select</span><br><span class="line">num,</span><br><span class="line">(num-num_1)/num_1 as day_ratio,</span><br><span class="line">(num-num_7)/num_7 as week_ratio</span><br><span class="line">from(</span><br><span class="line">    select</span><br><span class="line">    dt,</span><br><span class="line">    num,</span><br><span class="line">    lead(num,1) over(order by dt desc) as num_1,</span><br><span class="line">    lead(num,7) over(order by dt desc) as num_7</span><br><span class="line">    from app_mall.app_user_new_count</span><br><span class="line">where dt &gt;=regexp_replace(date_add('$&#123;dt_new&#125;',-7),'-','')</span><br><span class="line">) as t</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051622730.png" alt="image-20230405162216940"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051622364.png" alt="image-20230405162233516"></p><h4 id="需求二：每日活跃用户-主活-相关指标"><a href="#需求二：每日活跃用户-主活-相关指标" class="headerlink" title="需求二：每日活跃用户(主活)相关指标"></a>需求二：每日活跃用户(主活)相关指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">活跃用户的定义是指只要当天使用过 APP 就算是活跃用户，使用 APP 这种操作属于主动操作，所以这种活跃我们也会称为主动活跃，简称主活</span><br><span class="line">针对这个需求统计的指标和新增用户的指标类似</span><br><span class="line">1：每日主活用户量</span><br><span class="line">2：每日主活用户量的日环比和周同比</span><br><span class="line"></span><br><span class="line"> 首先看第一个指标：每日主活用户量</span><br><span class="line">主活的概念和定义我们知道了，其实就是统计每天使用过app的用户，所以我们可以直接使用dws层的dws_user_active_history这个表</span><br><span class="line">直接求和即可获取到当日的主活用户量,将最终的结果保存到app层的</span><br><span class="line">app_user_active_count表中</span><br><span class="line"></span><br><span class="line"> 接着看第二个指标：每日主活用户量的日环比和周同比</span><br><span class="line">这个指标直接基于每日主活用户量的表(app_user_active_coun)进行计算即可，把最终的结果保存到app层的app_user_active_count_ratio 表中</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_new_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_new_count'</span>;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_new_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_new_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_new_count_ratio(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span>,</span><br><span class="line">day_ratio    <span class="keyword">double</span>,</span><br><span class="line">week_ratio    <span class="keyword">double</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_new_count_ratio'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 日环比，周同比</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_new_count_ratio <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">num</span>,</span><br><span class="line">(<span class="keyword">num</span>-num_1)/num_1 <span class="keyword">as</span> day_ratio,</span><br><span class="line">(<span class="keyword">num</span>-num_7)/num_7 <span class="keyword">as</span> week_ratio</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">    dt,</span><br><span class="line">    <span class="keyword">num</span>,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_1,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">7</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_7</span><br><span class="line">    <span class="keyword">from</span> app_mall.app_user_new_count</span><br><span class="line"><span class="keyword">where</span> dt &gt;=regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line">) <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_active_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_active_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_active_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_active_count_ratio(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span>,</span><br><span class="line">day_ratio    <span class="keyword">double</span>,</span><br><span class="line">week_ratio    <span class="keyword">double</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_active_count_ratio'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_active_count_ratio <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">num</span>,</span><br><span class="line">(<span class="keyword">num</span>-num_1)/num_1 <span class="keyword">as</span> day_ratio,</span><br><span class="line">(<span class="keyword">num</span>-num_7)/num_7 <span class="keyword">as</span> week_ratio</span><br><span class="line"><span class="keyword">from</span>(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">    dt,</span><br><span class="line">    <span class="keyword">num</span>,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_1,</span><br><span class="line">    <span class="keyword">lead</span>(<span class="keyword">num</span>,<span class="number">7</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> dt <span class="keyword">desc</span>) <span class="keyword">as</span> num_7</span><br><span class="line">    <span class="keyword">from</span> app_mall.app_user_active_count</span><br><span class="line"><span class="keyword">where</span> dt &gt;=regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line">) <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_lost_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_lost_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_lost_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_lost_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_count(</span><br><span class="line">    pv    <span class="built_in">int</span>,</span><br><span class="line">uv    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_count'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>(times) <span class="keyword">as</span> pv,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_distrib(</span><br><span class="line">    ts_1    <span class="built_in">int</span>,</span><br><span class="line">ts_2    <span class="built_in">int</span>,</span><br><span class="line">ts_3_m    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">1</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_1,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">2</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_2,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times &gt;= <span class="number">3</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_3_m</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_platform_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_platform_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_platform_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_android_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_android_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_ios_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_ios_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_brand_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_brand_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_model_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_model_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_model_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_net_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_net_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_platform_all(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_platform_all'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_platform_all <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_android_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_android_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_android_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_ios_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_ios_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_ios_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_2.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_2.sh</span><br></pre></td></tr></table></figure><h5 id="app-mall-init-table-2-sh"><a href="#app-mall-init-table-2-sh" class="headerlink" title="app_mall_init_table_2.sh"></a>app_mall_init_table_2.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求二：每日活跃用户(主活)相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_active_count(</span><br><span class="line">    num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_active_count';</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_active_count_ratio(</span><br><span class="line">    num    int,</span><br><span class="line">day_ratio    double,</span><br><span class="line">week_ratio    double</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_active_count_ratio';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-mall-add-partition-2-sh"><a href="#app-mall-add-partition-2-sh" class="headerlink" title="app_mall_add_partition_2.sh"></a>app_mall_add_partition_2.sh</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求一：每日活跃用户(主活)相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 转换日期格式，<span class="variable">$&#123;dt&#125;</span>改为<span class="variable">$&#123;dt_new&#125;</span></span></span><br><span class="line">dt_new=`date +%Y-%m-%d --date="$&#123;dt&#125;"`</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_active_count partition(dt='$&#123;dt&#125;') select</span><br><span class="line">count(*) as num</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_active_count_ratio partition(dt='$&#123;dt&#125;') select</span><br><span class="line">num,</span><br><span class="line">(num-num_1)/num_1 as day_ratio,</span><br><span class="line">(num-num_7)/num_7 as week_ratio</span><br><span class="line">from(</span><br><span class="line">    select</span><br><span class="line">    dt,</span><br><span class="line">    num,</span><br><span class="line">    lead(num,1) over(order by dt desc) as num_1,</span><br><span class="line">    lead(num,7) over(order by dt desc) as num_7</span><br><span class="line">    from app_mall.app_user_active_count</span><br><span class="line">where dt &gt;=regexp_replace(date_add('$&#123;dt_new&#125;',-7),'-','')</span><br><span class="line">) as t</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051642004.png" alt="image-20230405164201393"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051642482.png" alt="image-20230405164229164"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051642965.png" alt="image-20230405164252247"></p><h6 id="需求扩展"><a href="#需求扩展" class="headerlink" title="需求扩展"></a>需求扩展</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如何统计每周主活？每月主活？</span><br><span class="line">周：按照自然周，每周一凌晨计算上一周的主活</span><br><span class="line">月：按照自然月，每月 1 号计算上一个月的主活</span><br></pre></td></tr></table></figure><h4 id="需求三：用户-7-日流失-push-提醒"><a href="#需求三：用户-7-日流失-push-提醒" class="headerlink" title="需求三：用户 7 日流失 push 提醒"></a>需求三：用户 7 日流失 push 提醒</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">什么是流失呢？</span><br><span class="line">假设这个用户在 2026年 2月 2 日是新增用户，如果他在后续的 7 天内，也就是在 2 月 9日</span><br><span class="line">内没有再使用 app，则认为是流失用户，具体多少天属于流失用户，这个是需要产品经理根</span><br><span class="line">据对应产品的特点来定的，一般业内使用比较多的是 7天这个时间点。</span><br><span class="line">push是什么意思呢</span><br><span class="line">大家平时是不是深受各种 app的提醒轰炸，我针对大部分的 app都禁用了消息推送，要不</span><br><span class="line">然每天手机上会有各种各样的推送消息，很烦，这个其实就是软件给你 push的消息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">什么是流失呢？</span><br><span class="line">假设这个用户在 2026年 2月 2 日是新增用户，如果他在后续的 7 天内，也就是在 2 月 9日</span><br><span class="line">内没有再使用 app，则认为是流失用户，具体多少天属于流失用户，这个是需要产品经理根</span><br><span class="line">据对应产品的特点来定的，一般业内使用比较多的是 7天这个时间点。</span><br><span class="line">push是什么意思呢</span><br><span class="line">大家平时是不是深受各种 app的提醒轰炸，我针对大部分的 app都禁用了消息推送，要不</span><br><span class="line">然每天手机上会有各种各样的推送消息，很烦，这个其实就是软件给你 push的消息</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">实现思路：</span><br><span class="line">1：基于 dws_user_active_histo表ry，获取表中最近 8 天的数据，根据 xaid进行分组，这样</span><br><span class="line">可以获取 xaid以及 xaid对应的多个日期(dt)</span><br><span class="line">2：接着需要对 xaid对应的 dt进行过滤，获取 xaid中最大的 dt，判断这个 dt是否等于(当天</span><br><span class="line">日期-7)，如果满足条件，则说明这个用户最近 7日内没有使用 app，就认为他属于 7 日流失</span><br><span class="line">用户</span><br><span class="line">例如：dws_user_active_histo表ry中有以下几条数据</span><br><span class="line">xaid dt</span><br><span class="line">a1 2026-02-01</span><br><span class="line">a1 2026-02-05</span><br><span class="line">b1 2026-02-01</span><br><span class="line">b1 2026-02-02</span><br><span class="line">c1 2026-02-03</span><br><span class="line">针对这份数据，我们想要在 02-09号统计用户 7 日流失量</span><br><span class="line">那也就意味着要统计表里面在 02-02使用过 APP，但是在之后的 7天内，一直到 02-09号没有再使用过app的用户</span><br><span class="line">根据xaid进行分组,获取里面最大的日期(最近一次使用app的时间)</span><br><span class="line">a1 2026-02-01,2026-02-05</span><br><span class="line">b1 2026-02-01,2026-02-02</span><br><span class="line">c1 2026-02-03</span><br><span class="line"></span><br><span class="line">判断这个时间是否等于 02-02，如果满足这个条件，就说明在 02-09 号之前的7天内没有使用过app，这里的b1满足条件，所以他就是7日流失用户了。</span><br><span class="line">依此类推，可以计算14日流失, 21日流失用户，针对流失的时间不同可以实现不同的策略给用户实现push提醒，告诉用户他关注的商品降价了，或者给用户推荐他经常浏览的类似商品，促进用户活跃，最终促进订单成交。</span><br><span class="line">3：将满足条件的 xaid 数据保存到dws层的dws_user_lost_item 表中</span><br><span class="line">4：对dws_user_lost_item表中的数据进行聚合统计，统计用户 7 日流失数据量，保存到app层的app_user_lost_count表中</span><br></pre></td></tr></table></figure><h5 id="dws-1"><a href="#dws-1" class="headerlink" title="dws"></a>dws</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_lost_item(</span><br><span class="line">    xaid    <span class="keyword">string</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_lost_item'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_lost_item <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">xaid</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt &gt;= regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> xaid</span><br><span class="line"><span class="keyword">having</span> <span class="keyword">max</span>(dt) = regexp_replace(<span class="keyword">date_add</span>(<span class="string">'2026-02-01'</span>,<span class="number">-7</span>),<span class="string">'-'</span>,<span class="string">''</span>);</span><br></pre></td></tr></table></figure><h6 id="提取脚本"><a href="#提取脚本" class="headerlink" title="提取脚本"></a>提取脚本</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_3.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_3.sh</span><br></pre></td></tr></table></figure><h6 id="dws-mall-init-table-3-sh"><a href="#dws-mall-init-table-3-sh" class="headerlink" title="dws_mall_init_table_3.sh"></a>dws_mall_init_table_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：用户7日流失push提醒</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_lost_item(</span><br><span class="line">    xaid    string</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_lost_item';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-3-sh"><a href="#dws-mall-add-partition-3-sh" class="headerlink" title="dws_mall_add_partition_3.sh"></a>dws_mall_add_partition_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：用户7日流失push提醒</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 转换日期格式，<span class="variable">$&#123;dt&#125;</span>改为<span class="variable">$&#123;dt_new&#125;</span></span></span><br><span class="line">dt_new=`date +%Y-%m-%d --date="$&#123;dt&#125;"`</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_user_lost_item partition(dt='$&#123;dt&#125;') select</span><br><span class="line">xaid</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt &gt;= regexp_replace(date_add('$&#123;dt_new&#125;',-7),'-','')</span><br><span class="line">group by xaid</span><br><span class="line">having max(dt) = regexp_replace(date_add('$&#123;dt_new&#125;',-7),'-','');</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h5 id="app-1"><a href="#app-1" class="headerlink" title="app"></a>app</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_lost_count(</span><br><span class="line">    <span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_lost_count'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_lost_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_lost_item</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br></pre></td></tr></table></figure><h6 id="提取脚本-1"><a href="#提取脚本-1" class="headerlink" title="提取脚本"></a>提取脚本</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_3.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_3.sh</span><br></pre></td></tr></table></figure><h6 id="app-mall-init-table-3-sh"><a href="#app-mall-init-table-3-sh" class="headerlink" title="app_mall_init_table_3.sh"></a>app_mall_init_table_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：用户7日流失push提醒</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_lost_count(</span><br><span class="line">    num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_lost_count';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-3-sh"><a href="#app-mall-add-partition-3-sh" class="headerlink" title="app_mall_add_partition_3.sh"></a>app_mall_add_partition_3.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求三：用户7日流失push提醒</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_lost_count partition(dt='$&#123;dt&#125;') select</span><br><span class="line">count(*) as num</span><br><span class="line">from dws_mall.dws_user_lost_item</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051726034.png" alt="image-20230405172638690"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051730420.png" alt="image-20230405173019710"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051731280.png" alt="image-20230405173138361"></p><h4 id="需求四：每日启动-App-次数相关指标"><a href="#需求四：每日启动-App-次数相关指标" class="headerlink" title="需求四：每日启动 App 次数相关指标"></a>需求四：每日启动 App 次数相关指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">这个需求就是对每日打开 app上报的数据进行统计</span><br><span class="line">针对这个需求我们需要统计两个指标</span><br><span class="line">1：每日人均启动App次数</span><br><span class="line">2：每日APP启动次数分布（1次，2次，3次及以上）</span><br><span class="line"></span><br><span class="line"> 首先看第一个指标：每日人均启动App次数</span><br><span class="line">每日人均启动App次数&#x3D;当日所有用户启动APP总次数&#x2F;当日所有人数</span><br><span class="line">针对这种需求，我们在计算结果的时候最好是把这个指标的分子和分母保存起来，这样这份数据后期还有可能被复用，如果直接保存最终的结果，这个数据就没办法复用了。</span><br><span class="line"></span><br><span class="line">实现思路如下：</span><br><span class="line">1：基于dws_user_active_history表，统计当日的数据，根据times字段的值求pv和uv即可</span><br><span class="line">2：将计算的结果保存到app层的app_user_open_app_count表中</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> 接下来是第二个指标：每日APP启动次数分布（1次，2次，3次及以上）</span><br><span class="line">这个指标也需要基于dws_user_active_history表</span><br><span class="line"></span><br><span class="line">实现思路如下：</span><br><span class="line">对这里面的 times字段进行统计，计算 times&#x3D;1的数据条数、times&#x3D;2的数据条数以及times&gt;&#x3D;3的数据条数即可，将最终的结果保存到app层的 app_user_open_app_distrib中即可</span><br></pre></td></tr></table></figure><h5 id="app-2"><a href="#app-2" class="headerlink" title="app"></a>app</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_count(</span><br><span class="line">    pv    <span class="built_in">int</span>,</span><br><span class="line">uv    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_count'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_count <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>(times) <span class="keyword">as</span> pv,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_open_app_distrib(</span><br><span class="line">    ts_1    <span class="built_in">int</span>,</span><br><span class="line">ts_2    <span class="built_in">int</span>,</span><br><span class="line">ts_3_m    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_open_app_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_open_app_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">1</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_1,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times = <span class="number">2</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_2,</span><br><span class="line"><span class="keyword">sum</span>( <span class="keyword">case</span> <span class="keyword">when</span> times &gt;= <span class="number">3</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) ts_3_m</span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_active_history</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span>;</span><br></pre></td></tr></table></figure><h5 id="开发脚本"><a href="#开发脚本" class="headerlink" title="开发脚本"></a>开发脚本</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_4.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_4.sh</span><br></pre></td></tr></table></figure><h6 id="app-mall-init-table-4-sh"><a href="#app-mall-init-table-4-sh" class="headerlink" title="app_mall_init_table_4.sh"></a>app_mall_init_table_4.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求四：每日启动APP次数相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_open_app_count(</span><br><span class="line">    pv    int,</span><br><span class="line">uv    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_open_app_count';</span><br><span class="line"> </span><br><span class="line">create external table if not exists app_mall.app_user_open_app_distrib(</span><br><span class="line">    ts_1    int,</span><br><span class="line">ts_2    int,</span><br><span class="line">ts_3_m    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_open_app_distrib';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-4-sh"><a href="#app-mall-add-partition-4-sh" class="headerlink" title="app_mall_add_partition_4.sh"></a>app_mall_add_partition_4.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求四：每日启动APP次数相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_open_app_count partition(dt='$&#123;dt&#125;') select</span><br><span class="line">sum(times) as pv,</span><br><span class="line">count(*) as uv</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_open_app_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">sum( case when times = 1 then 1 else 0 end) ts_1,</span><br><span class="line">sum( case when times = 2 then 1 else 0 end) ts_2,</span><br><span class="line">sum( case when times &gt;= 3 then 1 else 0 end) ts_3_m</span><br><span class="line">from dws_mall.dws_user_active_history</span><br><span class="line">where dt = '$&#123;dt&#125;';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051755754.png" alt="image-20230405175545120"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304051756799.png" alt="image-20230405175646747"></p><h4 id="需求五：操作系统活跃用户相关指标"><a href="#需求五：操作系统活跃用户相关指标" class="headerlink" title="需求五：操作系统活跃用户相关指标]"></a>需求五：操作系统活跃用户相关指标]</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">这个需求是统计一下我们产品的目前主要用户群体是使用什么类型的操作系统</span><br><span class="line">因为我们产品的app是有Android端和ios端的</span><br><span class="line">如果我们的用户80%以上使用的都是Android，那么我们肯定要针对Android端的 APP做更多的优化支持，这样可以保证大部分用户的使用体验。</span><br><span class="line"></span><br><span class="line">还有就是获取用户使用的手机型号，分辨率信息，这样可以更好的做适配。</span><br><span class="line">针对这个需求我们主要统计以下指标：</span><br><span class="line">1：操作系统活跃用户分布（安卓、IOS）</span><br><span class="line">2：安卓系统版本活跃用户分布</span><br><span class="line">3：IOS系统版本活跃用户分布</span><br><span class="line">4：设备品牌活跃用户分布</span><br><span class="line">5：设备型号活跃用户分布</span><br><span class="line">6：网络类型活跃用户分布</span><br><span class="line">针对这些指标统一分析，其实可以看出来，他们是有相似之处的。都是基于用户使用app时上报的数据相关的一些指标</span><br><span class="line">其实主要就是针对dwd_user_active表中的这些相关维度字段进行分组聚合统计</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">实现思路如下：</span><br><span class="line">1：利用咱们前面讲的维度建模的思想，使用星型模型，基于dwd_user_active表，在外层构建对应的维度表。</span><br><span class="line">2：在dws层基于以上6种维度创建对应的维度聚合表，按天建分区</span><br><span class="line">对应的表名为：</span><br><span class="line">dws_user_platform_distrib</span><br><span class="line">dws_user_andriod_osver_distrib</span><br><span class="line">dws_user_ios_osver_distrib</span><br><span class="line">dws_user_brand_distrib</span><br><span class="line">dws_user_model_distrib</span><br><span class="line">dws_user_net_distrib</span><br><span class="line">3：基于dws层的轻度聚合数据进行全局聚合，因为这些指标统计的时候需要统计所有数据，只统计某一天的没有多大意义，将最终聚合的结果保存到app层，这里面的表就是普通外部表了，里面也不需要日期字段，每天重新生成表里面的数据即可</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">注意了，咱们前面保存的有每天聚合的数据，如果后期有需求要统计一段时间内的这些维度的指标，那也很简单，直接基于dws层的表进行统计即可，从这也体现出来了数据分层的好处。</span><br><span class="line"></span><br><span class="line">在app层对应的表名为操作系统活跃用户分布（安卓、IOS）：app_user_platform_distrib</span><br><span class="line">安卓系统版本活跃用户分布：app_user_andriod_osver_distrib</span><br><span class="line">IOS 系统版本活跃用户分布：app_user_ios_osver_distrib</span><br><span class="line">设备品牌活跃用户分布：app_user_brand_distrib</span><br><span class="line">设备型号活跃用户分布：app_user_model_distrib</span><br><span class="line">网络类型活跃用户分布：app_user_net_distrib</span><br></pre></td></tr></table></figure><h5 id="dws-2"><a href="#dws-2" class="headerlink" title="dws"></a>dws</h5><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052112561.png" alt="image-20230405211242014"></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_platform_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_platform_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_android_osver_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">osver <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_ios_osver_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">osver <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_brand_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">brand <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> brand;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_model_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_model_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">model</span> <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">model</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_user_net_distrib <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> net</span><br><span class="line"><span class="keyword">when</span> <span class="number">0</span> <span class="keyword">then</span> <span class="string">'未知'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'WIFI'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'2G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">3</span> <span class="keyword">then</span> <span class="string">'3G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">4</span> <span class="keyword">then</span> <span class="string">'4G'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">5</span> <span class="keyword">then</span> <span class="string">'5G'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_user_active</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> net;</span><br></pre></td></tr></table></figure><h6 id="开发脚本-1"><a href="#开发脚本-1" class="headerlink" title="开发脚本"></a>开发脚本</h6><h6 id="dws-mall-init-table-5-sh"><a href="#dws-mall-init-table-5-sh" class="headerlink" title="dws_mall_init_table_5.sh"></a>dws_mall_init_table_5.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求五：操作系统活跃用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_platform_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_platform_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_android_osver_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_android_osver_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_ios_osver_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_ios_osver_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_brand_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_brand_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_model_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_model_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_user_net_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/user_net_distrib';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-5-sh"><a href="#dws-mall-add-partition-5-sh" class="headerlink" title="dws_mall_add_partition_5.sh"></a>dws_mall_add_partition_5.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求五：操作系统活跃用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_user_platform_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">case platform</span><br><span class="line">when 1 then 'android'</span><br><span class="line">when 2 then 'ios'</span><br><span class="line">end ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform in (1,2)</span><br><span class="line">group by platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_android_osver_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">osver as ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform = 1</span><br><span class="line">group by osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_ios_osver_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">osver as ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform = 2</span><br><span class="line">group by osver;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_brand_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">brand as ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by brand;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_model_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">model as ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by model;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table dws_mall.dws_user_net_distrib partition(dt='$&#123;dt&#125;') select</span><br><span class="line">case net</span><br><span class="line">when 0 then '未知'</span><br><span class="line">when 1 then 'WIFI'</span><br><span class="line">when 2 then '2G'</span><br><span class="line">when 3 then '3G'</span><br><span class="line">when 4 then '4G'</span><br><span class="line">when 5 then '5G'</span><br><span class="line">end ty,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_user_active</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by net;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052144416.png" alt="image-20230405214415743"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052144560.png" alt="image-20230405214454630"></p><h5 id="app-3"><a href="#app-3" class="headerlink" title="app"></a>app</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_platform_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_platform_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_platform_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_platform_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_android_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_android_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_android_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_android_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_ios_osver_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_ios_osver_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_ios_osver_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_ios_osver_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_brand_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_brand_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_brand_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_brand_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_model_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_model_distrib'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_model_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_model_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_user_net_distrib(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/user_net_distrib'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_user_net_distrib <span class="keyword">select</span></span><br><span class="line">ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_user_net_distrib</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> ty;</span><br></pre></td></tr></table></figure><h6 id="开发脚本-2"><a href="#开发脚本-2" class="headerlink" title="开发脚本"></a>开发脚本</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 app层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">app_mall_init_table_5.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">app_mall_add_partition_5.sh</span><br></pre></td></tr></table></figure><h6 id="app-mall-init-table-5-sh"><a href="#app-mall-init-table-5-sh" class="headerlink" title="app_mall_init_table_5.sh"></a>app_mall_init_table_5.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求五：操作系统活跃用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_platform_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_platform_distrib';</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_android_osver_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_android_osver_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_ios_osver_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_ios_osver_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_brand_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_brand_distrib';</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_model_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_model_distrib';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_user_net_distrib(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/user_net_distrib';</span><br><span class="line"></span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-5-sh"><a href="#app-mall-add-partition-5-sh" class="headerlink" title="app_mall_add_partition_5.sh"></a>app_mall_add_partition_5.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求五：操作系统活跃用户相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_user_platform_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_platform_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_android_osver_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_android_osver_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_ios_osver_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_ios_osver_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_brand_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_brand_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_model_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_model_distrib</span><br><span class="line">group by ty;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_user_net_distrib select</span><br><span class="line">ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_user_net_distrib</span><br><span class="line">group by ty;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052145058.png" alt="image-20230405214547266"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052146709.png" alt="image-20230405214646268"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052147527.png" alt="image-20230405214755005"></p><h4 id="需求六：APP-崩溃相关指标"><a href="#需求六：APP-崩溃相关指标" class="headerlink" title="需求六：APP 崩溃相关指标"></a>需求六：APP 崩溃相关指标</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">这个需求是统计在不同平台系统不同版本下APP崩溃的情况，统计这个数据可以方便排查定位问题，如果发现某一个版本的APP崩溃频繁，则需要及时修复问题，推送新版本，提升用户体验。</span><br><span class="line">针对这个需求主要统计下面几个指标</span><br><span class="line">1：每日操作系统崩溃总计（安卓、IOS）</span><br><span class="line">2：每日安卓系统-不同APP版本崩溃量</span><br><span class="line">3：每日IOS系统-不同APP版本崩溃量</span><br><span class="line"></span><br><span class="line">这里面这三个指标是有关联的，第一个是总的统计，第二个和第三个是不同维度的统计</span><br><span class="line">实现思路：</span><br><span class="line">针对第一个指标，使用dwd_app_close表中的数据，根据platform进行分组统计即可</span><br><span class="line"></span><br><span class="line">但是注意：第二个指标和第三个指标，也需要根据不同的platform进行统计，但是又多了一个操作系统的维度，如果按照我们刚才的分析，直接基于platform进行分组的话，针对后面两个指标还需要重新计算中间表，没有体现出来数据仓库的好处。</span><br><span class="line"></span><br><span class="line">所以我们可以这样做：</span><br><span class="line">针对dwd_app_close表中的数据，使用platform和vercode进行分组，做轻度聚合，将数据保存到dws层的dws_app_close_platform_vercode表中</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于dws_app_close_platform_vercode表中的数据就可以计算出来这三个指标了。</span><br><span class="line">这三个指标的结果分别保存到app层的以下这些表中：</span><br><span class="line">每日操作系统崩溃总计（安卓、IOS）：app_app_close_platform_all</span><br><span class="line">每日安卓系统-不同 APP 版本崩溃量 app_app_close_android_vercode</span><br><span class="line">每日苹果系统-不同 APP 版本崩溃量 app_app_close_ios_vercode</span><br></pre></td></tr></table></figure><h5 id="dws-层"><a href="#dws-层" class="headerlink" title="dws 层"></a>dws 层</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> dws_mall.dws_app_close_platform_vercode(</span><br><span class="line">    platform    <span class="keyword">string</span>,</span><br><span class="line">vercode    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/dws/app_close_platform_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_mall.dws_app_close_platform_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">platform,</span><br><span class="line">vercode,</span><br><span class="line"><span class="keyword">count</span>(*) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dwd_mall.dwd_app_close</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform <span class="keyword">in</span> (<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform,vercode;</span><br></pre></td></tr></table></figure><h6 id="开发脚本-3"><a href="#开发脚本-3" class="headerlink" title="开发脚本"></a>开发脚本</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">针对 dws 层抽取脚本</span><br><span class="line">1：表初始化脚本(初始化执行一次)</span><br><span class="line">dws_mall_init_table_6.sh</span><br><span class="line">2：添加分区数据脚本(每天执行一次)</span><br><span class="line">dws_mall_add_partition_6.sh</span><br></pre></td></tr></table></figure><h6 id="dws-mall-init-table-6-sh"><a href="#dws-mall-init-table-6-sh" class="headerlink" title="dws_mall_init_table_6.sh"></a>dws_mall_init_table_6.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求六：APP崩溃相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dws层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists dws_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists dws_mall.dws_app_close_platform_vercode(</span><br><span class="line">    platform    string,</span><br><span class="line">vercode    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/dws/app_close_platform_vercode';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="dws-mall-add-partition-6-sh"><a href="#dws-mall-add-partition-6-sh" class="headerlink" title="dws_mall_add_partition_6.sh"></a>dws_mall_add_partition_6.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求六：APP崩溃相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table dws_mall.dws_app_close_platform_vercode partition(dt='$&#123;dt&#125;') select</span><br><span class="line">platform,</span><br><span class="line">vercode,</span><br><span class="line">count(*) as num</span><br><span class="line">from dwd_mall.dwd_app_close</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform in (1,2)</span><br><span class="line">group by platform,vercode;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052215542.png" alt="image-20230405221537102"></p><h5 id="app层"><a href="#app层" class="headerlink" title="app层"></a>app层</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_platform_all(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_platform_all'</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_platform_all <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line"><span class="keyword">case</span> platform</span><br><span class="line"><span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="string">'android'</span></span><br><span class="line"><span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="string">'ios'</span></span><br><span class="line"><span class="keyword">end</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> platform;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_android_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_android_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_android_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> app_mall.app_app_close_ios_vercode(</span><br><span class="line">    ty    <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">num</span>    <span class="built_in">int</span></span><br><span class="line">)partitioned <span class="keyword">by</span>(dt <span class="keyword">string</span>)</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span>  </span><br><span class="line"> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> location <span class="string">'hdfs://bigdata01:9000/data/app/app_close_ios_vercode'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> app_mall.app_app_close_ios_vercode <span class="keyword">partition</span>(dt=<span class="string">'20260201'</span>) <span class="keyword">select</span></span><br><span class="line">vercode <span class="keyword">as</span> ty,</span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">num</span>) <span class="keyword">as</span> <span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> dws_mall.dws_app_close_platform_vercode</span><br><span class="line"><span class="keyword">where</span> dt = <span class="string">'20260201'</span> <span class="keyword">and</span> platform = <span class="number">2</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vercode;</span><br></pre></td></tr></table></figure><h6 id="开发脚本-4"><a href="#开发脚本-4" class="headerlink" title="开发脚本"></a>开发脚本</h6><h6 id="app-mall-init-table-6-sh"><a href="#app-mall-init-table-6-sh" class="headerlink" title="app_mall_init_table_6.sh"></a>app_mall_init_table_6.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求六：APP崩溃相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> app层数据库和表初始化脚本，只需要执行一次即可</span></span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">create database if not exists app_mall;</span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_app_close_platform_all(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/app_close_platform_all';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_app_close_android_vercode(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/app_close_android_vercode';</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">create external table if not exists app_mall.app_app_close_ios_vercode(</span><br><span class="line">    ty    string,</span><br><span class="line">num    int</span><br><span class="line">)partitioned by(dt string)</span><br><span class="line"> row format delimited  </span><br><span class="line"> fields terminated by '\t'</span><br><span class="line"> location 'hdfs://bigdata01:9000/data/app/app_close_ios_vercode';</span><br><span class="line">"</span><br></pre></td></tr></table></figure><h6 id="app-mall-add-partition-6-sh"><a href="#app-mall-add-partition-6-sh" class="headerlink" title="app_mall_add_partition_6.sh"></a>app_mall_add_partition_6.sh</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 需求六：APP崩溃相关指标</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每天凌晨执行一次</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认获取昨天的日期，也支持传参指定一个日期</span></span><br><span class="line">if [ "z$1" = "z" ]</span><br><span class="line">then</span><br><span class="line">dt=`date +%Y%m%d --date="1 days ago"`</span><br><span class="line">else</span><br><span class="line">dt=$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">hive -e "</span><br><span class="line">insert overwrite table app_mall.app_app_close_platform_all partition(dt='$&#123;dt&#125;') select</span><br><span class="line">case platform</span><br><span class="line">when 1 then 'android'</span><br><span class="line">when 2 then 'ios'</span><br><span class="line">end ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_app_close_platform_vercode</span><br><span class="line">where dt = '$&#123;dt&#125;'</span><br><span class="line">group by platform;</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_app_close_android_vercode partition(dt='$&#123;dt&#125;') select</span><br><span class="line">vercode as ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_app_close_platform_vercode</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform = 1</span><br><span class="line">group by vercode;</span><br><span class="line"></span><br><span class="line">insert overwrite table app_mall.app_app_close_ios_vercode partition(dt='$&#123;dt&#125;') select</span><br><span class="line">vercode as ty,</span><br><span class="line">sum(num) as num</span><br><span class="line">from dws_mall.dws_app_close_platform_vercode</span><br><span class="line">where dt = '$&#123;dt&#125;' and platform = 2</span><br><span class="line">group by vercode;</span><br><span class="line">"</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052216787.png" alt="image-20230405221619712"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052219532.png" alt="image-20230405221957019"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052221901.png" alt="image-20230405222129463"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052221386.png" alt="image-20230405222138929"></p><h2 id="用户行为数据数仓总结"><a href="#用户行为数据数仓总结" class="headerlink" title="用户行为数据数仓总结"></a>用户行为数据数仓总结</h2><h3 id="数据库和表梳理"><a href="#数据库和表梳理" class="headerlink" title="数据库和表梳理"></a>数据库和表梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052224849.png" alt="image-20230405222432071"></p><h3 id="任务脚本梳理"><a href="#任务脚本梳理" class="headerlink" title="任务脚本梳理"></a>任务脚本梳理</h3><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304052226522.png" alt="image-20230405222643594"></p><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库之用户行为数仓3</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933.html</id>
    <published>2023-03-30T13:22:30.000Z</published>
    <updated>2023-04-15T02:44:52.262Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="电商数据仓库之用户行为数仓3-数据生成与采集"><a href="#电商数据仓库之用户行为数仓3-数据生成与采集" class="headerlink" title="电商数据仓库之用户行为数仓3-数据生成与采集"></a>电商数据仓库之用户行为数仓3-数据生成与采集</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">接下来我们就来开发第一个模块：数据采集模块</span><br><span class="line">这一块内容在开发的时候，我们需要先生成测试数据，一份是服务端数据，还有一份是客户端数据</span><br></pre></td></tr></table></figure><h2 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h2><h3 id="【客户端数据】用户行为数据"><a href="#【客户端数据】用户行为数据" class="headerlink" title="【客户端数据】用户行为数据"></a>【客户端数据】用户行为数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先我们模拟生成用户行为数据，也就是客户端数据，主要包含用户打开APP、点击、浏览等行为数据</span><br><span class="line">用户行为数据：通过埋点上报，后端日志服务器(http)负责接收数据</span><br><span class="line">埋点上报数据基本格式：</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;uid&quot;：1001, &#x2F;&#x2F;用户ID</span><br><span class="line">&quot;xaid&quot;：&quot;ab25617-c38910-m2991&quot;, &#x2F;&#x2F;手机设备ID</span><br><span class="line">&quot;platform&quot;：2, &#x2F;&#x2F;设备类型, 1:Android-APP, 2:IOS-APP, 3:PC </span><br><span class="line">&quot;ver&quot;：&quot;3.5.10&quot;, &#x2F;&#x2F;大版本号</span><br><span class="line">&quot;vercode&quot;：&quot;35100083&quot;, &#x2F;&#x2F;子版本号</span><br><span class="line">&quot;net&quot;：1, &#x2F;&#x2F;网络类型, 0:未知, 1:WIFI, 2:2G , 3:3G, 4:4G, 5:5G</span><br><span class="line">&quot;brand&quot;：&quot;iPhone&quot;, &#x2F;&#x2F;手机品牌</span><br><span class="line">&quot;model&quot;：&quot;iPhone8&quot;, &#x2F;&#x2F;机型</span><br><span class="line">&quot;display&quot;：&quot;1334x750&quot;, &#x2F;&#x2F;分辨率</span><br><span class="line">&quot;osver&quot;：&quot;ios13.5&quot;, &#x2F;&#x2F;操作系统版本号</span><br><span class="line">&quot;data&quot;：[ &#x2F;&#x2F;用户行为数据</span><br><span class="line">&#123;&quot;act&quot;：1,&quot;acttime&quot;：1592486549819,&quot;ad_status&quot;：1,&quot;loading_time&quot;:100&#125;,</span><br><span class="line">&#123;&quot;act&quot;：2,&quot;acttime&quot;：1592486549819,&quot;goods_id&quot;：&quot;2881992&quot;&#125;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这个json串中的data是一个json数组，它里面包含了多种用户行为数据。</span><br><span class="line">json串中的其它字段属于公共字段</span><br><span class="line"></span><br><span class="line">注意：考虑到性能，一般数据上报都是批量上报，假设间隔10秒上报一次，这种数据延迟是可以接受的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">所以在每次上报的时候，公共字段只需要报一份就行，把不同的用户行为相关的业务字段放到data数组中，这样可以避免上报大量的重复数据，影响数据上报性能，我们只需要在后期解析的时候，把公共字段和data数组总的每一条业务字段进行拼装，就可以获取到每一个用户行为的所有字段信息。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">act代表具体的用户行为，在这列出来几种</span><br><span class="line">act&#x3D;1：打开APP</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型 </span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">ad_status 开屏广告展示状态, 1:成功 2:失败</span><br><span class="line">loading_time 开屏广告加载耗时(单位毫秒)</span><br><span class="line"></span><br><span class="line">act&#x3D;2：点击商品</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">goods_id 商品ID</span><br><span class="line">location 商品展示顺序：在列表页中排第几位，从0开始</span><br><span class="line"></span><br><span class="line">act&#x3D;3：商品详情页</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">goods_id 商品ID</span><br><span class="line">stay_time 页面停留时长(单位毫秒)</span><br><span class="line">loading_time 页面加载耗时(单位毫秒)</span><br><span class="line"></span><br><span class="line">act&#x3D;4：商品列表页</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br><span class="line">loading_time 页面加载耗时(单位毫秒)</span><br><span class="line">loading_type 加载类型：1:读缓存 2:请求接口</span><br><span class="line">goods_num 列表页加载商品数量</span><br><span class="line"></span><br><span class="line">act&#x3D;5：app崩溃数据</span><br><span class="line">属性 含义</span><br><span class="line">act 用户行为类型</span><br><span class="line">acttime 数据产生时间(时间戳)</span><br></pre></td></tr></table></figure><h4 id="生成用户行为测试数据"><a href="#生成用户行为测试数据" class="headerlink" title="生成用户行为测试数据"></a>生成用户行为测试数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里实现不了，课程提供的用户行为生成接口，需要提供uid、课程订单，然后执行提前编写好的测试数据生成代码。</span><br></pre></td></tr></table></figure><h4 id="部署日志采集服务"><a href="#部署日志采集服务" class="headerlink" title="部署日志采集服务"></a>部署日志采集服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">部署日志采集服务，模拟埋点上报数据的流程，代码在db_data_warehouse中的data_collect这个子项目中，将这个子项目打成jar包，部署到bigdata04服务器中，并且启动此HTTP服务。</span><br><span class="line">对data_collect执行打包操作，在cmd命令下执行 mvn clean package -DskipTests</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">D:\IdeaProjects\db_data_warehouse\data_collect&gt;mvn clean package -DskipTests</span><br><span class="line">[INFO] Scanning for projects...</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br><span class="line">[INFO] Building data_collect 1.0-SNAPSHOT</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br><span class="line">[INFO] Replacing main artifact with repackaged archive</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ----------------------------------------------------------------------</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在bigdata04的&#x2F;data&#x2F;soft&#x2F;目录下创建data_collect目录</span><br><span class="line"></span><br><span class="line">1 [root@bigdata04 soft]# mkdir data_collect</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">然后把target目录下的 data_collect-1.0-SNAPSHOT.jar 上传到bigdata04的 &#x2F;data&#x2F;soft&#x2F;data_collect</span><br><span class="line">里面</span><br><span class="line">接着就可以启动这个项目了，这个其实就是一个web项目。</span><br><span class="line">为了后面使用方便，我在这里面写一个启动脚本</span><br><span class="line"></span><br><span class="line">[root@bigdata04 data_collect]# vi start.sh</span><br><span class="line">nohup java -jar data_collect-1.0-SNAPSHOT.jar &gt;&gt; nohup.out &amp;</span><br><span class="line">[root@bigdata04 data_collect]# sh start.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">确认是否成功启动</span><br><span class="line">[root@bigdata04 data_collect]# jps -ml</span><br><span class="line">1601 sun.tools.jps.Jps -ml</span><br><span class="line">1563 data_collect-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">获取到的数据格式是这样的：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302339866.png" alt="image-20230330233905222"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302339704.png" alt="image-20230330233917373"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先解析data属性的值，里面包含了多个用户的行为数据</span><br><span class="line">并且每个用户的行为数据中还包含了多种具体的行为操作，因为客户端在上报数据的时候不是产生一条就上报一条，这样效率太低了，一般都会批量上报，所以内层json串中还有一个data参数，data参数的值是一个JSONArray，里面包含一个用户的多种行为数据</span><br><span class="line"></span><br><span class="line">然后通过接口模拟上报数据，data_collect接口接收到数据之后，会对数据进行拆分，将包含了多个用户行为的数据拆开，打平，输出多条日志数据</span><br><span class="line">日志数据格式是这样的：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302343266.png" alt="image-20230330234319650"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">最终的日志数据会保存在data_collect这个日志采集服务所在的机器上，通过log4j记录在&#x2F;data&#x2F;log目录下面。</span><br><span class="line">去确认一下：</span><br><span class="line">[root@bigdata04 log]# ll</span><br><span class="line">total 32</span><br><span class="line">-rw-r--r--. 1 root root 20881 Jun 30 18:00 user_action.log</span><br><span class="line">[root@bigdata04 log]# head -1 user_action.log </span><br><span class="line">&#123;&quot;ver&quot;:&quot;3.4.1&quot;,&quot;display&quot;:&quot;1920x1080&quot;,&quot;osver&quot;:&quot;7.1.1&quot;,&quot;platform&quot;:1,&quot;uid&quot;:&quot;1000</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302347512.png" alt="image-20230330234726391"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">到这为止，用户行为数据就生成好了。</span><br></pre></td></tr></table></figure><h3 id="【服务端数据】商品订单相关数据"><a href="#【服务端数据】商品订单相关数据" class="headerlink" title="【服务端数据】商品订单相关数据"></a>【服务端数据】商品订单相关数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">接下来需要生成商品订单相关数据，这些数据都是存储在mysql中的</span><br><span class="line">注意：MySQL在这里我使用的版本是8.x</span><br><span class="line"></span><br><span class="line">相关表名为：</span><br><span class="line">订单表：user_order </span><br><span class="line">商品信息表：goods_info</span><br><span class="line">订单商品表：order_item</span><br><span class="line">商品类目码表：category_code</span><br><span class="line">订单收货表：order_delivery </span><br><span class="line">支付流水表：payment_flow</span><br><span class="line">用户收货地址表：user_addr</span><br><span class="line">用户信息表：user</span><br><span class="line">用户扩展表：user_extend</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302357855.png" alt="image-20230330235704680"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">powerdesigner这个软件设计的</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">首先在MySQL中初始化数据库和表。</span><br><span class="line">使用这个脚本进行初始化： init_mysql_tables.sql</span><br><span class="line">初始化成功之后的效果如下：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310002439.png" alt="image-20230331000219996"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">接下来需要向表中初始化数据。</span><br><span class="line">使用generate_data项目中的这个类：GenerateGoodsOrderData</span><br><span class="line"></span><br><span class="line">在具体执行之前需要先修改GenerateGoodsOrderData中的几个参数值</span><br><span class="line">(1)code的值</span><br><span class="line">(2)date的值</span><br><span class="line">(3)user_num的值</span><br><span class="line">(4)order_num的值</span><br><span class="line">(5)修改项目的resources目录下的db.properties文件</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面就可以执行GenerateGoodsOrderData向MySQL中初始化数据了。</span><br></pre></td></tr></table></figure><h2 id="采集数据"><a href="#采集数据" class="headerlink" title="采集数据"></a>采集数据</h2><h3 id="采集用户行为数据"><a href="#采集用户行为数据" class="headerlink" title="采集用户行为数据"></a>采集用户行为数据</h3><h4 id="配置Flume的Agent"><a href="#配置Flume的Agent" class="headerlink" title="配置Flume的Agent"></a>配置Flume的Agent</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据接收到以后，需要使用flume采集数据，按照act值的不同，将数据分目录存储</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">flume Agent配置内容如下：</span><br><span class="line">useraction-to-hdfs.conf</span><br><span class="line"></span><br><span class="line"># agent的名称是a1</span><br><span class="line"># 指定source组件、channel组件和Sink组件的名称</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line"># 配置source组件</span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;data&#x2F;log&#x2F;user_action.log</span><br><span class="line"># 配置拦截器</span><br><span class="line">a1.sources.r1.interceptors &#x3D; i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type &#x3D; regex_extractor</span><br><span class="line">a1.sources.r1.interceptors.i1.regex &#x3D; &quot;act&quot;:(\\d)</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers &#x3D; s1</span><br><span class="line">a1.sources.r1.interceptors.i1.serializers.s1.name &#x3D; act</span><br><span class="line"># 配置channel组件</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line"># 配置sink组件</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&#x2F;user_action&#x2F;%Y%m%d&#x2F;%&#123;a</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat &#x3D; Text</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3600</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 134217728</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 0</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#增加文件前缀和后缀</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; data</span><br><span class="line">a1.sinks.k1.hdfs.fileSuffix &#x3D; .log</span><br><span class="line"># 把组件连接起来</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br></pre></td></tr></table></figure><h4 id="开始采集数据"><a href="#开始采集数据" class="headerlink" title="开始采集数据"></a>开始采集数据</h4><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310100507.png" alt="image-20230331010029253"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303310101359.png" alt="image-20230331010115320"></p><h3 id="采集商品订单相关数据"><a href="#采集商品订单相关数据" class="headerlink" title="采集商品订单相关数据"></a>采集商品订单相关数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面我们需要将商品订单数据采集到HDFS里面，咱们前面分析过，在这里针对关系型数据库数据的采集我们使用Sqoop</span><br><span class="line">使用sqoop的导入功能，将MySQL中的数据导入到HDFS上面</span><br><span class="line">那首先我们来看一下Sqoop的使用，因为Sqoop主要是一个工具，所以我们就快速的学习一下。</span><br></pre></td></tr></table></figure><h4 id="Sqoop的使用"><a href="#Sqoop的使用" class="headerlink" title="Sqoop的使用"></a>Sqoop的使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下面我们需要将商品订单数据采集到HDFS里面，咱们前面分析过，在这里针对关系型数据库数据的采集</span><br><span class="line"></span><br><span class="line">我们使用Sqoop</span><br><span class="line">使用sqoop的导入功能，将MySQL中的数据导入到HDFS上面</span><br><span class="line">那首先我们来看一下Sqoop的使用，因为Sqoop主要是一个工具，所以我们就快速的学习一下。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Sqoop目前有两大版本，Sqoop1和Sqoop2，这两个版本都是一直在维护者的，所以使用哪个版本都可以。</span><br><span class="line">这两个版本我都用过，还是感觉Sqoop1用起来比较方便，使用Sqoop1的时候可以将具体的命令全部都写到脚本中，这样看起来是比较清晰的，但是有一个弊端，就是在操作MySQL的时候，MySQL数据库的用户名和密码会明文暴露在这些脚本中，不过一般也没有什么问题，因为在访问生产环境下的MySQL的时候，是需要申请权限的，就算你知道了MySQL的用户名和密码，但是你压根无法访问MySQL的那台机器，所以这样也是安全的，只要运维那边权限控制到位了就没问题。</span><br><span class="line">sqoop2中引入了sqoop server(服务)，集中管理connector(连接)，而sqoop1只是客户端工具。</span><br><span class="line">相对来说，Sqoop1更加简洁，轻量级。</span><br><span class="line">Sqoop1的最后更新时间是2018年</span><br><span class="line">Sqoop2的最后更新时间是2016年</span><br><span class="line">Sqoop2我之前在使用的时候发现里面bug还是比较多的，相对来说Sqoop1更加稳定一些。</span><br><span class="line">所以在这我们采用Sqoop1。</span><br><span class="line">想要使用Sqoop1，先去官网下载安装包</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：最终下载的sqoop1.4.7的安装是这个sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这个安装包表示里面包含了hadoop-2.6.0的依赖，我们目前使用的是hadoop3.2.0，不过是可以兼容的，这样就没有必要重新编辑sqoop了。</span><br><span class="line">Sqoop的安装部署很简单，因为Sqoop1只是一个客户端工具，直接解压，修改一下配置文件就行，不需要启动任何进程</span><br><span class="line">Sqoop在执行的时候底层会生成MapReduce任务，所以Sqoop需要部署在Hadoop客户端机器上，因为它是依赖于Hadoop的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">我们在bigdata04机器上安装部署Sqoop</span><br><span class="line">1：把Sqoop的安装包上传到bigdata04机器的&#x2F;data&#x2F;soft目录下</span><br><span class="line">2：解压</span><br><span class="line"></span><br><span class="line">3：修改配置文件的名称</span><br><span class="line">[root@bigdata04 soft]# cd sqoop-1.4.7.bin__hadoop-2.6.0&#x2F;conf</span><br><span class="line">[root@bigdata04 conf]# mv sqoop-env-template.sh sqoop-env.sh</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">4：配置SQOOP_HOME环境变量</span><br><span class="line">[root@bigdata04 conf]# vi &#x2F;etc&#x2F;profile</span><br><span class="line">...</span><br><span class="line">export SQOOP_HOME&#x3D;&#x2F;data&#x2F;soft&#x2F;sqoop-1.4.7.bin__hadoop-2.6.0</span><br><span class="line">export PATH&#x3D;.......$SQOOP_HOME&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">5：将MySQL 8的驱动jar包，添加到SQOOP_HOME的lib目录下，因为我们需要使用Sqoop操作MySQL</span><br><span class="line">查看验证一下是否成功添加MySQL的驱动jar包</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：使用hadoop 3.2.0版本的时候，需要在SQOOP_HOME的lib目录下增加commons-lang2.6.jar</span><br><span class="line">查看验证一下是否成功添加commons-lang-2.6.jar</span><br><span class="line"></span><br><span class="line">[root@bigdata04 sqoop-1.4.7.bin__hadoop-2.6.0]# ll lib&#x2F;commons-lang-2.6.jar </span><br><span class="line">-rw-r--r--. 1 root root 284220 Nov 10 2015 lib&#x2F;commons-lang-2.6.jar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">6：开放MySQL远程访问权限【开放权限以后集群中的机器才可以连接windows上的MySQL服务,否则只能在windows本地访问】</span><br><span class="line">注意：我的MySQL的用户名为root，密码为admin，大家在执行下面命令的时候需要对应替换为自己的MySQL的真实用户名和密码。</span><br><span class="line"></span><br><span class="line">C:\Users\yehua&gt;mysql -uroot -padmin</span><br><span class="line">mysql&gt; USE mysql;</span><br><span class="line">mysql&gt; CREATE USER &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;admin&#39;;</span><br><span class="line">mysql&gt; GRANT ALL ON *.* TO &#39;root&#39;@&#39;%&#39;;</span><br><span class="line">mysql&gt; ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;admin&#39;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">到此为止，Sqoop相关的配置全部搞定。</span><br><span class="line">下面我们就来分析一下Sqoop中的两大功能。</span><br><span class="line">导入数据sqoop-import：从MySQL导入HDFS</span><br><span class="line">导出数据sqoop-export：从HDFS导出MySQL</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">来看一下Sqoop的文档</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041526800.png" alt="image-20230404152619779"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041526805.png" alt="image-20230404152632314"></p><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041527063.png" alt="image-20230404152729853"></p><h5 id="通用参数"><a href="#通用参数" class="headerlink" title="通用参数"></a>通用参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">整理了文档中一些参数的解释：</span><br><span class="line">Sqoop通用参数：</span><br><span class="line"></span><br><span class="line">选项 含义说明</span><br><span class="line">--connect &lt;jdbc-uri&gt; 指定JDBC连接字符串</span><br><span class="line">--connection-manager &lt;class-name&gt; 指定要使用的连接管理器类</span><br><span class="line">--driver &lt;class-name&gt; 指定要使用的JDBC驱动类</span><br><span class="line">--hadoop-mapred-home &lt;dir&gt; 指定$HADOOP_MAPRED_HOME路径</span><br><span class="line">--help 万能帮助</span><br><span class="line">--password-file 设置用于存放认证的密码信息文件的路径</span><br><span class="line">-P 从控制台读取输入的密码</span><br><span class="line">--password &lt;password&gt; 设置认证密码</span><br><span class="line">--username &lt;username&gt; 设置认证用户名</span><br><span class="line">--verbose 打印详细的运行信息</span><br><span class="line">--connection-param-file &lt;filename&gt; 可选，指定存储数据库连接参数的属性文件</span><br></pre></td></tr></table></figure><h5 id="导入参数"><a href="#导入参数" class="headerlink" title="导入参数"></a>导入参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">导入功能相关参数</span><br><span class="line"></span><br><span class="line">选项 含义说明</span><br><span class="line">--append 将数据追加到HDFS上一个已存在的数据集上</span><br><span class="line">--as-avrodatafile 将数据导入到Avro数据文件</span><br><span class="line">--as-sequencefile 将数据导入到SequenceFile</span><br><span class="line">--as-textfile 将数据导入到普通文本文件（默认）</span><br><span class="line">--boundary-query &lt;statement&gt; 边界查询，用于创建分片（InputSplit）</span><br><span class="line">--columns &lt;col,col,col…&gt; 从表中导出指定的一组列的数据</span><br><span class="line">--delete-target-dir 如果指定目录存在，则先删除掉</span><br><span class="line">--direct 使用直接导入模式（优化导入速度）</span><br><span class="line">--direct-split-size &lt;n&gt; 分割输入stream的字节大小（在直接导入模式下）</span><br><span class="line">--fetch-size &lt;n&gt; 从数据库中批量读取记录数</span><br><span class="line">--inline-lob-limit &lt;n&gt; 设置内联的LOB对象的大小</span><br><span class="line">-m,--num-mappers &lt;n&gt; 使用n个map任务并行导入数据</span><br><span class="line">-e,--query &lt;statement&gt; 导入的查询语句</span><br><span class="line">--split-by &lt;column-name&gt; 指定按照哪个列去分割数据</span><br><span class="line">--table &lt;table-name&gt; 导入的源表表名</span><br><span class="line">--target-dir &lt;dir&gt; 导入HDFS的目标路径</span><br><span class="line">--warehouse-dir &lt;dir&gt; HDFS存放表的根路径</span><br><span class="line">--where &lt;where clause&gt; 指定导出时所使用的查询条件</span><br><span class="line">-z,--compress 启用压缩</span><br><span class="line">--compression-codec &lt;c&gt; 指定Hadoop的codec方式（默认gzip）</span><br><span class="line">--null-string &lt;null-string&gt; 如果指定列为字符串类型，使用指定字符串替换值为</span><br><span class="line">--null-non-string &lt;null-string&gt; 如果指定列为非字符串类型，使用指定字符串替换值</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">下面来看一下数据导入功能：</span><br><span class="line">数据导入可以分为全表导入和查询导入</span><br><span class="line">(1)全表导入：直接把一个表中的所有数据全部导入到HDFS里面</span><br><span class="line">先在MySQL中创建一个数据库和表</span><br><span class="line">C:\Users\yehua&gt;mysql -uroot -padmin</span><br><span class="line">mysql&gt; create database imooc;</span><br><span class="line">Query OK, 1 row affected (0.09 sec)</span><br><span class="line">mysql&gt; use imooc;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; create table user(id int(10),name varchar(64));</span><br><span class="line">Query OK, 0 rows affected (0.60 sec)</span><br><span class="line">mysql&gt; insert into table user(id,name) values(1,&#39;jack&#39;);</span><br><span class="line">Query OK, 1 row affected (0.16 sec)</span><br><span class="line">mysql&gt; insert into table user(id,name) values(2,&#39;tom&#39;);</span><br><span class="line">Query OK, 1 row affected (0.08 sec)</span><br><span class="line">mysql&gt; insert into table user(id,name) values(3,&#39;mike&#39;);</span><br><span class="line">Query OK, 1 row affected (0.05 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">使用Sqoop将imooc.user表中的数据导入到HDFS中</span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table user \</span><br><span class="line">--target-dir &#x2F;out1 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#39;\t&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：如果表中没有主键则会报错(因为mapper数默认是4，需要分4个Task。但是info表又没有主键，MapReduce不知道以哪个字段为准来分Task。)</span><br><span class="line">解决办法有三种：</span><br><span class="line">可以选择在表中设置主键，默认根据主键字段分task</span><br><span class="line">使用–num-mappers 1 ，表示将map任务个数设置为1，sqoop默认是4</span><br><span class="line">使用–split-by ，后面跟上一个数字类型的列，会根据这个列分task</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(2)查询导入：使用sql语句查询表中满足条件的数据导入到HDFS里面</span><br><span class="line">注意：在使用–query指定sql的时候，则必须包含$CONDITIONS</span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--target-dir &#x2F;out2 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--query &#39;select id,name from user where id &gt;1 and $CONDITIONS;&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">注意：–query和–table不能同时指定</span><br><span class="line">问题：sqoop在导入数据的时候针对空值如何处理？</span><br><span class="line">默认情况下MySQL中的null值(无论字段类型是字符串类型还是数字类型)，使用Sqoop导入到HDFS文件中之后，都会显示为字符串null。</span><br><span class="line">针对字符串null类型：通过 --null-string &#39;*&#39; 来指定，单引号中指定一个字符即可，这个字符不能是--，因为 -- 是保留关键字</span><br><span class="line"></span><br><span class="line">针对非字符串的null类型：通过 --null-non-string &#39;&#x3D;&#39; 来指定，单引号中指定一个字符即可，这个字符不能是 -- ，因为 -- 是保留关键字</span><br><span class="line">这两个参数可以同时设置，这样在导入数据的时候，针对空值字段，会替换为指定的内容。</span><br><span class="line"></span><br><span class="line">例如：可以使用\N ，因为我们把数据导入到HDFS之后，最终是希望在Hive中查询的，Hive中针对NULL值在底层是使用\N存储的。</span><br><span class="line">当然了，我们也可以选择给NULL值指定一个默认的其它字符。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--target-dir &#x2F;out2 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--query &#39;select id,name from user where id &gt;1 and $CONDITIONS;&#39; \</span><br><span class="line">--null-string &#39;\\N&#39; \</span><br><span class="line">--null-non-string &#39;\\N&#39; \</span><br></pre></td></tr></table></figure><h5 id="导出参数"><a href="#导出参数" class="headerlink" title="导出参数"></a>导出参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">--validate &lt;class-name&gt; 启用数据副本验证功能，仅支持单表拷贝，可以</span><br><span class="line">--validation-threshold &lt;class-name&gt; 指定验证门限所使用的类</span><br><span class="line">--direct 使用直接导出模式（优化速度）</span><br><span class="line">--export-dir &lt;dir&gt; 导出过程中HDFS源路径</span><br><span class="line">--m,--num-mappers &lt;n&gt; 使用n个map任务并行导出</span><br><span class="line">--table &lt;table-name&gt; 导出的目的表名称</span><br><span class="line">--call &lt;stored-proc-name&gt; 导出数据调用的指定存储过程名</span><br><span class="line">--update-key &lt;col-name&gt; 更新参考的列名称，多个列名使用逗号分隔</span><br><span class="line">--update-mode &lt;mode&gt; 指定更新策略，包括：updateonly（默认）、</span><br><span class="line">--input-null-string &lt;null-string&gt; 使用指定字符串，替换字符串类型值为null的列</span><br><span class="line">--input-null-non-string &lt;null-string&gt; 使用指定字符串，替换非字符串类型值为null的</span><br><span class="line">--staging-table &lt;staging-table-name&gt; 在数据导出到数据库之前，数据临时存放的表名</span><br><span class="line">--clear-staging-table 清除工作区中临时存放的数据</span><br><span class="line">--batch 使用批量模式导出</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">接下来我们来看一下Sqoop的导出功能</span><br><span class="line">从HDFS导出到MySQL，将刚才导入到HDFS中的数据再导出来。</span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table user2 \</span><br><span class="line">--export-dir &#x2F;out2 \</span><br><span class="line">--input-fields-terminated-by &#39;\t&#39;</span><br><span class="line"></span><br><span class="line">注意：这里 --table 指定的表名需要提前创建，sqoop不会自动创建此表。</span><br><span class="line"></span><br><span class="line">C:\Users\yehua&gt;mysql -uroot -padmin</span><br><span class="line">mysql&gt; use imooc;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; create table user2(id int(10),name varchar(64));</span><br><span class="line">Query OK, 0 rows affected (0.24 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">验证结果，查询MySQL中的数据</span><br><span class="line">mysql&gt; select * from user2;</span><br><span class="line">+------+------+</span><br><span class="line">| id | name |</span><br><span class="line">+------+------+</span><br><span class="line">| 3 | mike |</span><br><span class="line">| 2 | tom |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">在导出的时候可以实现插入和更新功能</span><br><span class="line">如果存在就更新，不存在就插入</span><br><span class="line"></span><br><span class="line">注意：此时表中必须有一个主键字段</span><br><span class="line"></span><br><span class="line">将user2中的id字段设置为主键，</span><br><span class="line">然后修改user2中id为2那条数据的name字段的值为imooc</span><br><span class="line">删除id为3的那条数据</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041552875.png" alt="image-20230404155237504"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">修改之后的user2表中的数据如下：</span><br><span class="line">mysql&gt; select * from user2;</span><br><span class="line">+----+-------+</span><br><span class="line">| id | name |</span><br><span class="line">+----+-------+</span><br><span class="line">| 2 | imooc |</span><br><span class="line">+----+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">执行sqoop语句</span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table user2 \</span><br><span class="line">--export-dir &#x2F;out2 \</span><br><span class="line">--input-fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--update-key id \</span><br><span class="line">--update-mode allowinsert</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">再验证一下结果，会发现针对已有的数据更新，没有的数据新增。</span><br><span class="line">mysql&gt; select * from user2;</span><br><span class="line">+------+------+</span><br><span class="line">| id | name |</span><br><span class="line">+------+------+</span><br><span class="line">| 3 | mike |</span><br><span class="line">| 2 | tom |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这就是Sqoop的导入和导出功能。</span><br><span class="line">后期我们在使用Sqoop的时候，建议将Sqoop的命名写到shell脚本中，否则使用起来不方便。</span><br><span class="line">[root@bigdata04 soft]# vi sqoop-ex-user.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;imooc?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--table user2 \</span><br><span class="line">--export-dir &#x2F;out2 \</span><br><span class="line">--input-fields-terminated-by &#39;\t&#39;</span><br></pre></td></tr></table></figure><h4 id="数据采集方式"><a href="#数据采集方式" class="headerlink" title="数据采集方式"></a>数据采集方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">全量采集(数据量不大，每天都改变：一天采集一次；数据量不大，几十年都不变：只做一次全量采集)</span><br><span class="line"></span><br><span class="line">增量采集(数据量大，每天采集新增数据) </span><br><span class="line"></span><br><span class="line">hive不能对数据进行修改(比如mysql中的表的订单信息已改变，但hive中不支持修改)-&gt;解决：拉链表</span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933.htm/D:%5CGitHub%5Cmyblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%933%5C202303310122674.png" alt="image-20230331012207048"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意：手机号在采集的时候需要脱敏处理，因为数据进入到数据仓库之后会有很多人使用，为保护用户隐私，最好在采集的时候进行脱敏处理。</span><br><span class="line">所以在采集user和user_addr表中的数据时对手机号进行脱敏。</span><br><span class="line"></span><br><span class="line">18315138177</span><br><span class="line">183xxxxx177</span><br></pre></td></tr></table></figure><h4 id="数据采集脚本开发"><a href="#数据采集脚本开发" class="headerlink" title="数据采集脚本开发"></a>数据采集脚本开发</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">下面就开始进行数据采集，其实就是使用Sqoop实现的数据导入</span><br><span class="line">开发一个通用的sqoop数据采集脚本</span><br><span class="line">在bigdata04机器上创建目录 &#x2F;data&#x2F;soft&#x2F;warehouse_shell_good_order ，针对商品订单相关的脚本全部放在这里面</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 soft]# mkdir warehouse_shell_good_order</span><br><span class="line"></span><br><span class="line">创建脚本 sqoop_collect_data_util.sh</span><br><span class="line"></span><br><span class="line">[root@bigdata04 warehouse_shell_good_order]#vi sqoop_collect_data_util.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 采集MySQL中的数据导入到HDFS中</span><br><span class="line">if [ $# !&#x3D; 2 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;参数异常：sqoop_collect_data_util.sh &lt;sql&gt; &lt;hdfs_path&gt;&quot;</span><br><span class="line">exit 100</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># 数据SQL</span><br><span class="line"># 例如：select id,name from user where id &gt;1</span><br><span class="line">sql&#x3D;$1</span><br><span class="line"></span><br><span class="line"># 导入到HDFS的路径</span><br><span class="line">hdfs_path&#x3D;$2</span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;192.168.182.1:3306&#x2F;mall?serverTimezone&#x3D;UTC \</span><br><span class="line">--username root \</span><br><span class="line">--password admin \</span><br><span class="line">--target-dir &quot;$&#123;hdfs_path&#125;&quot; \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#39;\t&#39; \</span><br><span class="line">--query &quot;$&#123;sql&#125;&quot;&#39; and $CONDITIONS&#39; \</span><br><span class="line">--null-string &#39;\\N&#39; \</span><br><span class="line">--null-non-string &#39;\\N&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：如果在windows中使用notepad++开发shell脚本的时候，需要将此参数设置为UNIX。</span><br><span class="line">这个我们之前在讲shell的时候已经讲过了，在这再重复一遍。</span><br><span class="line">否则在windows中开发的脚本直接上传到linux中执行会报错。</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202304041610733.png" alt="image-20230404161003265"></p><h4 id="开始采集数据-1"><a href="#开始采集数据-1" class="headerlink" title="开始采集数据"></a>开始采集数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">针对全量数据采集和增量数据采集开发不同的脚本</span><br><span class="line">全量数据采集： collect_data_full.sh</span><br></pre></td></tr></table></figure><h5 id="全量采集"><a href="#全量采集" class="headerlink" title="全量采集"></a>全量采集</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata04 warehouse_shell_good_order]#vi collect_data_full.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 全量数据采集</span><br><span class="line"># 每天凌晨执行一次</span><br><span class="line"></span><br><span class="line"># 默认获取昨天的日期，也支持传参指定一个日期</span><br><span class="line">if [ &quot;z$1&quot; &#x3D; &quot;z&quot; ]</span><br><span class="line">then</span><br><span class="line">dt&#x3D;&#96;date +%Y%m%d --date&#x3D;&quot;1 days ago&quot;&#96;</span><br><span class="line">else</span><br><span class="line">dt&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># SQL语句</span><br><span class="line">user_sql&#x3D;&quot;select user_id,user_name,user_gender,user_birthday,e_mail,concat(left(mobile,3), &#39;****&#39; ,right(mobile,4)) as mobile,register_time,is_blacklist from user where 1&#x3D;1&quot;</span><br><span class="line">user_extend_sql&#x3D;&quot;select user_id,is_pregnant_woman,is_have_children,is_have_car,phone_brand,phone_cnt,change_phone_cnt,weight,height from user_extend where 1&#x3D;1&quot;</span><br><span class="line">user_addr_sql&#x3D;&quot;select addr_id,user_id,addr_name,order_flag,user_name,concat(left(mobile,3), &#39;****&#39; ,right(mobile,4)) as mobile from user_addr where 1&#x3D;1&quot;</span><br><span class="line">goods_info_sql&#x3D;&quot;select goods_id,goods_no,goods_name,curr_price,third_category_id,goods_desc,create_time from goods_info where 1&#x3D;1&quot;</span><br><span class="line">category_code_sql&#x3D;&quot;select first_category_id,first_category_name,second_category_id,second_catery_name,third_category_id,third_category_name from category_code where 1&#x3D;1&quot;</span><br><span class="line"></span><br><span class="line"># 路径后缀</span><br><span class="line">path_prefix&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&quot;</span><br><span class="line"></span><br><span class="line"># 输出路径</span><br><span class="line">user_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;user&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">user_extend_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;user_extend&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">user_addr_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;user_addr&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">goods_info_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;goods_info&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">category_code_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;category_code&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line"></span><br><span class="line"># 采集数据</span><br><span class="line">echo &quot;开始采集...&quot;</span><br><span class="line">echo &quot;采集表：user&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;user_sql&#125;&quot; &quot;$&#123;user_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：user_extend&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;user_extend_sql&#125;&quot; &quot;$&#123;user_extend_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：user_addr&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;user_addr_sql&#125;&quot; &quot;$&#123;user_addr_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：goods_info&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;goods_info_sql&#125;&quot; &quot;$&#123;goods_info_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：category_code&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;category_code_sql&#125;&quot; &quot;$&#123;category_code_path&#125;&quot;</span><br><span class="line">echo &quot;结束采集...&quot;</span><br></pre></td></tr></table></figure><h5 id="增量采集"><a href="#增量采集" class="headerlink" title="增量采集"></a>增量采集</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">增量数据采集： collect_data_incr.sh</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># 增量数据采集</span><br><span class="line"># 每天凌晨执行一次</span><br><span class="line"></span><br><span class="line"># 默认获取昨天的日期，也支持传参指定一个日期</span><br><span class="line">if [ &quot;z$1&quot; &#x3D; &quot;z&quot; ]</span><br><span class="line">then</span><br><span class="line">dt&#x3D;&#96;date +%Y%m%d --date&#x3D;&quot;1 days ago&quot;&#96;</span><br><span class="line">else</span><br><span class="line">dt&#x3D;$1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># 转换日期格式，20260101 改为 2026-01-01</span><br><span class="line">dt_new&#x3D;&#96;date +%Y-%m-%d --date&#x3D;&quot;$&#123;dt&#125;&quot;&#96;</span><br><span class="line"></span><br><span class="line"># SQL语句</span><br><span class="line">user_order_sql&#x3D;&quot;select order_id,order_date,user_id,order_money,order_type,order_status,pay_id,update_time from user_order where order_date &gt;&#x3D; &#39;$&#123;dt_new&#125; 00:00:00&#39; and order_date &lt;&#x3D; &#39;$&#123;dt_new&#125; 23:59:59&#39;&quot;</span><br><span class="line">order_item_sql&#x3D;&quot;select order_id,goods_id,goods_amount,curr_price,create_time from order_item where create_time &gt;&#x3D; &#39;$&#123;dt_new&#125; 00:00:00&#39; and create_time &lt;&#x3D; &#39;$&#123;dt_new&#125; 23:59:59&#39;&quot;</span><br><span class="line">order_delivery_sql&#x3D;&quot;select order_id,addr_id,user_id,carriage_money,create_time from order_delivery where create_time &gt;&#x3D; &#39;$&#123;dt_new&#125; 00:00:00&#39; and create_time &lt;&#x3D; &#39;$&#123;dt_new&#125; 23:59:59&#39;&quot;</span><br><span class="line">payment_flow_sql&#x3D;&quot;select pay_id,order_id,trade_no,pay_money,pay_type,pay_time from payment_flow where pay_time &gt;&#x3D; &#39;$&#123;dt_new&#125; 00:00:00&#39; and pay_time &lt;&#x3D; &#39;$&#123;dt_new&#125; 23:59:59&#39;&quot;</span><br><span class="line"></span><br><span class="line"># 路径后缀</span><br><span class="line">path_prefix&#x3D;&quot;hdfs:&#x2F;&#x2F;bigdata01:9000&#x2F;data&#x2F;ods&quot;</span><br><span class="line"></span><br><span class="line"># 输出路径</span><br><span class="line">user_order_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;user_order&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">order_item_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;order_item&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">order_delivery_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;order_delivery&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line">payment_flow_path&#x3D;&quot;$&#123;path_prefix&#125;&#x2F;payment_flow&#x2F;$&#123;dt&#125;&quot;</span><br><span class="line"></span><br><span class="line"># 采集数据</span><br><span class="line">echo &quot;开始采集...&quot;</span><br><span class="line">echo &quot;采集表：user_order&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;user_order_sql&#125;&quot; &quot;$&#123;user_order_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：order_item&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;order_item_sql&#125;&quot; &quot;$&#123;order_item_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：order_delivery&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;order_delivery_sql&#125;&quot; &quot;$&#123;order_delivery_path&#125;&quot;</span><br><span class="line">echo &quot;采集表：payment_flow&quot;</span><br><span class="line">sh sqoop_collect_data_util.sh &quot;$&#123;payment_flow_sql&#125;&quot; &quot;$&#123;payment_flow_path&#125;&quot;</span><br><span class="line">echo &quot;结束采集...&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">执行脚本</span><br><span class="line">验证结果：</span><br><span class="line"></span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F;data&#x2F;ods</span><br><span class="line">Found 10 items</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;category_</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;goods_inf</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;order_de</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;order_it</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:43 &#x2F;data&#x2F;ods&#x2F;payment_f</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:29 &#x2F;data&#x2F;ods&#x2F;user</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 11:04 &#x2F;data&#x2F;ods&#x2F;user_act</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;user_add</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:29 &#x2F;data&#x2F;ods&#x2F;user_ext</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:41 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F;data&#x2F;ods&#x2F;user_order&#x2F;20260101</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r-- 2 root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br><span class="line">-rw-r--r-- 2 root supergroup 74618 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">执行脚本</span><br><span class="line"></span><br><span class="line">[root@bigdata04 warehouse_shel_good_order]# sh collect_data_full.sh 20260101</span><br><span class="line">[root@bigdata04 warehouse_shel_good_order]# sh collect_data_incr.sh 20260101</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F;data&#x2F;ods</span><br><span class="line">Found 10 items</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;category_</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;goods_inf</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;order_de</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;order_it</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:43 &#x2F;data&#x2F;ods&#x2F;payment_f</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:29 &#x2F;data&#x2F;ods&#x2F;user</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 11:04 &#x2F;data&#x2F;ods&#x2F;user_act</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:30 &#x2F;data&#x2F;ods&#x2F;user_add</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:29 &#x2F;data&#x2F;ods&#x2F;user_ext</span><br><span class="line">drwxr-xr-x - root supergroup 0 2026-01-01 10:41 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br><span class="line">[root@bigdata01 hadoop-3.2.0]# hdfs dfs -ls &#x2F;data&#x2F;ods&#x2F;user_order&#x2F;20260101</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r-- 2 root supergroup 0 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br><span class="line">-rw-r--r-- 2 root supergroup 74618 2026-01-01 10:42 &#x2F;data&#x2F;ods&#x2F;user_ord</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据开发工程师-第十二周 综合项目:电商数据仓库之用户行为数仓2</title>
    <link href="http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%932.html"/>
    <id>http://tianyong.fun/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88-%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%91%A8-%E7%BB%BC%E5%90%88%E9%A1%B9%E7%9B%AE-%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%8B%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%932.html</id>
    <published>2023-03-30T12:27:10.000Z</published>
    <updated>2023-03-30T13:17:21.292Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><script type="text/javascript" src="/js/src/bai.js"></script><h1 id="第十二周-综合项目-电商数据仓库之用户行为数仓2"><a href="#第十二周-综合项目-电商数据仓库之用户行为数仓2" class="headerlink" title="第十二周 综合项目:电商数据仓库之用户行为数仓2"></a>第十二周 综合项目:电商数据仓库之用户行为数仓2</h1><h2 id="电商数仓技术选型"><a href="#电商数仓技术选型" class="headerlink" title="电商数仓技术选型"></a>电商数仓技术选型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">咱们前面对项目的需求进行了分析，整体上来说是需要三个大的功能模块，那下面我们就来分析一下，想要实现这些功能模块，具体使用哪些技术框架比较合适</span><br></pre></td></tr></table></figure><h3 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">首先是数据采集：</span><br><span class="line">咱们前面学习了Flume这个数据采集工具</span><br><span class="line">其实还有一些类似的数据采集工具，Logstash、FileBeat，这两个也可以实现数据采集</span><br><span class="line"></span><br><span class="line">那这三个日志采集工具我们需要如何选择呢？</span><br><span class="line">首先从性能消耗上面来说，Flume和Logstash的性能消耗差不多，都是基于JVM执行的，都是重量级的组件，支持多种数据源和目的地。</span><br><span class="line"></span><br><span class="line">FileBeat是一个只支持文件数据采集的工具，是一个轻量级组件，性能消耗比价低，它不是基于JVM执行的，它是使用go语言开发的。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们在采集数据的时候可以分为两种情况</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302033047.png" alt="image-20230330203300260"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">第一种是把采集工具部署到产生数据的服务器上面</span><br><span class="line">web项目产生的日志数据直接保存在服务器上面，并且这个服务器的性能比较高，可以允许我在上面部署Flume数据采集工具，这样也不会对上面的web项目的稳定性产生什么影响。</span><br><span class="line"></span><br><span class="line">第二种是把采集工具部署在一个独立的服务器上面</span><br><span class="line">web项目产生的日志数据直接保存在服务器上面，但是这个服务器的性能一般，并且对web项目的稳定性要求非常高，如果让你在上面部署一个其它服务，这样这个服务器的稳定性就没办法保证了，进而也就无法保证web项目的稳定性了，所以这个时候可以选择在产生日志的时候使用埋点上报的方式，通过http接</span><br><span class="line">口把日志数据传输到日志接收服务器中</span><br><span class="line"></span><br><span class="line">那针对第一种情况肯定是要选择一个性能消耗比较低的数据采集工具，优先选择FileBeat</span><br><span class="line">针对第二种情况的话就不需要考虑性能消耗了，因为采集工具是在独立的机器上，不会影响web项目，这个时候我们需要考虑的就是采集工具的功能是否完整，因为在采集数据的时候可能需要对数据进行一些简单的处理，以及后期可能会输出到不同的存储介质中。</span><br><span class="line"></span><br><span class="line">Flume和Logstash都是支持多种输入、多种输出、以及都可以在采集数据的时候对数据做一些处理，那这个时候该如何选择呢？</span><br><span class="line">注意了，这个时候我们就要考虑如果后期采集工具出现了问题，或者我们需要自定义一些功能，维护成本高不高，Flume是使用java开发的，而Logstash是使用ruby开发的，由于我们都是java出身，所以考虑到后期的维护成本，Flume是最优的选择。</span><br><span class="line"></span><br><span class="line">在采集数据的时候，除了日志数据，有时候还需要采集一些业务系统的数据，这些数据一般保存在关系型数据库中，例如：MySQL</span><br><span class="line">我们也需要把这些数据采集过来，如果MySQL开启了binlog，那我们可以使用Flume采集</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">什么是MySQL的binlog呢？</span><br><span class="line">可以这样理解，MySQL对数据库的任何修改都会记录在binlog中，所以如果开启了binlog，那我们就可以使用Flume采集这个日志数据，就可以获取到MySQL中数据的变化了。</span><br><span class="line"></span><br><span class="line">但是目前我们的MySQL没有开启binlog，并且也没有打算开启binlog，那怎么办？</span><br><span class="line">因为我们这个需求不需要实时采集MySQL中的数据，所以不开启binlog也是没有问题的，Flume默认不支持直接采集MySQL中的数据，如果想要实现的话需要自定义Source，这样就比较麻烦了</span><br><span class="line"></span><br><span class="line">其实采集MySQL中的数据有一个比较常用的方式是通过Sqoop实现。</span><br><span class="line">Sqoop中有两大功能，数据导入和数据导出</span><br><span class="line"></span><br><span class="line">数据导入是指把关系型数据库中的数据导入HDFS中</span><br><span class="line">数据导出是指把HDFS中的数据导出到关系型数据库中</span><br><span class="line"></span><br><span class="line">我们后期在做一些报表的时候其实也是需要把数据仓库中的数据(APP层)导出到MySQL中的，所以在这选择Sqoop也是非常实用的。</span><br><span class="line"></span><br><span class="line">所以针对数据采集这块，我们主要选择了Flume和Sqoop</span><br></pre></td></tr></table></figure><h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据采集过来以后，由于我们后面要构建数据仓库，数据仓库是使用Hive实现，Hive的数据是存储在HDFS中的，所以我们把采集到的数据也直接存储到HDFS里面</span><br><span class="line">还有一点是后期在做一些数据报表的时候，是需要把数据仓库中的数据导出到MySQL中的，所以数据存储也需要使用到MySQL。</span><br></pre></td></tr></table></figure><h3 id="数据计算"><a href="#数据计算" class="headerlink" title="数据计算"></a>数据计算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在构建数据仓库的时候，我们前面说了，是使用Hive构建数据仓库，一般的数据处理通过SQL是可以搞定的，如果遇到了比较复杂的处理逻辑，可能还需要和外部的数据进行交互的，这个时候使用SQL就比较麻烦了，内置的函数有时候搞不定，还需要开发自定义函数</span><br><span class="line"></span><br><span class="line">针对复杂的数据清洗任务我们也可以考虑使用Spark进行处理。</span><br></pre></td></tr></table></figure><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在数据可视化层面，我们可以使用Hue(无法出图表)数据查询</span><br><span class="line"></span><br><span class="line">如果想实现写SQL直接出图表(简单的图表)zeppelin</span><br><span class="line">如果想定制开发图表(复杂图表)的话可以使用Echarts(百度开源的)之类的图表库，这个时候是需要我们自己开发数据接口实现的。</span><br></pre></td></tr></table></figure><h2 id="整体架构设计"><a href="#整体架构设计" class="headerlink" title="整体架构设计"></a>整体架构设计</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">技术选型搞定后，下面我们来看一下整体的架构设计</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302116171.png" alt="image-20230330211658080"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">我们采集的数据主要分为服务端数据和客户端数据</span><br><span class="line">什么是服务端数据，就是网站上的商品详情数据以及你下的订单信息之类的数据，这些数据都是在服务端存储着的，一般是存储在类似于MySQL之类的关系型数据库中，这些数据对事务性要求比较严格，所以会存放在关系型数据库中。</span><br><span class="line"></span><br><span class="line">什么是客户端数据呢，就是用户在网站或者app上的一些滑动、点击、浏览、停留时间之类的用户行为数据，这些数据会通过埋点直接上报，这些其实就是一些日志类型的数据了，这种类型的数据没有事务性要求，并且对数据的完整性要求也不是太高，就算丢一些数据，对整体结果影响也不大。</span><br><span class="line"></span><br><span class="line">针对服务端数据，在采集的时候，主要是通过Sqoop进行采集，按天采集，每天凌晨的时候把昨天的数据采集过来，存储到HDFS上面。</span><br><span class="line">针对客户端数据，会通过埋点上报到日志接收服务器中，其实这里面就是一个Http服务，埋点上报就是调用了这个Http服务，把日志数据传输过来，日志接收服务收到数据之后，会把数据落盘，存储到本地，记录为日志文件，然后通过Flume进行采集，将数据采集到HDFS上面，按天分目录存储。</span><br><span class="line"></span><br><span class="line">服务端数据和客户端数据都进入到HDFS之后，就需要对数据进行ETL，构建数据仓库了。</span><br><span class="line">数据仓库构建好了以后可以选择把一些需要报表展现的数据导出到MySQL中，最终在页面进行展现。</span><br></pre></td></tr></table></figure><h2 id="服务器资源规划"><a href="#服务器资源规划" class="headerlink" title="服务器资源规划"></a>服务器资源规划</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">整体架构分析好了，下面我们来分析一下，想要实现这个架构，服务器资源应该如何划分</span><br><span class="line">针对我们的测试环境：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302105852.png" alt="image-20230330210548415"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">针对生产环境，至少需要这些机器：</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/ttyong/hexoBlog/raw/master/%E6%9E%97%E5%AD%90%E9%9B%A8%20%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/202303302106891.png" alt="image-20230330210649799"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">说明</span><br><span class="line">1：由于NameNode开启了HA，所以SecondaryNameNode进程就不需要了</span><br><span class="line">2：NameNode需要使用单独的机器，并且此机器的内存配置要比较高，建议128G</span><br><span class="line">3：DataNode和NodeManager需要部署在相同的集群上，这样可以实现数据本地化计算</span><br><span class="line">4：Hadoop Client需要部署在需要和Hadoop交互的机器上 </span><br><span class="line">5：数据接口服务器需要使用至少两台，为了实现负载均衡及故障转移，保证数据接收服务的稳定性</span><br><span class="line">6：Flume部署在日志服务器上面，便于采集本机保存的用户行为日志信息；还需要有单独的Flume机器，便于处理其它的日志采集需求</span><br><span class="line">7：Hive需要部署在所有业务机器上</span><br><span class="line">8：MySQL建议单独部署，至少两台，一主一备</span><br><span class="line">9：Sqoop需要部署在所有业务机器上</span><br><span class="line">10：Zeppelin可以单独部署在一台普通配置的机器上即可</span><br><span class="line">11：Azkaban建议至少使用三台，一主两从，这样可以保证一个从节点挂掉之后不影响定时任务的调度</span><br><span class="line">针对Hadoop集群的搭建在线上环境需要使用CDH或者HDP</span><br><span class="line">具体Hadoop集群需要使用多少台集群需要根据当前的数据规模来预估</span><br><span class="line">假设集群中的机器配置为8T，64 Core，128G</span><br><span class="line">1：如果每天会产生1T的日志数据，需要保存半年的历史数据： 1T*180天&#x3D;180T</span><br><span class="line">2：集群中的数据默认是3副本： 180T*3&#x3D;540T</span><br><span class="line">3：预留20%左右的空间： 540T&#x2F;0.8&#x3D;675T</span><br><span class="line">这样计算的话就需要675T&#x2F;8T&#x3D;85台服务器</span><br><span class="line">如果我们在数据仓库中对数据进行分层存储，这样数据会出现冗余，存储空间会再扩容1~2倍</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：没有必要一开始就上线全部的机器，我们可以前期上线30台，后面随着业务数据量的增长再去动态扩容机器即可。</span><br></pre></td></tr></table></figure><hr><blockquote></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
      
    
    </summary>
    
    
      <category term="大数据开发工程师" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/"/>
    
      <category term="大数据" scheme="http://tianyong.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
</feed>
